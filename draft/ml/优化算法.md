# 常见的优化算法有那些，以及这些优化算法有那些优缺点；

1)梯度下降法(Gradient Descent)
  - BGD
每一步迭代都需要遍历所有的样本数据,消耗时间长,但是一定能得到最优解.
 - SGD(Stochastic Gradient Descent）--->迭代速度快,得到局部最优解(凸函数时得到全局最优解)
- MBGD(Mini-batch Gradient Descent)--->小批量梯度下降法

2)牛顿法和拟牛顿法

牛顿法--->牛顿法是**二阶收敛,收敛速度快; 牛顿法是一种迭代算法**，每一步都需要求解目标函数的**Hessian矩阵的逆矩阵**，计算比较复杂。

拟牛顿法--->改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用**正定矩阵来近似Hessian矩阵的逆**，从而简化了运算的复杂度.

3)共轭梯度法（Conjugate Gradient）

共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

4) 启发式的方法

启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等。多目标优化算法(NSGAII算法、MOEA/D算法以及人工免疫算法)

5)解决约束的方法---拉格朗日乘子法
