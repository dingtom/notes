<!--
 * @Author: dingtom 2524370217@qq.com
 * @Date: 2021-08-23 07:03:35
 * @LastEditors: dingtom 2524370217@qq.com
 * @LastEditTime: 2022-05-28 11:37:55
 * @FilePath: \draft\数据分析\缺失值处理.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
# 删除
~~样本数据量十分大且缺失值不多~~的情况下非常有效，但如果样本量本身不大且缺失也不少，那么不建议使用。
```python
#删除数据表中含有空值的行
df.dropna(how='any')
```

# 不处理
补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，一些模型无法应对具有缺失值的数据，因此要对缺失值进行处理。然而还有一些模型本身就可以应对具有缺失值的数据，此时无需对数据进行处理，比如~~Xgboos等树模型~~。


# 分箱：（缺失值一个箱）
虚拟变量其实就是缺失值的一种衍生变量。具体做法是通过判断特征值是否有缺失值来定义一个新的二分类变量。比如，特征为A含有缺失值，我们~~衍生出一个新的特征B，如果A中特征值有缺失，那么相应的B中的值为1，如果A中特征值没有缺失，那么相应的B中的值为0。~~
```
data_train['CabinCat'] = data_train['Cabin'].copy()
data_train.loc[ (data_train.CabinCat.notnull()), 'CabinCat' ] = "No"
data_train.loc[ (data_train.CabinCat.isnull()), 'CabinCat' ] = "Yes"

fig, ax = plt.subplots(figsize=(10,5))
sns.countplot(x='CabinCat', hue='Survived',data=data_train)
plt.show()
```
```data_train[['Cabin','CabinCat']].head(10)```

# 补全
## 均值、众数、中位数、

~~对于定类数据：使用 众数（mode）填补~~，比如一个学校的男生和女生的数量，男生500人，女生50人，那么对于其余的缺失值我们会用人数较多的男生来填补。
~~对于定量（定比）数据：使用平均数（mean）或中位数（median）填补~~，比如一个班级学生的身高特征，对于一些同学缺失的身高值就可以使用全班同学身高的平均值或中位数来填补。一般如果特征分布为正太分布时，使用平均值效果比较好，而当分布由于异常值存在而不是正太分布的情况下，使用中位数效果比较好。

注：此方法虽然简单，但是不够精准，可能会引入噪声，或者会改变特征原有的分布。**如果缺失值是随机性的，那么用平均值比较适合保证无偏，否则会改变原分布。**
```
#使用price均值对NA进行填充
df['price'].fillna(df['price'].mean())
df['price'].fillna(df['price'].median())
```

## 建模预测、多重插补、压缩感知补全、矩阵补全等

利用其它变量做模型的输入进行缺失变量的预测，与我们正常建模的方法一样，只是目标变量变为了缺失值。如果其它特征变量与缺失变量无关，则预测的结果毫无意义。如果预测结果相当准确，则又说明这个变量完全没有必要进行预测，因为这必然是与特征变量间存在重复信息。
- 回归预测：
缺失值是连续的，即定量的类型，才可以使用回归来预测。

- 极大似然估计（Maximum likelyhood）：
在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计（Little and Rubin）。这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂，且仅限于线性模型。
- 多重插补（Mutiple imputation）：
多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。根据某种选择依据，选取最合适的插补值。 
- 热卡填补（Hot deck imputation）：
热卡填充法是在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。通常会找到超出一个的相似对象，在所有匹配对象中没有最好的，而是从中随机的挑选一个作为填充值。这个问题关键是不同的问题可能会选用不同的标准来对相似进行判定，以及如何制定这个判定标准。该方法概念上很简单，且利用了数据间的关系来进行空值估计，但缺点在于难以定义相似标准，主观因素较多。
- K最近距离邻法（K-means clustering）
通过K均值的聚类方法将所有样本进行聚类划分，然后再通过划分的种类的均值对各自类中的缺失值进行填补。归其本质还是通过找相似来填补缺失值。

- 多重插补：
我们看到，以上提出的拟合和替换方法都是单一的插补方法，而多重插补弥补了单一插补的缺陷，它并没有试图去通过模拟值去估计每个缺失值，而是提出缺失数据值的一个随即样本（这些样本可以是不同的模型拟合结果的组合）。这种程序的实施恰当地反映了由于缺失值引起的不确定性，使得统计推断有效。多重插补推断可以分为以下3个步骤：
>为每个缺失值产生一套可能的插补值，这些值反映了无响应模型的不确定性；
>每个插补数据集合都用针对完整数据集的统计方法进行统计分析；
>对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值；


- 随机森林：
这也是Kaggle竞赛中大佬们经常使用的一个办法，具体实现方式与正常一样，只是将缺失值作为目标变量即可.
```
def set_missing_ages(df):

    # 把已有的数值型特征取出来丢进Random Forest Regressor中
    age_df = df[['Age','Fare', 'Parch', 'SibSp', 'Pclass']]

    # 乘客分成已知年龄和未知年龄两部分
    known_age = age_df[age_df.Age.notnull()].as_matrix()
    unknown_age = age_df[age_df.Age.isnull()].as_matrix()

    # y即目标年龄
    y = known_age[:, 0]

    # X即特征属性值
    X = known_age[:, 1:]

    # fit到RandomForestRegressor之中
    rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)
    rfr.fit(X, y)

    # 用得到的模型进行未知年龄结果预测
    predictedAges = rfr.predict(unknown_age[:, 1:])
#     print predictedAges
    # 用得到的预测结果填补原缺失数据
    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges 

    return df, rfr
```
