

# 异常值检测特征分为类别特征和数字特征
## 数字特征
相关性分析、查看特征的偏度和峰度、数字特征相互之间的关系可视化、多变量互相回归关系可视化、数字特征的频数可视化      
## 类别特征
ounique分布、箱形图可视化、小提琴图可视化、类别柱形图可视化

# 异常值处理

## 箱线图(没有对数据作任何限制性要求)
## 3-$\sigma$(Sigma)(符合正态分布）
## BOX-COX转换（处理有偏分布）
## 长尾截断

聚类、k近邻、One Class SVM、Isolation Forest

>关于高势集特征model，也就是类别中取值个数非常多的， 一般可以使用聚类的方式，然后独热

# 很多模型假设数据服从正态分布
数据整体**服从正态分布，样本均值和方差则相互独立**。当样本不服从正态分布时，可以做如下转换：

- 线性变化z-scores：基于原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。将A的原始值x使用z-score标准化到x’
  


- yeo-johnson变换：是幂变换（power transformation）的方法之一，通过构建一组单调函数对随机变量进行数据变换。


- Boxcox变换：一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。在做线性回归的过程中，一般需要做线性模型假定。

关于box-cox转换，一般是用于连续的变量不满足正态的时候，在做线性回归的过程中，一般线性模型假定:   $Y=X\beta + \epsilon$

其中$\epsilon$满足正态分布，但是利用实际数据建立回归模型时，个别变量的系数通不过。例如往往不可观测的误差$\epsilon$可能是和预测变量相关的，不服从正态分布，于是给线性回归的最小二乘估计系数的结果带来误差，为了使模型满足线性性、独立性、方差齐性以及正态性，需改变数据形式，故应用BOX-COX转换。具体详情这里不做过多介绍，当然还有很多转换非正态数据分布的方式：
![](https://upload-images.jianshu.io/upload_images/18339009-03d35f2ce4fcf3bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
在一些情况下（P值<0.003）上述方法很难实现正态化处理，所以优先使用BOX-COX转换，但是当P值>0.003时两种方法均可，优先考虑普通的平方变换。
BOX-COX的变换公式：
![](https://upload-images.jianshu.io/upload_images/18339009-b259d819c0b52fe5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 类别不平衡

极端的例子如有998个反例，但是正例只有2个，那么学习方法只需要返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度

## 训练策略

有以下几种方式来缓解数据不均衡的问题，如迁移学习、解耦学习、集成学习。



迁移学习如在imageNet上训练好的模型，将其backbone模型迁移到下游任务上。在不平衡学习中还有另外一种迁移方式，**即先在完整的数据集上训练得到的模型，迁移到子数据集（重新构建的类别均衡数据集）上，冻结backbone特征提取部分，训练分类器。亲测效果提升明显。**
  

解耦学习是将学习过程分成用于特征提取的表征学习和用于分类的分类器学习两个部分，facebook[3] 则提出了两阶段训练的工作：

## 数据

- 将大类分解成多个小类
- 数据多的欠采样（删除，原型生成（k-means中心点代替整个簇）），数据少的过采样（复制）
- 合成样本： SMOTE，图像中的对比度、明亮度、平移、裁剪、旋转等能力，还有如copy-paste,mixup,mosica 等

## 模型/网络结构

决策树等。加权少类别的样本错分代价（Cost Sensitive算法代价敏感）

## loss函数

- Focal loss

  在标准交叉熵损失基础上修改得到的，通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。

## 评价指标

AUC等

![](https://upload-images.jianshu.io/upload_images/18339009-ad7a1a95d3fc3930.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)![](https://upload-images.jianshu.io/upload_images/18339009-e8164d86267b6091.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)








# 训练数据不足
迁移学习（ Transfer Learning)微调
生成对抗网络
图像处理（变换角度（旋转、平移、缩放、裁剪），添加噪声扰动，颜色变换。改变图像的亮度、清晰度）
上采样技术，数据合成SMOTE (Synthetic Minority Over-sampling Technique）


# 数据转换的方式有：
数据归一化(MinMaxScaler)；
标准化(StandardScaler)；
对数变换(log1p)；
转换数据类型(astype)；
独热编码(OneHotEncoder)；
标签编码(LabelEncoder)；
修复偏斜特征(boxcox1p)等。

## 使用独热编码需要注意以下问题
(1)使用稀疏向量来**节省空间**。在独热编码下，特征向量只有某一维取值为1,其他位置取值均为0.因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。
（2)**配合特征选择来降低维度**。高维度特征会带来几方面的问题。一是在K近邻算法中，**高维空间下两点之间的距离很难得到有效的衡量**；二是在逻辑回归模型中，**参数的数量会随着维度的增高而增加**，容易引起过拟合问题；三是通常只有**部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。**

