![](https://upload-images.jianshu.io/upload_images/18339009-48e19a2cced9c78e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 贪心调参 （坐标下降）
所谓贪心算法是指，在对问题求解时，总是做出在**当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的**局部最优解。

**选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。）**

## 贪心算法的基本思路
>1.建立数学模型来描述问题
2.把求解的问题分成若干个子问题
3.对每个子问题求解，得到子问题的局部最优解
4.把子问题的解局部最优解合成原来问题的一个解
## 例子
有一个背包，最多能承载150斤的重量，现在有7个物品，重量分别为[35, 30, 60, 50, 40, 10, 25]，它们的价值分别为[10, 40, 30, 50, 35, 40, 30]

分别求出子问题的最优解再堆叠出全局最优解
>按照制订的规则（价值）进行计算，顺序是：4 2 6 5 。
最终的总重量是：130。
最终的总价值是：165。
按照制订的规则（重量）进行计算，顺序是：6 7 2 1 5 。
最终的总重量是：140。
最终的总价值是：155。
按照制订的规则（单位密度）进行计算，顺序是：6 2 7 4 1。
最终的总重量是：150。
最终的总价值是：170。


三、该算法存在的问题
不能保证求得的最后解是最佳的
不能用来求最大值或最小值的问题
只能求满足某些约束条件的可行解的范围
四、贪心算法适用的问题

## 贪心策略适用的前提是：
>1、原问题复杂度过高；
2、求全局最优解的数学模型难以建立；
3、求全局最优解的计算量过大；
4、没有太大必要一定要求出全局最优解，“比较优”就可以。




坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。不循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索（line search）
```
#  -------------------------------贪心调参 （坐标下降）-----------------------------------
objective = ['regression', 'regression_l1', 'mape', 'huber', 'fair']
num_leaves = [3, 5, 10, 15, 20, 40, 55]
max_depth = [3, 5, 10, 15, 20, 40, 55]
greedy_parameters_lgb = dict()
greedy_score = 100
for o in objective:
    score = np.mean(cross_val_score(LGBMRegressor(objective=o), 
                                    X=x_train_tree, y=y_train_tree, verbose=0, cv=5, 
                                    scoring=make_scorer(mean_absolute_error)))
    if greedy_score > score:
        greedy_parameters_lgb['objective'] = o
        greedy_score = score
greedy_score = 100
for l in num_leaves:
    score = np.mean(cross_val_score(LGBMRegressor(objective=greedy_parameters_lgb['objective'],
                                                  num_leaves=l), 
                                    X=x_train_tree, y=y_train_tree, verbose=0, cv=5, 
                                    scoring=make_scorer(mean_absolute_error)))
    if greedy_score > score:
        greedy_parameter_lgb['num_leaves'] = l
        greedy_score = score
greedy_score = 100
for d in max_depth:
    score = np.mean(cross_val_score(LGBMRegressor(objective=greedy_parameters_lgb['objective'],
                                                  num_leaves=greedy_parameters_lgb['num_leaves'],
                                                  max_depth=d), 
                                    X=x_train_tree, y=y_train_tree, verbose=0, cv=5, 
                                    scoring=make_scorer(mean_absolute_error)))
    if greedy_score > score:
        greedy_parameters_lgb['max_depth'] = d
        greedy_score = score
print(greedy_parameter_lgb)  
print(np.mean(cross_val_score(LGBMRegressor(**greedy_parameter_lgb), 
                        X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))))
```
# 网格调参GridSearchCV

网格搜索可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低所需的时间和计算量，**但由于目标函数一般是非凸的，所以很可能会错过全局最优值。**
```
 # ------------------------------------Grid Search 调参---------------------------------
clf_lgb = GridSearchCV(LGBMRegressor(), cv=5,
                       {'objective': ['regression', 'regression_l1', 'mape', 'huber', 'fair'],
                        'num_leaves': [3, 5, 10, 15, 20, 40, 55], 
                        'max_depth': [3, 5, 10, 15, 20, 40, 55]})
clf_lgb = clf.fit(x_train_tree, y_train_tree)
print(clf_lgb.best_params_)
print(np.mean(cross_val_score(LGBMRegressor(**clf_lgb.best_params_), 
                        X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))))
```
# 贝叶斯优化

贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息；**而贝叶斯优化算法则充分利用了之前的信息**。

贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它学习目标函数形状的方法是，

首先根据先验分布，假设一个搜集函数；
然后，毎一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；
最后，算法测试由后验分布给出的全局最值最可能出现的位置的点。

对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了ー个局部最优值，**它会在该区域不断采样，所以很容易陷入局部最优值。**
为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。
```
# -----------------------------------------贝叶斯调参--------------------------
def bayes_cv_lgb(num_leaves, max_depth, subsample, min_child_samples):
    val = cross_val_score(LGBMRegressor(objective='regression_l1',
                                        num_leaves=int(num_leaves),  
                                        # 优化只能优化连续超参数加上int()转为离散超参数。
                                        max_depth=int(max_depth),
                                        subsample = subsample,
                                        min_child_samples = int(min_child_samples)),
                          X=x_train_tree, y=y_train_tree, verbose=0, cv=5, 
                          scoring=make_scorer(mean_absolute_error)).mean()
    return 1 - val  # 只支持最大值，需要在前面加上负号，


bo_lgb = BayesianOptimization(bayes_cv,
                             {'num_leaves': (2, 100), 
                              'max_depth': (2, 100),
                              'subsample': (0.1, 1),
                              'min_child_samples' : (2, 100)})
bo_lgb.maximize()
print(bo_lgb.max)
bo_parameters_lgb = bo_lgb.max['params']
for i in ['num_leaves', 'max_depth', 'min_child_samples']:
    bo_parameters_lgb[i] = int(bo_parameters_lgb[i])
print(np.mean(cross_val_score(LGBMRegressor(**bo_parameters), 
                        X=x_train_tree, y=y_train_tree, verbose=0, cv=5, 
                        scoring=make_scorer(mean_absolute_error)))    )
```
