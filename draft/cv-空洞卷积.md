# 空洞卷积

空洞卷积中文名也叫**膨胀卷积**或者**扩张卷积**,英文名也叫**Atrous Convolution**

空洞卷积最初的提出是为了解决图像分割的问题而提出的,常见的图像分割算法通常使用池化层和卷积层来增加感受野(Receptive Filed),同时也缩小了特征图尺寸(resolution),然后再利用上采样还原图像尺寸,特征图缩小再放大的过程造成了精度上的损失,因此需要一种操作可以**在增加感受野的同时保持特征图的尺寸不变**,**从而代替下采样和上采样操作**,在这种需求下,空洞卷积就诞生了(略有修改,引自[4])



## 原理

与正常的卷积不同的是,空洞卷积引入了一个称为 “**扩张率(dilation rate)**”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。

在此以3*3卷积为例,展示普通卷积和空洞卷积之间的区别

![quicker_1adcb4bc-878f-40dc-a302-c41d066d463c.png](https://s2.loli.net/2022/05/06/atHGAJnj7FqwxOl.png)

从左到右分别为a、b、c子图,三幅图是**相互独立**进行卷积的(区别于下面图4),大框表示输入图像(感受野默认为1),黑色的圆点表示3*3的卷积核,灰色地带表示卷积后的感受野

- a是普通的卷积过程(dilation rate = 1),卷积后的感受野为3
- b是dilation rate = 2的空洞卷积,卷积后的感受野为5
- c是dilation rate = 3的空洞卷积,卷积后的感受野为8



![quicker_28831eb3-2ba2-46dd-841d-2971bd6e417c.png](https://s2.loli.net/2022/05/06/AyL7B823DcbVWEU.png)



神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征



1个 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计可以大幅度的减少参数，有正则化的效果，参数少了就没那么容易发生过拟合。



## 作用

1. 扩大感受野。但需要明确一点，池化也可以扩大感受野，但空间分辨率降低了，相比之下，空洞卷积可以在扩大感受野的同时不丢失分辨率，且保持像素的相对空间位置不变。简单而言，空洞卷积可以同时控制感受野和分辨率。
2. 获取多尺度上下文信息。当多个带有不同dilation rate的空洞卷积核叠加时，不同的感受野会带来多尺度信息，这对于分割任务是非常重要的。
3. 可以降低计算量，不需要引入额外的参数，如上图空洞卷积示意图所示，实际卷积时只有带有红点的元素真正进行计算

# 转置卷积

**转置卷积（Transposed Convolution）** 在语义分割或者对抗神经网络（GAN）中比较常见，其主要作用就是做上采样（**UpSampling**）。**转置卷积不是卷积的逆运算**



首先回顾下普通卷积，以stride=1，padding=0，kernel_size=3为例，假设输入特征图大小是4x4的（假设输入输出都是单通道），通过卷积后得到的特征图大小为2x2。一般使用卷积的情况中，要么特征图变小（stride > 1），要么保持不变（stride = 1），当然也可以通过四周padding让特征图变大但没有意义。




转置卷积它只能恢复到原来的大小（shape）数值与原来不同。转置卷积的运算步骤可以归为以下几步：

- 在输入特征图元素间填充s-1行、列0（其中s表示转置卷积的步距）

- 在输入特征图四周填充k-p-1行、列0（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）

- 将卷积核参数上下、左右翻转

- 做正常卷积运算（填充0，步距1）

下面假设输入的特征图大小为2x2（假设输入输出都为单通道），通过转置卷积后得到4x4大小的特征图。这里使用的转置卷积核大小为k=3，stride=1，padding=0的情况（忽略偏执bias）。

首先在元素间填充s-1=0行、列0（等于0不用填充）
然后在特征图四周填充k-p-1=2行、列0
接着对卷积核参数进行上下、左右翻转
最后做正常卷积（填充0，步距1）
![quicker_8295a47a-6d1d-4c18-8ba8-b4a724e696d4.png](https://s2.loli.net/2022/05/07/6HUADX2yzZ3vkmt.png)



$$H_{out} = \frac{H_{in}+2p-k}{s}+1$$

$$H_{in} = (H_{out}-1)\times s +k-2p$$



```python
nn.ConvTranspose2d
n_channels, out_channels, kernel_size, stride, padding、
output_padding：在计算得到的输出特征图的高、宽方向各填充几行或列0（注意，这里只是在上下以及左右的一侧one side填充，并不是两侧都填充，有兴趣自己做个实验看下），默认为0不使用。
groups：当使用到组卷积时才会用到的参数，默认为1即普通卷积。
bias：是否使用偏执bias，默认为True使用。
dilation：当使用到空洞卷积（膨胀卷积）时才会使用到的参数，默认为1即普通卷积。
```

$$H_{in} = (H_{out}-1)\times s +dilation \times (k-1)-2p +out\_padding +1$$