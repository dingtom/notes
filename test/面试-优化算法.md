---
    # 文章标题
    title: 面试-优化算法.md
    # 分类
    categories: 面试
    # 标签
    #tags:
    # 文章内容摘要
    #description: "{{ .Name }}"
    
    # 发表日期
    #date: {{ .Date }}
    # 最后修改日期
    #lastmod: {{ .Date }}
    # 文章内容关键字
    #keywords: "{{replace .Name "-" ","}}"
    # 原文作者
    #author:
    # 原文链接
    #link:
    # 图片链接，用在open graph和twitter卡片上
    #imgs:
    # 在首页展开内容
    #expand: true
    # 外部链接地址，访问时直接跳转
    #extlink:
    # 在当前页面关闭评论功能
    #comment:
    # enable: false
    # 关闭当前页面目录功能
    # 注意：正常情况下文章中有H2-H4标题会自动生成目录，无需额外配置
    #toc: false
    # 绝对访问路径
    #url: "{{ lower .Name }}.html"
    # 开启文章置顶，数字越小越靠前
    #weight: 1
    #开启数学公式渲染，可选值： mathjax, katex
    #math: mathjax
    # 开启各种图渲染，如流程图、时序图、类图等
    #mermaid: true
--- 




主要有三大类：

1. 基本梯度下降法，包括 GD，BGD，SGD；
2. 动量优化法，包括 Momentum，NAG 等；
3. 自适应学习率优化法，包括 Adam，AdaGrad，RMSProp 等





1)梯度下降法(Gradient Descent)
  - BGD
每一步迭代都需要遍历所有的样本数据,消耗时间长,但是一定能得到最优解.
 - SGD(Stochastic Gradient Descent）--->迭代速度快,得到局部最优解(凸函数时得到全局最优解)
- MBGD(Mini-batch Gradient Descent)--->小批量梯度下降法

2)牛顿法和拟牛顿法

牛顿法--->牛顿法是**二阶收敛,收敛速度快; 牛顿法是一种迭代算法**，每一步都需要求解目标函数的**Hessian矩阵的逆矩阵**，计算比较复杂。

拟牛顿法--->改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用**正定矩阵来近似Hessian矩阵的逆**，从而简化了运算的复杂度.

3)共轭梯度法（Conjugate Gradient）

共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

4) 启发式的方法

启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等。多目标优化算法(NSGAII算法、MOEA/D算法以及人工免疫算法)

5)解决约束的方法---拉格朗日乘子法
