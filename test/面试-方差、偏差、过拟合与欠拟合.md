---
    # 文章标题
    title: 面试-方差、偏差、过拟合与欠拟合.md
    # 分类
    categories: 面试
    # 标签
    #tags:
    # 文章内容摘要
    #description: "{{ .Name }}"
    
    # 发表日期
    #date: {{ .Date }}
    # 最后修改日期
    #lastmod: {{ .Date }}
    # 文章内容关键字
    #keywords: "{{replace .Name "-" ","}}"
    # 原文作者
    #author:
    # 原文链接
    #link:
    # 图片链接，用在open graph和twitter卡片上
    #imgs:
    # 在首页展开内容
    #expand: true
    # 外部链接地址，访问时直接跳转
    #extlink:
    # 在当前页面关闭评论功能
    #comment:
    # enable: false
    # 关闭当前页面目录功能
    # 注意：正常情况下文章中有H2-H4标题会自动生成目录，无需额外配置
    #toc: false
    # 绝对访问路径
    #url: "{{ lower .Name }}.html"
    # 开启文章置顶，数字越小越靠前
    #weight: 1
    #开启数学公式渲染，可选值： mathjax, katex
    #math: mathjax
    # 开启各种图渲染，如流程图、时序图、类图等
    #mermaid: true
--- 




过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是模型在训练和预测时表现都不好的情况。

# 方差、偏差
- 模型的偏差，指的是模型**预测的期望值**与**真实值之间的差**；用于描述模型的**拟合能力**

- 模型的方差，指的是模型**预测的期望值**与**预测值之间的差平方和**；用于描述模型的**稳定性**

误差是观察值和总体平均值的偏差，而残差是观察值和样本平均值的偏差。
![](https://upload-images.jianshu.io/upload_images/18339009-63099bb759fe69ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**高偏差：Jtrain和Jcv都很大，并且Jtrain≈Jcv。对应欠拟合。**
**高方差：Jtrain较小，Jcv远大于Jtrain。对应过拟合。（受数据扰动影响大）**
![](https://upload-images.jianshu.io/upload_images/15873283-5bc6609aa48100cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# 过拟合
- 数据：获取更多的数据集、 对数据集做扰动和扩增、 交叉验证、 降维
- 模型：简化模型、决策树的剪枝操作、Droupout、BN（batch normalization）、L1/l2正则化、`Early Stopping` (提前终止训练)。
- 集成学习：把多个模型集成在一起，来降低单一模型的过拟合风险，如 Bagging方法。


# 欠拟合
- 数据：数据集未打乱、未进行归一化、特征工程中对数据特征的选取有问题
- 模型：结构简单、 权重初始化方案有问题、选择合适的激活函数、损失函数、 选择合适的优化器和学习速率、batch size过大、训练时间不足、模型训练遇到瓶颈(梯度爆炸和梯度弥散产生的根本原因是，根据链式法则，深度学习中的梯度在逐层累积。)、减小正则化系数

# 训练集loss不下降（欠拟合）
### 数据集有问题
当一个数据集噪声过多，或者数据标注有大量错误时，会使得神经网络难以从中学到有用的信息，从而出现摇摆不定的情况。

数据集未打乱，不打乱数据集的话，会导致网络在学习过程中产生一定的偏见问题。比如张三和李四常常出现在同一批数据中，那么结果就是，神经网络看见了张三就会“想起”李四。

未进行归一化，未进行归一化会导致尺度的不平衡，比如1km和1cm的不平衡，因此会导致误差变大，或者在同样的学习率下，模型会以秒速五厘米的步伐，左右两边摇摆不定地，向前走1km。

数据特征的选取不合理，就像数据标注错误一样，会使得神经网络难以找到数据的本质特征进行学习。而机器学习的本质就是在做特征工程，以及清洗数据(逃)。

获取更多的数据集，对数据集做扰动和扩增，比如对图像做旋转，对声音文件进行加噪处理等。最终的效果虽然比不上同等情况下的数据量的增加带来的效果增益，但是在现有条件下，算是扩增数据量的一个有效的方案。
### 模型结构存在问题
增加深度，也就是增加神经网络的层数就可以了。也可以增加神经网络的宽度，将每一层的神经单元数量增加，但是同等情况下，效果明显不如增加层数，而且要想达到较好的效果，需要增加的神经元数远超过增加一层增加的神经元数。深度深比宽度宽的模型更优这一点，是大家普遍认同的。
### 权重初始化方案有问题
https://keras.io/zh/initializers/
这么多初始化方案，其实按照大类来分，主要就三种：均匀分布、正太分布和相同固定值。
全0的初始化，一般只会被用于逻辑斯蒂回归之类的这种二分类问题上，最多是浅层的神经网络上。全为1或者某个其他相同的值的方案则很少见。因为这种初始化方案，会使得网络处于对称状态，导致的结果就是，每个神经元都具有相同的输出，然后在反向传播计算梯度时，会得到一个同一个梯度值，并且进行着同样的参数更新，这是我们不希望看到的。
keras中，神经网络默认初始化全部被初始化为了glorot_uniform，也就是一种均值为0，以0为中心的对称区间均匀分布的随机数。在我的模型上，这种接近于0的均匀分布会导致什么问题呢？那就是梯度的消失，使得训练时的loss难以收敛
### 正则化过度
### 选择合适的激活函数、损失函数

卷积层的输出，一般使用ReLu作为激活函数，因为可以有效避免梯度消失，并且线性函数在计算性能上面更加有优势。而循环神经网络中的循环层一般为tanh，或者ReLu，全连接层也多用ReLu，只有在神经网络的输出层，使用全连接层来分类的情况下，才会使用softmax这种激活函数。而在各种机器学习入门教程里面最常讲到的sigmoid函数，想都不要想它，它已经不适用于深度学习了，哪怕是作为其改进版的softmax函数，也仅仅是在输出层才使用。
而损失函数，对于一些分类任务，通常使用交叉熵损失函数，回归任务使用均方误差，有自动对齐的任务使用CTC loss等。
### 选择合适的优化器和学习速率
神经网络的优化器选取一般选取Adam，但是在有些情况下Adam难以训练，这时候需要使用如SGD之类的其他优化器。
![image](https://upload-images.jianshu.io/upload_images/18339009-e60ff74f6d1a801d.gif?imageMogr2/auto-orient/strip)
![image](https://upload-images.jianshu.io/upload_images/18339009-ef8dc26527e00952.gif?imageMogr2/auto-orient/strip)
学习率决定了网络训练的速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率，然后再训练一段时间，这时候基本上就完全收敛了。一般学习率的调整是乘以/除以10的倍数。不过现在也有一些自动调整学习率的方案了。
### 训练时间不足
模型训练遇到瓶颈
这里的瓶颈一般包括：梯度消失、大量神经元失活、梯度爆炸和弥散等。梯度消失时，模型的loss难以下降，可以通过梯度的检验来验证模型当前所处的状态。有时梯度的更新和反向传播代码存在bug时，也会有这样的问题。
在使用Relu激活函数的时候，当每一个神经元的输入X为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。有一种解决方案是使用LeakyRelu，这时，Y轴的左边图线会有一个很小的正梯度，使得神经网络在一定时间后可以得到恢复。不过LeakyRelu并不常用，因为部分神经元失活并不影响结果，相反，这种输出为0还有很多积极的作用。因为Relu方程输入为负时，输出值为0，利用此特性可以很好地忽略掉卷积核输出负相关信息，同时保留相关信息。梯度爆炸和梯度弥散产生的根本原因是，根据链式法则，深度学习中的梯度在逐层累积。
### batch size过大
batch size过小，会导致模型后期摇摆不定，迟迟难以收敛，而过大时，模型前期由于梯度的平均，导致收敛速度过慢。一般batch size 的大小常常选取为32，或者16，有些任务下比如NLP中，可以选取8作为一批数据的个数。不过，有时候，为了减小通信开销和计算开销的比例，也可以调整到非常大的值，尤其是在并行和分布式中。


# 验证集loss不下降（过拟合）
一种是训练集上的loss也不下降，这时问题主要在训练集的loss上，应当先参考上述方法解决。另一种是训练集上的loss可以下降，但验证集上的loss已经不降了，这里我们主要说明这种情况下的问题。

**由于验证集是从同一批训练数据中划分出来的，所以一般不存在数据集的问题，所以主要是过拟合。**

### 适当降低模型的规模

### 适当的正则化和降维
比如通过增加一个正则项，并且人为给定一个正则系数lambda，进行权重衰减，将一些相关性不大的特征项的参数衰减到几乎为0，相当于去掉了这一项特征，这跟降维类似，相当于减少了特征维度。而去掉基本无关的维度，那么就避免了模型对于这一维度特征的过分拟合。还有在神经网络两个层之间增加Dropout和Normal等，也起到了抑制过拟合的作用。

# 测试集loss不下降
由于训练集和验证集的loss不下降时，应归为前两节的内容，所以这一节中，我们默认训练集和验证集的loss情况是正常的。所以，如果测试集的loss很高，或者正确率很低，那么一般是**因为训练数据的分布和场景与测试数据的分布和应用场景不一致。**
### 获取更多的数据集
对数据集做扰动和扩增，比如对图像做旋转，对声音文件进行加噪处理等。最终的效果虽然比不上同等情况下的数据量的增加带来的效果增益，但是在现有条件下，算是扩增数据量的一个有效的方案。
### 应用场景不一致
比如，一个语音识别模型，输入的数据集都是女性的录音音频，那么对于男性的声音就不能很好的识别出来。
### 噪声问题
噪声问题是实际应用场景下，频繁遇到的问题。直接容易理解的案例就是，在语音识别中，标准语音数据集都是在安静环境下采集的数据，但是在实际应用中，我们录音时多多少少会有噪声，那么我们需要专门去处理噪声，比如进行一个降噪处理，或者在训练数据中添加噪声等。在图像的识别中，那么就需要考虑图片中的遮挡、雾霾、旋转、镜像和大小远近等问题。








