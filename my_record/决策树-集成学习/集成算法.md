集成学习包括Bagging方法和Boosting方法，下面详细分析这两种方法。

下面是决策树与这些算法框架进行结合所得到的新的算法：
1） Bagging + 决策树 = 随机森林
2）AdaBoost + 决策树 = 提升树
3）Gradient Boosting + 决策树 = GBDT

# Bagging即套袋法

**Bagging法假设训练样本集服从均匀分布**

## 算法过程如下：
（1） 从训练样本集中**随机可放回抽样（Bootstrapping )得到子训练集(训练子集的大小和原始数据集的大小相同)**，重复抽样K次，**得到K个训练集 。**
（2） 每个基学习器基于不同子训练集进行训练
（3） 分类问题：对K个模型采用**投票的方式得到分类结果**；回归问题：对K个模型的值**求平均得到分**类结果。

![](https://upload-images.jianshu.io/upload_images/18339009-cb6730dbe1fca107.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



# Boosting算法

Boosting 训练过程为阶梯状，基模型的训练是有顺序的，每个基模型都会在前一个基模型学习的基础上进行学习，最终综合所有基模型的预测值产生最终的预测结果，用的比较多的综合方式为加权法。


## 关于Boosting的两个核心问题
（1）每一轮如何改变训练数据的权值和概率分布？
- 通过**提高那些在前一轮被弱学习器分错样例的权值，减小前一轮正确样例的权值，使学习器重点学习分错的样本**，提高学习器的性能。

（2）通过什么方式来组合弱学习器？
- 通过**加法模型将弱学习器进行线性组合，学习器准确率大，则相应的学习器权值大**；反之，则学习器的权值小。即给学习器好的模型一个较大的确信度，提高学习器的性能。


![](https://upload-images.jianshu.io/upload_images/18339009-8f0b381f9f268f9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# Stacking
第一层原始训练集**训练多个基学习器，预测训练集、测试集**
第二层的模型**以第一层基学习器的训练集预测结果作为训练集进行再训练**，用第一层测试集预测结果作为测试集

![](https://upload-images.jianshu.io/upload_images/18339009-f8edd40b568beb56.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降，因此现在的问题变成了如何降低再训练的过拟合性，这里我们一般有两种方法：
- 次级模型尽量选择简单的线性模型
- 利用K折交叉验证



# Blending
采用了和stacking同样的方法，不过只**从训练集中选择一个fold的结果，再和原始特征进行concat作为元学习器meta learner的特征**，测试集上进行同样的操作。

在第一层，**70%的数据上训练多个模型，预测剩下30%数据和测试集**。
在第二层，用第一层**剩下30%数据预测的结果作为训练集继续训练，用第一层预测的测试集结果作为测试集**

其优点在于：
1. 比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）
2. 避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集

缺点在于：
1. 使用了很少的数据（第二阶段的blender只使用training set10%的量）
2. blender可能会过拟合，stacking使用多次的交叉验证会比较稳健 




# 集成学习之结合策略
集成学习得到多个学习器后，结合策略得到最终的结果。通常用到最多的是平均法，投票法和学习法。
## 平均法：简单平均、加权平均
适用范围：

**规模大的集成，学习的权重较多**，**加权平均法易导致过拟合**
$$H(x)=\sum_{i=1}^{T} w_{i} h_{i}(x)$$
**个体学习器性能相差较大时宜使用加权平均法，相近用简单平均法**。
$$H(x)=\frac{1}{T} \sum_{i=1}^{T} h_{i}(x)$$

## 投票法：

绝对多数投票法：某标记**超过半数**，也就是我们常说的要票过半数，否则就当会拒绝预测；
$$H(x)=\left\{\begin{array}{l}{c_{j}, \quad \text { if } \sum_{i=1}^{T} h_{i}^{j}(x)>0.5 \sum_{k=1}^{T} \sum_{i=1}^{T} h_{i}^{k}(x)} \\ {\text {reject,} \quad \text { otherwise. }}\end{array}\right.$$
相对多数投票法：预测为得票**最多**的标记，若同时有多个标记的票最高，则从中随机选取一个，也就是所谓的“少数服从多数”。
$$H(x)=c_{a r g} \max _{j} \sum_{i=1}^{T} h_{i}^{j}(x)$$
加权投票法：提供了预测结果，与加权平均法类似。
$$H(x)=c_{\arg \max _{j}} \sum_{i=1}^{T} w_{i} h_{i}^{j}(x)$$

## 学习法：
对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们**将训练集弱学习器的学习结果作为输入，**将训练集的输出作为输出，重新训练一个学习器来得到最终结果。
 在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。
![image.png](https://upload-images.jianshu.io/upload_images/18339009-d6148fe9159aa37d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# Bagging和Boosting两者之间的区别

1）训练样本集
Bagging：训练集是有放回抽样，从原始集中选出的K组训练集是相互独立的。
Boosting：每一次迭代的训练集不变。
2）训练样本权重
Bagging：每个训练样本的权重相等，即1/N。
Boosting：根据学习器的错误率不断调整样例的权值，错误率越大，权值越大。
3）预测函数的权重：
Bagging：K组学习器的权重相等，即1/K。
Boosting：学习器性能好的分配较大的权重，学习器性能差的分配较小的权重。
4）并行计算
Bagging：K组学习器模型可以并行生成。
Boosting：K组学习器只能顺序生成，因为后一个模型的样本权值需要前一个学习器模型的结果。

# Bagging和Boosting的方差和偏差问题讨论

## Bagging减小模型的方差

![](https://upload-images.jianshu.io/upload_images/18339009-f5f26c6a5a5c3a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对于 Bagging 来说，每个基模型的权重等于 1/m 且期望近似相等，故我们可以得到：

![](https://upload-images.jianshu.io/upload_images/18339009-4f621ef4819822fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


通过上式我们可以看到：

*   **整体模型的期望等于基模型的期望**，这也就意味着**整体模型的偏差和基模型的偏差近似**。
*   **整体模型的方差小于等于基模型的方差**，当且仅当相关性为 1 时取等号，随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于 1 吗？并不一定，当基模型数增加到一定程度时，方差公式第一项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。

在此我们知道了为什么 Bagging 中的基模型一定要为强模型，如果 Bagging 使用弱模型则会导致整体模型的偏差提高，而准确度降低。

Random Forest 是经典的基于 Bagging 框架的模型，并在此基础上通过**引入特征采样和样本采样来降低基模型间的相关性**，在公式中显著降低方差公式中的第二项，略微升高第一项，从而使得整体降低模型整体方差。
## Boosting是减小模型的偏差
对于 Boosting 来说，由于基模型共用同一套训练集，所以基模型间具有强相关性，故模型间的相关系数近似等于 1，针对 Boosting 化简公式为：

![](https://upload-images.jianshu.io/upload_images/18339009-bb90b601dcf110a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


通过观察整体方差的表达式我们容易发现：
*   **整体模型的方差等于基模型的方差**，如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。
*   此外 Boosting 框架中采用基于贪心策略的前向加法，整体模型的期望由基模型的期望累加而成，所以**随着基模型数的增多，整体模型的期望值增加**，整体模型的准确度提高。

基于 Boosting 框架的 Gradient Boosting Decision Tree 模型中基模型也为树模型，同 Random Forrest，我们也可以**对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果**。
# 总结
*   我们可以使用模型的偏差和方差来近似描述模型的准确度；
*   对于 Bagging 来说，**整体模型的偏差与基模型近似，而随着模型的增加可以降低整体模型的方差，故其基模型需要为强模型**；
*   对于 Boosting 来说，**整体模型的方差近似等于基模型的方差，而整体模型的偏差由基模型累加而成，故基模型需要为弱模型**。

不是所有集成学习框架中的基模型都是弱模型。**Bagging 和 Stacking 中的基模型为强模型（偏差低，方差高）**，而Boosting 中的基模型为弱模型（偏差高，方差低）





