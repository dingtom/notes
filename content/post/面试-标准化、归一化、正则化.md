---
    # 文章标题
    title: "面试-标准化、归一化、正则化"
    # 分类
    categories: 
        - 面试
    # 发表日期
    date: 2022-12-01T19:59:47+08:00
    
--- 





瘦长的椭圆，会导致趋向最值时`梯度下降的震荡`；所以需要缩放特征值，使得其取值范围相近。按经验，特征缩放到3倍或1/3是比较可以接受的。
![](https://upload-images.jianshu.io/upload_images/18339009-7b8e05b9f479e6b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-e94b1c23eb15233a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




# 标准化（Standardization）（Z-score）
标准化即为概率论与数理统计中常见的Z-score标准化。在特征值的均值（mean）和标准差（standard deviation）的基础上计算得出。
处理后特征符合`标准正态分布[-1,1]`。

$z=\frac{x-\mu}{\sigma}$
```
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(x_train)  
#标准化，将特征值映射到负无穷到正无穷
x_trainScaler = scaler.transform(x_train) 
x_testScaler = scaler.transform(x_test)
```


# 规范化 (normalization)\归一化：
归一化是`将每个样本缩放为单位范数（每个样本的范数为1）`。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都`转化为“单位向量”，[0,1]`
$X_{\text {norm }}=\frac{X-X_{\min }}{X_{\max }-X_{\min }}$



```
from sklearn.preprocessing import Normalizer
scaler = Normalizer().fit(x_train)#归一化
x_trainScaler = scaler.transform(x_train)
x_testScaler = scaler.transform(x_test)  
```

在实际应用中，`通过梯度下降法求解的模型通常是需要归一化的`，包括线性回归、逻辑回归、支持向量机、神经网络等模型。

但对于決策树模型则并不适用，以C4.5为例，`决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的因为归ー化并不会改变样本在特征x上的信息增益`。

- 原因1：因为它们`不关心变量的值，而是关心变量的分布和变量之间的条件概率`；
- 原因2：因为`数值缩放不影响分裂点位置`，对树模型的结构不造成影响。按照`特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同`。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此`树模型是阶跃的，阶跃点是不可导的`，并且求导没意义，也就不需要归一化。







向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性。你要把你的知识数学化告诉这个模型，对代价函数来说，就是加入对模型“长相”的惩罚

则化通过避免训练完美拟合数据样本的系数而有助于算法的泛化。为了防止过拟合， 增加训练样本是一个好的解决方案。此外， 还可使用`数据增强、 L1正则化、 L2 正则化、 Dropout、 DropConnect 和早停（Early stopping） 法`等。

# L1正则化、 L2 正则化
在损失函数中给`每个参数 w 加上权重`，引入模型复杂度指标，从而抑制模型噪声，`减小过拟合`。 

可以用于防止过拟合的原因在于`更小的权值表示神经网络的复杂度更低、网络参数越小，这说明模型相对简单，越简单的模型引起过度拟合的可能性越小`。对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，即抗扰动能力强。

## L1正则化
![L1 正则化时原始的损失函数后面加上一个 L1 正则化项， 即权值 w 绝对值的和除以 n](https://upload-images.jianshu.io/upload_images/18339009-3a8edd4d8c144f16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
##  L2 正则化
![L2 正则化就是在损失函数后面增加上 L2 正则化项，所有权值的平方和除以训练集中的样本大小](https://upload-images.jianshu.io/upload_images/18339009-24b804ecfe54eef4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

-  L1正则化向目标函数添加正则化项，以减少参数的绝对值总和；L1通常是`比L2更容易得到稀疏输出的`，会把一些不重要的特征直接置零`L1正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于0，因此它常用于特征选择`。同时一定程度上防止过拟合； 

- L2正则化中， 添加正则化项的目的在于`减少参数平方的总和`。机器学习中最常用的正则化方法是对权重施加L2范数约束。

- 线性回归中，使用`L1正则化的为Lasso回归`，使用`L2正则化的为Ridge回归(岭回归)`，既使用L1正则又使用L2正则的为 `ElasticNet`。

- L1相对于L2更能实现权值稀疏，L1是各元素绝对值之和，L2是各元素平方和的根，在对不同参数进行惩罚时，`L1无论参数大小如何，对它们的惩罚值都相同`，导致那些参数大小和惩罚值相等的参数，一减就变为 0，而L2对参数的惩罚值是根据参数本身的大小来变化的，`越小的参数惩罚值越小，越大的参数惩罚值越大`，所以最终使得所有参数都接近 0，但不会等于0。

![](https://upload-images.jianshu.io/upload_images/18339009-80f4594af295dd20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上图代表的意思就是目标函数-平方误差项的等值线和L1、L2范数等值线（左边是L1），我们正则化后的代价函数需要求解的目标就是在经验风险和模型复杂度之间的平衡取舍，在图中形象地表示就是黑色线与彩色线的交叉点。

对于L1范数，其图形为菱形，二维属性的等值线有4个角（高维的会有更多），“突出来的角”更容易与平方误差项进行交叉，而这些“突出来的角”都是在坐标轴上，即W1或则W2为0；
而对于L2范数，交叉点一般都是在某个象限中，很少有直接在坐标轴上交叉的。

因此L1范数正则化项比L2的更容易得到稀疏解。