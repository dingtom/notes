---
    # 文章标题
    title: "面试-激活、损失函数"
    # 分类
    categories: 
        - 面试
    # 发表日期
    date: 2022-12-01T19:59:47+08:00

---





## 激活函数

> 如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）
> 激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题；
>  在神经网络中，我们可以经常看到对于某一个隐藏层的节点，该节点的激活之计算一般分为两步：
> （1） 输入该节点的值后先进行一个线性变换，计算出值
> （2）再进行一个非线性变换，也就是经过一个非线性激活函数

常用的激活函数包括：sigmoid函数、tanh函数、ReLU函数。

### sigmoid函数：

当目标是解决一个二分类问题，可在输出层使用sigmoid函数进行二分类。

 该函数数将取值为(−∞,+∞) 的数映射到(0,1)之间，其公式以及函数图如下所示：
![image.png](https://upload-images.jianshu.io/upload_images/18339009-5bd89cfb15325e55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300)
![](https://upload-images.jianshu.io/upload_images/18339009-ce4352e48c3d9899.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300)
缺点

> 1.`当值非常大或者非常小是sigmoid的导数会趋近为0`，则会导致梯度消失
> 2.`函数的输出不是以0位均值，不便于下一层的计算。`这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：`???????????如x>0, f=wTx+b 那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。` 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
> 3.`解析式中含有幂运算`，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间

### tanh函数

tanh读作Hyperbolic Tangent，它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。
该函数数将取值为(−∞,+∞) 的数映射到(-1,1)之间，其公式以及函数图如下所示：
![](https://upload-images.jianshu.io/upload_images/18339009-23dc6503cdc17b26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300)
![](https://upload-images.jianshu.io/upload_images/18339009-037a253b499d575d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300)

### ReLU函数

ReLU函数又称为修正线性单元, 是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下：
![image.png](https://upload-images.jianshu.io/upload_images/18339009-f27b342366a20935.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 优点： 1. 当输入大于0时，不存在梯度消失的问题2. 由于ReLU函数只有线性关系，所以计算速度要快很多
> 缺点：1.当输入小于0时，梯度为0，会产生梯度消失问题。

ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
1） 解决了gradient vanishing问题 (在正区间)
2）计算速度非常快，只需要判断输入是否大于0
3）收敛速度远快于sigmoid和tanh

ReLU也有几个需要特别注意的问题：
1）ReLU的输出不是zero-centered
2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！



 



## 损失函数



### Cross Entropy Loss

交叉熵损失函数。马上就能说出它的二分类公式：

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-20-49-802.png)

在二分类问题模型，真实样本的标签为 [0，1]，分别表示负类和正类。模型的最后通常会经过一个 Sigmoid 函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大。

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-24-09-587.png)



![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-07-355.png)

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-51-982.png)

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-32-54-001.png)

横坐标是预测输出，纵坐标是交叉熵损失函数 L。显然，预测输出越接近真实样本标签 1，损失函数 L 越小；预测输出越接近 0，L 越大。因此，函数的变化趋势完全符合实际需要的情况。

无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。



BCE

BCE损失函数（Binary Cross-Entropy Loss）是交叉熵损失函数（Cross-Entropy Loss）的一种特例，BCE Loss只应用在二分类任务中。针对分类问题，单样本的交叉熵损失为：

![quicker_46e51217-3a10-40a0-a80b-a74c7eb0ee93.png](https://s2.loli.net/2022/04/01/hbuasvVTqPFCcop.png)

```python
import torch
import torch.nn as nn

bce = nn.BCELoss()
bce_sig = nn.BCEWithLogitsLoss()

input = torch.randn(5, 1, requires_grad=True)
target = torch.empty(5, 1).random_(2)
pre = nn.Sigmoid()(input)

loss_bce = bce(pre, target)
loss_bce_sig = bce_sig(input, target)

# 同时，pytorch还提供了已经结合了Sigmoid函数的BCE损失：torch.nn.BCEWithLogitsLoss()，相当于免去了实现进行Sigmoid激活的操作。


input = tensor([[-0.2296],
        		[-0.6389],
        		[-0.2405],
        		[ 1.3451],
        		[ 0.7580]], requires_grad=True)
output = tensor([[1.],
        		 [0.],
        		 [0.],
        		 [1.],
        		 [1.]])
pre = tensor([[0.4428],
        	  [0.3455],
        	  [0.4402],
        	  [0.7933],
        	  [0.6809]], grad_fn=<SigmoidBackward>)

print(loss_bce)
tensor(0.4869, grad_fn=<BinaryCrossEntropyBackward>)

print(loss_bce_sig)
tensor(0.4869, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)
```





### Label Smoothing

对于标注数据来说，这个时候我们认为其标注结果是准确的（不然这个结果就没意义了）。`但实际上，有一些标注数据并不一定是准确的`。那么这时候，使用交叉熵损失函数作为目标函数并不一定是最优的。这会导致模型对正确分类的情况奖励最大，错误分类惩罚最大。如果训练数据能覆盖所有情况，或者是完全正确，那么这种方式没有问题。但事实上，这不可能。`所以这种方式可能会带来泛化能力差的问题，即过拟合。`

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_14-45-42-974.png)

### Focal Loss

在目标检测领域对于one-stage的检测器准确率不高的问题，论文作者给出了解释：由于正负样本不均衡的问题（感觉理解成简单-难分样本不均衡比较好）。 什么意思呢，就是说one-stage中能够匹配到目标的候选框（正样本）个数一般只用十几个或几十个，而没匹配到的候选框（负样本）大概有$10^4 - 10^5$个。而负样本大多数都是简单易分的，对训练起不到什么作用，但是数量太多会淹没掉少数但是对训练有帮助的困难样本。

那么正负样本不均衡，会带来什么问题呢？

- 训练效率低下。 training is inefficient as most locations are easy negatives that contribute no useful learning signal;
- 模型精度变低。 过多的负样本会主导训练，使模型退化。en masse,the easy negatives can overwhelm training and lead to degenerate models.

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-24-56-371.png)

![quicker_b30327b2-4641-40a9-b9d2-53e0ed7c1ad2.png](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-27-48-233.png)

如上图，横坐标代表$p_t$，纵坐标表示各种样本所占的loss权重。对于正样本，我们希望p越接近1越好，也就是$p_t$越接近1越好；对于负样本，我们希望p越接近0越好，也就是$p_t$越接近1越好。所以不管是正样本还是负样本，我们总是希望他预测得到的$p_t$ 越大越好。如上图所示，$p_t\in[0.6, 1]$就是我们预测效果比较好的样本（也就是易分样本）了。显然可以想象这部分的样本数量很多，所以占比是比较高的（如图中蓝色线区域），我们用$(1-p_t)^\gamma$ 来降低易分样本的损失占比 / 损失贡献（如图其他颜色的曲线）。

优点：

- 解决了one-stage object detection中图片中正负样本（前景和背景）不均衡的问题；

- 降低简单样本的权重，使损失函数更关注困难样本；

缺点：

- 模型很容易收到噪声干扰：会将噪声当成复杂样本，使模型过拟合退化；
- 模型的初期，数量多的一类可能主导整个loss，所以训练初期可能训练不稳定；
- 两个参数$\alpha_t$ 和$\gamma$具体的值很难定义，需要自己调参，调的不好可能效果会更差（论文中的 $\alpha_t$=0.25，$\gamma$=2最好）。

```python
import torch.nn as nn
import torch
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits    # 如果BEC带logits则损失函数在计算BECloss之前会自动计算softmax/sigmoid将其映射到[0,1]
        self.reduce = reduce

    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)`self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss


FL1 = FocalLoss(logits=False)
FL2 = FocalLoss(logits=True)

inputs = torch.randn(5, 1, requires_grad=True)
targets = torch.empty(5, 1).random_(2)
pre = nn.Sigmoid()(inputs)

f_loss_1 = FL1(pre, targets)
f_loss_2 = FL2(inputs, targets)


print('inputs:', inputs)
inputs: tensor([[-1.3521],
        [ 0.4975],
        [-1.0178],
        [-0.3859],
        [-0.2923]], requires_grad=True)

print('targets:', targets)
targets: tensor([[1.],
        [1.],
        [0.],
        [1.],
        [1.]])

print('pre:', pre)
pre: tensor([[0.2055],
        [0.6219],
        [0.2655],
        [0.4047],
        [0.4274]], grad_fn=<SigmoidBackward>)

print('f_loss_1:', f_loss_1)
f_loss_1: tensor(0.3375, grad_fn=<MeanBackward0>)

print('f_loss_2', f_loss_2)
f_loss_2 tensor(0.3375, grad_fn=<MeanBackward0>)
```

### Lovász-Softmax

IoU是评价分割模型分割结果质量的重要指标，因此很自然想到能否用1−IoU（即Jaccard loss）来做损失函数，但是它是一个离散的loss，不能直接求导，所以无法直接用来作为损失函数。为了克服这个离散的问题，可以采用`Lovász extension将离散的Jaccard loss 变得连续，从而可以直接求导，使得其作为分割网络的loss function`。Lovász-Softmax相比于交叉熵函数具有更好的效果。



![quicker_b818041f-ffa0-4458-b1b3-988c1d4b3dd6.png](https://s2.loli.net/2022/04/01/JwYkyMQp19mqhRa.png)

```python
import torch
from torch.autograd import Variable
import torch.nn.functional as F
import numpy as np
try:
    from itertools import  ifilterfalse
except ImportError: # py3k
    from itertools import  filterfalse as ifilterfalse
    
# --------------------------- MULTICLASS LOSSES ---------------------------
def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):
    """
    Multi-class Lovasz-Softmax loss
      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).
              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].
      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)
      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.
      per_image: compute the loss per image instead of per batch
      ignore: void class labels
    """
    if per_image:
        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)
                          for prob, lab in zip(probas, labels))
    else:
        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)
    return loss


def lovasz_softmax_flat(probas, labels, classes='present'):
    """
    Multi-class Lovasz-Softmax loss
      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)
      labels: [P] Tensor, ground truth labels (between 0 and C - 1)
      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.
    """
    if probas.numel() == 0:
        # only void pixels, the gradients should be 0
        return probas * 0.
    C = probas.size(1)
    losses = []
    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes
    for c in class_to_sum:
        fg = (labels == c).float() # foreground for class c
        if (classes is 'present' and fg.sum() == 0):
            continue
        if C == 1:
            if len(classes) > 1:
                raise ValueError('Sigmoid output possible only with 1 class')
            class_pred = probas[:, 0]
        else:
            class_pred = probas[:, c]
        errors = (Variable(fg) - class_pred).abs()
        errors_sorted, perm = torch.sort(errors, 0, descending=True)
        perm = perm.data
        fg_sorted = fg[perm]
        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))
    return mean(losses)


def flatten_probas(probas, labels, ignore=None):
    """
    Flattens predictions in the batch
    """
    if probas.dim() == 3:
        # assumes output of a sigmoid layer
        B, H, W = probas.size()
        probas = probas.view(B, 1, H, W)
    B, C, H, W = probas.size()
    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C
    labels = labels.view(-1)
    if ignore is None:
        return probas, labels
    valid = (labels != ignore)
    vprobas = probas[valid.nonzero().squeeze()]
    vlabels = labels[valid]
    return vprobas, vlabels


def xloss(logits, labels, ignore=None):
    """
    Cross entropy loss
    """
    return F.cross_entropy(logits, Variable(labels), ignore_index=255)

# --------------------------- HELPER FUNCTIONS ---------------------------
def isnan(x):
    return x != x
    
def mean(l, ignore_nan=False, empty=0):
    """
    nanmean compatible with generators.
    """
    l = iter(l)
    if ignore_nan:
        l = ifilterfalse(isnan, l)
    try:
        n = 1
        acc = next(l)
    except StopIteration:
        if empty == 'raise':
            raise ValueError('Empty mean')
        return empty
    for n, v in enumerate(l, 2):
        acc += v
    if n == 1:
        return acc
    return acc / n
```

# 检测速度

`前传耗时`：从输入一张图像到输出最终结果所消耗的时间，`包括前处理耗时（如图`
`像归一化）、网络前传耗时、后处理耗时（如非极大值抑制）`
`每秒帧数`FPS(Frames Per Second)：每秒钟能处理的图像数量
`浮点运算量`(FLOPS floating-point operations per second)：`处理一张图像所需要的浮点运算数量`，跟具体软硬件没有关系，可以公平地比较不同算法之间的检测速度。