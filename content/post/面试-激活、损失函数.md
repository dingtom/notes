---
    # 文章标题
    title: "面试-激活、损失函数"
    # 分类
    categories: 
        - 面试
    # 发表日期
    date: 2022-12-01T19:59:47+08:00
    
--- 



# Cross Entropy Loss

交叉熵损失函数。马上就能说出它的二分类公式：

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-20-49-802.png)

在二分类问题模型，真实样本的标签为 [0，1]，分别表示负类和正类。模型的最后通常会经过一个 Sigmoid 函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大。

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-24-09-587.png)



![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-07-355.png)

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-51-982.png)

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-32-54-001.png)

横坐标是预测输出，纵坐标是交叉熵损失函数 L。显然，预测输出越接近真实样本标签 1，损失函数 L 越小；预测输出越接近 0，L 越大。因此，函数的变化趋势完全符合实际需要的情况。

无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。



# BCE

BCE损失函数（Binary Cross-Entropy Loss）是交叉熵损失函数（Cross-Entropy Loss）的一种特例，BCE Loss只应用在二分类任务中。针对分类问题，单样本的交叉熵损失为：

![quicker_46e51217-3a10-40a0-a80b-a74c7eb0ee93.png](https://s2.loli.net/2022/04/01/hbuasvVTqPFCcop.png)

```python
import torch
import torch.nn as nn

bce = nn.BCELoss()
bce_sig = nn.BCEWithLogitsLoss()

input = torch.randn(5, 1, requires_grad=True)
target = torch.empty(5, 1).random_(2)
pre = nn.Sigmoid()(input)

loss_bce = bce(pre, target)
loss_bce_sig = bce_sig(input, target)

# 同时，pytorch还提供了已经结合了Sigmoid函数的BCE损失：torch.nn.BCEWithLogitsLoss()，相当于免去了实现进行Sigmoid激活的操作。


input = tensor([[-0.2296],
        		[-0.6389],
        		[-0.2405],
        		[ 1.3451],
        		[ 0.7580]], requires_grad=True)
output = tensor([[1.],
        		 [0.],
        		 [0.],
        		 [1.],
        		 [1.]])
pre = tensor([[0.4428],
        	  [0.3455],
        	  [0.4402],
        	  [0.7933],
        	  [0.6809]], grad_fn=<SigmoidBackward>)

print(loss_bce)
tensor(0.4869, grad_fn=<BinaryCrossEntropyBackward>)

print(loss_bce_sig)
tensor(0.4869, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)
```





# Label Smoothing

对于标注数据来说，这个时候我们认为其标注结果是准确的（不然这个结果就没意义了）。`但实际上，有一些标注数据并不一定是准确的`。那么这时候，使用交叉熵损失函数作为目标函数并不一定是最优的。这会导致模型对正确分类的情况奖励最大，错误分类惩罚最大。如果训练数据能覆盖所有情况，或者是完全正确，那么这种方式没有问题。但事实上，这不可能。`所以这种方式可能会带来泛化能力差的问题，即过拟合。`

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_14-45-42-974.png)

# Focal Loss

在目标检测领域对于one-stage的检测器准确率不高的问题，论文作者给出了解释：由于正负样本不均衡的问题（感觉理解成简单-难分样本不均衡比较好）。 什么意思呢，就是说one-stage中能够匹配到目标的候选框（正样本）个数一般只用十几个或几十个，而没匹配到的候选框（负样本）大概有$10^4 - 10^5$个。而负样本大多数都是简单易分的，对训练起不到什么作用，但是数量太多会淹没掉少数但是对训练有帮助的困难样本。

那么正负样本不均衡，会带来什么问题呢？

- 训练效率低下。 training is inefficient as most locations are easy negatives that contribute no useful learning signal;
- 模型精度变低。 过多的负样本会主导训练，使模型退化。en masse,the easy negatives can overwhelm training and lead to degenerate models.

![](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-24-56-371.png)

![quicker_b30327b2-4641-40a9-b9d2-53e0ed7c1ad2.png](https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-27-48-233.png)

如上图，横坐标代表$p_t$，纵坐标表示各种样本所占的loss权重。对于正样本，我们希望p越接近1越好，也就是$p_t$越接近1越好；对于负样本，我们希望p越接近0越好，也就是$p_t$越接近1越好。所以不管是正样本还是负样本，我们总是希望他预测得到的$p_t$ 越大越好。如上图所示，$p_t\in[0.6, 1]$就是我们预测效果比较好的样本（也就是易分样本）了。显然可以想象这部分的样本数量很多，所以占比是比较高的（如图中蓝色线区域），我们用$(1-p_t)^\gamma$ 来降低易分样本的损失占比 / 损失贡献（如图其他颜色的曲线）。

优点：

- 解决了one-stage object detection中图片中正负样本（前景和背景）不均衡的问题；

- 降低简单样本的权重，使损失函数更关注困难样本；

缺点：

- 模型很容易收到噪声干扰：会将噪声当成复杂样本，使模型过拟合退化；
- 模型的初期，数量多的一类可能主导整个loss，所以训练初期可能训练不稳定；
- 两个参数$\alpha_t$ 和$\gamma$具体的值很难定义，需要自己调参，调的不好可能效果会更差（论文中的 $\alpha_t$=0.25，$\gamma$=2最好）。

```python
import torch.nn as nn
import torch
import torch.nn.functional as F

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.logits = logits    # 如果BEC带logits则损失函数在计算BECloss之前会自动计算softmax/sigmoid将其映射到[0,1]
        self.reduce = reduce

    def forward(self, inputs, targets):
        if self.logits:
            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)
        else:
            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)
        pt = torch.exp(-BCE_loss)
        F_loss = self.alpha * (1-pt)`self.gamma * BCE_loss

        if self.reduce:
            return torch.mean(F_loss)
        else:
            return F_loss


FL1 = FocalLoss(logits=False)
FL2 = FocalLoss(logits=True)

inputs = torch.randn(5, 1, requires_grad=True)
targets = torch.empty(5, 1).random_(2)
pre = nn.Sigmoid()(inputs)

f_loss_1 = FL1(pre, targets)
f_loss_2 = FL2(inputs, targets)


print('inputs:', inputs)
inputs: tensor([[-1.3521],
        [ 0.4975],
        [-1.0178],
        [-0.3859],
        [-0.2923]], requires_grad=True)

print('targets:', targets)
targets: tensor([[1.],
        [1.],
        [0.],
        [1.],
        [1.]])

print('pre:', pre)
pre: tensor([[0.2055],
        [0.6219],
        [0.2655],
        [0.4047],
        [0.4274]], grad_fn=<SigmoidBackward>)

print('f_loss_1:', f_loss_1)
f_loss_1: tensor(0.3375, grad_fn=<MeanBackward0>)

print('f_loss_2', f_loss_2)
f_loss_2 tensor(0.3375, grad_fn=<MeanBackward0>)
```

# Lovász-Softmax

IoU是评价分割模型分割结果质量的重要指标，因此很自然想到能否用1−IoU（即Jaccard loss）来做损失函数，但是它是一个离散的loss，不能直接求导，所以无法直接用来作为损失函数。为了克服这个离散的问题，可以采用`Lovász extension将离散的Jaccard loss 变得连续，从而可以直接求导，使得其作为分割网络的loss function`。Lovász-Softmax相比于交叉熵函数具有更好的效果。



![quicker_b818041f-ffa0-4458-b1b3-988c1d4b3dd6.png](https://s2.loli.net/2022/04/01/JwYkyMQp19mqhRa.png)

```python
import torch
from torch.autograd import Variable
import torch.nn.functional as F
import numpy as np
try:
    from itertools import  ifilterfalse
except ImportError: # py3k
    from itertools import  filterfalse as ifilterfalse
    
# --------------------------- MULTICLASS LOSSES ---------------------------
def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):
    """
    Multi-class Lovasz-Softmax loss
      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).
              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].
      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)
      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.
      per_image: compute the loss per image instead of per batch
      ignore: void class labels
    """
    if per_image:
        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)
                          for prob, lab in zip(probas, labels))
    else:
        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)
    return loss


def lovasz_softmax_flat(probas, labels, classes='present'):
    """
    Multi-class Lovasz-Softmax loss
      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)
      labels: [P] Tensor, ground truth labels (between 0 and C - 1)
      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.
    """
    if probas.numel() == 0:
        # only void pixels, the gradients should be 0
        return probas * 0.
    C = probas.size(1)
    losses = []
    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes
    for c in class_to_sum:
        fg = (labels == c).float() # foreground for class c
        if (classes is 'present' and fg.sum() == 0):
            continue
        if C == 1:
            if len(classes) > 1:
                raise ValueError('Sigmoid output possible only with 1 class')
            class_pred = probas[:, 0]
        else:
            class_pred = probas[:, c]
        errors = (Variable(fg) - class_pred).abs()
        errors_sorted, perm = torch.sort(errors, 0, descending=True)
        perm = perm.data
        fg_sorted = fg[perm]
        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))
    return mean(losses)


def flatten_probas(probas, labels, ignore=None):
    """
    Flattens predictions in the batch
    """
    if probas.dim() == 3:
        # assumes output of a sigmoid layer
        B, H, W = probas.size()
        probas = probas.view(B, 1, H, W)
    B, C, H, W = probas.size()
    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C
    labels = labels.view(-1)
    if ignore is None:
        return probas, labels
    valid = (labels != ignore)
    vprobas = probas[valid.nonzero().squeeze()]
    vlabels = labels[valid]
    return vprobas, vlabels


def xloss(logits, labels, ignore=None):
    """
    Cross entropy loss
    """
    return F.cross_entropy(logits, Variable(labels), ignore_index=255)

# --------------------------- HELPER FUNCTIONS ---------------------------
def isnan(x):
    return x != x
    
def mean(l, ignore_nan=False, empty=0):
    """
    nanmean compatible with generators.
    """
    l = iter(l)
    if ignore_nan:
        l = ifilterfalse(isnan, l)
    try:
        n = 1
        acc = next(l)
    except StopIteration:
        if empty == 'raise':
            raise ValueError('Empty mean')
        return empty
    for n, v in enumerate(l, 2):
        acc += v
    if n == 1:
        return acc
    return acc / n
```

# 检测速度

`前传耗时`：从输入一张图像到输出最终结果所消耗的时间，`包括前处理耗时（如图`
`像归一化）、网络前传耗时、后处理耗时（如非极大值抑制）`
`每秒帧数`FPS(Frames Per Second)：每秒钟能处理的图像数量
`浮点运算量`(FLOPS floating-point operations per second)：`处理一张图像所需要的浮点运算数量`，跟具体软硬件没有关系，可以公平地比较不同算法之间的检测速度。