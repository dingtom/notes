- [ 数据太大随机采样](#head1)
- [ 只关注DateTime和kWh两列。](#head2)
- [ 重采样](#head3)
- [ 横纵坐标轴交换](#head4)
- [ Prophet是设计用于分析在不同时间间隔上显示模式的日观测时间序列。](#head5)
- [ LSTM似乎非常适合于对时间序列的预测。](#head6)


```python
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```


```python
import pandas as pd
# import rawdata
raw_data_df = pd.read_csv(r"C:\Users\csi\Downloads\Power-Networks-LCL-June2015(withAcornGps)v2.csv", header=0)
print(raw_data_df.columns)
raw_data_df.head()
```

Index(['LCLid', 'stdorToU', 'DateTime', 'KWH/hh(perhalfhour)','Acorn','Acorn_grouped'],dtype='object')



![image.png](https://upload-images.jianshu.io/upload_images/18339009-11e420a5fb61ef08.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head1"> 数据太大随机采样</span>


```python
# DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)
# frac是抽取的比列。（有一些时候，我们并对具体抽取的行数不关系，我们想抽取其中的百分比，这个时候就可以选择使用frac，例如frac=0.8，就是抽取其中80%）
# replace：是否为有放回抽样，取replace=True时为有放回抽样。
# weights这个是每个样本的权重，字符索引或概率数组
# random_state这个在之前的文章已经介绍过了。
# axis是选择抽取数据的行还是列。axis=0的时是抽取行，axis=1时是抽取列（也就是说axis=1时，在列中随机抽取n列，在axis=0时，在行中随机抽取n行）
sampled_data_df = raw_data_df.sample(frac=0.01)
```

# <span id="head2"> 只关注DateTime和kWh两列。</span>


```python
sampled_data_df['date'] = pd.to_datetime(sampled_data_df['DateTime'])  # convert to datetime
selected_data_df = sampled_data_df.loc[:, ['KWH/hh (per half hour) ']]  # select kwh/hh
selected_data_df = selected_data_df.set_index(sampled_data_df.date)  # set data as first column 
selected_data_df['KWH/hh (per half hour) '] = pd.to_numeric(selected_data_df['KWH/hh (per half hour) '], downcast='float', errors='coerce')  # 将参数转换为数值类型
selected_data_df.head()
```

![image.png](https://upload-images.jianshu.io/upload_images/18339009-80a17f80d755d580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
# x axis is data y axis is KWH/hh
selected_data_df.plot()
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-1b8bee2797d347bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head3"> 重采样</span>


```python
# 重采样涉及到更改时间序列观测的频率。
# 特征工程可能是你对重新采样时间序列数据感兴趣的一个原因。
# 实际上，它可以用来为监督学习模型提供额外的架构或者是对学习问题的领会角度。
# pandas中的重采样方法与GroupBy方法相似，因为你基本上是按照特定时间间隔进行分组的。
# 然后指定一种方法来重新采样。让我们通过一些例子来把重采样技术描述的更具体些。
# 我们从每周的总结开始：

# data.resample（）方法将用于对DataFrame的kWh列数据重新取样；
# “W”表示我们要按每周重新取样；
weekly = selected_data_df.resample('W').sum()
weekly.plot(style=[':', '--', '-'])
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-2774a54b5ab1aa54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


```python
# 我们可以对每日的数据也这么做处理
daily = selected_data_df.resample('D').sum()
# Series.rolling 
# DataFrame.rolling(window, min_periods=None, freq=None, center=False, win_type=None, on=None, axis=0, closed=None)
# 第i+window个值为它之前window个值的和或平均，
# min_periods：最少需要有值的观测点的数量，
# center：是否使用i为中间值作为中间值前后取
daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-2d8b07d97b5a1a4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


```python
# 可以使用groupby和mean函数进行按小时处理
# DatetimeIndex.time：返回datetime.time的numpy数组。
hourly = selected_data_df.groupby(selected_data_df.index.time).mean()
hourly_ticks = 4*60*60*np.arange(6)
hourly.plot(xticks=hourly_ticks, style=[':', '--', '-'])
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-2d293ddb47301b7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head4"> 横纵坐标轴交换</span>


```python
chage_axis_data = selected_data_df.reset_index()
month = pd.Series(pd.DatetimeIndex(chage_axis_data['date']).month)
# Series.drop([labels, axis, index, columns, …])：返回已删除指定索引标签的系列。
chage_axis_data = chage_axis_data.drop(['date'], axis=1)
# DataFrame.join(other[, on, how, lsuffix, …])：在索引或键列上与其他DataFrame连接列。
chage_axis_data = chage_axis_data.join(month)
chage_axis_data.plot.scatter(x='KWH/hh (per half hour) ', y='date', figsize=(16,8), linewidth=5, fontsize=20)
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-b6765cd87121f235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


```python
# for trend analysis 
selected_data_df['KWH/hh (per half hour) '].rolling(5).mean().plot(figsize=(20,10), linewidth=5, fontsize=20)
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-3bf4ab55f1801f30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



```python
# For seasonal variations
# df.shift默认数据向下移动填第一行充为NAN， df.diff默认数据和前一行相减第一行为NaN
selected_data_df['KWH/hh (per half hour) '].diff(periods=30).plot(figsize=(20,10), linewidth=5, fontsize=20)
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-85fac29f20ffc168.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
pd.plotting.autocorrelation_plot(selected_data_df['KWH/hh (per half hour) '])
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-191f857b88318f34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head5"> Prophet是设计用于分析在不同时间间隔上显示模式的日观测时间序列。</span>

Prophet对于数据丢失情况和趋势的变化具有很强的鲁棒性，并且通常能够很好地处理异常值。
它还具有高级的功能，可以模拟假日在时间序列上产生的影响并执行自定义的变更点，
但我将坚持使用基本规则来启动和运行模型。
我认为Prophet可能是生产快速预测结果的一个好的选择，因为它有直观的参数，
并且可以由有良好领域知识背景的但缺乏预测模型的技术技能的人来进行调整。


```python
# 在使用Prophet之前，我们将数据里的列重新命名为正确的格式。
# Date列必须称为“ds”和要预测值的列为“y”。我们在下面的示例中使用了每日汇总的数据。
prophet_data_df = daily 
prophet_data_df.reset_index(inplace=True)
# Prophet requires columns ds (Date) and y (value)
prophet_data_df = prophet_data_df.rename(columns={'date': 'ds','KWH/hh (per half hour) ': 'y'})
prophet_data_df.head()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-6db28a56d0279c0b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
import fbprophet 
# Make the prophet model and fit on the data 
# changepoint_prior_scale参数用于控制趋势对变化的敏感度，越高的值会更敏感，越低的值则敏感度越低。
prophet_data_prophet= fbprophet.Prophet(changepoint_prior_scale=0.10)
prophet_data_prophet.fit(prophet_data_df)
```

```python
# 为了进行预测，我们需要创建一个称为未来数据框（future dataframe）的东西。
# 我们需要指定要预测的未来时间段的数量（在我们的例子中是两个月）和预测频率（每天）。
prophet_data_forecast = prophet_data_prophet.make_future_dataframe(periods=30*2, freq='D')
prophet_data_forecast = prophet_data_prophet.predict(prophet_data_forecast)
```


```python
# 我们可以用一个图表来进行可视化预测展示：
prophet_data_prophet.plot(prophet_data_forecast, xlabel='Date', ylabel='KMH')
plt.title('simple test')
plt.show()
# 图中的黑点代表了实际值，蓝线则代表了预测值，而浅蓝色阴影区域代表不确定性。
# 初始的不确定性随着时间的推移而扩散和增多。
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-37262560e4079e28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
# Prophet还可以允许我们轻松地对整体趋势和组件模式进行可视化展示：
# Plot the trends and patterns 
prophet_data_prophet.plot_components(prophet_data_forecast)
# 它看起来表明了居民的电量使用在秋季和冬季会增加，而在春季和夏季则会减少。
# 从每周的趋势来看，周日的使用量似乎比一周中其它时间都要多。
# 最后，总体的趋势表明，使用量增长了一年，然后才缓慢地下降。
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-d6daa16069e08f68.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head6"> LSTM似乎非常适合于对时间序列的预测。</span>



```python
lstm_data_df = daily.loc[:, ['KWH/hh (per half hour) ']]
lstm_data_df = lstm_data_df.set_index(daily.date)
lstm_data_df.head()
```

![image.png](https://upload-images.jianshu.io/upload_images/18339009-0ab7c37bc5291f4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
# LSTM对输入数据的大小很敏感，特别是当使用Sigmoid或Tanh这两个激活函数的时候。
# 通常，将数据重新调整到[0，1]或[-1，1]这个范围是一个不错的实践，也称为规范化。
# 我们可以使用scikit-learn库中的MinMaxScaler预处理类来轻松地规范化数据集。
from sklearn.preprocessing import MinMaxScaler 
values = lstm_data_df['KWH/hh (per half hour) '].values.reshape(-1,1)
values = values.astype('float32')
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(values)
```


```python
# 现在我们可以将已排好序的数据集拆分为训练数据集和测试数据集。
train_size = int(len(scaled)*0.8)
test_size = len(scaled)-train_size 
train, test = scaled[0: train_size,:], scaled[train_size: len(scaled),:]
print(len(train), len(test))
```

663 166



```python
# 我们可以定义一个函数来创建一个新的数据集，X为i到i+look_back；Y为i+look_back
def create_dataset(dataset, look_back=1): 
    dataX, dataY=[], []
    for i in range(len(dataset) - look_back): 
        a = dataset[i:(i+look_back), 0]
        dataX.append(a)
        dataY.append(dataset[i+look_back, 0])
    print(len(dataY))
    return np.array(dataX), np.array(dataY)
look_back = 2
trainX,trainY = create_dataset(train, look_back)
testX,testY = create_dataset(test, look_back)
```

661
164


```python
# LSTM网络要求输入的数据以如下的形式提供特定的数组结构：[样本、时间间隔、特征]。
# 数据目前都规范成了[样本，特征]的形式，我们正在为每个样本设计两个时间间隔。
# 可以将准备好的分别用于训练和测试的输入数据转换为所期望的结构，如下所示：
trainX = np.reshape(trainX, (trainX.shape[0], 1, 2))
testX = np.reshape(testX, (testX.shape[0], 1, 2))
```


```python
# 就是这样，现在已经准备好为示例设计和设置LSTM网络了。
from keras.models import Sequential 
from keras.layers import Dense 
from keras.layers import LSTM 
model = Sequential()
model.add(LSTM(100, input_shape=(trainX.shape[1], trainX.shape[2])))
model.add(Dense(1))
model.compile(loss='mse', optimizer="adam")
```


```python
history = model.fit(trainX, trainY, epochs=300, batch_size=100, validation_data=(testX, testY), verbose=0, shuffle=False)
```


```python
# 从下面的损失图可以看出，该模型在训练数据集和测试数据集上都具有可比较的表现。
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-e29afadf6f9b7626.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
# 在下图中，我们看到LSTM在拟合测试数据集方面做得非常好。
y_pred = model.predict(testX, batch_size=100)
plt.plot(y_pred, label='y_pred')
plt.plot(testY, label='actual')
plt.legend()
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-621e4548d3d2f948.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
# 聚类（Clustering）
# 执行聚类有很多不同的方式，但一种方式是按结构层次来形成聚类。
# 你可以通过两种方式形成一个层次结构：从顶部开始来拆分，或从底部开始来合并。

# 只需简单地导入原始数据，并为某年中的某日和某日中的某一小时添加两列。
cluster_data_df = selected_data_df.reset_index()
cluster_data_df['dy'] = cluster_data_df['date'].dt.dayofyear
cluster_data_df['heure'] = cluster_data_df['date'].dt.time
temp = cluster_data_df.loc[:, ['dy', 'KWH/hh (per half hour) ']]
temp.set_index(cluster_data_df.heure)

# pivot_table 将列数据设定为行索引和列索引，并可以聚合运算。
# df.pivot_table(values=None, index=None, columns=None, aggfunc='mean', fill_value=None, margins=False, dropna=True, margins_name='All')
# values被计算的数据项;index作为结果DataFrame的行索引;columns作为结果DataFrame的列索引;aggfunc聚合函数或函数列表
temp = cluster_data_df.pivot_table(values=['KWH/hh (per half hour) '], index=['heure'], columns=['dy'], fill_value=0)
temp.head()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-728acb2398504372.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

```python
pd.plotting.register_matplotlib_converters()  # index is timedata
temp.plot(figsize=(30, 20))
plt.show()
```

    IOPub data rate exceeded.
    The notebook server will temporarily stop sending output
    to the client in order to avoid crashing it.
    To change this limit, set the config variable
    `--NotebookApp.iopub_data_rate_limit`.



```python
# Linkage和Dendrograms

# linkage函数根据对象的相似性，将距离信息和对象对分组放入聚类中。
# 这些新形成的聚类随后相互连接，以创建更大的聚类。
# 这个过程将会进行迭代，直到在原始数据集中的所有对象在层次树中都连接在了一起。

# 对数据进行聚类：
from scipy.cluster.hierarchy import dendrogram, linkage
# “ward”是可以用来计算新形成的聚类之间距离的一个方法。
# 关键字“ward”让linkage函数使用Ward方差最小化算法。
# 其它常见的linkage方法，如single、complete、average，
# 还有不同的距离度量标准，如euclidean、manhattan、hamming、cosine
Z = linkage(temp.iloc[:, 0:365], 'ward')
```


```python
# 来看dendogram图是聚类的层次图，其中那些条形的长度表示到下一个聚类中心的距离。
plt.figure(figsize=(25, 10))
plt.title('Hierarchical clustering Desdrogram')
plt.xlabel('sample index')
plt.ylabel('diatance')
dendrogram(
    Z,
    leaf_rotation=90.,        # Rotates the x axis labels 
    leaf_font_size=8., # font size for the x axis labels 
)
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-815683f15ef584f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


```python
# 在x轴上标签，如果你没有指定任何其它内容那么这些标签就是X上样本的索引；
# 在y轴上，你可以看到那些距离长度（在我们的例子中是ward方法）；

# 水平线是聚类的合并；
# 那些垂线告诉你哪些聚类或者标签是合并的一部分，从而形成了新的聚类；
# 水平线的高度是用来表示需要被“桥接”以形成新聚类的距离；
# 即使有解释说明，之前的dendogram图看起来仍然不明显。我们可以减少一点，以便能更好地查看数据。
plt.title('Hierarchical Clustering Dendrogram (truncated)')
plt.xlabel('sample index')
plt.ylabel('distance')
dendrogram(
    Z,
    truncate_mode='lastp',# shov only the last p merged clusters 
    p=12,        # show only the lastp merged clusters 
    show_leaf_counts=False,# othervise nusbers in brackets are counts 
    leaf_rotation=90., 
    leaf_font_size=12., 
    show_contracted=True,# to get a distribution impression in truncated branches 
)
plt.show()
```
![image.png](https://upload-images.jianshu.io/upload_images/18339009-ac18c146d66db7e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

