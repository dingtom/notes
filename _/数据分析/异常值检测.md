- [简单统计 ](#head1)
- [ 散点图](#head2)
- [ 3∂原则](#head3)
- [ 箱型图](#head4)
- [ 基于模型检测](#head5)
- [ 基于近邻度的离群点检测](#head6)
- [ 基于密度的离群点检测](#head7)
- [ 基于聚类的方法来做异常点检测](#head8)
- [ 专门的离群点检测](#head9)

# <span id="head1">简单统计 </span>
```
df.describe()
```
![](https://upload-images.jianshu.io/upload_images/18339009-6a679dfcfb9de935.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# <span id="head2"> 散点图</span>
![](https://upload-images.jianshu.io/upload_images/18339009-6be9fc217cd4afbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# <span id="head3"> 3∂原则</span>
**这个原则有个条件：数据需要服从正态分布。**在3∂原则下，**异常值如超过3倍标准差，那么可以将其视为异常值。**正负3∂的概率是99.7%，那么距离平均值3∂之外的值出现的概率为P(|x-u| > 3∂) <= 0.003，属于极个别的小概率事件。
![](https://upload-images.jianshu.io/upload_images/18339009-7248f7ed5f503f8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# <span id="head4"> 箱型图</span>
利用箱型图的四分位距（IQR）对异常值进行检测
![](https://upload-images.jianshu.io/upload_images/18339009-be96c9561d21b050.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
**四分位距(IQR)就是上四分位与下四分位的差值。**而我们通过IQR的1.5倍为标准，**规定：超过（上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离）的点为异常值。**下面是Python中的代码实现，主要使用了numpy的percentile方法。
```
Percentile = np.percentile(df['length'],[0,25,50,75,100])
IQR = Percentile[3] - Percentile[1]
UpLimit = Percentile[3]+ageIQR*1.5
DownLimit = Percentile[1]-ageIQR*1.5
```
也可以使用seaborn的可视化方法boxplot来实现：
```
f,ax=plt.subplots(figsize=(10,8))
sns.boxplot(y='length',data=df,ax=ax)
plt.show()
```

# <span id="head5"> 基于模型检测</span>
>**构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点。**如果模型是簇的集合，则异常是不显著属于任何簇的对象；如果模型是回归时，异常是相对远离预测值的对象。
离群点的概率定义：离群点是一个对象，关于数据的概率分布模型，它具有低概率。**这种情况的前提是必须知道数据集服从什么分布，如果估计错误就造成了重尾分布。**

优缺点：（1）有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；（2）对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。

# <span id="head6"> 基于近邻度的离群点检测</span>
统计方法是**利用数据的分布来观察异常值，一些方法甚至需要一些分布条件，**而在实际中数据的分布很难达到一些假设条件，在使用上有一定的局限性。
>确定数据集的有意义的邻近性度量比确定它的统计分布更容易。这种方法比统计学方法更一般、更容易使用，因为一个对象的离群点得分由到它的k-最近邻（KNN）的距离给定。
需要注意的是：离群点得分对k的取值高度敏感。**如果k太小，则少量的邻近离群点可能导致较低的离群点得分；**如果K太大，则点数少于k的簇中所有的对象可能都成了离群点。为了使该方案对于k的选取更具有鲁棒性，可以使用k个最近邻的平均距离。

优缺点：（1）简单；（2）缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；（3）该方法对参数的选择也是敏感的；（4）不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。

# <span id="head7"> 基于密度的离群点检测</span>

>从基于密度的观点来说，**离群点是在低密度区域中的对象。基于密度的离群点检测与基于邻近度的离群点检测密切相关，因为密度通常用邻近度定义。**

一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用**DBSCAN聚类**算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。

优缺点：（1）给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；（2）与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；（3）参数选择是困难的。虽然LOF算法通过观察不同的k值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。

# <span id="head8"> 基于聚类的方法来做异常点检测</span>

>基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。

离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。**这也是k-means算法的缺点，对离群点敏感。**为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。

优缺点：（1）基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；（2）簇的定义通常是离群点的补，因此可能同时发现簇和离群点；（3）产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；（4）聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。

# <span id="head9"> 专门的离群点检测</span>

其实以上说到聚类方法的本意是是无监督分类，并不是为了寻找离群点的，只是恰好它的功能可以实现离群点的检测，算是一个衍生的功能。

除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：**One Class SVM和Isolation Forest**



<!-- more -->

<!-- more -->
