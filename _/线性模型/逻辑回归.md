- [ 逻辑回归](#head1)
- [ 损失函数](#head2)
- [ 逻辑回归常用的优化方法有哪些](#head3)
	- [ 一阶方法](#head4)
	- [ 二阶方法：牛顿法、拟牛顿法： ](#head5)
- [ 逻辑斯特回归为什么要对特征进行离散化。](#head6)
- [ 逻辑回归有什么优点](#head7)
- [ 代码](#head8)
	- [ sklearn](#head9)
	- [ numpy](#head10)














## <span id="head1"> 逻辑回归</span>

逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞, +∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。

也就是把Y的结果带入一个非线性变换的**Sigmoid函数**中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。



对数几率回归（logistics regression）和一般回归分析有什么区别？：

- 对数几率回归是设计用来预测事件可能性的

- 对数几率回归可以用来度量模型拟合程度

- 对数几率回归可以用来估计回归系数



![](https://upload-images.jianshu.io/upload_images/18339009-bedfb412211c5431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## <span id="head2"> 损失函数</span>

逻辑回归的损失函数是 **log loss**，也就是**对数似然函数**，函数公式如下：

![](https://i.loli.net/2021/05/16/ORFpMvK8ADCPuir.png)

损失函数 可导、凸函数、不好时快速下降





## <span id="head3"> 逻辑回归常用的优化方法有哪些</span>

### <span id="head4"> 一阶方法</span>

梯度下降、随机梯度下降、mini 随机梯度下降降法。随机梯度下降不但速度上比梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。 

### <span id="head5"> 二阶方法：牛顿法、拟牛顿法： </span>

这里详细说一下牛顿法的基本原理和牛顿法的应用方式。牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解。在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了二阶求解问题，比一阶方法更快。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。

缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。

拟牛顿法： 不用二阶偏导而是构造出Hessian矩阵的近似正定对称矩阵的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、 L-BFGS（可以减少BFGS所需的存储空间）。



## <span id="head6"> 逻辑斯特回归为什么要对特征进行离散化。</span>

1. 非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
2. 速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
3. 鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 
4. 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 

## <span id="head7"> 逻辑回归有什么优点</span>

- LR能以概率的形式输出结果，而非只是0,1判定。
- LR的可解释性强，可控度高。
- 训练快，feature engineering之后效果赞。
- 因为结果是概率，可以做ranking model。

# <span id="head8"> 代码</span>

## <span id="head9"> sklearn</span>


```

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.datasets.samples_generator import make_classification
# n_features :特征个数= n_informative（） + n_redundant + n_repeated
# n_informative：多信息特征的个数
# n_redundant：冗余信息，informative特征的随机线性组合
# n_repeated ：重复信息，随机提取n_informative和n_redundant 特征
#　n_classes：分类类别
# n_clusters_per_class ：某一个类别是由几个cluster构成的

X, Y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=2)
Y = Y.reshape((-1, 1))
# print(X.shape, Y.shape, Y[:10])  # (100, 2) (100, 1)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

# 将类别数据数字化
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
onehotencoder = OneHotEncoder(sparse = False)
Y = onehotencoder.fit_transform(Y)
#.toarray()

#### 特征缩放
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

### 步骤2 | 逻辑回归模型
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

for i, j in zip(y_pred, y_test):
    print(i, j)
```

## <span id="head10"> numpy</span>

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_classification

class logistic_regression():    
    def __init__(self):        
        pass
    def create_data(self):
        X, labels = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=2)
        labels = labels.reshape((-1, 1))
        offset = int(X.shape[0] * 0.9)
        X_train, y_train = X[:offset], labels[:offset]
        X_test, y_test = X[offset:], labels[offset:]        
        return X_train, y_train, X_test, y_test    
        
    def plot_logistic(self, X_train, y_train, params):
        n = X_train.shape[0]
        xcord1 = []
        ycord1 = []
        xcord2 = []
        ycord2 = []        
        for i in range(n):            
            if y_train[i] == 1:
                xcord1.append(X_train[i][0])
                ycord1.append(X_train[i][1])            
            else:
                xcord2.append(X_train[i][0])
                ycord2.append(X_train[i][1])
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.scatter(xcord1, ycord1, s=32, c='red')
        ax.scatter(xcord2, ycord2, s=32, c='green')
        x = np.arange(-1.5, 3, 0.1)
        y = (-params['b'] - params['W'][0] * x) / params['W'][1]
        ax.plot(x, y)
        plt.xlabel('X1')
        plt.ylabel('X2')
        plt.show()
    # sigmoid 函数
    def sigmoid(self, x):
        z = 1 / (1 + np.exp(-x))        
        return z    
    # 模型参数初始化函数
    def initialize_params(self, dims):
        W = np.zeros((dims, 1))
        b = 0
        return W, b    
    # 逻辑回归模型主体部分，包括模型计算公式、损失函数和参数的梯度公式
    def logistic(self, X, y, W, b):
        num_train = X.shape[0]
        num_feature = X.shape[1]

        pre = self.sigmoid(np.dot(X, W) + b)
        cost = -1 / num_train * np.sum(y * np.log(pre) + (1 - y) * np.log(1 - pre))

        dW = np.dot(X.T, (pre - y)) / num_train
        db = np.sum(pre - y) / num_train
        cost = np.squeeze(cost)        
        return pre, cost, dW, db    
    # 基于梯度下降的参数更新训练过程
    def logistic_train(self, X, y, learning_rate, epochs):    
        # 初始化模型参数
        W, b = self.initialize_params(X.shape[1])  
        cost_list = []  
        # 迭代训练
        for i in range(epochs):       
            # 计算当前次的模型计算结果、损失和参数梯度
            pre, cost, dW, db = self.logistic(X, y, W, b)    
            # 参数更新
            W = W -learning_rate * dW
            b = b -learning_rate * db        
            # 记录损失
            if i % 100 == 0:
                cost_list.append(cost)   
                print('epoch %d cost %f' % (i, cost)) 
        # 保存参数
        params = {            
            'W': W,            
            'b': b
        }        
        # 保存梯度
        grads = {            
            'dW': dW,            
            'db': db
        }           
        return cost_list, params, grads    
    # 对测试数据的预测函数
    def predict(self, X, params):
        y_prediction = self.sigmoid(np.dot(X, params['W']) + params['b'])        
        for i in range(len(y_prediction)):            
            if y_prediction[i] > 0.5:
                y_prediction[i] = 1
            else:
                y_prediction[i] = 0

        return y_prediction    
    def accuracy(self, y_test, y_pred):
        correct_count = 0
        for i in range(len(y_test)):            
            for j in range(len(y_pred)):                
                if y_test[i] == y_pred[j] and i == j:
                    correct_count += 1

        accuracy_score = correct_count / len(y_test)        
        return accuracy_score    
        
    
if __name__ == "__main__":
    model = logistic_regression()
    X_train, y_train, X_test, y_test = model.create_data()
    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)
    cost_list, params, grads = model.logistic_train(X_train, y_train, 0.01, 1000)
    print(params)
    y_train_pred = model.predict(X_train, params)
    accuracy_score_train = model.accuracy(y_train, y_train_pred)
    print('train accuracy is:', accuracy_score_train)
    y_test_pred = model.predict(X_test, params)
    accuracy_score_test = model.accuracy(y_test, y_test_pred)
    print('test accuracy is:', accuracy_score_test)
    model.plot_logistic(X_train, y_train, params)
```
