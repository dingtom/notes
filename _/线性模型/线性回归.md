- [ 线性回归](#head1)
	- [ 线性](#head2)
	- [ 回归](#head3)
	- [ 公式](#head4)
- [ Lasso回归(L1)](#head5)
	- [ 什么场景下使用L1正则化](#head6)
- [ 岭回归(L2)](#head7)
	- [ 什么场景下用L2正则化](#head8)
- [ ElasticNet回归](#head9)
	- [ ElasticNet回归的使用场景](#head10)
- [ 线性回归的假设](#head11)
- [ 代码](#head12)
	- [ sklearn](#head13)
	- [ numpy](#head14)






# <span id="head1"> 线性回归</span>



## <span id="head2"> 线性</span>

![](https://i.loli.net/2021/05/15/LeYyvSUWoBsAcRT.png)



答案是：**方程一和方程二为线性回归，方程三为非线性回归**。线性回归中线性的含义：因变量y对于未知的回归系数(β0,β1….,k)是线性的。这就是可中所说的参数线性性。换句话说只要**系数β是线性的就称为线性回归**，方程一和方程二中的回归系数都是线性的，而方程三中自变量X1的回归系数为非线性，因此，方程一和方程二为线性回旧，方程三为非线性回归这个问题弄错的原因是，大家把“线性回归方程”等价于“线性函数”。**如方程二，出现了二次方，它是非线性函数**，但是根据线性回归中对线性的定义，它是线性回归方程因此**线性函数≠线性回归方程**，两者的概念不一样。



## <span id="head3"> 回归</span>

到底什么是回归，回归到哪里？就是回归到真实值。当**数据量足够大的时候，我们得到测量数据的均值越接近事物的本质——真实值**，也就是说**线性回归方程就是回归到事物的本质——真实值。**



## <span id="head4"> 公式</span>



![](https://latex.codecogs.com/gif.latex?Y=wx+b)

w叫做x的系数，b叫做偏置项是独立同分布的，服从均值为0,方差为$\sigma^2$ 的高斯分布。。

Loss Function--MSE

![](https://latex.codecogs.com/gif.latex?J=\frac{1}{2m}\sum^{i=1}_{m}(y^{'}-y)^2)






![](https://upload-images.jianshu.io/upload_images/18339009-fa2925e7ff4a4120.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-f8ff35f15c8f3361.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-6bac62bb98afef52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-0cde73129b5a0f80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

值得注意的上式中存在计算矩阵的逆，一般来讲当样本数大于数据维度时，矩阵可逆，可以采用最小二乘法求得目标函数的闭式解。当数据维度大于样本数时，矩阵线性相关，不可逆。此时最小化目标函数解不唯一，且非常多，出于这样一种情况，我们可以考虑奥卡姆剃刀准则来简化模型复杂度，使其不必要的特征对应的w为0。所以引入正则项使得模型中w非0个数最少。当然，岭回归，lasso回归的最根本的目的不是解决不可逆问题，而是防止过拟合。

使用**L1正则化的模型建叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）**。

# <span id="head5"> Lasso回归(L1)</span>

L1正则化是指权值向量$w$中**各个元素的绝对值之和**，通常表示为$||w||$，L1正则化有助于**生成一个稀疏权值矩阵，进而可以用于特征选择**

![](https://latex.codecogs.com/gif.latex?J=J_0+\lambda(|w_1|+|w_2|))

## <span id="head6"> 什么场景下使用L1正则化</span>

**L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0**，从而增强模型的泛化能力 。对于**高维的特征数据,尤其是线性关系是稀疏的，就采用L1正则化(Lasso回归)**,或者是要**在一堆特征里面找出主要的特征**，那么L1正则化(Lasso回归)更是首选了。

# <span id="head7"> 岭回归(L2)</span>

**L2正则化**是指权值向量$w$中各个**元素的平方和然后再求平方根**，L2正则化在拟合过程中通常都倾向于**让权值尽可能小**，最后构造一个所有参数都比较小的模型。

> 因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移 得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』 

方程：

![](https://latex.codecogs.com/gif.latex?J=J_0+\lambda\sum_{w}w^2)

![](https://latex.codecogs.com/gif.latex?J_0)表示上面的 loss function ，在loss function的基础上加入w参数的平方和乘以 ![](https://latex.codecogs.com/gif.latex?\lambda) ，假设：

![](https://latex.codecogs.com/gif.latex?L=\lambda({w_1}^2+{w_2}^2))

## <span id="head8"> 什么场景下用L2正则化</span>

只要数据线性相关，用LinearRegression拟合的不是很好，**需要正则化**，可以考虑使用岭回归(L2), 如果输入特征的维度很高,而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。

**Lasso回归采用一范数**来约束，使参数非零个数最少。而Lasso和岭回归的区别很好理解，**在优化过程中，最优解为函数等值线与约束空间的交集，正则项可以看作是约束空间。**可以看出二范的约束空间是一个球形，而一范的约束空间是一个方形，这也就是**二范会得到很多参数接近0的值，而一范则尽可能非零参数最少。**
![](https://upload-images.jianshu.io/upload_images/18339009-749aa26357bb67ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



# <span id="head9"> ElasticNet回归</span>

**ElasticNet综合了L1正则化项和L2正则化项**，以下是它的公式：

![](https://latex.codecogs.com/gif.latex?min(\frac{1}{2m}[\sum_{i=1}^{m}({y_i}^{'}-y_i)^2+\lambda\sum_{j=1}^{n}\theta_j^2]+\lambda\sum_{j=1}^{n}|\theta|))

## <span id="head10"> ElasticNet回归的使用场景</span>

ElasticNet在我们发现用**Lasso回归太过(太多特征被稀疏为0),而岭回归也正则化的不够(回归系数衰减太慢)的时候**，可以考虑使用ElasticNet回归来综合，得到比较好的结果。



# <span id="head11"> 线性回归的假设</span>

零均值假定、
随机项独立同方差假定、
解释变量的非随机性假定、
解释变量之间不存在线性相关关系假定、
随机误差项服从均值为0方差为σ的正态分布假定。

 **假设1：X 与 Y 有线性关系（多项式关系）**
**假设2：自变量x服从正态分布。**
这条假设的目的是为了让对整个回归分析模型的F检验和对某个参数的t检验有效，但是，这是一条非常强的假设。很多来自实际的数据并不满足这条假设，换言之，即便是自变量x不服从正态分布，只要残差服从正态分布，也可以保障F检验和t检验有效。

**假设3：解释变量X是确定性变量，随着样本量的增加，x的方差趋近于一个有界常数。**
这一假设的前半部分是由回归分析的本质所决定的，回归分析就是基于自变量x的变化来解释因变量y的变化的。如果所有自变量x都处于同一水平值上，无论因变量如何变化，也无法估计出自变量的系数。这一假设的后半部分旨在排除时间序列数据中持续上升或下降的变量作为自变量，这类数据不仅使得统计推断无效，还会产生其他问题。

**假设4：随机误差项μ服从零均值、同方差、零协方差的正态分布。**

这一假设的目的在于保证统计推断的可靠性。当样本量不大时，这条假设必须得到重视；当样本量非常大时，可以放松这条假设，因为由中心极限定理可知，当样本量足够大时，随机误差项的分布接近正态分布。

**假设5：μ与x不相关。**

这一假设表明：随机误差项μ和自变量x不具有任何形式的相关性。这一假设意味着，随机误差项的方差不随着自变量的变化而变化。如果这一假设成立，自变量x就被称为外生解释变量；如果这一假设不成立，自变量x就被称为内生解释变量。
如果回归分析模型中的自变量就是因变量的最主要影响因素，而且自变量不受到模型之外的其他因素影响，那么，自变量自然就与随机因素无关了。反之，如果回归分析模型中的自变量虽然是因变量的影响因素之一，但自变量受到了尚未包含在模型中的其他因素的影响，那么，自变量就会与随机因素有关了。


多元回归模型中包含了多个自变量，除了满足上述五条假设，还要满足下列假设：

**假设6：各个自变量之间不存在严格线性相关性。**

这一假设是多元线性回归分析所特有的，这一假设可以写成：n×（k+1）矩阵X的秩为（k+1），即矩阵X满秩，如果矩阵X并非满秩，就无法估计出（k+1）个需要估计的参数，包括k个自变量的系数，1个常数项。





# <span id="head12"> 代码</span>

## <span id="head13"> sklearn</span>


```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.utils import shuffle

# 第一步：数据预处理
diabetes = load_diabetes()
data = diabetes.data
target = diabetes.target 

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/4, random_state=0) 

#　第二步：训练集使用简单线性回归模型来训练
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor = regressor.fit(X_train, Y_train)

y_pred = regressor.predict(X_test)

plt.plot(y_pred)
plt.plot(Y_test)
plt.show()
```
![](https://upload-images.jianshu.io/upload_images/18339009-f15c4abfe606c0f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## <span id="head14"> numpy</span>


```
import numpy as np
from sklearn.utils import shuffle
from sklearn.datasets import load_diabetes

class lr_model():    
    def __init__(self):        
        pass
    # 初始化参数
    def initialize_params(self, dims):
        w = np.zeros((dims, 1))
        b = 0
        return w, b 
    # 生成数据
    def prepare_data(self):
        data = load_diabetes().data
        target = load_diabetes().target
        X, y = shuffle(data, target, random_state=42)
        X = X.astype(np.float32)
        y = y.reshape((-1, 1))
        data = np.concatenate((X, y), axis=1)        
        return data 
    # 生成交叉验证数据
    def linear_cross_validation(self, data, k, randomize=True):        
        if randomize:
            data = list(data)
            shuffle(data)

        slices = [data[i::k] for i in range(k)]        
        for i in range(k):
            validation = slices[i]
            train = [data  for s in slices if s is not validation for data in s]
            train = np.array(train)
            validation = np.array(validation)            
            yield train, validation
    #　求损失、参数偏导       
    def linear_loss(self, X, y, w, b):
        num_train = X.shape[0]
        num_feature = X.shape[1]
        # 模型公式
        y_hat = np.dot(X, w) + b  
        # 损失函数
        loss = np.sum((y_hat-y)**2) / num_train
        # 参数的偏导
        dw = np.dot(X.T, (y_hat - y)) / num_train
        db = np.sum((y_hat - y)) / num_train        
        return y_hat, loss, dw, db    
    # 训练更新参数
    def linear_train(self, X, y, learning_rate, epochs):
        w, b = self.initialize_params(X.shape[1])        
        for i in range(1, epochs):
            # 基于梯度下降的参数更新过程
            y_hat, loss, dw, db = self.linear_loss(X, y, w, b)
            # 基于梯度下降的参数更新过程
            w += -learning_rate * dw
            b += -learning_rate * db            
            if i % 10000 == 0:
                print('epoch %d loss %f' % (i, loss))
            
            params = {                
                'w': w,                
                'b': b
            }
            grads = {                
                'dw': dw,                
                'db': db
            }        
        return loss, params, grads    
    # 预测
    def predict(self, X, params):
        w = params['w']
        b = params['b']
        y_pred = np.dot(X, w) + b        
        return y_pred    
    
            
            
if __name__ == '__main__':
    lr = lr_model()
    # 生成数据，最后一列是标签
    data = lr.prepare_data()   
    # 五折交叉验证训练
    for train, validation in lr.linear_cross_validation(data, 5):
        X_train = train[:, :10]
        y_train = train[:, -1].reshape((-1, 1))
        X_valid = validation[:, :10]
        y_valid = validation[:, -1].reshape((-1, 1))

        loss5 = []
        # 训练
        loss, params, grads = lr.linear_train(X_train, y_train, 0.001, 100000)
        loss5.append(loss)
        l = np.mean(loss5)
        print('five kold cross train loss is', l)
        y_pred = lr.predict(X_valid, params)
        valid_l = np.sum(((y_pred - y_valid) ** 2)) / len(X_valid)
        print('valid loss is', valid_l)
```
epoch 50000 loss 4492.067734
epoch 60000 loss 4308.511724
epoch 70000 loss 4151.375892
epoch 80000 loss 4016.352800
epoch 90000 loss 3899.866551
five kold cross train loss is 3798.9563843996484
valid loss is 4214.092765494475








