- [L1正则化、 L2 正则化](#head1)
	- [ L1正则化](#head2)
	- [ L2 正则化](#head3)
向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性。你要把你的知识数学化告诉这个模型，对代价函数来说，就是加入对模型“长相”的惩罚

则化通过避免训练完美拟合数据样本的系数而有助于算法的泛化。为了防止过拟合， 增加训练样本是一个好的解决方案。此外， 还可使用**数据增强、 L1正则化、 L2 正则化、 Dropout、 DropConnect 和早停（Early stopping） 法**等。

# <span id="head1">L1正则化、 L2 正则化</span>
在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。 

可以用于防止过拟合的原因在于更小的权值表示神经网络的复杂度更低、网络参数越小，这说明模型相对简单，越简单的模型引起过度拟合的可能性越小。对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，即抗扰动能力强。

## <span id="head2"> L1正则化</span>
![L1 正则化时原始的损失函数后面加上一个 L1 正则化项， 即权值 w 绝对值的和除以 n](https://upload-images.jianshu.io/upload_images/18339009-3a8edd4d8c144f16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
## <span id="head3"> L2 正则化</span>
![L2 正则化就是在损失函数后面增加上 L2 正则化项，所有权值的平方和除以训练集中的样本大小](https://upload-images.jianshu.io/upload_images/18339009-24b804ecfe54eef4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

-  L1正则化向目标函数添加正则化项，以减少参数的绝对值总和；L1通常是**比L2更容易得到稀疏输出的**，会把一些不重要的特征直接置零**L1正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于0，因此它常用于特征选择**。同时一定程度上防止过拟合； 

- L2正则化中， 添加正则化项的目的在于**减少参数平方的总和**。机器学习中最常用的正则化方法是对权重施加L2范数约束。

- 线性回归中，使用**L1正则化的为Lasso回归**，使用**L2正则化的为Ridge回归(岭回归)**，既使用L1正则又使用L2正则的为 **ElasticNet**。

- L1相对于L2更能实现权值稀疏，L1是各元素绝对值之和，L2是各元素平方和的根，在对不同参数进行惩罚时，**L1无论参数大小如何，对它们的惩罚值都相同**，导致那些参数大小和惩罚值相等的参数，一减就变为 0，而L2对参数的惩罚值是根据参数本身的大小来变化的，**越小的参数惩罚值越小，越大的参数惩罚值越大**，所以最终使得所有参数都接近 0，但不会等于0。

![](https://upload-images.jianshu.io/upload_images/18339009-80f4594af295dd20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
上图代表的意思就是**目标函数-平方误差项**的等值线和**L1、L2范数等值线**（左边是L1），我们正则化后的代价函数需要**求解的目标就是在经验风险和模型复杂度之间**的平衡取舍，在图中形象地表示就是**黑色线与彩色线的交叉点**。

对于**L1范数，其图形为菱形，二维属性的等值线有4个角**（高维的会有更多），“突出来的角”更容易与平方误差项进行交叉，而这些“突出来的角”都是在坐标轴上，即W1或则W2为0；
而对于L2范数，**交叉点一般都是在某个象限中，很少有直接在坐标轴上交叉的**。

因此L1范数正则化项比L2的更容易得到稀疏解。
