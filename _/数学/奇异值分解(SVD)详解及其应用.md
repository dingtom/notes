- [1 特征值：](#head1)
- [2 奇异值：](#head2)
参考：[https://blog.csdn.net/shenziheng1/article/details/52916278](https://blog.csdn.net/shenziheng1/article/details/52916278)
特征值分解和奇异值分解两者有着很紧密的关系，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。先谈谈特征值分解吧：
# <span id="head1">1 特征值：</span>
如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：
$Av=\lambda v$
>这时候$\lambda$就被称为特征向量$v$对应的特征值，
一个矩阵的一组特征向量是一组正交向量。







特征值分解是将一个矩阵分解成下面的形式：
$A=Q\sum Q^{-1}$
>其中Q是这个矩阵A的特征向量组成的矩阵，
Σ是一个对角阵，每一个对角线上的元素就是一个特征值。首先，要明确的是，一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。比如:
![image.png](https://upload-images.jianshu.io/upload_images/18339009-032265613242434c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换（每一个对角线上的元素将会对一个维度进行拉伸变换，当值>1时拉长，当值<1时缩短）
反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）
我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：提取这个矩阵最重要的特征。总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。
# <span id="head2">2 奇异值：</span>
下面重点谈谈奇异值分解。特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，我们怎样才能描述这样普通的矩阵呢的重要特征呢？奇异值分解可以用来干这个事情，奇异值分解是一个能适用于任意的矩阵的一种分解的方法：
$  A=U\sum V^{\top}$
 >假设A是一个N * M的矩阵，那么得到的U是一个m * m的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个m *n的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），$V^{\top}$是一个n * n的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）

那么奇异值和特征值是怎么对应起来的呢？首先，我们将矩阵$A$乘 $A^{\top}$，将会得到一个方阵，我们用这个方阵求特征值可以得到：
$(A^{\top}A)v_i=\lambda_i v_i$
这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：
$\sigma_i=\sqrt{\lambda_i}$
$u_i=\frac{1}{\sigma_i}Av_i$
这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：
$A_{m \times n} \approx U_{m \times r} \Sigma_{r \times r} V_{r \times n}^{T}$
