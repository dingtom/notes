- [ Self-attention在图像](#head1)
	- [ 二维位置嵌入](#head2)
	- [ 注意力增强卷积](#head3)
![](https://upload-images.jianshu.io/upload_images/18339009-5f4cef03231a00b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
对于每个空间位置（h，w），通过queries和keys计算图像上的$N_h$个attention map。这些attention map用于计算values V的$N_h个加权平均值。然后将结果连接、reshape以匹配原始体积的空间维度，并与逐点卷积混合。将多头注意与标准卷积运算并行应用，并串联输出。




# <span id="head1"> Self-attention在图像</span>
给定一个输入的形状张量$ (H, W, F_{in})$ （为了简单起见，我们省略了批处理维度）我们把它平成一个矩阵$X \in \mathbb{R}^{H W \times F_{i n}}$ ，并按照Transformer架构中提出的方法进行多头注意，self-attention机制的输出为一个单一的头h，可以表述为:
$$O_{h}=\operatorname{Softmax}\left(\frac{\left(X W_{q}\right)\left(X W_{k}\right)^{T}}{\sqrt{d_{k}^{h}}}\right)\left(X W_{v}\right)$$


$ W_q, W_k \in \mathbb{R}^{F_{in}×d^{h}_{k}}$ ,$ W_v \in \mathbb{R}^{F_{in}×d^{h}_{v}}$

是学习到的线性变换，将输入X 映射 $queries \ Q =
XW_q$, $keys \  K = XW_k$ and $values \  V = XW_v$. 
然后，所有头的输出将再次连接和投影如下:


$ \operatorname{MHA}(X)=\text { Concat }\left[O_{1}, \ldots, O_{N_h}\right] W^{O}$


$ W^O \in \mathbb{R}^{d{v}×d{v}}$ 是学习到的线性变换。然后将$MHA(X)$重塑为形状的张量$(H，W，d_v)$，以匹配原始空间尺寸。 我们注意到，多头注意力会导致$O((HW)^2d_k)$的复杂性和$O((HW)^2N_h)$的存储成本，因为它需要存储每个头的注意力图。


## <span id="head2"> 二维位置嵌入</span>
没有关于位置的明确信息，自我注意是permutation等变的：

$\operatorname{MHA}( \pi (X)) = \pi (MHA(X))$

对于像素位置的任何permutation π，使其对建模高度结构化的数据（例如图像）无效。 已经提出了用明确的空间信息来增强激活图的多个位置编码以缓解相关问题。 特别地，图像Transformer[32]将最初在原始Transformer[43]中引入的正弦波扩展到二维输入，而CoordConv [29]将位置通道连接到激活图。 但是，这些编码对我们进行图像分类和物体检测的实验没有帮助（请参见第4.5节）。 我们假设这是因为这样的位置编码虽然不是permutation等变的，但不满足translation等变，这是处理图像时的理想特性。 作为解决方案，~~我们建议将相对位置编码[37]的使用扩展到二维，并提出一种基于Music Transformer [18]的内存高效实现。~~

**相对位置嵌入：** 在[37]中为了语言建模而引入的相对自我注意，通过相对位置嵌入来增强自我注意，并在防止permutation等变的同时实现translation等变。通过独立添加相对高度信息和相对宽度信息，实现了二维相对自注意。多少像素$ i = (i_x，i_y)$参与像素$ j = (j_x，j_y)$的注意logit计算如下:

$$l_{i, j}=\frac{q_{i}^{T}}{\sqrt{d_{k}^{h}}}\left(k_{j}+r_{j_{x}-i_{x}}^{W}+r_{j_{y}-i_{y}}^{H}\right)$$

 其中$q_i$是像素$i$（Q的第i行）的query矢量，$k_j$是像素$j$（K的第j行）的key矢量，$r^W_{ j_x-i_x}$和$r^H_{ j_y-i_y}$分别是针对相对宽度$j_x - i_x$和相对高度$j_y-i_y$学习到的嵌入。 现在，头$h$的输出变为：

$O_{h}=\operatorname{Softmax} (\frac{Q K^{T}+S_{H}^{r e l}+S_{W}^{r e l}}{\sqrt{d_{k}^{h}}}) V$

 其中$S^{rel}_H，S^{rel}_W \in R^{HW×HW}$是沿着高度和宽度维度的相对位置对数矩阵，满足$S ^{rel}_H [i，j] = q^T_i r^H_{j_y-i_y}$和$S ^{rel}_W [i，j] = q^T_i r^W_{j_x-i_x}$

[37]中的相对注意力算法将所有相对嵌入$r_{ij}$显式存储在一个张量形状$(HW，HW，d^h_k)$中，因此产生了额外的存储成本$O((HW)^2d^h_k)$。 这与$O((HW)^2N_h)$相比，它不使用位置编码的位置未知版本自我注意。 由于我们通常具有$Nh <d^h_k$，因此这种实现可能会被证明极其禁止，并限制了可用于小批量处理的图像数量。 取而代之的是，~~我们将[18]中提出的高效存储的相对masked注意力算法扩展到二维输入上的未masked相对自我注意力~~。 我们的实现的内存成本为O（HW dh k）。 我们将算法的Tensorflow代码保留在附录中。
 相对位置嵌入物$r^H$和$r^W$被学习并跨heads共享，而不是跨layers共享。 对于每一层，我们沿高度和宽度的相对距离给模型添加$2(H + W)− 2)d^h_k$参数。


## <span id="head3"> 注意力增强卷积</span>
先前在图像上提出的多种注意力机制[17、16、31、46]表明，卷积算子受到其局部性和对全局上下文理解的限制。 这些方法通过重新校准卷积特征图来捕获远程依赖性。 特别是，, Squeeze-and-Excitation（SE）[17]和GatherExcite（GE）[16]执行通道式reweighing，而BAM [31]和CBAM [46]独立地重新reweigh通道和空间位置。 与这些方法相比，我们1.使用一种可以共同参与空间和特征子空间（每个头部对应一个特征子空间）的注意力机制，以及2.引入其他特征图而不是对其进行完善。 图2总结了我们提出的增强卷积。


连接卷积和注意力特征图：形式上，考虑具有内核大小k，Fin输入滤波器和Fout输出滤波器的原始卷积算子。 相应的注意力增强卷积可以写成

我们表示υ= dv Fout注意通道与原始输出滤波器数量的比率，而κ= dk Fout键深度与原始输出滤波器数量的比率。 与卷积类似，拟议的注意力增强卷积1）与平移等价，并且2）可以轻松地在不同空间维度的输入上进行操作。 我们在附录A.3中包含了Tensorflow代码，以用于拟议的注意力增强卷积。


 对参数数量的影响：多头注意力引入了带有Fin输入滤波器和（2dk + dv）= Fout（2κ+υ）输出滤波器的1x1卷积来计算查询，键和值，以及带有dv =Foutυ输入和输出的附加1x1卷积 过滤器来混合不同负责人的贡献。 考虑到卷积部分中滤波器的减少，这导致参数发生以下变化：

为了简化起见，我们忽略了相对位置嵌入引入的参数，因为这些参数可以忽略不计。 实际上，这在替换3x3卷积时会导致参数略有减少，而在替换1x1卷积时会导致参数略有增加。 有趣的是，我们在实验中发现，注意力增强网络在使用较少参数的情况下仍明显优于其完全卷积的网络。



注意增强卷积体系结构：
在我们所有的实验中，增强卷积之后是批处理归一化[20]层，该层可以学习缩放卷积特征图和注意特征图的贡献。 与其他视觉注意机制[17、16、31、46]类似，我们对每个残差块应用一次增强卷积，并在内存允许的情况下沿整个体系结构应用（更多详细信息，请参见第4节）。 由于内存成本O（（Nh（HW）2）对于较大的空间尺寸可能是禁止的，因此从最后一层（具有最小的空间尺寸）开始，直到达到内存限制为止，我们都应注意增加卷积。 网络中，我们通常会采用较小的批处理大小，有时还会对输入进行降采样，以在应用空间的最大空间维度中进行自我关注。 通过使用步长为2的3x3平均池进行下采样，同时通过双线性插值获得后续的上采样（级联所需）
