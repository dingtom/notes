- [ 改进点](#head1)
- [3D 卷积](#head2)
- [1×1 卷积](#head3)
- [ 两个3\*3代替一个5\*5](#head4)
- [ 　转置卷积（去卷积、棋盘效应）](#head5)
	- [ 转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。](#head6)
- [ 多尺度卷积](#head7)
- [ 扩张卷积](#head8)
- [ 可分卷积（空间可分卷积，深度可分卷积）](#head9)
- [ 平展卷积](#head10)
- [ 分组卷积](#head11)
- [ 　混分组卷积](#head12)
- [ 　SE](#head13)
- [Dilated Convolution](#head14)
- [ Resnet](#head15)
- [ Pytorch](#head16)
[TOC]

卷积主要有三⼤特点：

- 局部连接。⽐起全连接，局部连接会⼤⼤减少⽹络的参数。在⼆维图像中，局部像素的关联性很强，
  设计局部连接保证了卷积⽹络对图像局部特征的强响应能⼒。
  
- 权值共享。参数共享也能减少整体参数量，增强了⽹络训练的效率。⼀个卷积核的参数权重被整张图
  ⽚共享，不会因为图像内位置的不同⽽改变卷积核内的参数权重。
  
- 下采样。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特
  征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。
  
  $ n_{output}=\lfloor \frac{n_{\text {input}}-F+2P}{S}+1\rfloor
  $

# <span id="head1"> 改进点</span>

![](https://upload-images.jianshu.io/upload_images/18339009-acae173c2caeea35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head2">3D 卷积</span>

实际上是对一个 3D 体积执行卷积。但通常而言，我们仍在深度学习中称之为 2D 卷积。这是在 3D 体积数据上的 2D 卷积。**过滤器深度与输入层深度一样。这个 3D 过滤器仅沿两个方向移动（图像的高和宽）。**这种操作的输出是一张 2D 图像（仅有一个通道）。

3D 卷积确实存在。这是 2D 卷积的泛化。**其过滤器深度小于输入层深度（核大小<通道大小）。因此，3D 过滤器可以在所有三个方向（图像的高度、宽度、通道）上移动。**在每个位置，逐元素的乘法和加法都会提供一个数值。因为过滤器是滑过一个 3D 空间，所以输出数值也按 3D 空间排布。也就是说输出是一个 3D 数据。

3D 卷积可以描述 3D 空间中目标的空间关系。对某些应用（比如生物医学影像中的 3D 分割/重构）而言，这样的 3D 关系很重要，比如在 CT 和 MRI 中，血管之类的目标会在 3D 空间中蜿蜒曲折。

# <span id="head3">1×1 卷积</span>

⾸发于NIN（Network in Network），后续也在GoogLeNet和ResNet等⽹络中使⽤。感兴趣的朋友
可追踪这些论⽂研读细节。

- 跨通道交流信息
- 降维、升维
- 减少参数量
- 1×1 卷积+激活函数 增加⾮线性，提升⽹络表达能⼒。

![](https://upload-images.jianshu.io/upload_images/18339009-cc270f72d13957ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-95f1db364ebe56bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head4"> 两个3\*3代替一个5\*5</span>

两个3\*3卷积核和⼀个5\*5卷积核的感受野相同，但是**减少了参数量和计算量**，加快了模型训练。与此同时由于卷积核的增加，**模型的⾮线性表达能⼒⼤⼤增强**。

参数量：（3\*3+1）*2；5\*5+1

过⼤卷积核也有使⽤的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应⽤

# <span id="head5"> 　转置卷积（去卷积、棋盘效应）</span>

**上采样生成高分辨率图像、将低维特征图映射到高维空间**

###### <span id="head6"> 转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。</span>

转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。

我们一直都可以使用直接的卷积实现转置卷积。对于下图的例子，我们在一个 2×2 的输入（周围加了 2×2 的单位步长的零填充）上应用一个 3×3 核的转置卷积。上采样输出的大小是 4×4。

# <span id="head7"> 多尺度卷积</span>

![](https://upload-images.jianshu.io/upload_images/18339009-9386a104067200fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head8"> 扩张卷积</span>

# <span id="head9"> 可分卷积（空间可分卷积，深度可分卷积）</span>

# <span id="head10"> 平展卷积</span>

# <span id="head11"> 分组卷积</span>

特征图局部链接
![传统](https://upload-images.jianshu.io/upload_images/18339009-f180d01ed3f2ddf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
通道局部链接
![参数量减少](https://upload-images.jianshu.io/upload_images/18339009-d02525a2f6f26516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![Alexnet](https://upload-images.jianshu.io/upload_images/18339009-ee939e0c39d7e299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head12"> 　混分组卷积</span>
![](https://upload-images.jianshu.io/upload_images/18339009-81cba71c6faa2e9c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head13"> 　SE</span>
![](https://upload-images.jianshu.io/upload_images/18339009-566314bb6c0d6470.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head14">Dilated Convolution</span>

图像分割领域，图像输入到CN(典型的网络比如FCN)中有两个关键
一个是 pooling减小图像尺寸增大感受野，
另一个是 upsampling扩大图像尺寸。

在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作不通过 Pooling也能有较大的感受野看到更多的信息呢？
![](https://upload-images.jianshu.io/upload_images/18339009-829e5cdf94e031be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head15"> Resnet</span>

![](https://upload-images.jianshu.io/upload_images/18339009-cdeb9f1d81bd6470.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head16"> Pytorch</span>

```
from __future__ import print_function 
from __future__ import division

import torch.nn as nn
from torchvision import datasets, models, transforms
from torch.autograd import Variable
from PIL import Image


def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
        # Initialize these variables which will be set in this if statement. Each of these
        #   variables is model specific.
        model_ft = None
        input_size = 0

        if model_name == "resnet":
            """ Resnet18
            """
            model_ft = models.resnet18(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            num_ftrs = model_ft.fc.in_features
            model_ft.fc = nn.Linear(num_ftrs, num_classes)
            input_size = 224

        elif model_name == "alexnet":
            """ Alexnet
            """
            model_ft = models.alexnet(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            num_ftrs = model_ft.classifier[6].in_features
            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
            input_size = 224

        elif model_name == "vgg":
            """ VGG11_bn
            """
            model_ft = models.vgg11_bn(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            num_ftrs = model_ft.classifier[6].in_features
            model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
            input_size = 224

        elif model_name == "squeezenet":
            """ Squeezenet
            """
            model_ft = models.squeezenet1_0(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))
            model_ft.num_classes = num_classes
            input_size = 224

        elif model_name == "densenet":
            """ Densenet
            """
            model_ft = models.densenet121(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            num_ftrs = model_ft.classifier.in_features
            model_ft.classifier = nn.Linear(num_ftrs, num_classes) 
            input_size = 224

        elif model_name == "inception":
            """ Inception v3 
            Be careful, expects (299,299) sized images and has auxiliary output
            """
            model_ft = models.inception_v3(pretrained=use_pretrained)
            set_parameter_requires_grad(model_ft, feature_extract)
            # Handle the auxilary net
            num_ftrs = model_ft.AuxLogits.fc.in_features
            model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)
            # Handle the primary net
            num_ftrs = model_ft.fc.in_features
            model_ft.fc = nn.Linear(num_ftrs,num_classes)
            input_size = 299

        else:
            print("Invalid model name, exiting...")
            exit()
        
        return model_ft

def extract_feature(model, imgpath):
    model.eval()        # 必须要有，不然会影响特征提取结果
    
    img=Image.open(imgpath)     # 读取图片
    img=img.resize((128, 128))
    to_tensor=transforms.ToTensor()   # 将图片转化成tensor
    tensor=to_tensor(img).cuda()    # 如果只是在cpu上跑的话要将这行去掉
    print(tensor.view(1, 3, 128, 128).shape)
    result=model(Variable(tensor.view(1, 3, 128, 128).repeat(64)))
    result_npy=result.data.cpu().numpy()    # 保存的时候一定要记得转成cpu形式的，不然可能会出错
    
    return result_npy  

if __name__=="__main__":
    # Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]
    model = initialize_model(model_name="vgg", num_classes=2, feature_extract=True, use_pretrained=True)
    imgpath = '/home/zut_csi/tomding/1.jpg'
    tmp = extract_feature(model, imgpath)
    print(tmp.shape)
    print(tmp)     
```
