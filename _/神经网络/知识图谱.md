- [Deep Learning in Knowledge Graph](#head1)
	- [ 1.概念和典型的知识图](#head2)
		- [1.2经典的知识图谱 ](#head3)
	- [2.三大类基于深度学习的知识图谱技术 ](#head4)
		- [ 2.1知识表示学习](#head5)
		- [ 2.2神经关系提取](#head6)
			- [ 2.2.1语句级NRE](#head7)
				- [ 1.输入编码器](#head8)
				- [ 2.句子编码器](#head9)
				- [ 3.关系分类器](#head10)
		- [ 2.2.2文档级NRE](#head11)
			- [ 2.2.2.1文档编译器](#head12)
			- [ 2.2.2.2关系分类器](#head13)
		- [ 2.3基于深度学习的实体链接技术](#head14)
			- [ 2.3.1实体链接框架](#head15)
			- [ 2.3.2实体链接的深度学习](#head16)
				- [ 2.3.2.1通过神经网络表示异构实例](#head17)
				- [ 2.3.2.2对上下文实例之间语义交互的建模](#head18)
				- [ 2.3.2.3学习本地兼容性度量](#head19)
	- [ 3.总结](#head20)

# <span id="head1">Deep Learning in Knowledge Graph</span>
 the world is made up of entities instead of strings



## <span id="head2"> 1.概念和典型的知识图</span>
###1.1基本概念
知识图(KG)又称知识库，是将人类关于世界的知识以**结构化**的形式组织起来的一个重要数据集，其中**知识被表示为具体的实体及其之间的多关系**抽象概念。

**构建知识图**的方法主要有两种：
1.一在资源描述框架(RDF)中使用现有的语义web数据集，并借助于**手工**注释。
2.是利用**机器学习或深度学习**的方法，从互联网上大量的明文中自动提取知识。

由于这种结构良好的统一知识表示，KG可以为复杂的现实世界提供有效的结构化信息。因此，它开始在人工智能的许多应用中发挥重要作用，特别是在**自然语言处理和信息检索领域**，如网络搜索、问答、语音识别等，近年来受到学术界和工业界的广泛关注。

一个典型的知识图谱由两部分组成，**实体**（包括具体的实体和现实世界的抽象概念）和实体之间的**关系**.它将各种知识以**(e1, relation, e2)**的形式排列成大量的三重事实，其中e1表示头实体，e2表示尾实体。例如：(美国，总统，唐纳德·特朗普)。此外，需要注意的是，在现实世界中，相同头部实体和关系可能有几个不同的尾部实体**（既一对多、多对一、多对多）**。


### <span id="head3">1.2经典的知识图谱 </span>
1.Freebase：世界上最受欢迎的知识图表之一。它是一个大型的协作数据库，由主要由社区成员组成的数据组成。信息以结构化的形式列出，**就像简历一样**。
<table>
<td> 
<img src="./a.png" style="width:200px;height:300px;"> <br>
</td>  
</table>



2.DBpedia：这是一个众包社区项目，旨在从维基百科中提取结构化信息，并将这些信息发布到网络上。它可以随着信息的变化进行动态更新。

3.Wikidata：维基媒体基金会合作编辑的知识库。每个条目表示一个主题(或用于维护Wikipedia的管理页面)，并由一个惟一的编号标识。信息通过创建语句添加到项中。语句采用**键-值对**的形式，每个语句由一个属性(键)和一个与该属性链接的值组成。
<table>
<td> 
<img src="./b.png" style="width:200px;height:300px;"> <br>
</td>  
</table>

4.YAGO：是由马克斯·普朗克信息学研究所和巴黎电信技术大学开发的一个巨大的高质量知识库。人工评估证实了其准确率为95%，并对每个关系进行了**置信值标注**。YAGO结合了wordnet的干净分类和wikipedia分类系统的丰富内容，将实体分配给超过350,000个类。YAGO是一个同时锚定在时间和空间的本体，这意味着**它将一个时间维度和一个空间维度附加到它的许多事实和实体上。**

<table>
<td> 
<img src="./c.png" style="width:200px;height:300px;"> <br>
</td>  
</table>



## <span id="head4">2.三大类基于深度学习的知识图谱技术 </span>

### <span id="head5"> 2.1知识表示学习</span>
**知识表示学习技术将知识表示中的实体和关系嵌入到一个密集的、低维的、实值的语义空间中。**

**基于翻译的表示学习**方法是一种高效、有效的对关系事实进行KG编码的方法，具有实体和关系的低维表示，可以缓解数据稀疏性的问题，进一步应用于知识获取、融合和推理。

TransE 是一种典型的基于翻译的知识表示学习方法，它对实体和关系都学习低维向量，非常简单有效。TransE将关系三元组中的关系看作是头尾实体嵌入之间的**转换**，即当三元组(h, r, t)成立时，h + r $\approx$t。在知识图补全任务中取得了惊人的效果。



TransE存在两个问题：
1.如何**处理复杂关系**(1到n, n到1,n到n的关系模型)是知识表示学习的关键挑战之一。很多transeE的扩展都在关注这一挑战。
TransH和TransR 当**涉及到不同的关系时实体具有不同的表示**。TransH 模型将关系转换为超平面上的平移向量，并用法向量将实体嵌入到超平面中。TransR表示实体语义空间中的实体，当涉及到不同的关系时，使用关系特定的转换矩阵将其投影到不同的关系空间中。此外，研究人员提出了TransR的两个扩展，包括考虑投影矩阵中实体信息的TransD和通过稀疏矩阵考虑关系的异质性和不平衡性的TranSpace。此外，TransE还有很多其他的扩展，分别侧重于关系的不同特征，包括TransG 和 采用高斯嵌入对实体和关系进行建模的KG2E；ManifoldE在知识表示学习中采用了基于流形的嵌入原理;等等.



2.TransE还有一个**只考虑实体间直接关系**的问题。为了解决这个问题，Lin等人提出了基于路径的TransE，通过选择合理的关系路径并用低维向量表示，将TransE扩展到模型关系路径。几乎与此同时,还有其他研究人员成功使用**神经网络**考虑知识图谱关系路径。此外，基于知识图谱的QA也使用了关系路径学习。

**现有的知识表示学习方法大多只关注KG中的结构信息，而忽略了文本信息、类型信息、视觉信息等丰富的多源信息。这些跨模态信息可以为关系事实较少的实体提供实体补充知识，在学习知识表示时具有重要意义。**

对于文本信息，Wang等人和Zhon等人提出将实体和单词与实体名称、描述和Wikipedia锚对齐，共同嵌入到一个统一的语义空间中。此外，Xie等人建议使用CBOW（连续词袋）或CNN编码器学习基于实体描述的实体表示。
对于类型信息,Krompaßet等人将类型信息作为每一个关系头和尾的约束信息，来区分属于同一类型的实体。Xie等人不仅将类型信息视为类型约束，还利用分层类型结构通过投影矩阵的构建来增强TransR。
对于视觉信息，Xie等人提出了一种基于图像的知识表示学习方法，通过使用其对应的图形学习实体表示来考虑视觉信息。
我们在现实世界中学习各种各样的信息是很自然的事。多源信息，如纯文本、层次类型，甚至图像和视频，在建模复杂的世界和构建跨模式表示时非常重要。此外，其他类型的信息也可以编码成知识表示学习，以提高性能。


### <span id="head6"> 2.2神经关系提取</span>
**从文本中提取事实/关系的神经关系提取技术，可用于构建/完成KG**

近年来，随着深度学习技术的发展，为了丰富现有的知识体系，神经关系提取采用端到端神经网络对关系提取任务进行建模。神经关系提取的**框架包括:捕获输入句子语义的句子编码器，并将其表示为句子向量;根据句子向量生成提取关系的概率分布的关系提取器**。神经关系提取(NRE)主要包括语句级NRE和文档级NRE。
#### <span id="head7"> 2.2.1语句级NRE</span>
句子级NRE旨在**预测句子中实体(或名词)对之间的语义关系**。形式上，给定由m个单词组成的输入句子$x = (w_1,w_2，…w_m)$和对应的实体对$e_1,e_2$作为输入，语句级NRE想要通过神经网络得到关系$r (r \in \mathbb{R})$的条件概率$p(r |x, e1, e2)$，可以形式化为
$$
p\left(r | x, e_{1}, e_{2}\right)=p\left(r | x, e_{1}, e_{2}, \theta\right)
$$
$θ$是神经网络的参数,$r$是一组关系集$R$中的一个关系

句子级NRE基本形式包括三个部分:(a)一个**输入编码器**给出输入单词的表示,(b)一个**句子编码器**计算代表原始句子的一个向量或一个向量序列,(c)**关系分类器**计算所有关系的条件概率分布。


##### <span id="head8"> 1.输入编码器</span>
首先，句子级NRE系统将离散的源句子词投射到连续向量空间中，并获得源句的输入表示$$w = \{w_1; w_2; ···;w_m \}$$

**Word嵌入**学习单词的低维实值表示，可以反映单词之间的句法和语义关系。形式上，每个字$w_i$由嵌入矩阵$\mathbf{V} \in \mathbb{R}^{d^{a} \times|V|}$中的相应列向量编码，其中$\mathbf{V}$表示固定大小的词汇表

**位置嵌入**旨在针对句子中的两个对应实体指定单词的位置信息。形式上，每个字$w_i$分别相对于从字到两个目标实体的相对距离由两个位置向量编码。例如，在句子“New York is a city of United States”中，从城市到纽约的相对距离是3，美国是-2。

**词性标签嵌入**表示句子中目标词的词汇信息。由于单词嵌入是从大规模的通用语料库中获得的，因此它们包含的信息可能与特定句子中的含义不一致，有必要将每个单词与其语言信息对齐，形式上，每个字$w_i$由嵌入矩阵$\mathbf{V} P \in \mathbb{R}^{d^{p} \times\left|V^{p}\right|}$中的相应列向量编码，其中$d^p$是嵌入向量的维数$V^p$表示固定大小的词性标签词汇表。

**WordNet 上位词嵌入**旨在利用上位词的先验知识来促进关系提取。当在WordNet中给出每个词的上位词信息时，在不同但概念性的相似词之间建立链接更容易。形式上，每个词$w_i$由嵌入矩阵$\mathbf{V}^{h} \in \mathbb{R}^{d^{h} \times\left|V^{h}\right|}$中的相应列向量编码，其中$d^h$是嵌入向量的维度，$V^h$表示固定大小的上位词。


##### <span id="head9"> 2.句子编码器</span>
接下来，句子编码器将输入表示编码成单个向量或向量x的序列。

**卷积神经网络编码器**使用卷积神经网络（CNN）嵌入输入语句，该卷积神经网络通过**卷积层提取局部特征**并通过**最大池化操作组合所有局部特征**以获得固定大小输入句子的向量。形式上，如图所示

<table>
<td> 
<img src="./e.png" style="width:200px;height:300px;"> <br>
</td>  
</table>


卷积运算被定义为向量序列与卷积矩阵$W$和具有滑动窗口的偏置向量$b$之间的矩阵乘法。让我们将向量$q_i$定义为第i个窗口中输入表示序列的串联，我们有

$$
[\mathbf{x}]_{j}=\max _{i}\left[f\left(\mathbf{W} \mathbf{q}_{i}+\mathbf{b}\right)\right]_{j}
$$

其中$f$表示非线性函数，如sigmoid或tangent函数。此外，为了更好地捕获两个实体之间的结构信息，提出了**分段最大池化操作**而不是传统的最大化池化操作。分段最大池化操作返回输入句子的三个段中的最大值，这三个段被分成两个目标实体。


**循环神经网络编码器**使用具有**学习时间特征的能力**的循环神经网络（RNN）来嵌入输入语句。如图所示，
<table>
<td> 
<img src="./d.png" style="width:200px;height:300px;"> <br>
</td>  
</table>

每个单词表示向量逐步放入循环层。对于每个步骤$i$，网络采用单词的表示向量$w_i$和前一步骤i-1的输出$h_{i-1}$作为输入，然后我们有
$$
\mathbf{h}_{i}=f\left(\mathbf{w}_{t}, \mathbf{h}_{i-1}\right)
$$
其中$f$表示RNN单元内部的变换函数，可以是LSTM单元或GRU单元。另外，当预测句子中间的语义时，可以采用双向RNN网络来充分利用未来词的信息。接下来，RNN将来自前向和后向网络的信息组合为局部特征，并使用最大池化操作来提取全局特征，形成整个输入句子的表示。最大池化层可以表示为
$$
[\mathbf{x}]_{j}=\max _{i}\left[\mathbf{h}_{i}\right]_{j}
$$
除了最大池化之外，单词还可以将所有局部特征向量组合在一起。使用注意力机制来学习每一步的注意力。假设$H = [h_1，h_2，... ，h_m]$是由循环层产生的所有输出向量组成的矩阵，整个句子的特征向量$x$由每个步骤输出的加权和形成。
$$\alpha=\operatorname{softmax}\left(\mathbf{s}^{T} \tanh (\mathbf{H})\right)$$
$$\mathbf{x}=\mathbf{H} \alpha^{T}$$
其中$s$是可训练的查询向量，$s^T$表示其转置。



##### <span id="head10"> 3.关系分类器</span>
最后，当获得输入句子的表示x时，关系分类器通过softmax层计算条件概率$p（r | x,e1,e2）$如下
$$
p\left(r | x, e_{1}, e_{2}\right)=\operatorname{softmax}(\mathbf{M} \mathbf{x}+\mathbf{b})
$$
其中$M$表示关系矩阵，$b$是偏差向量

### <span id="head11"> 2.2.2文档级NRE</span>
尽管现有的神经模型在提取新的关系事实方面取得了巨大成功，但它总是受到**训练数据不足**的影响。为了解决这个问题，研究人员提出了**远程监督假说**，通过对齐KG和明文来自动生成训练数据。从大量文本中，对实体进行标注，通过聚类等方法抽取实体之间的关系字符串。这种方法可以抽取非常大量的关系对，但是抽取结果很难映射到知识库中。

远程监督假设的直觉是**所有包含两个实体的句子都会在知识图谱中表达他们的关系**。例如，（纽约，美国的城市）是知识图谱中的关系事实。远程监督假设将所有包含这两个实体的句子视为关系城市的有效实例。它提供了一种自然的方式来利用来自多个句子（文档级别）而不是单个句子（句子级别）的信息来判断两个实体之间是否存在关系。因此，文档级NRE**旨在使用所有涉及的句子来预测实体对之间的语义关系**。给定由n个句子$S =（x_1，x_2，...，x_n）$及其对应的实体对$e1$和$e2$组成的输入句集$S$作为输入，文档级NRE想要通过神经网络获得关系$r（r∈ \mathbb{R}）$的条件概率$p（r | S，e1，e2）$，可以形式化为


$$
p\left(r | S, e_{1}, e_{2}\right)=p\left(r | S, e_{1}, e_{2}, \theta\right)
$$




文档级NRE的基本形式由四个部分组成：（a）**类似于句子级NRE的输入编码器**（b）**类似于句子级NRE的句子编码器**（c）**计算代表所有相关句子的向量的文档编码器 **（d）**类似于句子级NRE的关系分类器，它将文档向量作为输入而不是句子向量。**
##### <span id="head12"> 2.2.2.1文档编译器</span>
将所有句子向量编码为单个向量$S$.我们将在下面介绍不同的文档编码器。

**随机编码器**它简单地假设每个句子可以表达两个目标实体之间的关系，并**随机选择**一个句子来表示文档。形式上，文档表示被定义为
$$
\mathbf{S}=\mathbf{x}_{i}(i=1,2, \dots, n)
$$
其中$x_i$表示$x_i$的句子表示，$i$是随机索引。

**最大编码器**实际上，如上所述，并非所有包含两个目标实体的句子都可以表达它们的关系。例如，句子“纽约市是合法移民到美国的首要门户”并不表达city_of的关系。因此，在（Zeng等人，2015）中，他们遵循至少一个假设，该假设**假设包含这两个目标实体的至少一个句子可以表达他们的关系，并选择具有最高概率的句子**来表示关系该文件。形式上，文档表示被定义为
$$
\mathbf{S}=\mathbf{x}_{i}\left(i=\operatorname{argmax}_{i} p\left(r | x_{i}, e_{1}, e_{2}\right)\right)
$$
##### <span id="head13"> 2.2.2.2关系分类器</span>
与句子级NRE类似，当获得文档表示S时，关系分类器通过softmax层计算条件概率$p（r | S，e1，e2）$如下
$$
p\left(r | S, e_{1}, e_{2}\right)=\operatorname{softmax}\left(\mathbf{M}^{\prime} \mathbf{S}+\mathbf{b}^{\prime}\right)
$$
$M^{\prime}$表示关系矩阵和$b^{\prime  }$是偏向量。

### <span id="head14"> 2.3基于深度学习的实体链接技术</span>
**将知识图与文本数据连接起来，可以方便地完成许多不同的任务。**
知识图包含有关世界实体，它们的属性以及不同实体之间的语义关系的丰富知识。使用文本数据桥接知识图可以促进许多不同的任务，例如信息提取，文本分类和问答。例如，如果我们知道“史蒂夫乔布斯是苹果公司的首席执行官”，那么理解“乔布斯离开苹果公司”是有帮助的。
目前，**利用文本数据桥接知识图的主要研究问题是实体链接（EL）**。
给定一组文档d中的名称$M = {m_1，m_2 ， ...，m_k}$和包含一组实体$E = {e_1，e_2，...，e_n}$的知识图KB，实体链接系统是一个函数$δ：M→E$，**它将名称提及映射到它们的指称实体（KB）**。
<table>
<td> 
<img src="./f.png" style="width:200px;height:300px;"> <br>
</td>  
</table>

其中EL系统将识别三个实体的参考实体，WWDC，Apple和Lion相应地是Apple Worldwide Developers Conference，Apple Inc.和MacOS X Lion。基于链接结果的实体，可以使用KB中有关这些实体的所有知识来理解文本，例如，我们可以将给定文档分类为IT类别，而不是基于知识“狮子是操作系统”的动物类别 ”。
实体链接的**主要挑战是名称歧义问题和名称变化问题**。
名称歧义问题与**名称可能指不同上下文中的不同实体的事实**有关。例如，苹果公司的名称可以指维基百科中的20多个实体，如水果苹果公司，IT公司苹果公司和苹果银行。名称变化问题意味着可以**以不同方式提及实体**，例如其全名，别名，首字母缩写词和拼写错误。例如，可以使用超过10个名称来提及IBM公司，例如IBM，International Business Machine及其昵称Big Blue。为了解决名称歧义问题和名称变异问题，已经提出了许多方法用于实体链接。

#### <span id="head15"> 2.3.1实体链接框架</span>
给定文档d和知识图KB，实体链接系统链接文档中的名称提及如下:

**名称提及识别**：在此步骤中，将标识文档中的所有名称提及实体链接。
例如，EL系统应该从上图的文档中识别出三个提及{WWDC，Apple，Lion}。
目前，大多数EL系统采用两种技术来完成这项任务。
一种是经典的**命名实体识别（NER）**技术，它可以识别文档中人物，位置和组织的名称，然后这些实体名称将用作实体链接的名称提及。NER技术的主要缺点是它只能识别有限类型的实体，而忽略了许多常用的实体，如音乐，电影和书籍。
名称提及检测的另一种技术是**基于字典的匹配**，它首先为知识图中的所有实体构建名称字典（例如，从维基百科Mihalcea和Csomai 2007中的锚文本中收集），然后文档中匹配的所有名称将 用作名称提及。基于字典的匹配的主要缺点是它可以匹配许多嘈杂的名称提及，例如，甚至停用词is 和 an在维基百科中用作实体名称。为了解决这个问题，已经提出了许多技术来过滤嘈杂的名称提及。



**候选实体选择**：在该步骤中，EL系统为在步骤1中检测到的每个名称提取选择候选实体。例如，系统可以将{Apple（fruit），Apple Inc.，Apple Bank}识别为名称Apple的可能参考。由于名称变化问题，大多数EL系统依赖于参考表来进行候选实体选择。具体来说，参考表使用（名称，实体）对记录名称的所有可能的指示对象，并且可以从维基百科锚文本，网络，或查询日志。

**本地兼容性计算**：给定文档d中的名称m及其候选引用实体$E = \{e_1，e_2，... , e_n\}$。EL系统的关键步骤是计算提及$m$和实体$e$之间的局部兼容性$sim（m，e）$，即估计提及$m$将与实体$e$链接的可能性。
根据本地兼容性分数，名称提及m将链接到具有最大兼容性得分的实体：
$$
e^{*}=\operatorname{argmax}_{e} \operatorname{sim}(m, e)
$$
例如，要在下面的句子中确定名称apple的指示实体：
          苹果树是蔷薇科的落叶树
我们需要计算它与实体Apple（水果）和Apple Inc.的兼容性，最后根据上下文词“树”，“蔷薇科”等将apple与Apple（水果）联系起来。目前，已经提出了许多用于局部兼容性计算的方法，基本思想是从提及的上下文和特定实体的描述（例如，实体的维基百科页面）中提取判别特征（例如，重要词，频繁共现实体，属性值），然后兼容性是 由他们共同的共同特征决定。

**全局推断**：长期以来，已经证明全局推断可以显着提高实体链接的性能。全局推断的基本假设是**主题一致性假设，即文档中的所有实体应该在语义上与文档的主要主题相关**。基于这一假设，指称实体不仅应与其局部语境兼容，而且应与同一文件中的其他参照实体保持一致。



例如，如果我们知道名称中的指示实体Lion是上图中的Mac OSX（Lion），我们可以使用语义关系产品（Apple Inc.，Mac OSX（Lion）），轻松确定Apple的参照实体是Apple Inc. Mac OSX（Lion））。
这些例子强烈表明，通过联合而不是独立地解决同一文档中的问题的实体，可以提升实体连接的性能。形式上，给出所有提及$M = \{m_1，m_2 ,  . ..，m_k\}$在文档$d$中，全局推理算法旨在找到最大的指示实体，从而使全局一致性得分最大化：
$$
\left[e_{1}^{*}, \ldots, e_{k}^{*}\right]=\operatorname{argmax}\left(\sum_{i} \operatorname{sim}\left(m_{i}, e_{i}\right)+\text { Coherence }\left(e_{1}, e_{2}, \ldots, e_{k}\right)\right)
$$

近年来，已经提出了许多用于实体链接的全局推理算法，包括基于图的算法，基于主题模型的方法 和基于优化的算法。这些方法在如何建立文档一致性模型以及如何推断全局最优EL决策方面各不相同。例如，Han等人(2011)将一致性建模为所有指称实体之间语义关联的总和:
$$
\left(e_{1}, e_{2}, \ldots, e_{k}\right)=\sum_{(i, j)} \text { SemanticRelatedness }\left(e_{i}, e_{j}\right)
$$
然后通过图形随机游走算法获得全局最优决策。相比之下，Han和Sun提出了一个实体-主题模型，其中一致性被建模为从文档的主要主题生成的所有参照实体的概率，并且通过Gibbs采样算法获得全局最优决策。


#### <span id="head16"> 2.3.2实体链接的深度学习</span>

如上所述，EL的一个主要问题是**名称歧义问题**; 因此，关键的挑战是如何通过有效地使用上下文实例来计算名称提及与实体之间的兼容性。
已经观察到实体链接的性能很大程度上取决于本地兼容性模型。现有研究通常使用手工制作的特征来表示不同类型的上下文实例（例如，提及，背景和实体描述），并使用启发式相似性度量来衡量局部兼容性。
然而，这些基于特征工程的方法具有以下缺点：

•特征工程是**劳动密集型的，并且难以手动设计判别特征**。
例如，设计能够捕获猫和狗之间的语义相似性的特征是具有挑战性的。

•实体链接的**上下文实例通常是异构的，并且可能具有不同的粒度**。使用手工制作的特征，异构实例的建模和利用并不简单。到目前为止，许多不同类型的上下文实例已被用于实体链接，包括实体名称，实体类别，实体描述，实体流行度，实体之间的语义关系，提及名称，提及上下文，提及文档等。很难设计 可以将所有这些实例投射到相同的特征空间中的特征，或者将所有这些实例汇总到EL决策的统一框架中。

•最后，传统的实体链接方法通常以**启发式的方式定义提及与实体之间的兼容性**，这在发现和捕获实体链接决策的所有有用因素的能力较弱。

近年来已经采用了许多深度学习技术来实现实体链接。接下来，我们首先描述如何通过神经网络表示异构实例，然后介绍如何对不同类型的上下文实例之间的语义交互进行建模，最后，我们描述如何使用深度学习技术优化实体链接的本地兼容性度量



##### <span id="head17"> 2.3.2.1通过神经网络表示异构实例</span>
神经网络的一个主要优点是它可以从不同类型的原始输入（例如文本，图像和视频）自动学习良好的表示。
在实体链接中，已经利用神经网络来表示异构的上下文实例，例如提及名称，提及上下文和实体描述。
通过对连续向量空间中适合于实体链接的所有上下文实例进行编码，神经网络避免了设计手工制作的特征的需要。
在下文中，我们将介绍如何详细表示不同类型的上下文实例。

**名称提及表示**
$m = [m_1，m_2，...]$通常由一到三个单词组成，例如Apple Inc.，President Obama。以前的方法大多代表一个提及它所包含的单词的嵌入的平均值
$$
\mathbf{v}_{m}=\operatorname{average}\left(\mathbf{e}_{m_{1}}, \mathbf{e}_{m_{2}}, \ldots\right)
$$
其中$e_{m_i}$是单词$m_i$的嵌入，可以使用CBOW或Skip-Gram模型学习。
上述嵌入平均表示未考虑到一个词的重要性和位置。
为了解决这个问题，一些方法采用卷积神经网络（CNN）来表示提及，它提供了更灵活的表示名称提及的能力。

**局部上下文表示**:提及的局部上下文为实体链接决策提供了关键信息。
例如，“苹果树是蔷薇科中的落叶树”中的上下文词{tree, deciduous, rose family}提供了关联苹果名称的关键信息。
Sun等人提出使用CNN表示本地上下文，其中上下文的表示由其包含的单词组成，通过考虑单词的语义及其相对位置。
下图演示了如何使用CNN表示本地上下文。



<table>
<td> 
<img src="./g.png" style="width:100px;height:200px;"> <br>
</td>  
</table>


形式上，给定上下文$c = [w_1，w_2 ,...，w_{ | c |}]$中的单词，我们将每个单词$w$表示为$x = [e_w，e_p]$，其中$e_w$是单词 $w$的嵌入，$e_p$是单词$w$的位置嵌入，$d_w$和$d_p$是单词向量和位置向量的维度 。一个单词$w_i$的位置是它在局部上下文中与提及词的距离。
为了表示上下文$c$，我们首先将它的单词的所有向量连接起来,如下：
$$\mathbf{X}=\left[\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{|c|}\right]$$
然后将卷积运算应用于$X$，并且卷积层的输出为
$$
\mathbf{Z}=\left[\mathbf{M}_{\mathbf{g}} \mathbf{X}_{[1, K+1]}, \mathbf{M}_{\mathbf{g}} \mathbf{X}_{[2, K+2]}, \ldots, \mathbf{M}_{\mathrm{g}} \mathbf{X}_{[|c|-K,|c|]}\right]$$
其中$M_g∈ \mathbb{R}^{n1×n2}$是线性变换矩阵，$K$是卷积层的上下文大小。
由于局部上下文长度各异，并且为了确定特征向量的每个维度中最有用的特征，我们对卷积层的输出执行最大池化操作（或其他池化操作）如下
$$m_{i}=\max \quad \mathbf{Z}(i, .) \quad 0 \leq i \leq|c|$$
最后，我们使用向量$\mathbf{m}_c = [m_1，m_2 ,. ..]$代表提到的$m$的局部上下文$c$。


**文档表示**:如先前的研究中所述，文档和名称提及的局部上下文提供了用于实体链接的不同粒度的信息。例如，文档通常捕获比本地上下文更大的主题信息。基于这种观察，大多数实体链接系统将文档和本地上下文视为两种不同的实例，并单独学习它们的表示。目前，已经利用两种类型的神经网络用于实体链接中的文档表示。
第一个是卷积神经网络，它与我们在局部上下文表示中引入的相同。
第二个是去噪自动编码器（DA），其旨在学习可以在原始文档中保留最大信息的紧凑文档表示。
具体地，首先将文档表示为二进制词袋向量$\mathbf{x}_d$，其中$\mathbf{x}$的每个维度表示是否出现词$\mathbf{x}_i$。
给定文档表示$\mathbf{x}$，去噪自动编码器试图学习一个模型，该模型可以在给定随机损坏的$\mathbf{x}^{\prime}$的情况下重构$\mathbf{x}$
通过以下过程：（1）通过将掩蔽噪声（随机掩蔽1或0）应用于原始$\mathbf{x}$来随机破坏$\mathbf{x}$; （2）通过编码过程将$\mathbf{x}$编码为紧凑表示$h(\mathbf{x})$; （3）通过解码过程$g(h(\mathbf{x}))$从$h(\mathbf{x})$重构$\mathbf{x}$。
DA的学习目标是最小化重建误差$L(x，g(h(\mathbf{x}))$。
下图演示了DA的编码和解码过程。

<table>
<td> 
<img src="./h.png" style="width:200px;height:300px;"> <br>
</td>  
</table>

DA进行文档表示具有几个优点。
首先，自动编码器试图学习文档的紧凑表示，因此可以将相似的单词分组成簇。
其次，通过随机破坏原始输入，DA能捕获主题并忽略无意义的单词，例如功能词是，和，等等。第三，自动编码器可以重复堆叠在先前学习的$h(\mathbf{x})$上; 因此，DA可以学习多级别的文档表示。

**实体知识表示**:目前，大多数实体链接系统使用维基百科（或源自维基百科的知识库，如Yago，DBPedia等）作为其目标知识库。维基百科包含有关实体的丰富知识，例如标题，描述，包含其重要属性的信息框， 语义类别，有时是与其他实体的关系。
如图显示了维基百科中包含的Apple Inc.的知识。
<table>
<td> 
<img src="./i.png" style="width:200px;height:300px;"> <br>
</td>  
</table>



在下文中，我们描述了如何使用神经网络来表示来自实体知识的实例。

•实体标题表示:与名称一样，实体标题通常由一到三个单词组成; 因此，大多数实体链接系统采用与名称提及表示相同的神经网络来表示实体标题，即字嵌入或CNN的平均值。

•实体描述:目前，大多数实体链接系统将实体描述建模为普通文档，并且通过CNN或DA了解其与文档表示相同的表示。
从上面的介绍中，深度学习技术提出了一系列神经网络，用于表示从词嵌入，去噪自动编码器到卷积神经网络的上下文实例。
这些神经网络可以有效地学习上下文实例的表示，而无需手工制作的特征。
近年来，许多其他类型的实例也被用于实体链接。

例如，实体流行度告诉实体出现在文档中的可能性，语义关系捕获不同实体之间的语义关联/关系（(e.g., CEO-of(Steve Jobs, Apple Inc.) and Employeeof(Michael I. Jordan, UC Berkeley)),为实体提供关键概括信息的类别entity(e.g., apple ISA fruit, Steve Jobs is a Businessman, MichaelJeffery Jordan ISA NBA player).
使用神经网络表示这些上下文实例仍然不是直截了当的。对于未来的工作，使用神经网络来表示这些上下文实例仍然不是很直观。在今后的工作中，设计出能够有效表达这些上下文实例的其他神经网络是很有用的。

##### <span id="head18"> 2.3.2.2对上下文实例之间语义交互的建模</span>
如上所示，实体链接存在许多类型的上下文实例。为了做出准确的EL决策，EL系统需要考虑所有不同类型的上下文实例。此外，近年来，跨语言实体链接的任务使得必须比较不同语言的语境实例。例如，EL系统需要将中文名称“pingguo (Apple)fabu
(released) xin(new) iPhone”与维基百科中“Apple Inc.”的英文描述进行比较，以便进行中英文实体链接。为了考虑所有上下文实例，最近的研究采用神经网络来模拟不同上下文实例之间的语义相互作用。
通常，使用两种策略来对不同上下文实例之间的语义交互进行建模：




•第一种是通过神经网络将不同类型的上下文实例映射到相同的连续特征空间，然后可以使用它们的表示之间的相似性（主要是余弦相似性）来捕获上下文实例之间的语义交互。

•第二个是学习一种新的代表，它可以汇总来自不同上下文实例的信息，然后根据新的表示做出实体链接决策。
在下文中，我们将描述如何在实体链接系统中使用这两种策略。
Francis-Landau等人提出学习卷积神经网络，将项目名称提及，提及的本地上下文，源文档，实体标题和实体描述投射到同一个连续特征空间; 然后，不同实例之间的语义交互被建模为它们的表示之间的相似性。
具体而言，给定CNN学习的连续向量表示，Francis-Landau等提出捕获提及与实体之间的语义交互如下

$$
\mathbf{f}(c, e)=\left[\cos \left(s_{d}, e_{n}\right), \cos \left(s_{c}, e_{n}\right), \cos \left(s_{m}, e_{n}\right), \cos \left(s_{d}, e_{d}\right), \cos \left(s_{c}, e_{d}\right), \cos \left(s_{m}, e_{d}\right)\right]
$$
其中$s_d，s_m和s_c$相应地是提及的文档，上下文和名称的学习到的向量，并且$e_n$和$e_d$相应地是实体名称和描述的学习到的向量。
最后，将上述语义相似性与链路计数等其他信号相结合，预测局部兼容性。

Sun等人提出为每次提及学习一种新的表示形式，该表示由提及的名称和基于其表示的本地上下文的实例组成。
具体来说，新的表示使用神经张量网络来组成提及向量$(v_m)$和上下文向量$(v_c)$：



$$
\mathbf{v}_{m c}=\left[\mathbf{v}_{m}, \mathbf{v}_{c}\right]^{T}\left[M_{i}^{a p p r}\right]^{[1, L]}\left[\mathbf{v}_{m}, \mathbf{v}_{c}\right]
$$

这样，不同上下文实例之间的语义交互被总结为新的特征向量$v_{mc}$。
Sun等人还通过组合其实体名称表示和实体类别表示来学习每个实体的新表示。最后，提及与实体之间的局部兼容性计算为其新表示之间的余弦相似度。
在Tsai和Roth提出了一种用于跨语言实体链接的多语言嵌入方法。跨语言实体链接旨在将以非英语文档编写的广泛的提及与英语维基百科中的条目相对应。
Tsai和Roth将外语和英语中的词和实体名称投射到新的连续向量空间，然后可以有效地计算外语提及与英语维基百科条目之间的实体链接。
具体来说，给定对齐的英语和外语标题$\mathbf{A}_{e n} \in \mathbb{R}^{a \times k_{1}}$ 和 $\mathbf{A}_{f} \in \mathbb{R}^{a \times k_{2}}$的嵌入，其中$a$是对齐的标题号，$k1$和$k2$相应地是英语和外语的嵌入维度，Tsai和Roth 对这两个矩阵应用典范相关分析（CCA）：

$$
\left[\mathbf{P}_{e n}, \mathbf{P}_{f}\right]=\operatorname{CCA}\left(\mathbf{A}_{e n}, \mathbf{A}_{f}\right)
$$
然后，将英语嵌入和外语嵌入投射到新的特征空间中

$$
\begin{aligned} \mathbf{E}_{e n}^{\prime} &=\mathbf{E}_{e n} \mathbf{P}_{e n} \\ \mathbf{E}_{f}^{\prime} &=\mathbf{E}_{f} \mathbf{P}_{f} \end{aligned}
$$
$E_{en}$和$E_f$是英语和外语中所有单词的原始嵌入，$\mathbf{E}_{e n}^{\prime}$与$\mathbf{E}^{\prime}_f$是英语和外语中所有单词的新嵌入。



##### <span id="head19"> 2.3.2.3学习本地兼容性度量</span>
上下文实例表示学习和语义交互建模都依赖于大量参数以获得良好的性能。
深度学习技术提供端到端框架，可以使用反向传播算法和基于梯度的优化算法有效地优化所有参数。
在下图中，我们展示了一种常用的本地兼容性学习架构。
<table>
<td> 
<img src="./l.png" style="width:200px;height:300px;"> <br>
</td>  
</table>


我们可以看到提及的实例和实体的实例将首先使用上下文实例表示神经网络编码到连续特征空间中，然后将使用语义交互建模神经网络计算提及与实体之间的兼容性信号，最后，所有这些信号将是总结为本地兼容性得分。
为了学习上述神经网络以获得本地兼容性，我们需要从不同资源（例如，来自维基百科超链接）收集实体链接注释$(d，e，m)$。
然后，训练目标是最小化排名损失：
$$
L=\sum_{(m, e)} L(m, e)
$$
其中$L(m, e)=\max \left\{0,1-\operatorname{sim}(m, e)+\operatorname{sim}\left(m, e^{\prime}\right)\right\}$是每个训练实例$(m，e)$的成对排序标准，如果排名第1的实体$e^{\prime}$不是真正的参照实体$e$。
我们可以看到，在上述学习过程中，深度学习技术可以通过微调提及表示和实体表示来优化相似性度量，并学习不同兼容性信号的权重。通过这种方式，它通常可以实现比启发式设计的相似性度量更好的性能。

## <span id="head20"> 3.总结</span>
知识图是自然语言理解和常识推理的基础知识库，其中包含有关世界实体，其属性和实体之间语义关系的丰富知识。

在本章中，我们将介绍几个重要的知识图，包括DBPedia，Freebase，Wikidata，Yago和HowNet。
之后，我们介绍知识图的三个重要任务，并描述深度学习技术如何应用于这些问题：
第一个是**表示学习**，可用于嵌入实体、关系，到连续的特征空间; 
第二个是**神经关系抽取**，它展示了如何通过从网页和文本中提取知识来构建知识图; 第三个是**实体链接**，可用于将知识与文本联系起来。深度学习技术被用于知识图表示的实体和关系嵌入，并表示知识图构建的关系提取中的关系实例，并表示实体链接的异构实例。上述技术将为在不同的任务中理解，表示，构建并利用KGs提供坚实的基础，例如问答，文本理解和常识推理。

除了有利于KG的建设，知识表示学习为我们提供了一个令人兴奋的方法来应用KG。将来，重要的是探索如何更好地将KGs考虑到自然语言理解和生成的深度学习模型，并为自然语言处理开发知识渊博的神经模型。



