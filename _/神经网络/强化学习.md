- [ Q-learning](#head1)
- [ 用神经网络来逼近Q](#head2)
	- [ 兼顾探索和利用](#head3)
	- [ 经验回放](#head4)
- [ 用另一个$Q$网络来作为$L$的预估值](#head5)
![](https://upload-images.jianshu.io/upload_images/18339009-9ba160a5d5fac798.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-3e20c4ed415cbcb9.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
下一状态只与当前状态有关
![](https://upload-images.jianshu.io/upload_images/18339009-faa2a2e55f7bb684.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

$R$为之后的奖励和，我们的目标是使其最大化
![](https://upload-images.jianshu.io/upload_images/18339009-b813984a4e8626d1.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
# <span id="head1"> Q-learning</span>
![](https://upload-images.jianshu.io/upload_images/18339009-bd114e08937ace56.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
初始化$Q$值表
![](https://upload-images.jianshu.io/upload_images/18339009-8a9d94b47df06fce.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-9918ffd3e5865715.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-f6862f2520f57f57.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-6daea7dd36b9d34d.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-d79c0cdde1ae6294.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-206f7b037435ef4d.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# <span id="head2"> 用神经网络来逼近Q</span>
$q$计算量太大
![](https://upload-images.jianshu.io/upload_images/18339009-b6e9f71d53d2cde5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-aca2eee03097858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-1e70687444ae8358.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##### <span id="head3"> 兼顾探索和利用</span>
![](https://upload-images.jianshu.io/upload_images/18339009-777cd791399a637b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

##### <span id="head4"> 经验回放</span>
状态间高度相关
玩多轮游戏把片段储存，训练时从若干轮若干片段抽取作为训练集
![](https://upload-images.jianshu.io/upload_images/18339009-5a5912abd249f881.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-6540ba775587baac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


# <span id="head5"> 用另一个$Q$网络来作为$L$的预估值</span>
![](https://upload-images.jianshu.io/upload_images/18339009-8917e56dd4364ba9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

每轮训练后让把训练好的network$Q(s, a)$赋值给target network$Q\left(s^{\prime}, a^{\prime \prime}\right)$
训练时只训练预估的network
