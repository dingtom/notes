<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>欢迎加入 Hugo NexT 组织！</title><url>/post/external-link/</url><categories><category>示例</category></categories><tags><tag>Hugo</tag><tag>NexT</tag></tags><content type="html">欢迎来到 Hugo NexT 文档站点！ 它是从 Theme NexT 移植过来的为 Hugo 打造的高品质优雅主题，保持简单易用的特性和强大的功能。
用户指南 设置 NexT 主题很容易。只需遵循文档，就可快速创建您的个人网站！
反馈 访问 Awesome NexT 列表，与其他用户分享插件和教程。 加入我们的 Gitter 聊天。 在几秒钟内添加或改进翻译。 在 GitHub Issues 中报告一个 🐛。 在 GitHub 上申请一个新特性。 为最受欢迎的功能请求投票。</content></entry><entry><title>世界，你好！</title><url>/post/hello-world.html</url><categories><category>博客</category></categories><tags><tag>Hugo</tag><tag>开始</tag></tags><content type="html"> “使用 weight 关键字置顶文章。”
Hugo 是现今世界上最快的网站建设框架，也是最流行的开源静态站点生成器之一。 凭借其惊人的速度和灵活性，Hugo 让建设网站再次变得有趣起来。
快速开始 发表新文章 $ hugo new hello-world.md 更多信息： 内容格式 启动服务 $ hugo server 更多信息： Hugo 服务命令行 生成静态文件 $ hugo 更多信息： Hugo 建站 部署到服务器 $ hugo deploy 更多信息： Hugo 发布 祝你好运，相信你会喜欢上 Hugo 建站的旅程！</content></entry><entry><title>Id</title><url>/post/id/</url><categories/><tags/><content type="html"></content></entry><entry><title>cv-CNN</title><url>/post/cv-cnn/</url><categories><category>cv</category></categories><tags/><content type="html">
卷积层 卷积提取底层特征减少神经网络中参数个数
局部连接。⽐起全连接，局部连接会⼤⼤减少⽹络的参数。在⼆维图像中，局部像素的关联性很强， 设计局部连接保证了卷积⽹络对图像局部特征的强响应能⼒。+ 下采样。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特 征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。 权值共享。参数共享也能减少整体参数量，增强了⽹络训练的效率。⼀个卷积核的参数权重被整张图 ⽚共享，不会因为图像内位置的不同⽽改变卷积核内的参数权重。 为了避免尺寸的变化可以在当前层的矩阵的边界加入全０填充（zero-padding）．否则中间的像素会多次进入卷积野而边上的进入次数少 还可以通过设置过滤器移动步长调整结果矩阵的大小 池化层 下采样。降维、扩大感知野、减小计算量 实现非线性 实现不变性，（平移不变性、旋转不变性和尺度不变性）
卷积层输出矩阵大小 padding = “SAME”$n_{output}=[ n_{input} ]$
padding = “VALID”$n_{\text {output}}=\left[\frac{n_{\text {input}}- \text{kernel_size} +1}{s}\right]$
池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为 $ \frac{n- \text{kernel_size} }{s} +1 $
改进点 3D 卷积 实际上是对一个 3D 体积执行卷积。但通常而言，我们仍在深度学习中称之为 2D 卷积。这是在 3D 体积数据上的 2D 卷积。**过滤器深度与输入层深度一样。这个 3D 过滤器仅沿两个方向移动（图像的高和宽）。**这种操作的输出是一张 2D 图像（仅有一个通道）。
3D 卷积确实存在。这是 2D 卷积的泛化。**其过滤器深度小于输入层深度（核大小&amp;lt;通道大小）。因此，3D 过滤器可以在所有三个方向（图像的高度、宽度、通道）上移动。**在每个位置，逐元素的乘法和加法都会提供一个数值。因为过滤器是滑过一个 3D 空间，所以输出数值也按 3D 空间排布。也就是说输出是一个 3D 数据。
3D 卷积可以描述 3D 空间中目标的空间关系。对某些应用（比如生物医学影像中的 3D 分割/重构）而言，这样的 3D 关系很重要，比如在 CT 和 MRI 中，血管之类的目标会在 3D 空间中蜿蜒曲折。
1×1 卷积 ⾸发于NIN（Network in Network），后续也在GoogLeNet和ResNet等⽹络中使⽤。
跨通道交流信息 降维、升维 减少参数量 1×1 卷积+激活函数 增加⾮线性，提升⽹络表达能⼒。 两个3*3代替一个5*5 两个3*3卷积核和⼀个5*5卷积核的感受野相同，但是减少了参数量和计算量，加快了模型训练。与此同时由于卷积核的增加，模型的⾮线性表达能⼒⼤⼤增强。
参数量：（3*3+1）*2；5*5+1
过⼤卷积核也有使⽤的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应⽤
转置卷积（去卷积、棋盘效应） 上采样生成高分辨率图像、将低维特征图映射到高维空间
转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。
转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。
我们一直都可以使用直接的卷积实现转置卷积。对于下图的例子，我们在一个 2×2 的输入（周围加了 2×2 的单位步长的零填充）上应用一个 3×3 核的转置卷积。上采样输出的大小是 4×4。
多尺度卷积 深度可分卷积 深度可分离卷积将传统的卷积分两步进行，分别是depthwise和pointwise。
首先按照通道进行计算按位相乘的计算，深度可分离卷积中的卷积核都是单通道的，输出不能改变feature map的通道数，此时通道数不变；
然后将得到将第一步的结果，使用1*1的卷积核进行传统的卷积运算，此时通道数可以进行改变。
分组卷积 特征图局部链接 通道局部链接 混分组卷积 SE注意力机制 空洞卷积 扩大感受野。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征
获取多尺度上下文信息。当多个带有不同dilation rate的空洞卷积核叠加时，不同的感受野会带来多尺度信息，这对于分割任务是非常重要的。
可以降低计算量，不需要引入额外的参数，如上图空洞卷积示意图所示，实际卷积时只有带有红点的元素真正进行计算
图像分割领域，图像输入到CN(典型的网络比如FCN)中有两个关键
一个是 pooling减小图像尺寸增大感受野，另一个是 upsampling扩大图像尺寸。
在先减小再增大尺寸的过程中，肯定有一些信息损失掉了，那么能不能设计一种新的操作不通过 Pooling也能有较大的感受野看到更多的信息呢？ 与正常的卷积不同的是,空洞卷积引入了一个称为 “扩张率(dilation rate)”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。
在此以3*3卷积为例,展示普通卷积和空洞卷积之间的区别
从左到右分别为a、b、c子图,三幅图是相互独立进行卷积的(区别于下面图4),大框表示输入图像(感受野默认为1),黑色的圆点表示3*3的卷积核,灰色地带表示卷积后的感受野
a是普通的卷积过程(dilation rate = 1),卷积后的感受野为3 b是dilation rate = 2的空洞卷积,卷积后的感受野为5 c是dilation rate = 3的空洞卷积,卷积后的感受野为8 1个 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计可以大幅度的减少参数，有正则化的效果，参数少了就没那么容易发生过拟合。
转置卷积 **转置卷积（Transposed Convolution）**它和空洞卷积的思路正好相反，是为上采样而生，也应用于语义分割当中，而且他的计算也和空洞卷积正好相反，先对输入的feature map间隔补0，卷积核不变，然后使用标准的卷积进行计算，得到更大尺寸的feature map。转置卷积不是卷积的逆运算
首先回顾下普通卷积，以stride=1，padding=0，kernel_size=3为例，假设输入特征图大小是4x4的（假设输入输出都是单通道），通过卷积后得到的特征图大小为2x2。一般使用卷积的情况中，要么特征图变小（stride &amp;gt; 1），要么保持不变（stride = 1），当然也可以通过四周padding让特征图变大但没有意义。
转置卷积它只能恢复到原来的大小（shape）数值与原来不同。转置卷积的运算步骤可以归为以下几步：
在输入特征图元素间填充s-1行、列0（其中s表示转置卷积的步距）
在输入特征图四周填充k-p-1行、列0（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）
将卷积核参数上下、左右翻转
做正常卷积运算（填充0，步距1）
下面假设输入的特征图大小为2x2（假设输入输出都为单通道），通过转置卷积后得到4x4大小的特征图。这里使用的转置卷积核大小为k=3，stride=1，padding=0的情况（忽略偏执bias）。
首先在元素间填充s-1=0行、列0（等于0不用填充） 然后在特征图四周填充k-p-1=2行、列0 接着对卷积核参数进行上下、左右翻转 最后做正常卷积（填充0，步距1） $$H_{out} = \frac{H_{in}+2p-k}{s}+1$$
$$H_{in} = (H_{out}-1)\times s +k-2p$$
nn.ConvTranspose2d n_channels, out_channels, kernel_size, stride, padding、 output_padding：在计算得到的输出特征图的高、宽方向各填充几行或列0（注意，这里只是在上下以及左右的一侧one side填充，并不是两侧都填充，有兴趣自己做个实验看下），默认为0不使用。 groups：当使用到组卷积时才会用到的参数，默认为1即普通卷积。 bias：是否使用偏执bias，默认为True使用。 dilation：当使用到空洞卷积（膨胀卷积）时才会使用到的参数，默认为1即普通卷积。 $$H_{in} = (H_{out}-1)\times s +dilation \times (k-1)-2p +out_padding +1$$
Resnet Addition / Concatenate Addition和Concatenate分支操作统称为shortcut，
Addition是在ResNet中提出，两个相同维度的feature map相同位置点的值直接相加，得到新的相同维度feature map，这个操作可以融合之前的特征，增加信息的表达，
Concatenate操作是在Inception中首次使用，被DenseNet发扬光大，和addition不同的是，它只要求两个feature map的HW相同，通道数可以不同，然后两个feature map在通道上直接拼接，得到一个更大的feature map，它保留了一些原始的特征，增加了特征的数量，使得有效的信息流继续向后传递。</content></entry><entry><title>cv-CTPN</title><url>/post/cv-ctpn/</url><categories><category>cv</category></categories><tags/><content type="html">CTPN算法 文本通常都是从左往右写的（水平），并且字之间的宽度都大致相同 固定宽度，来检测文本高度即可，但是如何应对变长序列呢？ 本质上还是RPN方法(可参考faster&amp;ndash;rcnn),可将检测到的框拼在一起！
CTPN第一步和通用的目标检测网络一样，先用一个backbone，这里用的是VGG16来提取空间特征，取VGG的conv5层的输出，输出维度为B × W × H × C（批次batchsize×宽×高×通道数）。这里要注意因为是第五层卷积输出，所以下采样倍数为16，也就是输出的feature map中的每个特征点对应原图16个像素。
接下来我们会对feature map做一个编码，在feature map上使用3×3的滑窗来提取空间特征（也就是卷积），经过滑窗后得到的feature map大小依旧是B × W × H × C，但这里的每一个像素点都融合了周围3 × 3的信息（在论文中作者使用caffe中的im2col实现的滑窗操作，将B × W × H × C大小的feature map转换为B × W × H × 9C，本文遵从tf版本）
接着将feature map reshape成(NH) × W × C输入双向LSTM提取每一行的序列特征。最后双向LSTM输出(NH) × W × 256，然后重新reshape回N × 256 × H × W
将输出经过一个卷积层（图中的FC），变成N × H × W × 512
N × H × W × 512 最后会经过一个类似RPN的网络，分成三个预测支路：如上图所示，
其中一个分支输出N × H × W × 2k，这里的k指的是每个像素对应k个anchor，这里的2K指的是对某一个anchor的预测$v=[v_y,v_h]$(y坐标和到原点高度，宽度是固定的)；
第二个分支输出N × H × W × 2k，这里的2K指的是2K个前景背景得分，记做$s=[text,non−text]$。
最后一个分支输出N × H × W × k，这里是K个side-refinement，调整边界位置。
经过上面步骤，可以得到密密麻麻的text proposal，这里使用nms来过滤掉多余的文本框。
假如理想的话（文本水平），会将上述得到的一个文本小框使用文本线构造方法合成一个完整文本行，如果还有些倾斜，会做一个矫正的操作。
以上就是CTPN的架构，看不懂没关系，接下来我会可视化每个过程帮你了解。
torch.size([1,3,900,1600]) 3通道 VGG torch.size([1,512,56,100]) RPN(卷积） torch.size([1,512,56,100]) transpose 以w为序列方向，h为每个样本 torch.size([1,56,100,512]) reshape torch.size([56,100,512]) Bi-LSTM torch.size([1,56,100,256]) channel first torch.size([1,256,56,100]) fc torch.size([1,512,56,100]) 10个anchor 2 个位置 torch.size([1,20,56,100]) 56个特征点，每个产生10个anchor
torch.size([1,5600,2])
特殊的anchor 文本长度的剧烈变化是文本检测的挑战之一，作者认为文本在长度的变化比高度的变化剧烈得多，文本边界开始与结束的地方难以和Faster-rcnn一样去用anchor匹配回归，所以作者提出一种vertical anchor的方法，即我们只去预测文本的竖直方向上的位置，不去预测水平方向的位置，水平位置的确定只需要我们检测一个一个小的固定宽度的文本段，将他们对应的高度预测准确，最后再将他们连接在一起，就得到了我们的文本行，如下图所示：
所以，我们对于每个像素点设置的anchor的宽度都是固定的，为16像素（第二部分提到，feature map上的每一点对应原图16个像素），这样就可以覆盖到原图的所有位置，而高度则是从11到273变化，这里我们每个像素点取k=10个anchor。
因为宽度是固定的，所以只需要anchor的中心的y坐标以及anchor的高度就可以确定一个anchor，其中带星号的为ground-truth，没有带星号的则是预测值，带a的则是对应anchor的值，具体那些回归的原理和之前讲解的检测算法一致就不多赘述。
双向LSTM VGG16提取的是空间特征，而LSTM学习的就是序列特征，而这里使用的是双向LSTM，更好的避免RNN当中的遗忘问题，更完整地提取出序列特征。
RPN层 CTPN的RPN层和Faster R-CNN很像，第一个分支输出的是我们anchor的位置，也就是我们上面讲解anchor提到的两个参数($v_c,v_h$)，因为每个特征点配置10个anchor，所以这个分支的输出20个channel。
第二个分支则是输出前景背景的得分情况(text/non-text scores)，通过softmax计算得分，所以这里也是输出20个channel。
第三个分支则是输出最后水平精修side-refinement的比例$o$，这是由于我们每个anchor的宽是一定的，所以有时候会导致水平方向有一点不准，所以这时候就需要校准一下我们的框（在我自己的实验中这个帮助不大），精修的公式如下：
和anchor一样，带星号的是ground-truth，$X_{side}$表示文本框的左边界或者右边界，$C^x_a$表示anchor中心的横坐标，$Wa$是anchor固定的宽度16像素，所以我们可以把这个$o$理解为一个缩放的比例，来对最后的结果做一个准确的拉伸，下面这张图中红色的就是使用了side-refinement，黄色的则是没有使用的结果。
这里要注意，作者选择与真值IoU大于0.7的anchor作为正样本，与真值IoU最大的那个anchor也定义为正样本，这样做有助于检测出小文本，避免小样本因得分低而被判断为负样本；与真值IOU小于0.5的anchor则定义为负样本，经过RPN之后我们只保留那些正样本。
nms 经过RPN，就会输出密密麻麻的检测框，这时候使用一个nms来过滤掉多的框。
文本线构造方法 经过上一部分我们已经得到了一系列的小的文本框，接下来我们就是用文本线构造方法将他们连起来。论文中说的不太清楚，
有2个text proposal，即蓝色和红色2组Anchor，CTPN采用如下算法构造文本线：
这只是一个概述，接下来展开叙述。假设每个Anchor index是绿色数字，每个Anchor Softmax score是黑色数字：
文本框矫正 很多网上的文章忽略了文本框矫正这一点，加入文本并不是理想的，也就是存在倾斜，文本框是需要矫正的，矫正的步骤如下：
（1）上一步我们得到了一些判断为同一个文本序列的anchor，我们首先要求一条直线L使得所有中心到这条直线的距离最小，也就是最小二乘法线性回归。
（2）这时候我们已经得到了每一段文字的基本走向，与此同时我们可以根据每一段里的text proposal得到这段文字的最大区域，我们可视化一下：
3）现在有了最大范围和拟合出的文本的直线，我们要生成最终符合文字倾斜角度和区域的box，CTPN作者使用一种巧妙方法来生成text proposal：首先求每段text proposal的平均高度，并以此和拟合出的文字中的直线做上下平移，来生成候选区域。这个时候我们生成的box的上下边都是我们刚才的拟合出的直线的平行线，左右边则是由上下边生成的垂线生成的平行四边形。
（4）现在我们生成了一个平行四边形，但是我们传入识别部分肯定是一个矩形，所以作者根据框上下边斜度来对左右两条边做出斜度变化的补偿方法，来确定最终的矩形框。
这里要注意，CTPN在实际当中对一些倾斜样本的鲁棒性还是略显不足的。
损失函数 CTPN的损失函数如上图，一眼看上去又是一个Muti-task的loss，分为三个部分：
LS：每个anchor是否是正样本的classification loss Lv:每个anchor的中心y坐标和高度loss L0:文本区域两侧精修的x损失 和Faster-RCNN一样，以上的loss都采用smooth L1 loss，$λ$是权重系数，$N$则是归一化系数，这部分很清晰没什么好讲的。
CRNN 1:torch.Size([1,1,32,258]) （c,h,w)灰度图单通道 2:torch.size([1,512,1,65]) 3:torch.size([65,1,512]) transpose 65表序列长度，512表序列维度 4:torch.size([65,1,5835]) 5835表词库</content></entry><entry><title>cv-Deeplab</title><url>/post/cv-deeplab/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[DeeplabV1 原论文名称：Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs 论文下载地址：https://arxiv.org/abs/1412.7062 参考源码：https://github.com/TheLegendAli/DeepLab-Context
在论文的引言部分(INTRODUCTION)首先抛出了两个问题（针对语义分割任务）: 信号下采样导致分辨率降低和空间“不敏感” 问题。
There are two technical hurdles in the application of DCNNs to image labeling tasks: signal downsampling, and spatial ‘insensitivity’ (invariance).
对于第一个问题信号下采样，作者说主要是采用Maxpooling导致的，为了解决这个问题作者引入了&rsquo;atrous&rsquo;(with holes) algorithm（空洞卷积 / 膨胀卷积 / 扩张卷积）。
对于第二个问题空间“不敏感”，作者说分类器自身的问题（分类器本来就具备一定空间不变性），我个人认为其实还是Maxpooling导致的。为了解决这个问题作者采用了fully-connected CRF(Conditional Random Field)方法，这个方法只在DeepLabV1-V2中使用到了，从V3之后就不去使用了，而且这个方法挺耗时的。
优势 速度更快，论文中说是因为采用了膨胀卷积的原因，但fully-connected CRF很耗时 准确率更高，相比之前最好的网络提升了7.2个点 模型很简单，主要由DCNN和CRF联级构成 搭建细节 LargeFOV 首先网络的backbone是当时比较火的VGG-16，并且和FCN网络一样将全连接层的权重转成了卷积层的权重，构成全卷积网络。然后关于膨胀卷积的使用，论文中是这么说的：
We skip subsampling after the last two max-pooling layers in the network of Simonyan &amp; Zisserman (2014) and modify the convolutional filters in the layers that follow them by introducing zeros to increase their length (2×in the last three convolutional layers and 4× in the first fully connected layer).
感觉文中的skip subsampling说的有点模糊（可能是自己英语水平太菜）什么叫做跳过下采样。既然看不懂论文的表述，就去看看代码。根据代码我绘制了如下所示的网络结构（DeepLab-LargeFOV）。
通过分析发现虽然backbone是VGG-16但所使用Maxpool略有不同，**VGG论文中是kernel=2，stride=2，但在DeepLabV1中是kernel=3，stride=2，padding=1。**接着就是最后两个Maxpool层的stride全部设置成1了（这样下采样的倍率就从32变成了8）。最后三个3x3的卷积层采用了膨胀卷积，膨胀系数r=2。然后关于将全连接层卷积化过程中，对于第一个全连接层（FC1）在FCN网络中是直接转换成卷积核大小7x7，卷积核个数为4096的卷积层，但在DeepLabV1中作者说是对参数进行了下采样最终得到的是卷积核大小3x3，卷积核个数为1024的卷积层（这样不仅可以减少参数还可以减少计算量，详情可以看下论文中的Table2），对于第二个全连接层（FC2）卷积核个数也由4096采样成1024。
After converting the network to a fully convolutional one, the first fully connected layer has 4,096 filters of large 7 × 7 spatial size and becomes the computational bottleneck in our dense score map computation. We have addressed this practical problem by spatially subsampling (by simple decimation) the first FC layer to 4×4 (or 3×3) spatial size.
将FC1卷积化后，还设置了膨胀系数，论文3.1中说的是r=4但在Experimental Evaluation中Large of View章节里设置的是r=12对应LargeFOV。对于FC2卷积化后就是卷积核1x1，卷积核个数为1024的卷积层。接着再通过一个卷积核1x1，卷积核个数为num_classes（包含背景）的卷积层。最后通过8倍上采样还原回原图大小。
下表是关于是否使用LargeFOV（Field of View）的对比。
第一行DeepLab-CRF-7x7就是直接将FC1按照FCN论文中的方法转换成7x7大小的卷积层，并且膨胀因子r=4（receptive field=224）。 第二行DeepLab-CRF是将7x7下采样到4x4大小的卷积层，同样膨胀因子r=4（receptive field=128），可以看到参数减半，训练速度翻倍，但mean IOU下降了约4个点。 第三行DeepLab-CRF-4x4，是在DeepLab-CRF的基础上把膨胀因子r改成了8（receptive field=224），mean IOU又提升了回去了。 第四行DeepLab-CRF-LargeFOV，是将7x7下采样到3x3大小的卷积层，膨胀因子r=12（receptive field=224），相比DeepLab-CRF-7x7，参数减少了6倍，训练速度提升了3倍多，mean IOU不变。
MSc(Multi-Scale) 其实在论文的4.3中还提到了Multi-Scale Prediction，即融合多个特征层的输出。关于MSc(Multi-Scale)的结构论文中是这么说的：
Specifically, we attach to the input image and the output of each of the first four max pooling layers a two-layer MLP (first layer: 128 3x3 convolutional filters, second layer: 128 1x1 convolutional filters) whose feature map is concatenated to the main network’s last layer feature map. The aggregate feature map fed into the softmax layer is thus enhanced by 5 * 128 = 640 channels.
即，除了使用之前主分支上输出外，还融合了来自原图尺度以及前四个Maxpool层的输出，更详细的结构参考下图。论文中说使用MSc大概能提升1.5个点，使用fully-connected CRF大概能提升4个点。但在源码中作者建议使用的是不带MSc的版本，以及看github上的一些开源实现都没有使用MSc。我个人猜测是因为这里的MSc不仅费时而且很吃显存。根据参考如下代码绘制了DeepLab-MSc-LargeFOV结构。
DeeplabV2 论文名称：Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs 论文下载地址：https://arxiv.org/abs/1606.00915
个人感觉相对DeepLab V1，DeepLab V2就是换了个backbone（VGG -&gt; ResNet ，简单换个backbone就能涨大概3个点）然后引入了一个新的模块ASPP（Atros Spatial Pyramid Pooling），其他的没太大区别。
在文章的引言部分作者提出了DCNNs应用在语义分割任务中遇到的问题。
分辨率被降低（主要由于下采样stride&gt;1的层导致） 目标的多尺度问题 DCNNs的不变性(invariance)会降低定位精度 针对分辨率被降低的问题，一般就是将最后的几个Maxpooling层的stride给设置成1(如果是通过卷积下采样的，比如resnet，同样将stride设置成1即可)，然后在配合使用膨胀卷积。
In order to overcome this hurdle and efficiently produce denser feature maps, we remove the downsampling operator from the last few max pooling layers of DCNNs and instead upsample the filters in subsequent convolutional layers, resulting in feature maps computed at a higher sampling rate. Filter upsampling amounts to inserting holes (‘trous’ in French) between nonzero filter taps.
针对目标多尺度的问题，最容易想到的就是将图像缩放到多个尺度分别通过网络进行推理，最后将多个结果进行融合即可。这样做虽然有用但是计算量太大了。为了解决这个问题，DeepLab V2 中提出了ASPP模块（atrous spatial pyramid pooling），具体结构后面会讲。
A standard way to deal with this is to present to the DCNN rescaled versions of the same image and then aggregate the feature or score maps. We show that this approach indeed increases the performance of our system, but comes at the cost of computing feature responses at all DCNN layers for multiple scaled versions of the input image. Instead, motivated by spatial pyramid pooling, we propose a computationally efficient scheme of resampling a given feature layer at multiple rates prior to convolution. This amounts to probing the original image with multiple filters that have complementary effective fields of view, thus capturing objects as well as useful image context at multiple scales. Rather than actually resampling features, we efficiently implement this mapping using multiple parallel atrous convolutional layers with different sampling rates; we call the proposed technique “atrous spatial pyramid pooling” (ASPP).
针对DCNNs不变性导致定位精度降低的问题，和DeepLab V1差不多还是通过CRFs解决，不过**这里用的是fully connected pairwise CRF，相比V1里的fully connected CRF要更高效点。**在DeepLab V2中CRF涨点就没有DeepLab V1猛了，在DeepLab V1中大概能提升4个点，在DeepLab V2中通过Table4可以看到大概只能提升1个多点了。
Our work explores an alternative approach which we show to be highly effective. In particular, we boost our model’s ability to capture fine details by employing a fully-connected Conditional Random Field (CRF) [22]. CRFs have been broadly used in semantic segmentation to combine class scores computed by multi-way classifiers with the low-level information captured by the local interactions of pixels and edges [23], [24] or superpixels [25]. Even though works of increased sophistication have been proposed to model the hierarchical dependency [26], [27], [28] and/or high-order dependencies of segments [29], [30], [31], [32], [33], we use the fully connected pairwise CRF proposed by [22] for its efficient computation, and ability to capture fine edge details while also catering for long range dependencies.
优势 速度更快 准确率更高（当时的state-of-art） 模型结构简单，还是DCNNs和CRFs联级 From a practical standpoint, the three main advantages of our DeepLab system are: (1) Speed: by virtue of atrous convolution, our dense DCNN operates at 8 FPS on an NVidia Titan X GPU, while Mean Field Inference for the fully-connected CRF requires 0.5 secs on a CPU. (2) Accuracy: we obtain state-of-art results on several challenging datasets, including the PASCAL VOC 2012 semantic segmentation benchmark [34], PASCAL-Context [35], PASCAL-Person-Part [36], and Cityscapes [37]. (3) Simplicity: our system is composed of a cascade of two very well-established modules, DCNNs and CRFs.
ASPP(atrous spatial pyramid pooling) 个人觉得在DeepLab V2中值得讲的就这个ASPP模块了，其他的都算不上啥亮点。这个ASPP模块给我感觉就像是DeepLab V1中LargeFOV的升级版（加入了多尺度的特性）。下图是原论文中介绍ASPP的示意图，就是在backbone输出的Feature Map上并联四个分支，每个分支的第一层都是使用的膨胀卷积，但不同的分支使用的膨胀系数不同（即每个分支的感受野不同，从而具有解决目标多尺度的问题）。
下图(b)有画出更加详细的ASPP结构（这里是针对VGG网络为例的），将Pool5输出的特征层（这里以VGG为例）并联4个分支，每个分支分别通过一个3x3的膨胀卷积层，1x1的卷积层，1x1的卷积层（卷积核的个数等于num_classes）。最后将四个分支的结果进行Add融合即可。 如果是以ResNet101做为Backbone的话，每个分支只有一个3x3的膨胀卷积层，卷积核的个数等于num_classes（看源码分析得到的）。
在论文中有给出两个ASPP的配置，ASPP-S（四个分支膨胀系数分别为2,4,8,12）和ASPP-L（四个分支膨胀系数分别为6,12,18,24），下表是对比LargeFOV、ASPP-S以及ASPP-L的效果。这里只看CRF之前的（before CRF）对比，ASPP-L优于ASPP-S优于LargeFOV。
网络结构 这里以ResNet101作为backbone为例，下图是根据官方源码绘制的网络结构（这里不考虑MSC即多尺度）。在ResNet的Layer3中的Bottleneck1中原本是需要下采样的（3x3的卷积层stride=2），但在DeepLab V2中将stride设置为1，即不在进行下采样。而且3x3卷积层全部采用膨胀卷积膨胀系数为2。在Layer4中也是一样，取消了下采样，所有的3x3卷积层全部采用膨胀卷积膨胀系数为4。最后需要注意的是ASPP模块，在以ResNet101做为Backbone时，每个分支只有一个3x3的膨胀卷积层，且卷积核的个数都等于num_classes。 Learning rate policy 在DeepLab V2中训练时采用的学习率策略叫poly，相比普通的step策略（即每间隔一定步数就降低一次学习率）效果要更好。文中说最高提升了3.63个点，真是炼丹大师。poly学习率变化策略公式如下： $$ lr \times (1 - \frac{iter}{max_iter})^{power}$$
其中$lr$为初始学习率，$iter$为当前迭代的step数，$max_{iter}$为训练过程中总的迭代步数。
DeeplabV3 论文名称：Rethinking Atrous Convolution for Semantic Image Segmentation 论文下载地址：https://arxiv.org/abs/1706.05587 非官方Pytorch实现代码：pytorch_segmentation/deeplab_v3
相比DeepLab V2有三点变化：1）引入了Multi-grid，2）改进了ASPP结构，3）把CRFs后处理给移除掉了。
两种模型结构 给出两个模型，cascaded model和ASPP model，在cascaded model中是没有使用ASPP模块的，在ASPP model中是没有使用cascaded blocks模块的，如果没有弄清楚这两个模型的区别，那么这篇文章就算不上看懂。注意，虽然文中提出了两种结构，但作者说ASPP model比cascaded model略好点。包括在Github上开源的一些代码，大部分也是用的ASPP model。
Both our best cascaded model (in Tab. 4) and ASPP model (in Tab. 6) (in both cases without DenseCRF post-processing or MS-COCO pre-training) already outperform DeepLabv2.
cascaded model 在这篇论文中，大部分实验基本都是围绕cascaded model来做的。如下图所示，论文中提出的cascaded model指的是图(b)。其中Block1，Block2，Block3，Block4是原始ResNet网络中的层结构，但在Block4中将第一个残差结构里的3x3卷积层以及捷径分支上的1x1卷积层步距stride由2改成了1（即不再进行下采样），并且所有残差结构里3x3的普通卷积层都换成了膨胀卷积层。Block5，Block6和Block7是额外新增的层结构，他们的结构和Block4是一模一样的，即由三个残差结构构成。
注意，原论文中说在训练cascaded model时output_stride=16（即特征层相对输入图片的下采样率），但验证时使用的output_stride=8（这里论文里虽然没有细讲，但我猜应该是把Block3中的下采样取消了）。因为output_stride=16时最终得到的特征层H和W会更小，这意味着可以设置更大的batch_size并且能够加快训练速度。但特征层H和W变小会导致特征层丢失细节信息（文中说变的更“粗糙”），所以在验证时采用的output_stride=8。其实只要你GPU显存足够大，算力足够强也可以直接把output_stride设置成8。
Also note that training with output stride = 16 is several times faster than output stride = 8 since the intermediate feature maps are spatially four times smaller, but at a sacrifice of accuracy since output stride = 16 provides coarser feature maps.
另外需要注意的是，图中标注的rate并不是膨胀卷积真正采用的膨胀系数。 真正采用的膨胀系数应该是图中的rate乘上Multi-Grid参数，比如Block4中rate=2，Multi-Grid=(1, 2, 4)那么真正采用的膨胀系数是2 x (1, 2, 4)=(2, 4, 8)。关于Multi-Grid参数后面会提到。
The final atrous rate for the convolutional layer is equal to the multiplication of the unit rate and the corresponding rate. For example, when output stride = 16 and Multi Grid = (1, 2, 4), the three convolutions will have rates = 2 · (1, 2, 4) = (2, 4, 8) in the block4, respectively.
ASPP model 虽然论文大篇幅的内容都在讲cascaded model以及对应的实验，但实际使用的最多的还是ASPP model，ASPP model结构如下图所示：
注意，和cascaded model一样，原论文中说在训练时output_stride=16（即特征层相对输入图片的下采样率），但验证时使用的output_stride=8。但在Pytorch官方实现的DeepLabV3源码中就直接把output_stride设置成8进行训练的。
接下来分析下DeepLab V3中ASPP结构。首先回顾下上篇博文中讲的DeepLab V2中的ASPP结构，DeepLab V2中的ASPP结构其实就是通过四个并行的膨胀卷积层，每个分支上的膨胀卷积层所采用的膨胀系数不同（注意，这里的膨胀卷积层后没有跟BatchNorm并且使用了偏执Bias）。接着通过add相加的方式融合四个分支上的输出。
我们再来看下DeepLab V3中的ASPP结构。这里的ASPP结构有5个并行分支，分别是一个1x1的卷积层，三个3x3的膨胀卷积层，以及一个全局平均池化层（后面还跟有一个1x1的卷积层，然后通过双线性插值的方法还原回输入的W和H）。关于最后一个全局池化分支作者说是为了增加一个全局上下文信息global context information。然后通过Concat的方式将这5个分支的输出进行拼接（沿着channels方向），最后在通过一个1x1的卷积层进一步融合信息。 关于原论文中ASPP（Atrous Spatial Pyramid Pooling）结构介绍可以看下下面这段话。
Specifically, we apply global average pooling on the last feature map of the model, feed the resulting image-level features to a 1 × 1 convolution with 256 filters (and batch normalization [38]), and then bilinearly upsample the feature to the desired spatial dimension. In the end, our improved ASPP consists of (a) one 1×1 convolution and three 3 × 3 convolutions with rates = (6, 12, 18) when output stride = 16 (all with 256 filters and batch normalization), and (b) the image-level features, as shown in Fig. 5. Note that the rates are doubled when output stride = 8. The resulting features from all the branches are then concatenated and pass through another 1 × 1 convolution (also with 256 filters and batch normalization) before the final 1 × 1 convolution which generates the final logits.
Multi-grid 在之前的DeepLab模型中虽然一直在使用膨胀卷积，但设置的膨胀系数都比较随意。在DeepLab V3中作者有去做一些相关实验看如何设置更合理。下表是以cascaded model（ResNet101作为Backbone为例）为实验对象，研究采用不同数量的cascaded blocks模型以及cascaded blocks采用不同的Multi-Grid参数的效果。注意，刚刚在讲cascaded model时有提到，blocks中真正采用的膨胀系数应该是图中的rate乘上这里的Multi-Grid参数。通过实验发现，当采用三个额外的Block时（即额外添加Block5，Block6和Block7）将Multi-Grid设置成(1, 2, 1)效果最好。另外如果不添加任何额外Block（即没有Block5，Block6和Block7）将Multi-Grid设置成(1, 2, 4)效果最好，因为在ASPP model中是没有额外添加Block层的，后面讲ASPP model的消融实验时采用的就是Multi-Grid等于(1, 2, 4)的情况。
]]></content></entry><entry><title>cv-Inception、Xception</title><url>/post/cv-inceptionxception/</url><categories><category>cv</category></categories><tags/><content type="html">如果 ResNet 是为了更深，那么 Inception 家族就是为了更宽。Inception 的作者对训练更大型网络的计算效率尤其感兴趣。换句话说：我们怎样在不增加计算成本的前提下扩展神经网络？ Inception 最早的论文关注的是一种用于深度网络的新型构建模块，现在这一模块被称为「Inception module」。究其核心，这种模块源自两种思想见解的交汇。
第一个见解与对层的操作有关。在传统的卷积网络中，每一层都会从之前的层提取信息，以便将输入数据转换成更有用的表征。但是，不同类型的层会提取不同种类的信息。5×5 卷积核的输出中的信息就和 3×3 卷积核的输出不同，又不同于最大池化核的输出……在任意给定层，我们怎么知道什么样的变换能提供最「有用」的信息呢？ 见解 1：为什么不让模型选择？ Inception 模块会并行计算同一输入映射上的多个不同变换，并将它们的结果都连接到单一一个输出。换句话说，对于每一个层，Inception 都会执行 5×5 卷积变换、3×3 卷积变换和最大池化。然后该模型的下一层会决定是否以及怎样使用各个信息。 这种模型架构的信息密度更大了，这就带来了一个突出的问题：计算成本大大增加。不仅大型（比如 5×5）卷积过滤器的固有计算成本高，并排堆叠多个不同的过滤器更会极大增加每一层的特征映射的数量。而这种计算成本增长就成为了我们模型的致命瓶颈。 想一下，每额外增加一个过滤器，我们就必须对所有输入映射进行卷积运算以计算单个输出。如下图所示：从单个过滤器创建一个输出映射涉及到在之前一层的每个单个映射上执行计算。 假设这里有 M 个输入映射。增加一个过滤器就意味着要多卷积 M 次映射；增加 N 个过滤器就意味着要多卷积 N*M 次映射。换句话说，正如作者指出的那样：「过滤器数量的任何统一增长都会导致计算量的 4 倍增长。」我们的朴素 Inception 模块只是将过滤器的数量增加了三四倍。但从计算成本上看，这简直就是一场大灾难。
这就涉及到了见解 2：
使用 1×1 卷积来执行降维。为了解决上述计算瓶颈，Inception 的作者使用了 1×1 卷积来「过滤」输出的深度。一个 1×1 卷积一次仅查看一个值，但在多个通道上，它可以提取空间信息并将其压缩到更低的维度。比如，使用 20 个 1×1 过滤器，一个大小为 64×64×100（具有 100 个特征映射）的输入可以被压缩到 64×64×20。通过减少输入映射的数量，Inception 可以将不同的层变换并行地堆叠到一起，从而得到既深又宽（很多并行操作）的网络。 这能达到多好的效果？Inception 的第一个版本是 GoogLeNet，也就是前面提及的赢得了 ILSVRC 2014 比赛的 22 层网络。一年之后，研究者在第二篇论文中发展出了 Inception v2 和 v3，并在原始版本上实现了多种改进——其中最值得一提的是将更大的卷积重构成了连续的更小的卷积，让学习变得更轻松。比如在 v3 中，5×5 卷积被替换成了两个 连续的 3×3 卷积。
Inception 很快就变成了一种具有决定性意义的模型架构。最新的版本 Inception v4 甚至将残差连接放进了每一个模组中，创造出了一种 Inception-ResNet 混合结构。但更重要的是，Inception 展现了经过良好设计的「网中有网」架构的能力，让神经网络的表征能力又更上了一层楼。 Xception
Xception 表示「extreme inception」。和前面两种架构一样，它重塑了我们看待神经网络的方式——尤其是卷积网络。而且正如其名字表达的那样，它将 Inception 的原理推向了极致。
它的假设是：「跨通道的相关性和空间相关性是完全可分离的，最好不要联合映射它们。」
这是什么意思？在传统的卷积网络中，卷积层会同时寻找跨空间和跨深度的相关性。让我们再看一下标准的卷积层： 在上图中，过滤器同时考虑了一个空间维度（每个 2×2 的彩色方块）和一个跨通道或「深度」维度（4 个方块的堆叠）。在输入图像的输入层，这就相当于一个在所有 3 个 RGB 通道上查看一个 2×2 像素块的卷积过滤器。那问题来了：我们有什么理由去同时考虑图像区域和通道？
在 Inception 中，我们开始将两者稍微分开。我们使用 1×1 的卷积将原始输入投射到多个分开的更小的输入空间，而且对于其中的每个输入空间，我们都使用一种不同类型的过滤器来对这些数据的更小的 3D 模块执行变换。Xception 更进一步。不再只是将输入数据分割成几个压缩的数据块，而是为每个输出通道单独映射空间相关性，然后再执行 1×1 的深度方面的卷积来获取跨通道的相关性。 其作者指出这本质上相当于一种已有的被称为「深度方面可分的卷积（depthwise separable convolution）」的运算，它包含一个深度方面的卷积（一个为每个通道单独执行的空间卷积），后面跟着一个逐点的卷积（一个跨通道的 1×1 卷积）。我们可以将其看作是首先求跨一个 2D 空间的相关性，然后再求跨一个 1D 空间的相关性。可以看出，这种 2D+1D 映射学起来比全 3D 映射更加简单。
而且这种做法是有效的！在 ImageNet 数据集上，Xception 的表现稍稍优于 Inception v3，而且在一个有 17000 类的更大规模的图像分类数据集上的表现更是好得多。最重要的是，它的模型参数的数量和 Inception 一样多，说明它的计算效率也更高。Xception 非常新（2017 年 4 月才公开），但正如前面提到的那样，这个架构已经在通过 MobileNet 助力谷歌的移动视觉应用了。
有趣的事实：
Xception 的作者也是 Keras 的作者。Francois Chollet 是真正的大神。
未来发展
这就是 ResNet、Inception 和 Xception！我坚信我们需要对这些网络有很好的直观理解，因为它们在研究界和产业界的应用越来越普遍。我们甚至可以通过所谓的迁移学习将它们用在我们自己的应用中。
迁移学习是一种机器学习技术，即我们可以将一个领域的知识（比如 ImageNet）应用到目标领域，从而可以极大减少所需要的数据点。在实践中，这通常涉及到使用来自 ResNet、Inception 等的预训练的权重初始化模型，然后要么将其用作特征提取器，要么就在一个新数据集上对最后几层进行微调。使用迁移学习，这些模型可以在任何我们想要执行的相关任务上得到重新利用，从自动驾驶汽车的目标检测到为视频片段生成描述。
要了解迁移学习，Keras 有一个关于微调模型的很棒的指南：https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html。如果你对此有兴趣，一定不要错过！</content></entry><entry><title>cv-OpenCV</title><url>/post/cv-opencv/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[图像处理中的坐标系，水平向右为x轴正方向，竖直向下为y轴正方向。
安装OpenCV-Python,
pip install opencv-python==3.4.2.17 要利用SIFT和SURF等进行特征提取时，还需要安装：
pip install opencv-contrib-python==3.4.2.17 core、highgui、imgproc是最基础的模块，该课程主要是围绕这几个模块展开的，分别介绍如下：
core模块实现了最核心的数据结构及其基本运算，如绘图函数、数组操作相关函数等。 highgui模块实现了视频与图像的读取、显示、存储等接口。 imgproc模块实现了图像处理的基础方法，包括图像滤波、图像的几何变换、平滑、阈值分割、形态学处理、边缘检测、目标检测、运动分析和对象跟踪等。 对于图像处理其他更高层次的方向及应用，OpenCV也有相关的模块实现
features2d模块用于提取图像特征以及特征匹配，nonfree模块实现了一些专利算法，如sift特征。 objdetect模块实现了一些目标检测的功能，经典的基于Haar、LBP特征的人脸检测，基于HOG的行人、汽车等目标检测，分类器使用Cascade Classification（级联分类）和Latent SVM等。 stitching模块实现了图像拼接功能。 FLANN模块（Fast Library for Approximate Nearest Neighbors），包含快速近似最近邻搜索FLANN 和聚类Clustering算法。 ml模块机器学习模块（SVM，决策树，Boosting等等）。 photo模块包含图像修复和图像去噪两部分。 video模块针对视频处理，如背景分离，前景检测、对象跟踪等。 calib3d模块即Calibration（校准）3D，这个模块主要是相机校准和三维重建相关的内容。包含了基本的多视角几何算法，单个立体摄像头标定，物体姿态估计，立体相似性算法，3D信息的重建等等。 G-API模块包含超高效的图像处理pipeline引擎 opencv 的接口使用BGR模式，而 matplotlib.pyplot 接口使用的是RGB模式
b, g, r = cv2.split(srcImage) srcImage_new = cv2.merge([r, g, b]) plt.imshow(srcImage_new) # 通道变换之后对灰度图进行输出的图片颜色仍然为绿色,这是因为我们还是直接使用plt显示图像，它默认使用三通道显示图像 grayImage = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) # 灰度变换 plt.imshow(grayImage, cmap=&#34;gray&#34;) 图像处理 二值化 # 全局阈值 thresh, dst = cv2.threshold(src, thresh, maxVal, type) type: cv2.THRESH_BINARY 大于阈值的为maxVal,小于的为0 cv2.THRESH_BINARY_INV # 自适应阈值 dst = cv2.adaptiveThreshold(src, maxVal, adaptiveMethod, type, blockSize, C) #type:cv2.THRESH_BINARY #adapttiveMethod:cv2.ADAPTIVE_THRESH_MEAN_C #thresh=blockSize*blockSize矩阵平均值灰度-C，大于thresh的为maxValue 寻找轮廓 contours, hierarchy = cv2.findContours(image, mode, method) # 轮廓检索模式 # Cv2.RETR_EXTERNAL检测外轮廓 # Cv2.RETR_TREE等级树结构的轮廓 # 轮廓近似方法 # Cv2.CHAIN_APPROX_NONE所有点 # Cv2.CHAIN_APPROX_SIMPLE直线两端点 # contours：list结构，列表中每个元素代表一个边沿信息。每个元素是(x,1,2)的三维向量，x表示该条边沿里共有多少个像素点，第三维的那个“2”表示每个点的横、纵坐标； # 注意：如果输入选择cv2.CHAIN_APPROX_SIMPLE，则contours中一个list元素所包含的x点之间应该用直线连接起来，这个可以用cv2.drawContours()函数观察一下效果。 # hierarchy：返回类型是(x,4)的二维ndarray。x和contours里的x是一样的意思。如果输入选择cv2.RETR_TREE，则以树形结构组织输出，hierarchy的四列分别对应下一个轮廓编号、上一个轮廓编号、父轮廓编号、子轮廓编号，该值为负数表示没有对应项。 iamge = cv2.drawContours(image, contours, i, color, thickness) # i：列表中第几个轮廓，-1所有；color：绘制颜色；thickness：线条粗细，-1填充 x, y, w, h = cv2.boundingRect(contours) 用一个最小的矩形，把找到的形状包起来。 x，y是矩阵左上点的坐标，w，h是矩阵的宽和高 几何变换 # 读取图像 img = cv.imread(&#39;messi5.jpg&#39;,0) 参数：要读取的图像；读取方式的标志 cv.IMREAD*COLOR：以彩色模式加载图像，任何图像的透明度都将被忽略。这是默认参数。 cv.IMREAD*GRAYSCALE：以灰度模式加载图像 cv.IMREAD_UNCHANGED：包括alpha通道的加载图像模式。 可以使用1、0或者-1来替代上面三个标志 # 显示图像 # opencv中显示 cv.imshow(&#39;image&#39;,img) cv.waitKey(0) # matplotlib中展示 plt.imshow(img[:,:,::-1]) 参数：显示图像的窗口名称，以字符串类型表示，要加载的图像 注意：在调用显示图像的API后，要调用cv.waitKey()给图像绘留下时间，否则窗口会出现无响应情况，并且图像无法显示出来 # 保存图像 cv.imwrite(&#39;messigray.png&#39;,img) 参数：文件名，要保存在哪里；要保存的图像 # 向图像中添加文字 cv.putText(img,text,station, font, fontsize,color,thickness,cv.LINE_AA) 参数： img: 图像 text：要写入的文本数据 station：文本的放置位置 font：字体 Fontsize :字体大小 通过行和列的坐标值获取该像素点的像素值。对于BGR图像，它返回一个蓝，绿，红值的数组。对于灰度图像，仅返回相应的强度值。使用相同的方法对像素值进行修改。 img = cv.imread(&#39;messi5.jpg&#39;) # 获取某个像素点的值 px = img[100,100] # 仅获取蓝色通道的强度值 blue = img[100,100,0] # 修改某个位置的像素值 img[100,100] = [255,255,255] # 通道拆分 b,g,r = cv.split(img) # 通道合并 img = cv.merge((b,g,r)) # 色彩空间的改变 cv.cvtColor(image，flag) cv.COLOR_BGR2GRAY : BGR↔Gray cv.COLOR_BGR2HSV: BGR→HSV # 图像的加法 OpenCV加法和Numpy加法之间存在差异。OpenCV的加法是饱和操作，而Numpy添加是模运算。 尽量使用 OpenCV 中的函数。 &gt;&gt;&gt; x = np.uint8([250]) &gt;&gt;&gt; y = np.uint8([10]) &gt;&gt;&gt; print( cv.add(x,y) ) # 250+10 = 260 =&gt; 255 &gt;&gt;&gt; print( x+y ) # 250+10 = 260 % 256 = 4 # 图像的混合 这其实也是加法，但是不同的是两幅图像的权重不同，这就会给人一种混合或者透明的感觉。图像混合的计算公式如下：dst = α⋅img1 + β⋅img2 + γ img3 = cv.addWeighted(img1,α,img2,β,γ) 图像旋转 仿射变换 变换前后满足平直性（变换前是直线变换后还是直线）和平行性（变换前平行的线变换后依旧平行）
透射变换 图像金字塔 # 图像缩放 cv2.resize(src,dsize,fx=0,fy=0,interpolation=cv2.INTER_LINEAR) src : 输入图像 dsize: 绝对尺寸，直接指定调整后图像的大小 (2*cols,2*rows) fx,fy: 相对尺寸，将dsize设置为None，(img1,None,fx=0.5,fy=0.5) interpolation：插值方法， cv2.INTER_LINEAR 双线性插值法 cv2.INTER_NEAREST 最临近插值 cv2.INTER_AREA 像素区域重采样{默认} cv2.INTER_CUBIC 双三次插值 # 图像平移 M = np.float32([[1,0,100],[0,1,50]])# 将图像的像素点移动(50,100)的距离： dst = cv.warpAffine(img1,M,dsize=(cols,rows)，borderValue=(0,0,0)) img: 输入图像 M： 2*∗3移动矩阵 dsize: 输出图像的大小，它应该是(宽度，高度)的形式。请记住,width=列数，height=行数。 borderValue为边界填充颜色（注意是BGR顺序，( 0 , 0 , 0 ) (0,0,0)(0,0,0)代表黑色）: # 图像旋转 旋转中图像仍保持这原始尺寸。图像旋转后图像的水平对称轴、垂直对称轴及中心坐标原点都可能会发生变换，因此需要对图像旋转中的坐标进行相应转换。 # 生成旋转矩阵 M = cv.getRotationMatrix2D((cols/2,rows/2),90,1) # center：旋转中心；angle：旋转角度；scale：缩放比例 # 进行旋转变换 dst = cv.warpAffine(img,M,(cols,rows)) # 仿射变换 涉及到图像的形状位置角度的变化，是深度学习预处理中常到的功能,仿射变换主要是对图像的缩放，旋转，翻转和平移等操作的组合。 pts1 = np.float32([[50,50],[200,50],[50,200]])# 2.1 创建变换矩阵 pts2 = np.float32([[100,100],[200,50],[100,250]]) M = cv.getAffineTransform(pts1,pts2) dst = cv.warpAffine(img,M,(cols,rows))# 2.2 完成仿射变换 # 透射变换 pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]]) # 2.1 创建变换矩阵 pts2 = np.float32([[100,145],[300,100],[80,290],[310,300]]) T = cv.getPerspectiveTransform(pts1,pts2) # 2.2 进行变换 dst = cv.warpPerspective(img,T,(cols,rows)) # 图像金字塔 cv.pyrUp(img) #对图像进行上采样 cv.pyrDown(img) #对图像进行下采样 形态学操作 连通性 腐蚀、膨胀 开闭运算
腐蚀、开 消灭噪音
膨胀、闭 填补空洞
礼帽和黑帽 礼帽：噪音提取
黑帽：空洞提取
# 腐蚀、膨胀 cv.erode(img,kernel,iterations) cv.dilate(img,kernel,iterations) # 开闭运算# 礼帽和黑帽 kernel = np.ones((10, 10), np.uint8)# 2 创建核结构 cvOpen = cv.morphologyEx(img1,cv.MORPH_OPEN,kernel) # 开运算 cvClose = cv.morphologyEx(img2,cv.MORPH_CLOSE,kernel)# 闭运算 cvOpen = cv.morphologyEx(img1,cv.MORPH_TOPHAT,kernel) # 礼帽运算 cvClose = cv.morphologyEx(img2,cv.MORPH_BLACKHAT,kernel)# 黑帽运算 图像平滑 图像噪声 椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）。椒盐噪声的成因可能是影像讯号受到突如其来的强烈干扰而产生、类比数位转换器或位元传输错误等。例如失效的感应器导致像素值为最小值，饱和的感应器导致像素值为最大值。
高斯噪声是指噪声密度函数服从高斯分布的一类噪声。由于高斯噪声在空间和频域中数学上的易处理性，这种噪声(也称为正态噪声)模型经常被用于实践中。高斯随机变量z的概率密度函数由下式给出：
图像平滑从信号处理的角度看就是去除其中的高频信息，保留低频信息。因此我们可以对图像实施低通滤波。低通滤波可以去除图像中的噪声，对图像进行平滑。
均值滤波的优点是算法简单，计算速度较快，缺点是在去噪的同时去除了很多细节部分，将图像变得模糊。
高斯平滑在从图像中去除高斯噪声方面非常有效。
正态分布是一种钟形曲线，越接近中心，取值越大，越远离中心，取值越小。计算平滑结果时，只需要将&quot;中心点&quot;作为原点，其他点按照其在正态曲线上的位置，分配权重，就可以得到一个加权平均值。
中值滤波对椒盐噪声（salt-and-pepper noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值。是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值。
# 均值滤波 cv.blur(src, ksize, anchor, borderType) src：输入图像 ksize：卷积核的大小 anchor：默认值 (-1,-1) ，表示核中心 borderType：边界类型 # 高斯滤波 cv2.GaussianBlur(src,ksize,sigmaX,sigmay,borderType) src: 输入图像 ksize:高斯卷积核的大小，注意 ： 卷积核的宽度和高度都应为奇数，且可以不同 sigmaX: 水平方向的标准差 sigmaY: 垂直方向的标准差，默认值为0，表示与sigmaX相同 borderType:填充边界类型 # 中值滤波 cv.medianBlur(src, ksize ) src：输入图像 ksize：卷积核的大小 直方图 图像直方图（Image Histogram）是用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素个数。这种直方图中，横坐标的左侧为较暗的区域，而右侧为较亮的区域。因此一张较暗图片的直方图中的数据多集中于左侧和中间部分，而整体明亮、只有少量阴影的图像则相反。
“直方图均衡化”是把原始图像的灰度直方图从比较集中的某个灰度区间变成在更广泛灰度范围内的分布。直方图均衡化就是对图像进行非线性拉伸，重新分配图像像素值，使一定灰度范围内的像素数量大致相同。
这种方法提高图像整体的对比度，特别是有用数据的像素值分布比较接近时，在X光图像中使用广泛，可以提高骨架结构的显示，另外在曝光过度或不足的图像中可以更好的突出细节。
上述的直方图均衡，我们考虑的是图像的全局对比度。 的确在进行完直方图均衡化之后，图片背景的对比度被改变了，在猫腿这里太暗，我们丢失了很多信息，所以在许多情况下，这样做的效果并不好。
需要使用自适应的直方图均衡化
整幅图像会被分成很多小块，这些小块被称为“tiles”（在 OpenCV 中 tiles 的 大小默认是 8x8），然后再对每一个小块分别进行直方图均衡化。 所以在每一个的区域中， 直方图会集中在某一个小的区域中）。如果有噪声的话，噪声会被放大。为了避免这种情况的出现要使用对比度限制。对于每个小块来说，**如果直方图中的 bin 超过对比度的上限的话，就把其中的像素点均匀分散到其他 bins 中，然后在进行直方图均衡化。**最后，为了 去除每一个小块之间的边界，再使用双线性差值，对每一小块进行拼接。
# 直方图 cv2.calcHist(images,channels,mask,histSize,ranges[,hist[,accumulate]]) images: 原图像。当传入函数时应该用中括号 [] 括起来，例如：[img]。 channels: 如果输入图像是灰度图，它的值就是 [0]；如果是彩色图像的话，传入的参数可以是 [0]，[1]，[2] 它们分别对应着通道 B，G，R。 mask: 掩模图像。要统计整幅图像的直方图就把它设为 None。但是如果你想统计图像某一部分的直方图的话，你就需要制作一个掩模图像，并使用它。（后边有例子） histSize:BIN 的数目。也应该用中括号括起来，例如：[256]。 ranges: 像素值范围，通常为 [0，256] mask = np.zeros(img.shape[:2], np.uint8)# 2. 创建蒙版 mask[400:650, 200:500] = 255 # 查找直方图的区域上创建一个白色的掩膜图像，否则创建黑色 masked_img = cv.bitwise_and(img,img,mask = mask)# 3.掩模 mask_histr = cv.calcHist([img],[0],mask,[256],[1,256]) # 4. 统计掩膜后图像的灰度图 # 直方图均衡化 dst = cv.equalizeHist(img) # 自适应的直方图均衡化 cv.createCLAHE(clipLimit, tileGridSize) clipLimit: 对比度限制，默认是40 tileGridSize: 分块的大小，默认为8*88∗8 边缘检测 图像边缘检测大幅度地减少了数据量，并且剔除了可以认为不相关的信息，保留了图像重要的结构属性。有许多方法用于边缘检测，它们的绝大部分可以划分为两类：基于搜索和基于零穿越。
Sobel边缘检测算法比较简单，实际应用中效率比canny边缘检测效率要高，但是边缘不如Canny检测的准确，但是很多实际应用的场合，sobel边缘却是首选，Sobel算子是高斯平滑与微分操作的结合体，所以其抗噪声能力很强，用途较多。尤其是效率要求较高，而对细纹理不太关心的时候。
Laplacian是利用二阶导数来检测边缘 。
Canny 边缘检测算法被认为是最优的边缘检测算法。
# sobel边缘检测 Sobel_x_or_y = cv2.Sobel(src, ddepth, dx, dy, dst, ksize, scale, delta, borderType) src：传入的图像 ddepth: 图像的深度 dx和dy: 指求导的阶数，0表示这个方向上没有求导，取值为0、1。 ksize: 是Sobel算子的大小，即卷积核的大小，必须为奇数1、3、5、7，默认为3。注意：如果ksize=-1，就演变成为3x3的Scharr算子。 scale：缩放导数的比例常数，默认情况为没有伸缩系数。 borderType：图像边界的模式，默认值为cv2.BORDER_DEFAULT。 Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数，所以Sobel建立的图像位数不够，会有截断。因此要使用16位有符号的数据类型，即cv2.CV_16S。处理完图像后，再使用cv2.convertScaleAbs()函数将其转回原来的uint8格式，否则图像无法显示。Sobel算子是在两个方向计算的，最后还需要用cv2.addWeighted( )函数将其组合起来 x = cv.Sobel(img, cv.CV_16S, 1, 0)# 2 计算Sobel卷积结果 y = cv.Sobel(img, cv.CV_16S, 0, 1) Scale_absX = cv.convertScaleAbs(x) # convert 转换 scale 缩放 Scale_absY = cv.convertScaleAbs(y) result = cv.addWeighted(Scale_absX, 0.5, Scale_absY, 0.5, 0)# 4 结果合成 # laplacian算子 cv2.Laplacian(src, ddepth[, dst[, ksize[, scale[, delta[, borderType]]]]]) Src: 需要处理的图像， Ddepth: 图像的深度，-1表示采用的是原图像相同的深度，目标图像的深度必须大于等于原图像的深度； ksize：算子的大小，即卷积核的大小，必须为1,3,5,7。 result = cv.Laplacian(img,cv.CV_16S) Scale_abs = cv.convertScaleAbs(result) # canny检测 canny = cv2.Canny(image, threshold1, threshold2) image:灰度图， threshold1: minval，较小的阈值将间断的边缘连接起来 threshold2: maxval，较大的阈值检测图像中明显的边缘 lowThreshold = 0 max_lowThreshold = 100 canny = cv.Canny(img, lowThreshold, max_lowThreshold) 模板匹配和霍夫变换 模板匹配，就是在给定的图片中查找和模板最相似的区域，该算法的输入包括模板和图片，整个任务的思路就是按照滑窗的思路不断的移动模板图片，计算其与图像中对应区域的匹配度，最终将匹配度最高的区域选择为最终的结果。
霍夫变换常用来提取图像中的直线和圆等几何形状
res = cv.matchTemplate(img,template,method) img: 要进行模板匹配的图像 Template ：模板 method：实现模板匹配的算法，主要有： 平方差匹配(CV_TM_SQDIFF)：利用模板与图像之间的平方差进行匹配，最好的匹配是0，匹配越差，匹配的值越大。 相关匹配(CV_TM_CCORR)：利用模板与图像间的乘法进行匹配，数值越大表示匹配程度较高，越小表示匹配效果差。 利用相关系数匹配(CV_TM_CCOEFF)：利用模板与图像间的相关系数匹配，1表示完美的匹配，-1表示最差的匹配。 完成匹配后，使用cv.minMaxLoc()方法查找最大值所在的位置即可。如果使用平方差作为比较方法，则最小值位置是最佳匹配位置。 res = cv.matchTemplate(img, template, cv.TM_CCORR)# 2.1 模板匹配 min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)# 2.2 返回图像中最匹配的位置，确定左上角的坐标，并将匹配位置绘制在图像上 # top_left = min_loc# 使用平方差时最小值为最佳匹配位置 top_left = max_loc bottom_right = (top_left[0] + w, top_left[1] + h) cv.rectangle(img, top_left, bottom_right, (0,255,0), 2) # 霍夫线检测 cv.HoughLines(img, rho, theta, threshold) img: 检测的图像，要求是二值化的图像，所以在调用霍夫变换之前首先要进行二值化，或者进行Canny边缘检测 rho、theta: \rhoρ 和\thetaθ的精确度 threshold: 阈值，只有累加器中的值高于该阈值时才被认为是直线。 img = cv.imread(&#39;./image/rili.jpg&#39;)# 1.加载图片，转为二值图 gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) edges = cv.Canny(gray, 50, 150) lines = cv.HoughLines(edges, 0.8, np.pi / 180, 150)# 2.霍夫直线变换 for line in lines:# 3.将检测的线绘制在图像上（注意是极坐标噢） rho, theta = line[0] a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho x1 = int(x0 + 1000 * (-b)) y1 = int(y0 + 1000 * (a)) x2 = int(x0 - 1000 * (-b)) y2 = int(y0 - 1000 * (a)) cv.line(img, (x1, y1), (x2, y2), (0, 255, 0)) plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;霍夫变换线检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() 图像特征提取和描述 模板匹配不适用于尺度变换，视角变换后的图像，这时我们就要使用关键点匹配算法，比较经典的关键点检测算法包括SIFT和SURF等，主要的思路是首先通过关键点检测算法获取模板和测试图片中的关键点；然后使用关键点匹配算法处理即可，这些关键点可以很好的处理尺度变化、视角变换、旋转变化、光照变化等，具有很好的不变性。
角点特征 在角点的地方，无论你向哪个方向移动小图，结果都会有很大的不同。所以可以把它们当 成一个好的特征。
Harris和Shi-Tomas算法 Harris
优点：
旋转不变性，椭圆转过一定角度但是其形状保持不变（特征值保持不变） 对于图像灰度的仿射变化具有部分的不变性，由于仅仅使用了图像的一介导数，对于图像灰度平移变化不变；对于图像灰度尺度变化不变 缺点：
对尺度很敏感，不具备几何尺度不变性。 提取的角点是像素级的 Shi-Tomasi
对Harris算法的改进，能够更好地检测角点
#Hariis检测使用的API是： dst=cv.cornerHarris(src, blockSize, ksize, k) img：数据类型为 ﬂoat32 的输入图像。 blockSize：角点检测中要考虑的邻域大小。 ksize：sobel求导使用的核大小 k ：角点检测方程中的自由参数，取值参数为 [0.04，0.06]. # 1 读取图像，并转换成灰度图像 img = cv.imread(&#39;./image/chessboard.jpg&#39;) gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # 2 角点检测 # 2.1 输入图像必须是 float32 gray = np.float32(gray) # 2.2 最后一个参数在 0.04 到 0.05 之间 dst = cv.cornerHarris(gray,2,3,0.04) # 3 设置阈值，将角点绘制出来，阈值根据图像进行选择 img[dst&gt;0.001*dst.max()] = [0,0,255] # 4 图像显示 plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;Harris角点检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() # Shi-Tomasi corners = cv2.goodFeaturesToTrack ( image, maxcorners, qualityLevel, minDistance ) Image: 输入灰度图像 maxCorners : 获取角点数的数目。 qualityLevel：该参数指出最低可接受的角点质量水平，在0-1之间。 minDistance：角点之间最小的欧式距离，避免得到相邻特征点。 返回： Corners: 搜索到的角点，在这里所有低于质量水平的角点被排除掉，然后把合格的角点按质量排序，然后将质量较好的角点附近（小于最小欧式距离）的角点删掉，最后找到maxCorners个角点返回。 import numpy as np import cv2 as cv import matplotlib.pyplot as plt # 1 读取图像 img = cv.imread(&#39;./image/tv.jpg&#39;) gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2 角点检测 corners = cv.goodFeaturesToTrack(gray,1000,0.01,10) # 3 绘制角点 for i in corners: x,y = i.ravel() cv.circle(img,(x,y),2,(0,0,255),-1) # 4 图像展示 plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;shi-tomasi角点检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() SIFT/SURF算法 Harris和Shi-Tomasi角点检测算法，这两种算法具有旋转不变性，但不具有尺度不变性
SIFT算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点，但并不完美，仍然存在实时性不高，有时特征点较少，对边缘光滑的目标无法准确提取特征点等缺陷
SIFT原理：
尺度空间极值检测：构建高斯金字塔，高斯差分金字塔，检测极值点。 关键点定位：去除对比度较小和边缘对极值点的影响。 关键点方向确定：利用梯度直方图确定关键点的方向。 关键点描述：对关键点周围图像区域分块，计算块内的梯度直方图，生成具有特征向量，对关键点信息进行描述。 使用 SIFT 算法进行关键点检测和描述的执行速度比较慢， 需要速度更快的算法。 2006 年 Bay提出了 SURF 算法，是SIFT算法的增强版，它的计算量小，运算速度快，提取的特征与SIFT几乎相同，将其与SIFT算法对比如下：
# 实例化sift sift = cv.xfeatures2d.SIFT_create() # 利用sift.detectAndCompute()检测关键点并计算 kp,des = sift.detectAndCompute(gray,None) gray: 进行关键点检测的图像，注意是灰度图像 返回： kp: 关键点信息，包括位置，尺度，方向信息 des: 关键点描述符，每个关键点对应128个梯度信息的特征向量 # 将关键点检测结果绘制在图像上 cv.drawKeypoints(image, keypoints, outputimage, color, flags) image: 原始图像 keypoints：关键点信息，将其绘制在图像上 outputimage：输出图片，可以是原始图像 color：颜色设置，通过修改（b,g,r）的值,更改画笔的颜色，b=蓝色，g=绿色，r=红色。 flags：绘图功能的标识设置 cv2.DRAW_MATCHES_FLAGS_DEFAULT：创建输出图像矩阵，使用现存的输出图像绘制匹配对和特征点，对每一个关键点只绘制中间点 cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG：不创建输出图像矩阵，而是在输出图像上绘制匹配对 cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS：对每一个特征点绘制带大小和方向的关键点图形 cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制 # 1 读取图像 img = cv.imread(&#39;./image/tv.jpg&#39;) gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2 sift关键点检测 # 2.1 实例化sift对象 sift = cv.xfeatures2d.SIFT_create() # 2.2 关键点检测：kp关键点信息包括方向，尺度，位置信息，des是关键点的描述符 kp,des=sift.detectAndCompute(gray,None) # 2.3 在图像上绘制关键点的检测结果 cv.drawKeypoints(img,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # 3 图像显示 plt.figure(figsize=(8,6),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;sift检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() 视频操作 import numpy as np import cv2 as cv # 1.获取视频对象 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 获取视频属性 # retval = cap.get(propId) #0.cv2.CAP_PROP POS MSEC视频文件的当前位置(ms) #1.cv2.CAP_PROP POS FRAMES从0开始索引帧，帧位置 #2.cv2.CAP_PROP_POS AVI RATIO视频文件的相对位置(0表示开始，1表示结束) #3.cv2.CAP_PROP FRAME WIDTH视频流的帧宽度 #4.cv2.CAP PROP FRAME HEIGHT视频流的帧高度 #5.cv2.CAP PROP FPS帧率 #6.cv2.CAP PROP FOURCC编解码器四字符代码 #7.cv2.CAP PROP FRAME COUNT视频文件的帧 # 修改视频的属性信息 # cap.set(propId，value) # 2.判断是否读取成功 while(cap.isOpened()): # 3.获取每一帧图像 ret, frame = cap.read() #ret: 若获取成功返回True，获取失败，返回False #Frame: 获取到的某一帧的图像 # 4. 获取成功显示图像 if ret == True: cv.imshow(&#39;frame&#39;,frame) # 5.每一帧间隔为25ms if cv.waitKey(25) &amp; 0xFF == ord(&#39;q&#39;): break # 6.释放视频对象 cap.release() cv.destoryAllwindows() # 1. 读取视频 cap = cv.VideoCapture(&#34;DOG.wmv&#34;) # 3. 创建保存视频的对象，设置编码格式，帧率，图像的宽高等 out = cv.VideoWriter(&#39;outpy.avi&#39;,cv.VideoWriter_fourcc(&#39;M&#39;,&#39;J&#39;,&#39;P&#39;,&#39;G&#39;), 10, (frame_width,frame_height)) #ilename：视频保存的位置 #fourcc：指定视频编解码器的4字节代码cv2.VideoWriter_fourcc( c1, c2, c3, c4 ) c1,c2,c3,c4: 是视频编解码器的4字节代码，在fourcc.org中找到可用代码列表，与平台紧密相关 #fps：帧率 #frameSize：帧大小 while(True): # 4.获取视频中的每一帧图像 ret, frame = cap.read() if ret == True: # 5.将每一帧图像写入到输出文件中 out.write(frame) else: break # 6.释放资源 cap.release() out.release() cv.destroyAllWindows() 视频追踪 图像是一个矩阵信息，如何在一个视频当中使用meanshift算法来追踪一个运动的物体呢？ 大致流程如下：
首先在图像上选定一个目标区域
计算选定区域的直方图分布，一般是HSV色彩空间的直方图。
对下一帧图像b同样计算直方图分布。
计算图像b当中与选定区域直方图分布最为相似的区域，使用meanshift算法将选定区域沿着最为相似的部分进行移动，直到找到最相似的区域，便完成了在图像b中的目标追踪。
重复3到4的过程，就完成整个视频目标追踪。
通常情况下我们使用直方图反向投影得到的图像和第一帧目标对象的起始位置，当目标对象的移动会反映到直方图反向投影图中，meanshift 算法就把我们的窗口移动到反向投影图像中灰度密度最大的区域了。
直方图反向投影的流程是：
假设我们有一张100x100的输入图像，有一张10x10的模板图像，查找的过程是这样的：
从输入图像的左上角(0,0)开始，切割一块(0,0)至(10,10)的临时图像； 生成临时图像的直方图； 用临时图像的直方图和模板图像的直方图对比，对比结果记为c； 直方图对比结果c，就是结果图像(0,0)处的像素值； 切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像； 重复1～5步直到输入图像的右下角，就形成了直方图的反向投影。 import numpy as np import cv2 as cv # 1.获取图像 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 2.获取第一帧图像，并指定目标位置 ret,frame = cap.read() # 2.1 目标位置（行，高，列，宽） r,h,c,w = 197,141,0,208 track_window = (c,r,w,h) # 2.2 指定目标的感兴趣区域 roi = frame[r:r+h, c:c+w] # 3. 计算直方图 # 3.1 转换色彩空间（HSV） hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV) # 3.2 去除低亮度的值 # mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.))) # 3.3 计算直方图 roi_hist = cv.calcHist([hsv_roi],[0],None,[180],[0,180]) # 3.4 归一化 cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX) # 4. 目标追踪 # 4.1 设置窗口搜索终止条件：最大迭代次数，窗口中心漂移最小值 term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 ) while(True): # 4.2 获取每一帧图像 ret ,frame = cap.read() if ret == True: # 4.3 计算直方图的反向投影 hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # 4.4 进行meanshift追踪 ret, track_window = cv.meanShift(dst, track_window, term_crit) # 4.5 将追踪的位置绘制在视频上，并进行显示 x,y,w,h = track_window img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow(&#39;frame&#39;,img2) if cv.waitKey(60) &amp; 0xFF == ord(&#39;q&#39;): break else: break # 5. 资源释放 cap.release() cv.destroyAllWindows() 视频追踪 meanshift算法除了应用在视频追踪当中，在聚类，平滑等等各种涉及到数据以及非监督学习的场合当中均有重要应用，是一个应用广泛的算法。
图像是一个矩阵信息，如何在一个视频当中使用meanshift算法来追踪一个运动的物体呢？ 大致流程如下：
首先在图像上选定一个目标区域
计算选定区域的直方图分布，一般是HSV色彩空间的直方图。
对下一帧图像b同样计算直方图分布。
计算图像b当中与选定区域直方图分布最为相似的区域，使用meanshift算法将选定区域沿着最为相似的部分进行移动，直到找到最相似的区域，便完成了在图像b中的目标追踪。
重复3到4的过程，就完成整个视频目标追踪。
通常情况下我们使用直方图反向投影得到的图像和第一帧目标对象的起始位置，当目标对象的移动会反映到直方图反向投影图中，meanshift 算法就把我们的窗口移动到反向投影图像中灰度密度最大的区域了。
直方图反向投影的流程是：
假设我们有一张100x100的输入图像，有一张10x10的模板图像，查找的过程是这样的：
从输入图像的左上角(0,0)开始，切割一块(0,0)至(10,10)的临时图像； 生成临时图像的直方图； 用临时图像的直方图和模板图像的直方图对比，对比结果记为c； 直方图对比结果c，就是结果图像(0,0)处的像素值； 切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像； 重复1～5步直到输入图像的右下角，就形成了直方图的反向投影。 # Meanshift的API是： cv.meanShift(probImage, window, criteria) probImage: ROI区域，即目标的直方图的反向投影 window： 初始搜索窗口，就是定义ROI的rect criteria: 确定窗口搜索停止的准则，主要有迭代次数达到设置的最大值，窗口中心的漂移值大于某个设定的限值等。 import numpy as np import cv2 as cv # 1.获取图像 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 2.获取第一帧图像，并指定目标位置 ret,frame = cap.read() # 2.1 目标位置（行，高，列，宽） r,h,c,w = 197,141,0,208 track_window = (c,r,w,h) # 2.2 指定目标的感兴趣区域 roi = frame[r:r+h, c:c+w] # 3. 计算直方图 # 3.1 转换色彩空间（HSV） hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV) # 3.2 去除低亮度的值 # mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.))) # 3.3 计算直方图 roi_hist = cv.calcHist([hsv_roi],[0],None,[180],[0,180]) # 3.4 归一化 cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX) # 4. 目标追踪 # 4.1 设置窗口搜索终止条件：最大迭代次数，窗口中心漂移最小值 term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 ) while(True): # 4.2 获取每一帧图像 ret ,frame = cap.read() if ret == True: # 4.3 计算直方图的反向投影 hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # 4.4 进行meanshift追踪 ret, track_window = cv.meanShift(dst, track_window, term_crit) # 4.5 将追踪的位置绘制在视频上，并进行显示 x,y,w,h = track_window img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow(&#39;frame&#39;,img2) if cv.waitKey(60) &amp; 0xFF == ord(&#39;q&#39;): # cv2.waitKey(1)在有按键按下的时候返回按键的ASCII值，否则返回-1 # &amp; 0xFF的按位与操作只取cv2.waitKey(1)返回值最后八位，因为有些系统cv2.waitKey(1)的返回值不止八位 # ord(‘q’)表示q的ASCII值 # 总体效果：按下q键后break else: break # 5. 资源释放 cap.release() cv.destroyAllWindows() meanshift检测的窗口的大小是固定的，而狗狗由近及远是一个逐渐变小的过程，固定的窗口是不合适的。所以我们需要根据目标的大小和角度来对窗口的大小和角度进行修正。CamShift可以帮我们解决这个问题。
CamShift算法全称是“Continuously Adaptive Mean-Shift”（连续自适应MeanShift算法），是对MeanShift算法的改进算法，可随着跟踪目标的大小变化实时调整搜索窗口的大小，具有较好的跟踪效果。
Camshift算法首先应用meanshift，一旦meanshift收敛，它就会更新窗口的大小，还计算最佳拟合椭圆的方向，从而根据目标的位置和大小更新搜索窗口。
#进行camshift追踪 ret, track_window = cv.CamShift(dst, track_window, term_crit) # 绘制追踪结果 pts = cv.boxPoints(ret) pts = np.int0(pts) img2 = cv.polylines(frame,[pts],True, 255,2) 人脸识别 Haar 特征会被使用，就像我们的卷积核，每一个特征是一 个值，这个值等于黑色矩形中的像素值之后减去白色矩形中的像素值之和。
Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。
Haar特征可用于于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征。
# 训练好的检测器，包括面部，眼睛，猫脸等，都保存在XML文件中，我们可以通过以下程序找到他们： import cv2 as cv print(cv.__file__) # 实例化人脸和眼睛检测的分类器对象 # 实例化级联分类器 classifier =cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) # 加载分类器 classifier.load(&#39;haarcascade_frontalface_default.xml&#39;) # 进行人脸和眼睛的检测 rect = classifier.detectMultiScale(gray, scaleFactor, minNeighbors, minSize,maxsize) Gray: 要进行检测的人脸图像 scaleFactor: 前后两次扫描中，搜索窗口的比例系数 minneighbors：目标至少被检测到minNeighbors次才会被认为是目标 minsize和maxsize: 目标的最小尺寸和最大尺寸 import cv2 as cv import matplotlib.pyplot as plt # 1.以灰度图的形式读取图片 img = cv.imread(&#34;16.jpg&#34;) gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2.实例化OpenCV人脸和眼睛识别的分类器 face_cas = cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) face_cas.load(&#39;haarcascade_frontalface_default.xml&#39;) eyes_cas = cv.CascadeClassifier(&#34;haarcascade_eye.xml&#34;) eyes_cas.load(&#34;haarcascade_eye.xml&#34;) # 3.调用识别人脸 faceRects = face_cas.detectMultiScale( gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32)) for faceRect in faceRects: x, y, w, h = faceRect # 框出人脸 cv.rectangle(img, (x, y), (x + h, y + w),(0,255,0), 3) # 4.在识别出的人脸中进行眼睛的检测 roi_color = img[y:y+h, x:x+w] roi_gray = gray[y:y+h, x:x+w] eyes = eyes_cas.detectMultiScale(roi_gray) for (ex,ey,ew,eh) in eyes: cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) # 5. 检测结果的绘制 plt.figure(figsize=(8,6),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;检测结果&#39;) plt.xticks([]), plt.yticks([]) plt.show() # 我们也可在视频中对人脸进行检测： import cv2 as cv import matplotlib.pyplot as plt # 1.读取视频 cap = cv.VideoCapture(&#34;movie.mp4&#34;) # 2.在每一帧数据中进行人脸识别 while(cap.isOpened()): ret, frame = cap.read() if ret==True: gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # 3.实例化OpenCV人脸识别的分类器 face_cas = cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) face_cas.load(&#39;haarcascade_frontalface_default.xml&#39;) # 4.调用识别人脸 faceRects = face_cas.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32)) for faceRect in faceRects: x, y, w, h = faceRect # 框出人脸 cv.rectangle(frame, (x, y), (x + h, y + w),(0,255,0), 3) cv.imshow(&#34;frame&#34;,frame) if cv.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break # 5. 释放资源 cap.release() cv.destroyAllWindows() ]]></content></entry><entry><title>cv-RCNN</title><url>/post/cv-rcnn/</url><categories><category>cv</category></categories><tags/><content type="html">RCNN Rich feature hierarchies for accurate object detection and semantic segmentation
R-CNN存在的问题：
测试速度慢： 测试一张图片约53s(CPU)。用Selective Search:算法提取候选框用时约2秒，一张图像内候选框之间存在大量重叠，提取特征操作冗余。 训练速度慢： 过程及其繁琐 训练所需空间大： 对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16,从VOC07训练集上的5k图像上提取的特征需要数百G的存储空间。 Fast R-CNN 艾佛森括号：u&amp;gt;=1 为1 其他值为0，负样本没有边界框回归损失
Faster R-CNN 官方用的二交叉熵</content></entry><entry><title>cv-SSD</title><url>/post/cv-ssd/</url><categories><category>cv</category></categories><tags/><content type="html">原理 让图片经过卷积神经网络（VGG）提取特征，生成feature map
抽取其中六层的feature map，然后分别在这些feature map层上面的每一个点构造4个不同尺度大小的default box
然后分别进行检测和分类（各层的个数不同，但每个点都有）将生成的所有default box都集合起来，全部丢到NMS中，输出筛选后的default box。</content></entry><entry><title>cv-Swin-Transformer</title><url>/post/cv-swin-transformer/</url><categories><category>cv</category></categories><tags/><content type="html">网络整体框架 层次化构建方法（Hierarchical feature maps），比如特征图尺寸中有对图像下采样4倍的，8倍的以及16倍的，这样的backbone有助于在此基础上构建目标检测，实例分割等任务。
使用了Windows Multi-Head Self-Attention(W-MSA)的概念，比如在下图的4倍下采样和8倍下采样中，将特征图划分成了多个不相交的区域（Window），并且Multi-Head Self-Attention只在每个窗口（Window）内进行。相对于Vision Transformer中直接对整个（Global）特征图进行Multi-Head Self-Attention，这样做的目的是能够减少计算量的，尤其是在浅层特征图很大的时候。这样做虽然减少了计算量但也会隔绝不同窗口之间的信息传递，所以在论文中作者**又提出了 Shifted Windows Multi-Head Self-Attention(SW-MSA)**的概念
首先将图片输入到Patch Partition模块中进行分块，即每4x4相邻的像素为一个Patch，然后在channel方向展平（flatten）。
假设输入的是RGB三通道图片，那么每个patch就有4x4=16个像素，然后每个像素有R、G、B三个值所以展平后是16x3=48，所以通过Patch Partition后图像shape由 [H, W, 3]变成了 [H/4, W/4, 48]。
然后在通过Linear Embeding层对每个像素的channel数据做线性变换，由48变成C，即图像shape再由 [H/4, W/4, 48]变成了 [H/4, W/4, C]。
其实在源码中Patch Partition和Linear Embeding就是直接通过一个卷积层实现的，和之前Vision Transformer中讲的 Embedding层结构一模一样。
然后就是通过四个Stage构建不同大小的特征图，除了Stage1中先通过一个Linear Embeding层外，剩下三个stage都是先通过一个Patch Merging层进行下采样（后面会细讲）。然后都是重复堆叠Swin Transformer Block
注意这里的Block其实有两种结构，如图(b)中所示，这两种结构的不同之处仅在于一个使用了W-MSA结构，一个使用了SW-MSA结构。而且这两个结构是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。所以你会发现堆叠Swin Transformer Block的次数都是偶数（因为成对使用）。
最后对于分类网络，后面还会接上一个Layer Norm层、全局池化层以及全连接层得到最终输出。图中没有画，但源码中是这样做的。
Patch Mergin详解 前面有说，在每个Stage中首先要通过一个Patch Merging层进行下采样（Stage1除外）。
如下图所示，假设输入Patch Merging的是一个4x4大小的单通道特征图（feature map），Patch Merging会将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map。接着将这四个feature map在深度方向进行concat拼接，然后在通过一个LayerNorm层。最后通过一个全连接层在feature map的深度方向做线性变化，将feature map的深度由C变成C/2。通过这个简单的例子可以看出，通过Patch Merging层后，feature map的高和宽会减半，深度会翻倍。
W-MSA详解 **引入Windows Multi-head Self-Attention（W-MSA）模块是为了减少计算量。**如下图所示，左侧使用的是普通的Multi-head Self-Attention（MSA）模块，对于feature map中的每个像素（或称作token，patch）在Self-Attention计算过程中需要和所有的像素去计算。但在图右侧，在使用Windows Multi-head Self-Attention（W-MSA）模块时，首先将feature map按照MxM（例子中的M=2）大小划分成一个个Windows，然后单独对每个Windows内部进行Self-Attention。
MSA模块计算量 W-MSA模块计算量 SW-MSA详解 采用W-MSA模块时，只会在每个窗口内进行自注意力计算，所以窗口与窗口之间是无法进行信息传递的。为了解决这个问题，作者引入了Shifted Windows Multi-Head Self-Attention（SW-MSA）模块，即进行偏移的W-MSA。如下图所示，左侧使用的是刚刚讲的W-MSA（假设是第L层），那么根据之前介绍的W-MSA和SW-MSA是成对使用的，那么第L+1层使用的就是SW-MSA（右侧图）。
根据左右两幅图对比能够发现窗**口（Windows）发生了偏移（可以理解成窗口从左上角分别向右侧和下方各偏移了$\left \lfloor \frac {M} {2} \right \rfloor $个像素）。**看下偏移后的窗口（右侧图），比如对于第一行第2列的2x4的窗口，它能够使第L层的第一排的两个窗口信息进行交流。再比如，第二行第二列的4x4的窗口，他能够使第L层的四个窗口信息进行交流，其他的同理。那么这就解决了不同窗口之间无法进行信息交流的问题。
可以发现通过将窗口进行偏移后，由原来的4个窗口变成9个窗口了。后面又要对每个窗口内部进行MSA，这样做感觉又变麻烦了。为了解决这个麻烦，作者又提出而了Efficient batch computation for shifted configuration，一种更加高效的计算方法。下面是原论文给的示意图。
下图左侧是刚刚通过偏移窗口后得到的新窗口，右侧是为了方便大家理解，对每个窗口加上了一个标识。然后0对应的窗口标记为区域A，3和6对应的窗口标记为区域B，1和2对应的窗口标记为区域C。
移动完后，4是一个单独的窗口；将5和3合并成一个窗口；7和1合并成一个窗口；8、6、2和0合并成一个窗口。这样又和原来一样是4个4x4的窗口了，所以能够保证计算量是一样的。这里肯定有人会想，把不同的区域合并在一起（比如5和3）进行MSA，这信息不就乱窜了吗？是的，为了防止这个问题，在实际计算中使用的是masked MSA即带蒙板mask的MSA，这样就能够通过设置蒙板来隔绝不同区域的信息了。关于mask如何使用，可以看下下面这幅图，下图是以上面的区域5和区域3为例。
Relative Position Biasi详解 关于相对位置偏执，论文里也没有细讲，就说了参考的哪些论文，然后说使用了相对位置偏执后给够带来明显的提升。根据原论文中的表4可以看出，在Imagenet数据集上如果不使用任何位置偏执，top-1为80.1，但使用了相对位置偏执（rel. pos.）后top-1为83.3，提升还是很明显的。
模型详细配置参数 下图是原论文中给出的关于不同Swin Transformer的配置，T(Tiny)，S(Small)，B(Base)，L(Large)，其中：
win. sz. 7x7表示使用的窗口（Windows）的大小 dim表示feature map的channel深度（或者说token的向量长度） head表示多头注意力模块中head的个数</content></entry><entry><title>cv-TensorRT</title><url>/post/cv-tensorrt/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[TensorRT TensorRT支持几乎所有主流深度学习框架，将python框架转换成C++的TensorRT，从而可以加速推理。
算子融合(层与张量融合)：简单来说就是通过融合一些计算op或者去掉一些多余op来减少数据流通次数以及显存的频繁使用来提速 量化：量化即IN8量化或者FP16以及TF32等不同于常规FP32精度的使用，这些精度可以显著提升模型执行速度并且不会保持原先模型的精度 内核自动调整：根据不同的显卡构架、SM数量、内核频率等(例如1080TI和2080TI)，选择不同的优化策略以及计算方式，寻找最合适当前构架的计算方式 动态张量显存：我们都知道，显存的开辟和释放是比较耗时的，通过调整一些策略可以减少模型中这些操作的次数，从而可以减少模型运行的时间 多流执行：使用CUDA中的stream技术，最大化实现并行操作 当然，TensorRT主要缺点是与特定GPU绑定，在不同型号上转换出来的模型不能通用(这一点笔者暂未去从实践证实)
TensorRT官方在其 仓库 提供了三个开源工具，之后有需要可以使用。
三个工具大致用途[1]：
ONNX GraphSurgeon
可以修改我们导出的ONNX模型，增加或者剪掉某些节点，修改名字或者维度等等 Polygraphy
各种小工具的集合，例如比较ONNX和trt模型的精度，观察trt模型每层的输出等等，主要用-来debug一些模型的信息 PyTorch-Quantization
可以在Pytorch训练或者推理的时候加入模拟量化操作，从而提升量化模型的精度和速度，并且支持量化训练后的模型导出ONNX和TensorRT Open Neural Network Exchange（ONNX，开放神经网络交换）格式，是微软和Facebook提出用来表示深度学习模型的开放格式，定义了一组和环境，平台均无关的标准格式，可使模型在不同框架之间进行转移[2]。
典型的几个线路[3]：
Pytorch -&gt; ONNX -&gt; TensorRT Pytorch -&gt; ONNX -&gt; TVM TF -&gt; onnx -&gt; ncnn Pytorch -&gt; ONNX -&gt; tensorflow ONNX结构是将每一个网络的每一层或者每一个算子当作节点Node，再由这些Node去构建一个Graph，最后将Graph和这个onnx模型的其他信息结合在一起，生成一个model。
可以通过在线网站 https://netron.app/ 来查看ONNX模型结构。
实践上手 操作系统：Windows10 显卡：RTX2060 CUDA版本：11.6 Pytorch版本：1.7.1 Python版本：3.8 查看CUDA版本 TensortRT非常依赖CUDA版本，在安装之前，需要先查看本机安装好的CUDA版本，查看方式有多种，第一种方式可以通过NVIDIA 控制面板查看；第二种方式可以通过在控制台输入nvcc -V进行查看。
安装TensorRT 首先需要到 Nvidia官网 去下载对应Cuda版本的TensorRT安装包。
我这里下载的是红框选中的这一个，这个版本支持CUDA11.0-11.7
cd TensorRT-8.4.3.1\python pip install tensorrt-8.4.3.1-cp38-none-win_amd64.whl cd TensorRT-8.4.3.1\graphsurgeon pip install graphsurgeon-0.4.6-py2.py3-none-any.whl cd TensorRT-8.4.3.1\onnx_graphsurgeon pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl cd TensorRT-8.4.3.1\uff pip install uff-0.6.9-py2.py3-none-any.whl 然后需要移动安装包里的一些文件：
将TensorRT-8.4.3.1\include中头文件拷贝到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\include 将TensorRT-8.4.3.1\lib中所有lib文件拷贝到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\lib\x64 将TensorRT-8.4.3.1\lib中所有dll文件拷贝到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin 注：这里的v11.6根据自己的Cuda版本号即可
之后，需要手动将C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin路径添加到用户Path环境变量中
之后进行验证：
python import tensorrt print(tensorrt.__version__) 在import时发生报错，
FileNotFoundError: Could not find: nvinfer.dll. Is it on your PATH?
此时只需要将缺少的文件找到，然后添加到上面的bin目录下即可，我这里是在安装的torch中lib文件下找到的部分文件，缺什么移什么即可。
如无报错，再次验证，可以输出tensorrt版本：
下面运行安装包里面的一个sample.py文件，以确保tensorrt能够正常工作。
进入到下图所示的路径，运行sample.py，如果正常输出，则代表tensorrt安装成功。
如果提示没装pycuda，还需要再安装一下
pip install pycuda YOLOv5 使用TensorRT加速 这部分内容看到不少博客都是用Cmake编译生成yolov5的VS工程，非常繁琐麻烦，主要是这些博文写作时间较早。
而在YOLOv5 6.0版本更新后，官方新增了一个export.py文件，支持大部分框架模型的导出，包括TensorRT。
下面我所使用的是 YOLOv5官方 最新的6.2版本。
下面直接导出官方提供的yolov5s模型试试，终端输入：
python export.py --weights yolov5s.pt --data data/coco128.yaml --include engine --device 0 --half 注：这里的--half表示半精度模型，使用半精度可以加快推理速度，但会损失一定精度，直接导出可以不加
初次导出，遇到如下报错
ONNX: export failure 0.4s: Exporting the operator silu to ONNX opset version 12 is not supported.
这个报错需要修改pytorch的激活函数，找到该函数位置：D:\anaconda\envs\pytorch\Lib\sitepackages\torch\nn\modules\activation.py(此处结合自己的anaconda实际安装位置来更改)
修改代码如下：
class SiLU(Module): __constants__ = [&#39;inplace&#39;] inplace: bool def __init__(self, inplace: bool = False): super(SiLU, self).__init__() self.inplace = inplace def forward(self, input: Tensor) -&gt; Tensor: # ------------------------------------- # # 把F.silu替换掉，修改后如下 return input * torch.sigmoid(input) #原来的代码 return F.silu(input, inplace=self.inplace) 再次导出，不再报错。
值得注意的是，YOLOv5并不会直接导出TensorRT模型，而是会先导出ONNX模型，然后将ONNX模型转换成TensorRT模型，因此导出完成后，会在模型位置处生成yolov5s.onnx和yolov5s.engine，yolov5s.engine就是可以用来直接推理的TensorRT模型。
经过实测，不添加半精度导出yolov5s模型花费时间99.5s，添加半精度之后，导出yolov5s模型花费时间404.2s。
实验结果 图片检测 首先是来检测一下图片的推理速度，首先修改detect.py，统计程序花费时间。
if __name__ == &#34;__main__&#34;: begin_time = time.time() opt = parse_opt() main(opt) end_time = time.time() print(&#34;程序花费时间{}秒&#34;.format(end_time-begin_time)) 然后依次使用不同模型进行推理
python detect.py --weights yolov5s.pt python detect.py --weights yolov5s.engine python val.py --weights yolov5s.pt python val.py --weights yolov5s.engine 这里数据源选择的是coco128中128张图片，整体实验结果如下表所示：
模型 推理花费时间(s) AP50 原始模型 8.39 71.4% TensorRT(全精度) 5.45 71.1% TensorRT(半精度) 4.83 70.8% 从数据可以发现，使用TensorRT加速之后，模型推理速度提升了约35%，但是模型精度并没有下降太多。coco数据集128张图片，模型训练和检测都是用同一份数据，这可能会对AP产生一定影响，于是再换用Visdrone数据集在进行实验。
下面对Visdrone数据集进行实验，使用yolov5m模型，训练100个epoch.
使用VisDrone2019-DET-test-dev中的1610张图片进行验证和检测：
验证命令：
python val.py --data data/VisDrone.yaml --weights runs/train/exp3/weights/best.engine --batch-size 4 --task test 使用VisDrone2019-DET-test-dev中的
检测命令：
python detect.py --weights runs/train/exp3/half/best.engine --source D:/Desktop/Work/Dataset/VisDrone/VisDrone2019-DET-test-dev/images --data data/VisDrone.yaml 实验结果如下表所示：
模型 验证花费时间(s) P R AP50 推理花费时间(s) yolov5m 101.28 43.1% 34.9% 32.0% 157.51 onnx 156.16 42.8% 34.9% 32.0% 158.97 TensorRT(全精度) 124.37 42.8% 34.9% 32.0% 144.93 TensorRT(半精度) 127.85 42.8% 34.8% 31.9% 139.97 由表可见，使用TensorRT加速之后，推理速度提升约了10%，同时精度只掉了0.3%，AP50基本上变化不大。
下面选一张图片来直观对比一下，左侧图为原始模型推理图，右侧图为TensorRT(半精度) 推理图，两者大致上差异不大，各有各的漏检对象。
视频检测 视频检测用了王者荣耀数据集做一个实验，比较了常规检测，tensorrt和onnx推理速度和帧率。推理所花费的时间分别为：
常规推理 程序花费时间20.88s onnx推理 程序花费时间25.68s tensorrt推理 程序花费时间16.03s 视频帧率如下视频所示：
YOLOv5：使用TensorRT加速效果对比
综合比较来看，tensorrt的推理速度最快，并且帧率会比原本检测帧率提升约20%左右，而用onnx模型推理，速度和帧率反而会变慢，这一点我是存在一些疑问的，看到一些博客说onnx模型对比pytorch模型速度更快，但我的实验结果并非如此，个人猜测是pytorch对模型优化已经做得比较出色，onnx模型作为一个通用标准模型，为了翻译功能，损失了部分性能。
]]></content></entry><entry><title>cv-Vision Transformer(ViT)</title><url>/post/cv-vision-transformervit/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[下图是原论文中给出的关于Vision Transformer(ViT)的模型框架。简单而言，模型由三个模块组成：
Linear Projection of Flattened Patches(Embedding层) Transformer Encoder(图右侧有给出更加详细的结构) MLP Head（最终用于分类的层结构）
Embedding层结构详解 对于标准的Transformer模块，要求输入的是token（向量）序列，即二维矩阵[num_token, token_dim]。对于图像数据而言，其数据格式为[H, W, C]是三维矩阵明显不是Transformer想要的。所以需要先通过一个Embedding层来对数据做个变换。
如下图所示，首先将一张图片按给定大小分成一堆Patches。以ViT-B/16为例，将输入图片(224x224)按照16x16大小的Patch进行划分，划分后会得$(224/16)^2=196$个Patches。接着通过线性映射将每个Patch映射到一维向量中，以ViT-B/16为例，每个Patche数据shape为[16, 16, 3]通过映射得到一个长度为768的向量（后面都直接称为token）。[16, 16, 3] -&gt; [768]
在代码实现中，直接通过一个卷积层来实现。 以ViT-B/16为例，直接使用一个卷积核大小为16x16，步距为16，卷积核个数为768的卷积来实现。通过卷积[224, 224, 3] -&gt; [14, 14, 768]，然后把H以及W两个维度展平即可[14, 14, 768] -&gt; [196, 768]，此时正好变成了一个二维矩阵，正是Transformer想要的。在输入Transformer Encoder之前注意需要加上[class]token以及Position Embedding。 在原论文中，作者说参考BERT，在刚刚得到的一堆tokens中插入一个专门用于分类的[class]token，这个[class]token是一个可训练的参数，数据格式和其他token一样都是一个向量，以ViT-B/16为例，就是一个长度为768的向量，与之前从图片中生成的tokens拼接在一起，Cat([1, 768], [196, 768]) -&gt; [197, 768]。然后关于Position Embedding就是之前Transformer中讲到的Positional Encoding，这里的Position Embedding采用的是一个可训练的参数（1D Pos. Emb.），是直接叠加在tokens上的（add），所以shape要一样。以ViT-B/16为例，刚刚拼接[class]token后shape是[197, 768]，那么这里的Position Embedding的shape也是[197, 768]。 MLP Head详解 上面通过Transformer Encoder后输出的shape和输入的shape是保持不变的，以ViT-B/16为例，输入的是[197, 768]输出的还是[197, 768]。
注意，在Transformer Encoder后其实还有一个Layer Norm没有画出来，后面有我自己画的ViT的模型可以看到详细结构。这里我们只是需要分类的信息，所以我们只需要提取出[class]token生成的对应结果就行，即[197, 768]中抽取出[class]token对应的[1, 768]。接着我们通过MLP Head得到我们最终的分类结果。MLP Head原论文中说在训练ImageNet21K时是由Linear+tanh激活函数+Linear组成。但是迁移到ImageNet1K上或者你自己的数据上时，只用一个Linear即可。
Hybrid模型详解 在论文4.1章节的Model Variants中有比较详细的讲到Hybrid混合模型，就是将传统CNN特征提取和Transformer进行结合。
下图绘制的是以ResNet50作为特征提取器的混合模型，但这里的Resnet与之前讲的Resnet有些不同。首先这里的R50的卷积层采用的StdConv2d不是传统的Conv2d，然后将所有的BatchNorm层替换成GroupNorm层。在原Resnet50网络中，stage1重复堆叠3次，stage2重复堆叠4次，stage3重复堆叠6次，stage4重复堆叠3次，但在这里的R50中，把stage4中的3个Block移至stage3中，所以stage3中共重复堆叠9次。
通过R50 Backbone进行特征提取后，得到的特征矩阵shape是[14, 14, 1024]，接着再输入Patch Embedding层，注意Patch Embedding中卷积层Conv2d的kernel_size和stride都变成了1，只是用来调整channel。后面的部分和前面ViT中讲的完全一样，就不在赘述。
下表是论文用来对比ViT，Resnet（和刚刚讲的一样，使用的卷积层和Norm层都进行了修改）以及Hybrid模型的效果。通过对比发现，在训练epoch较少时Hybrid优于ViT，但当epoch增大后ViT优于Hybrid。
ViT模型搭建参数 在论文的Table1中有给出三个模型（Base/ Large/ Huge）的参数，在源码中除了有Patch Size为16x16的外还有32x32的。其中的Layers就是Transformer Encoder中重复堆叠Encoder Block的次数，Hidden Size就是对应通过Embedding层后每个token的dim（向量的长度），MLP size是Transformer Encoder中MLP Block第一个全连接的节点个数（是Hidden Size的四倍），Heads代表Transformer中Multi-Head Attention的heads数。
Model Patch Size Layers Hidden Size D MLP size Heads Params ViT-Base 16x16 12 768 3072 12 86M ViT-Large 16x16 24 1024 4096 16 307M ViT-Huge 14x14 32 1280 5120 16 632M ]]></content></entry><entry><title>cv-yolo</title><url>/post/cv-yolo/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[YOLO vs Faster R-CNN Faster R-CNN将检测结果分为两部分求解：物体类别（分类问题）、物体位置即bounding box（回归问题），YOLO统一为一个回归问题。
统一网络：YOLO没有显示求取region proposal的过程。Faster R-CNN中尽管RPN与fast rcnn共享卷积层，但是在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。相对于R-CNN系列的&quot;看两眼&quot;(候选框提取与分类)，YOLO只需要Look Once.
YOLOv1论文名以及论文地址：You Only Look Once:Unified, Real-Time Object Detection、 You Only Look Once:Unified, Real-Time Object Detection: https://arxiv.org/pdf/1506.02640.pdf YOLOv1开源代码：YOLOv1-Darkent YOLOv1-Darkent: https://github.com/pjreddie/darknet YOLOv2论文名以及论文地址：YOLO9000:Better, Faster, Stronger YOLO9000:Better, Faster, Stronger: https://arxiv.org/pdf/1612.08242v1.pdf YOLOv2开源代码：YOLOv2-Darkent YOLOv2-Darkent: https://github.com/pjreddie/darknet YOLOv3论文名以及论文地址：YOLOv3: An Incremental Improvement YOLOv3: An Incremental Improvement: https://arxiv.org/pdf/1804.02767.pdf YOLOv3开源代码：YOLOv3-PyTorch YOLOv3-PyTorch: https://github.com/ultralytics/yolov3 YOLOv4论文名以及论文地址：YOLOv4: Optimal Speed and Accuracy of Object Detection YOLOv4: Optimal Speed and Accuracy of Object Detection: https://arxiv.org/pdf/2004.10934.pdf YOLOv4开源代码：YOLOv4-Darkent YOLOv4-Darkent: https://github.com/AlexeyAB/darknet YOLOv5论文名以及论文地址：无 YOLOv5开源代码：YOLOv5-PyTorch YOLOv5-PyTorch: https://github.com/ultralytics/yolov5 YOLOx论文名以及论文地址：YOLOX: Exceeding YOLO Series in 2021 YOLOX: Exceeding YOLO Series in 2021: https://arxiv.org/pdf/2107.08430.pdf YOLOx开源代码：YOLOx-PyTorch YOLOx-PyTorch: https://github.com/Megvii-BaseDetection/YOLOX YOLOv6论文名以及论文地址：YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications: https://arxiv.org/pdf/2209.02976.pdf YOLOv6开源代码：YOLOv6-PyTorch YOLOv6-PyTorch: https://github.com/meituan/YOLOv6 YOLOv7论文名以及论文地址：YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors: https://arxiv.org/pdf/2207.02696.pdf YOLOv7开源代码：Official YOLOv7-PyTorch Official YOLOv7-PyTorch: https://github.com/WongKinYiu/yolov7 V1 you only look once:unified, real-time object detection
思想 yolo的核心思想是将输入的图像经过backbone特征提取后，将的到的特征图划分为S x S的网格，物体的中心落在哪一个网格内，这个网格就负责预测该物体的置信度、类别以及坐标位置。
将一幅图像分成SxS个网格(grid cell),如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。
每个网格要预测B个bounding box,每个bounding box除了要预测位置之外，还要附带预测一个confidence值。每个网格还要预测C个类别的分数。
​ 该表达式含义：如果有object落在一个grid cell里，则第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。
B=2， 7*7*30包含了坐标、置信度、类别结果
x.y是相对每一个gird cell左上角点的坐标，w,h是相对整幅图像的宽高，都是0-1的值。
网络结构 最后一层全连接层用线性激活函数，其余层采用 Leaky ReLU。
后处理 2个框重合度很高，大概率是一个目标，那就只取一个框。
首先从所有的检测框中找到置信度最大的那个，然后遍历剩余的框，计算其与最大框之间的IOU。如果其值大于一定阈值，则表示重合度过高，那么就将该框就会被剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。
损失函数 损失函数有三个部分组成，分别是边框损失，置信度损失，以及类别损失。并且三个损失函数都使用均方差损失函数(MSE)。
只有当某个网格中有object的时候才对classification error进行惩罚。
只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大
紫色的框是当gird cell有真实框的中心点的时候取1，否则取0
红色的框为第i个gird cell第j个bounding box是负责预测物体，是为1否则为0
绿色的框呢是第i个gird cell第j个bounding box不负责预测物体为1否则为0
再说两个λ，对于负责预测物体的框呢要严重惩罚所以赋值为５，不负责的呢意思意思为０.５再说每一项，
第一项是中心点定位误差，
第二项是宽高误差，开根号主要是为了惩罚小框，对大框公平一点
第三项是置信度误差，标签是通过计算这个框与ground truth的iou
最后一项是类别预测误差，这个概率就是条件概率乘以置信度，所得到的20维度的类别概率
宽和高开根号，是因为偏移相同的距离，对小目标影响更大
缺陷 YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类； 准确度低，recall低 同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱； 由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。 v2 YOLOv2 也叫 YOLO9000，因为使用了 COCO 数据集以及 Imagenet 数据集来联合训练，最终可以检测9000个类别。
思想 使用 Darknet19 作为网络的主干网络。Darknet19 有点类似 VGG，在 Darknet19 中，使用的是 3 x 3 大小的卷积核，并且在每次Pooling 之后都增加一倍通道数，以及将特征图的宽高缩减为原来的一半。网络中有19个卷积层，所以叫 Darknet19，以及有5个 Max Pooling 层，所以这里进行了32倍的下采样。
网络结构 使用 Darknet19 作为网络的主干网络。Darknet19 有点类似 VGG，在 Darknet19 中，使用的是 3 x 3 大小的卷积核，并0且在每次Pooling 之后都增加一倍通道数，以及将特征图的宽高缩减为原来的一半。网络中有19个卷积层，所以叫 Darknet19，以及有5个 Max Pooling 层，所以这里进行了32倍的下采样。
改进 Batch Normalization 在卷积层的后面、激活函数的前面加上了BN层，
使得网络可以加速收敛，
解决了梯度弥散的问题。
不太受初始化的影响，
还可以起到正则化的作用。
零均值标准差为1，sigmod、双曲正切函数(tanh)在0附近有比较大的梯度
高分辨率分类器 High Resolution Classifier
先来看看yolov1是怎么做的，首先让ImageNet的图像resize到224*224训练特征提取层，使用更高的448*448的分辨率训练检测头
yolov2避免了这个突变，首先对224*224的预训练之后，然后在448*448的分辨率上微调10个epoch，这样的过度使得mAP又增加了3.7个百分点
Anchor Convolutional With Anchor Boxes
先受Faster-rcnn的启发，使用锚框然后预测它的偏移量，而不是直接预测坐标位置，这样网络学习起来会更容易。所谓锚框，就是之前设定好不同宽高比的先验框
为了更好的理解anchor，我们先来回忆一下yolov1没有anchor的时候是怎么回事？四个坐标呢完全就是由网络计算出来的，(x, y)坐标表示相对于网格单元格边界的方框中心，宽高是相对于整个图像的宽度和高度，这个值生成的宽高比、中心点的位置幔帐图像乱跑的，明显更难预测。
作者输入的是416*416，而不是448*448，因为下采样32倍之后416*416的图像是13*13，它是一个奇数，为什么我们想要奇数呢？因为图片的中心往往存在大物体，作者希望有一个单独的grid cell来负责预测这个物体，而不是周围的4个
作者将原图划分为13*13个gird cell，每个grid cell会有5个预测框，13*13*5这个数远远大于v1中的7*7*2，所以作者去掉了全连接层，参数太多，计算太慢
但是添加了锚框之后mAP下降了0.2%，这依然是一个很好的改进，因为他解决了V1的召回率低的问题，使recall增加到了88%
聚类anchor Dimension Clusters
先验框先验框，这个宽高比该怎么定呢，该选择几个锚框呢，3个、5个？手动选择吗？显然不是那么合理，所以作何采用了对训练集的bounding_box进行k_means聚类的方法。
Direct location prediction 虽然说现在使用了锚框，但是它的中心点还是可以乱跑的。作者对边框进行了限制，
保证锚框的中心点只能在负责预测的这个grid cell里面（通过sigmoid函数映射到0-1），宽高不设限制。
这里的cx,cy是经过归一化了的，一个cx,cy是1，也就是如果要可视化在原图上还要乘以13
Fine-Grained Features 类似 Pixel-shuffle，**融合高层和低层的信息，这样可以保留一些细节信息，这样可以更好检测小物体。**具体来说，就是进行一拆四的操作，直接传递到池化后的特征图中，进行卷积后再叠加两者，最后一起作为输出特征图进行输出。通过使用 Pass through 层来检测细粒度特征使 mAP 提升了1个点。
多尺度训练 Multi-Scale Training
由于这个模型只有卷积层、池化层，所以可以动态的调整输入图像的大小，为了让网络适应不同的尺度的图像，每10个batch，网络随机选择一个新的图像维度大小，他们都是32的倍数,这一步是在检测数据集上fine tune时候采用的
损失函数 V3 网络结构 上图三个蓝色方框内表示Yolov3的三个基本组件：
CBL：Yolov3网络结构中的最小组件，由Conv+Bn+Leaky_relu激活函数三者组成。 Res unit：借鉴Resnet网络中的残差结构，让网络可以构建的更深。 ResX：由一个CBL和X个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19大小。 其他基础操作：
**Concat：**张量拼接，会扩充两个张量的维度，例如26*26*256和26*26*512两个张量拼接，结果是26*26*768。Concat和cfg文件中的route功能一样。 **add：**张量相加，张量直接相加，不会扩充维度，例如104*104*128和104*104*128相加，结果还是104*104*128。add和cfg文件中的shortcut功能一样。 每个ResX中包含1+2*X个卷积层，因此整个主干网络Backbone中一共包含1+（1+2*1）+（1+2*2）+（1+2*8）+（1+2*8）+（1+2*4）=52，再加上一个FC全连接层，即可以组成一个Darknet53分类网络。不过在目标检测Yolov3中，去掉FC层，不过为了方便称呼，仍然把Yolov3的主干网络叫做Darknet53结构。
卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid。
上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。
原Darknet53中的尺寸是在图片分类训练集上训练的，所以输入的图像尺寸是256x256，下图是以YOLO v3 416模型进行绘制的，所以输入的尺寸是416x416，预测的三个特征层大小分别是52，26，13。
在上图中我们能够很清晰的看到三个预测层分别来自的什么地方，以及Concatenate层与哪个层进行深度方向拼接（FPN对应维度上相加）。注意Convolutional是指Conv2d+BN+LeakyReLU，和Darknet53图中的一样，而生成预测结果的最后三层都只是Conv2d。
目标边界框的预测 损失函数 V3 SPP 而Yolov3和Yolov3_spp的不同点在于，Yolov3的主干网络后面，添加了spp组件，这里需要注意。
V4 YOLOv4 : Optimal Speed and Accuracy of Object Detection
网络结构 先整理下Yolov4的五个基本组件：
**CBM：**Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。 **CBL：**由Conv+Bn+Leaky_relu激活函数三者组成。 **Res unit：**借鉴Resnet网络中的残差结构，让网络可以构建的更深。 **CSPX：**借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。 **SPP：**采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。 其他基础操作：
**Concat：**张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。 **add：**张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。 Backbone中卷积层的数量：
和Yolov3一样，再来数一下Backbone里面的卷积层数量。
每个CSPX中包含5+2*X个卷积层，因此整个主干网络Backbone中一共包含1+（5+2*1）+（5+2*2）+（5+2*8）+（5+2*8）+（5+2*4）=72。
Backbone: CSPDarknet53 Neck: SPP，PAN Head: YOLOv3
相比之前的YOLOv3，改进了下Backbone，在Darknet53中引入了CSP模块（来自CSPNet）。在Neck部分，采用了SPP模块（Ultralytics版的YOLOv3 SPP就使用到了）以及PAN模块（来自PANet）。Head部分没变还是原来的检测头。
CSPDarknet53网络结构 CSPDarknet53就是将CSP结构融入了Darknet53中。CSP结构是在CSPNet（Cross Stage Partial Network）论文中提出的，CSPNet作者说在目标检测任务中使用CSP结构有如下好处：
Strengthening learning ability of a CNN Removing computational bottlenecks Reducing memory costs 即减少网络的计算量以及对显存的占用，同时保证网络的能力不变或者略微提升。CSP结构的思想参考原论文中绘制的CSPDenseNet，进入每个stage（一般在下采样后）先将数据划分成俩部分，如下图所示的Part1和Part2。但具体怎么划分呢，在CSPNet中是直接按照通道均分，但在YOLOv4网络中是通过两个1x1的卷积层来实现的。在Part2后跟一堆Blocks然后在通过1x1的卷积层（图中的Transition），接着将两个分支的信息在通道方向进行Concat拼接，最后再通过1x1的卷积层进一步融合（图中的Transition）。
CSPDarknet53详细结构（以输入图片大小为416 × 416 × 3 为例）
注意，CSPDarknet53 Backbone中所有的激活函数都是Mish激活函数
网络详细结构 改进 YoloV4的创新之处进行讲解，让大家一目了然。
输入端：这里指的创新主要是训练时对输入端的改进，主要包括Mosaic数据增强、cmBN、SAT自对抗训练 **BackBone主干网络：**将各种新的方式结合起来，包括：CSPDarknet53、Mish激活函数、Dropblock Neck：目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构 Prediction：输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms 输入端 这里指的创新主要是训练时对输入端的改进，主要包括Mosaic数据增强、cmBN、SAT自对抗训练
Mosaic数据增强 Yolov4中使用的Mosaic是参考2019年底提出的CutMix数据增强的方式，但CutMix只使用了两张图片进行拼接，而Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。
主要有几个优点：
丰富数据集：随机使用4张图片，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。 **减少GPU：**可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU，因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。 Backbone CSPDarknet53 CSPDarknet53是在Yolov3主干网络Darknet53的基础上，借鉴2019年CSPNet的经验，产生的Backbone结构，其中包含了5个CSP模块。
每个CSP模块前面的卷积核的大小都是3*3，stride=2，因此可以起到下采样的作用。因为Backbone有5个CSP模块，输入图像是608*608，所以特征图变化的规律是：608-&gt;304-&gt;152-&gt;76-&gt;38-&gt;19.经过5次CSP模块后得到19*19大小的特征图。
CSPNet论文地址： https://arxiv.org/pdf/1911.11929.pdf CSPNet全称是Cross Stage Paritial Network，CSPNet的作者认为推理计算过高的问题是由于网络优化中的梯度信息重复导致的。因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过跨阶段层次结构将它们合并，在减少了计算量的同时可以保证准确率。
Mish激活函数 而且作者只在Backbone中采用了Mish激活函数，网络后面仍然采用Leaky_relu激活函数。
Dropblock Yolov4中使用的Dropblock，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。
Dropblock在2018年提出，论文地址： https://arxiv.org/pdf/1810.12890.pdf 传统的Dropout很简单，一句话就可以说的清：随机删除减少神经元的数量，使网络变得更简单。
中间Dropout的方式会随机的删减丢弃一些信息，但Dropblock的研究者认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：卷积+激活+池化层，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到相同的信息。
因此，在全连接层上效果很好的Dropout在卷积层上效果并不好。
所以右图Dropblock的研究者则干脆整个局部区域进行删减丢弃。
这种方式其实是借鉴2017年的cutout数据增强的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程线性的增加这个比率。
Dropblock的研究者与Cutout进行对比验证时，发现有几个特点：
**优点一：**Dropblock的效果优于Cutout
**优点二：**Cutout只能作用于输入层，而Dropblock则是将Cutout应用到网络中的每一个特征图上
**优点三：**Dropblock可以定制各种组合，在训练的不同阶段可以修改删减的概率，从空间层面和时间层面，和Cutout相比都有更精细的改进。
Yolov4中直接采用了更优的Dropblock，对网络的正则化过程进行了全面的升级改进。
Neck 目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构
SPP模块 第一个预测特征层添加了SPP（Spatial Pyramid Pooling）模块，实现了不同尺度的特征融合 注意：这里的SPP和SPPnet中的SPP结构不一样
SPP模块，其实在Yolov3中已经存在了，在Yolov4的C++代码文件夹中有一个Yolov3_spp版本，但有的同学估计从来没有使用过，在Yolov4中，SPP模块仍然是在Backbone主干网络之后：
作者在SPP模块中，使用**k={1*1,5*5,9*9,13*13}**的最大池化的方式，再将不同尺度的特征图进行Concat操作。
注意：这里最大池化采用padding操作，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，padding=2，因此池化后的特征图仍然是13×13大小。
FPN+PAN PAN结构比较有意思，看了网上Yolov4关于这个部分的讲解，大多都是讲的比较笼统的，而PAN是借鉴 图像分割领域PANet 的创新点，有些同学可能不是很清楚。
下面大白将这个部分拆解开来，看下Yolov4中是如何设计的。
Yolov3结构：
我们先来看下Yolov3中Neck的FPN结构
可以看到经过几次下采样，三个紫色箭头指向的地方，输出分别是76*76、38*38、19*19。
以及最后的Prediction中用于预测的三个特征图①19*19*255、②38*38*255、③76*76*255。[注：255表示80类别(1+4+80)×3=255]
我们将Neck部分用立体图画出来，更直观的看下两部分之间是如何通过FPN结构融合的。
如图所示，FPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。
Yolov4结构：
而Yolov4中Neck这部分除了使用FPN外，还在此基础上使用了PAN结构：
前面CSPDarknet53中讲到，每个CSP模块前面的卷积核都是3*3大小，步长为2，相当于下采样操作。
因此可以看到三个紫色箭头处的特征图是76*76、38*38、19*19。
以及最后Prediction中用于预测的三个特征图：①76*76*255，②38*38*255，③19*19*255。
我们也看下Neck部分的立体图像，看下两部分是如何通过FPN+PAN结构进行融合的。
和Yolov3的FPN层不同，Yolov4在FPN层的后面还添加了一个自底向上的特征金字塔。
其中包含两个PAN结构。
这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合,这样的操作确实很皮。
FPN+PAN借鉴的是18年CVPR的PANet，当时主要应用于图像分割领域，但Alexey将其拆分应用到Yolov4中，进一步提高特征提取的能力。
不过这里需要注意几点：
注意一：
Yolov3的FPN层输出的三个大小不一的特征图①②③直接进行预测
但Yolov4的FPN层，只使用最后的一个76*76特征图①，而经过两次PAN结构，输出预测的特征图②和③。
这里的不同也体现在cfg文件中，这一点有很多同学之前不太明白，
比如Yolov3.cfg最后的三个Yolo层，
第一个Yolo层是最小的特征图19*19，mask=6,7,8，对应最大的anchor box。
第二个Yolo层是中等的特征图38*38，mask=3,4,5，对应中等的anchor box。
第三个Yolo层是最大的特征图76*76，mask=0,1,2，对应最小的anchor box。
而Yolov4.cfg则恰恰相反
第一个Yolo层是最大的特征图76*76，mask=0,1,2，对应最小的anchor box。
第二个Yolo层是中等的特征图38*38，mask=3,4,5，对应中等的anchor box。
第三个Yolo层是最小的特征图19*19，mask=6,7,8，对应最大的anchor box。
注意点二：
原本的PANet网络的PAN结构中，两个特征图结合是采用shortcut操作，而Yolov4中则采用**concat（route）**操作，特征图融合后的尺寸发生了变化。
Prediction 输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms
目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。
Bounding Box Regeression的Loss近些年的发展过程是：
Smooth L1 Loss
IoU Loss（2016）
GIoU Loss（2019）
DIoU Loss（2020）
CIoU Loss（2020）
CIOU_Loss IOU_Loss 可以看到IOU的loss其实很简单，主要是交集/并集，但其实也存在两个问题。
问题1：即状态1的情况，当预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。
问题2：即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。
GIOU_Loss
可以看到右图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。
但为什么仅仅说缓解呢？因为还存在一种不足：
问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。
DIOU_Loss 好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。
针对IOU和GIOU存在的问题，作者从两个方面进行考虑
一：如何最小化预测框和目标框之间的归一化距离？
二：如何在预测框和目标框重叠时，回归的更准确？
针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）
DIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。
但就像前面好的目标框回归函数所说的，没有考虑到长宽比。
比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。
但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。
CIOU_Loss CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。
这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。
再来综合的看下各个Loss函数的不同点：
**IOU_Loss：**主要考虑检测框和目标框重叠面积。
**GIOU_Loss：**在IOU的基础上，解决边界框不重合时的问题。
**DIOU_Loss：**在IOU和GIOU的基础上，考虑边界框中心点距离的信息。
**CIOU_Loss：**在DIOU的基础上，考虑边界框宽高比的尺度信息。
Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。
DIOU_nms Nms主要用于预测框的筛选，常用的目标检测算法中，一般采用普通的nms的方式，Yolov4则借鉴上面D/CIOU loss的论文： https://arxiv.org/pdf/1911.08287.pdf 将其中计算IOU的部分替换成DIOU的方式：
再来看下实际的案例
在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。因此在重叠目标的检测中，DIOU_nms的效果优于传统的nms。
注意：有读者会有疑问，这里为什么不用CIOU_nms，而用DIOU_nms?
**答：**因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。
**总体来说，**YOLOv4的论文称的上良心之作，将近几年关于深度学习领域最新研究的改进s移植到Yolov4中做验证测试，将Yolov3的精度提高了不少。虽然没有全新的创新，但很多改进之处都值得借鉴，借用Yolov4作者的总结。
Yolov4 主要带来了 3 点新贡献：
（1）提出了一种高效而强大的目标检测模型，使用 1080Ti 或 2080Ti 就能训练出超快、准确的目标检测器。
（2）在检测器训练过程中，验证了最先进的一些研究成果对目标检测器的影响。
（3）改进了 SOTA 方法，使其更有效、更适合单 GPU 训练。
优化策略 有关训练Backbone时采用的优化策略就不讲了有兴趣自己看下论文的4.2章节，这里直接讲下训练检测器时作者采用的一些方法。在论文4.3章节，作者也罗列了一堆方法，并做了部分消融实验。这里我只介绍确实在代码中有使用到的一些方法。
V5 网络结构 Yolov5s网络最小，速度最快，AP精度也最低。但如果检测的以大目标为主，追求速度，倒也是个不错的选择。
大家可能对Yolov3比较熟悉，因此大白列举它和Yolov3的一些主要的不同点，并和Yolov4进行比较。
**（1）输入端：**Mosaic数据增强、自适应锚框计算、自适应图片缩放 **（2）Backbone：**Focus结构，CSP结构 **（3）Neck：**FPN+PAN结构 **（4）Prediction：**GIOU_Loss
改进 输入端 Mosaic数据增强 Mosaic数据增强则采用了4张图片，随机缩放、随机裁剪、随机排布的方式进行拼接。
自适应锚框计算 在Yolo算法中，针对不同的数据集，都会有初始设定长宽的锚框。在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新，迭代网络参数。
因此初始锚框也是比较重要的一部分，比如Yolov5在Coco数据集上初始设定的锚框：
在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。
但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。
当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能关闭。
自适应图片缩放 在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。
比如Yolo算法中常用416*416，608*608等尺寸，比如对下面800*600的图像进行缩放。
作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像自适应的添加最少的黑边。
图像高度上两端的黑边变少了，在推理时，计算量也会减少，即目标检测速度会得到提升。
这种方式在之前github上Yolov3中也进行了讨论：
https://github.com/ultralytics/yolov3/issues/232 在讨论中，通过这种简单的改进，推理速度得到了37%的提升，可以说效果很明显。
但是有的同学可能会有大大的问号？？如何进行计算的呢？大白按照Yolov5中的思路详细的讲解一下，在datasets.py的letterbox函数中也有详细的代码。
此外，需要注意的是：
这里大白填充的是黑色，即**（0，0，0），而Yolov5中填充的是灰色，即（114,114,114）**，都是一样的效果。
训练时没有采用缩减黑边的方式，还是采用传统填充的方式，即缩放到416*416大小。只是在测试，使用模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度。
为什么np.mod函数的后面用32？因为Yolov5的网络经过5次下采样，而2的5次方，等于32。所以至少要去掉32的倍数，再进行取余。
Backbone Focus结构 具体操作为把一张feature map每隔一个像素拿到一个值，类似于邻近下采样，这样我们就拿到了4张feature map
在减少特征信息损失的情况下，减少特征图尺寸的大小提高计算力。通道多少对计算量影响不大
切片顺序不同 focus列优先 passthrough行优先
Focus结构，在Yolov3&amp;Yolov4中并没有这个结构，其中比较关键是切片操作。比如右图的切片示意图，4*4*3的图像切片后变成2*2*12的特征图。以Yolov5s的结构为例，原始608*608*3的图像输入Focus结构，采用切片操作，先变成304*304*12的特征图，再经过一次32个卷积核的卷积操作，最终变成304*304*32的特征图。
需要注意的是：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加，先注意下，后面会讲解到四种结构的不同点。
CSP结构 Yolov4网络结构中，借鉴了CSPNet的设计思路，在主干网络中设计了CSP结构。Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。
而Yolov5中设计了两种CSP结构，以Yolov5s网络为例，CSP1_X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。
Neck FPN+PAN Yolov5现在的Neck和Yolov4中一样，都采用FPN+PAN的结构，但在Yolov5刚出来时，只使用了FPN结构，后面才增加了PAN结构，此外网络中其他部分也进行了调整。
但如上面CSPNet结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。
Prediction CIOU_Loss—Bounding box损失函数 Yolov5中采用其中的CIOU_Loss做Bounding box的损失函数。
DIOU_nms 在目标检测的后处理过程中，针对很多目标框的筛选，通常需要nms操作。
因为CIOU_Loss中包含影响因子v，涉及groudtruth的信息，而测试推理时，是没有groundtruth的。所以Yolov4在DIOU_Loss的基础上采用DIOU_nms的方式，而Yolov5中采用加权nms的方式。
可以看出，采用DIOU_nms，下方中间箭头的黄色部分，原本被遮挡的摩托车也可以检出。
Yolov5四种网络结构的不同点 Yolov5代码中的四种网络，和之前的Yolov3，Yolov4中的cfg文件不同，都是以yaml的形式来呈现。
而且四个文件的内容基本上都是一样的，只有最上方的depth_multiple和width_multiple两个参数不同，很多同学看的一脸懵逼，不知道只通过两个参数是如何控制四种结构的？
Yolov5四种网络的深度 在上图中，大白画了两种CSP结构，CSP1和CSP2，其中CSP1结构主要应用于Backbone中，CSP2结构主要应用于Neck中。
需要注意的是，四种网络结构中每个CSP结构的深度都是不同的。
以yolov5s为例，第一个CSP1中，使用了1个残差组件，因此是CSP1_1。而在Yolov5m中，则增加了网络的深度，在第一个CSP1中，使用了2个残差组件，因此是CSP1_2。而Yolov5l中，同样的位置，则使用了3个残差组件，Yolov5x中，使用了4个残差组件。其余的第二个CSP1和第三个CSP1也是同样的原理。
在第二种CSP2结构中也是同样的方式，以第一个CSP2结构为例，Yolov5s组件中使用了2×X=2×1=2个卷积，因为Ｘ=1，所以使用了1组卷积，因此是CSP2_1。而Yolov5m中使用了2组，Yolov5l中使用了3组，Yolov5x中使用了4**组。**其他的四个CSP2结构，也是同理。
Yolov5中，网络的不断加深，也在不断增加网络特征提取和特征融合的能力。
控制四种网络结构的核心代码是yolo.py中下面的代码，存在两个变量，n和gd。
我们再将n和gd带入计算，看每种网络的变化结果。
Yolov5四种网络的宽度 如上图表格中所示，四种yolov5结构在不同阶段的卷积核的数量都是不一样的，因此也直接影响卷积后特征图的第三维度，即厚度，大白这里表示为网络的宽度。
以Yolov5s结构为例，第一个Focus结构中，最后卷积操作时，卷积核的数量是32个，因此经过Focus结构，特征图的大小变成304*304*32。而yolov5m的Focus结构中的卷积操作使用了48个卷积核，因此Focus结构后的特征图变成304*304*48。yolov5l，yolov5x也是同样的原理。
第二个卷积操作时，yolov5s使用了64个卷积核，因此得到的特征图是152*152*64。而yolov5m使用96个特征图，因此得到的特征图是152*152*96。yolov5l，yolov5x也是同理。
后面三个卷积下采样操作也是同样的原理，这样大白不过多讲解。
四种不同结构的卷积核的数量不同，这也直接影响网络中，比如CSP1，CSP2等结构，以及各个普通卷积，卷积操作时的卷积核数量也同步在调整，影响整体网络的计算量。当然卷积核的数量越多，特征图的厚度，即宽度越宽，网络提取特征的学习能力也越强。
在yolov5的代码中，控制宽度的核心代码是yolo.py文件里面的这一行：
损失函数 code 和YOLOv4对比，其实YOLOv5在Backbone部分没太大变化。但是YOLOv5在v6.0版本后相比之前版本有一个很小的改动，把网络的第一层（原来是Focus模块）换成了一个6x6大小的卷积层。两者在理论上其实等价的，但是对于现有的一些GPU设备（以及相应的优化算法）使用6x6大小的卷积层比使用Focus模块更加高效。详情可以参考这个issue #4825。下图是原来的Focus模块(和之前Swin Transformer中的Patch Merging类似)，将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map，然后在接上一个3x3大小的卷积层。这和直接使用一个6x6大小的卷积层等效。
在Neck部分的变化还是相对较大的，首先是将SPP换成成了SPPF（Glenn Jocher自己设计的），这个改动我个人觉得还是很有意思的，两者的作用是一样的，但后者效率更高。SPPF比SPP计算速度快了不止两倍。SPP结构如下图所示，是将输入并行通过多个不同大小的MaxPool，然后做进一步融合，能在一定程度上解决目标多尺度问题。
SPPF结构是将输入串行通过多个5x5大小的MaxPool层，这里需要注意的是串行两个5x5大小的MaxPool层是和一个9x9大小的MaxPool层计算结果是一样的，串行三个5x5大小的MaxPool层是和一个13x13大小的MaxPool层计算结果是一样的。
在Neck部分另外一个不同点就是New CSP-PAN了，在YOLOv4中，Neck的PAN结构是没有引入CSP结构的，但在YOLOv5中作者在PAN结构中加入了CSP。每个C3模块里都含有CSP结构。在Head部分，YOLOv3, v4, v5都是一样的
数据增强 这里简单罗列部分方法：
Mosaic，将四张图片拼成一张图片，讲过很多次了 Random affine(Rotation, Scale, Translation and Shear)，随机进行仿射变换，但根据配置文件里的超参数发现只使用了Scale和Translation即缩放和平移。 MixUp，就是将两张图片按照一定的透明度融合在一起，具体有没有用不太清楚，毕竟没有论文，也没有消融实验。代码中只有较大的模型才使用到了MixUp，而且每次只有10%的概率会使用到。 Albumentations，主要是做些滤波、直方图均衡化以及改变图片质量等等，我看代码里写的只有安装了albumentations包才会启用，但在项目的requirements.txt文件中albumentations包是被注释掉了的，所以默认不启用。 Augment HSV(Hue, Saturation, Value)，随机调整色度，饱和度以及明度。 Random horizontal flip，随机水平翻转 X 网络结构 **输入端：**Strong augmentation数据增强
**BackBone主干网络：**主干网络没有什么变化，还是Darknet53。
**Neck：**没有什么变化，Yolov3 baseline的Neck层还是FPN结构。
**Prediction：**Decoupled Head、End-to-End YOLO、Anchor-free、Multi positives。
改进 输入端 在网络的输入端，Yolox主要采用了Mosaic、Mixup两种数据增强方法。
而采用了这两种数据增强，直接将Yolov3 baseline，提升了2.4个百分点。
Mosaic数据增强 Mosaic增强的方式，是U版YOLOv3引入的一种非常有效的增强策略。而且在Yolov4、Yolov5算法中，也得到了广泛的应用。通过随机缩放、随机裁剪、随机排布的方式进行拼接，对于小目标的检测效果提升，还是很不错的。
MixUp数据增强 调整透明度两张图像叠加在一起。
主要来源于2017年，顶会ICLR的一篇论文《mixup: Beyond Empirical Risk Minimization》。当时主要应用在图像分类任务中，可以在几乎无额外计算开销的情况下，稳定提升1个百分点的分类精度。而在Yolox中，则也应用到目标检测中，代码在yolox/datasets/mosaicdetection.py这个文件中。
其实方式很简单，比如我们在做人脸检测的任务。先读取一张图片，图像两侧填充，缩放到640*640大小，即Image_1，人脸检测框为红色框。再随机选取一张图片，图像上下填充，也缩放到640*640大小，即Image_2，人脸检测框为蓝色框。然后设置一个融合系数，比如上图中，设置为0.5，将Image_1和Image_2，加权融合，最终得到右面的Image。从右图可以看出，人脸的红色框和蓝色框是叠加存在的。
我们知道，在Mosaic和Mixup的基础上，Yolov3 baseline增加了2.4个百分点。不过有两点需要注意：
（1）在训练的最后15个epoch，这两个数据增强会被关闭掉。而在此之前，Mosaic和Mixup数据增强，都是打开的，这个细节需要注意。
（2）由于采取了更强的数据增强方式，作者在研究中发现，ImageNet预训练将毫无意义，因此，所有的模型，均是从头开始训练的。
Backbone Yolox-Darknet53的Backbone主干网络，和原本的Yolov3 baseline的主干网络都是一样的。
Neck 在Neck结构中，Yolox-Darknet53和Yolov3 baseline的Neck结构，也是一样的，都是采用FPN的结构进行融合。
而在Yolov4、Yolov5、甚至后面讲到的Yolox-s、l等版本中，都是采用FPN+PAN的形式，这里需要注意。
Prediction 在输出层中，主要从四个方面进行讲解：Decoupled Head、Anchor Free、标签分配、Loss计算。
Decoupled	Head 在很多一阶段网络中都有类似应用，比如RetinaNet、FCOS等。
而在Yolox中，作者增加了三个Decoupled Head，俗称“解耦头”。大白这里从两个方面对Decoupled Head进行讲解：
从上图右面的Prediction中，我们可以看到，有三个Decoupled Head分支。
① 为什么使用Decoupled Head？
在了解原理前，我们先了解下改进的原因。为什么将原本的Yolo head，修改为Decoupled Head呢？
作者想继续改进，比如输出端改进为End-to-end的方式（即无NMS的形式）。在实验中还发现，不单单是精度上的提高。替换为Decoupled Head后，网络的收敛速度也加快了。
**但是需要注意的是：将检测头解耦，会增加运算的复杂度。**因此作者经过速度和性能上的权衡，最终使用 1个1x1 的卷积先进行降维，并在后面两个分支里，各使用了 2个3x3 卷积，最终调整到仅仅增加一点点的网络参数。而且这里解耦后，还有一个更深层次的重要性：Yolox的网络架构，可以和很多算法任务，进行一体化结合。
比如：
（1）YOLOX + Yolact/CondInst/SOLO ，实现端侧的实例分割。
（2）YOLOX + 34 层输出，实现端侧人体的 17 个关键点检测。
Decoupled Head的细节？
我们将Yolox-Darknet53中，Decoupled Head①提取出来，经过前面的Neck层，这里Decouple Head①输入的长宽为20*20。
从图上可以看出，Concat前总共有三个分支：
（1）cls_output：主要对目标框的类别，预测分数。因为COCO数据集总共有80个类别，且主要是N个二分类判断，因此经过Sigmoid激活函数处理后，变为202080大小。
（2）obj_output：主要判断目标框是前景还是背景，因此经过Sigmoid处理好，变为20201大小。
（3）reg_output：主要对目标框的坐标信息（x，y，w，h）进行预测，因此大小为20204。
最后三个output，经过Concat融合到一起，得到20*20*85的特征信息。
当然，这只是Decoupled Head①的信息，再对Decoupled Head②和③进行处理。
Decoupled Head②输出特征信息，并进行Concate，得到40*40*85特征信息。
Decoupled Head③输出特征信息，并进行Concate，得到80*80*85特征信息。
再对①②③三个信息，进行Reshape操作，并进行总体的Concat，得到8400*85的预测信息。
并经过一次Transpose，变为85*8400大小的二维向量信息。
这里的8400，指的是预测框的数量，而85是每个预测框的信息（reg，obj，cls）。
有了预测框的信息，下面我们再了解，如何将这些预测框和标注的框，即groundtruth进行关联，从而计算Loss函数，更新网络参数呢？
Anchor-free 在Yolov3、Yolov4、Yolov5中，通常都是采用Anchor Based的方式，来提取目标框，进而和标注的groundtruth进行比对，判断两者的差距。
① Anchor Based方式
比如输入图像，经过Backbone、Neck层，最终将特征信息，传送到输出的Feature Map中。
这时，就要设置一些Anchor规则，将预测框和标注框进行关联。从而在训练中，计算两者的差距，即损失函数，再更新网络参数。比如在下图的，最后的三个Feature Map上，基于每个单元格，都有三个不同尺寸大小的锚框。
当输入为416*416时，网络最后的三个特征图大小为13*13，26*26，52*52。我们可以看到，黄色框为小狗的Groundtruth，即标注框。而蓝色的框，为小狗中心点所在的单元格，所对应的锚框，每个单元格都有3个蓝框。当采用COCO数据集，即有80个类别时。基于每个锚框，都有x、y、w、h、obj（前景背景）、class（80个类别），共85个参数。
因此会产生3*(13*13+26*26+52*52）*85=904995个预测结果。
如果将输入从416*416，变为640*640，最后的三个特征图大小为20*20,40*40,80*80。
则会产生3*（20*20+40*40+80*80）*85=2142000个预测结果。
② Anchor Free方式
而Yolox-Darknet53中，则采用Anchor Free的方式。我们从两个方面，来对Anchor Free进行了解。
a.输出的参数量
当输入为640*640时，最终输出得到的特征向量是85*8400。通过计算，8400*85=714000个预测结果，比基于Anchor Based的方式，少了2/3的参数量。
b.Anchor框信息
在前面Anchor Based中，我们知道，每个Feature map的单元格，都有3个大小不一的锚框。
那么Yolox-Darknet53就没有吗？其实并不然，这里只是巧妙的，将前面Backbone中，下采样的大小信息引入进来。
比如上图中，最上面的分支，下采样了5次，2的5次方为32。并且Decoupled Head①的输出，为20*20*85大小。
最后8400个预测框中，其中有400个框，所对应锚框的大小，为32*32。同样的原理，中间的分支，最后有1600个预测框，所对应锚框的大小，为16*16。最下面的分支，最后有6400个预测框，所对应锚框的大小，为8*8。当有了8400个预测框的信息，每张图片也有标注的目标框的信息。
这时的锚框，就相当于桥梁。这时需要做的，就是将8400个锚框，和图片上所有的目标框进行关联，挑选出正样本锚框。而相应的，正样本锚框所对应的位置，就可以将正样本预测框，挑选出来。这里采用的关联方式，就是标签分配。
标签分配 当有了8400个Anchor锚框后，这里的每一个锚框，都对应85*8400特征向量中的预测框信息。
这些预测框只有少部分是正样本，绝大多数是负样本。
那么到底哪些是正样本呢？
这里需要利用锚框和实际目标框的关系，挑选出一部分适合的正样本锚框。
比如第3、10、15个锚框是正样本锚框，则对应到网络输出的8400个预测框中，第3、10、15个预测框，就是相应的**正样本预测框。**训练过程中，在锚框的基础上，不断的预测，然后不断的迭代，从而更新网络参数，让网络预测的越来越准。
那么在Yolox中，是如何挑选正样本锚框的呢？
① 初步筛选
初步筛选的方式主要有两种：根据中心点来判断、根据目标框来判断；
这部分的代码，在models/yolo_head.py的get_in_boxes_info函数中。
a. 根据中心点来判断：
规则：寻找anchor_box中心点，落在groundtruth_boxes矩形范围的所有anchors。
比如在get_in_boxes_info的代码中，通过groundtruth的[x_center,y_center，w，h]，计算出每张图片的每个groundtruth的左上角、右下角坐标。
深入浅出Yolo系列之Yolox核心基础完整讲解 - 知乎 (zhihu.com) ]]></content></entry><entry><title>cv-残差网络(ResNet)</title><url>/post/cv-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/</url><categories><category>cv</category></categories><tags/><content type="html">残差操作这一思想起源于论文《Deep Residual Learning for Image Recognition》。如果存在某个K层的网络f是当前最优的网络，那么可以构造一个更深的网络，其最后几层仅是该网络f第K层输出的恒等映射（IdentityMapping），就可以取得与f一致的结果；也许K还不是所谓“最佳层数”，那么更深的网络就可以取得更好的结果。**总而言之，与浅层网络相比，更深的网络的表现不应该更差。但是如下图所示，56层的神经网络表现明显要比20层的差。**证明更深的网络在训练过程中的难度更大，因此作者提出了残差网络的思想。+ 定义 ResNet 的作者将这些问题归结成了一个单一的假设：直接映射是难以学习的。而且他们提出了一种修正方法：不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是「残差（residual）」。然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。假设残差为 F(x)=H(x)-x，那么现在我们的网络不会直接学习 H(x) 了，而是学习 F(x)+x。
这就带来了你可能已经见过的著名 ResNet（残差网络）模块： ResNet 的每一个「模块（block）」都由一系列层和一个「捷径（shortcut）」连接组成，这个「捷径」将该模块的输入和输出连接到
了一起。然后在元素层面上执行「加法（add）」运算，当输入和输出维度一致时，可以直接将输入加到输出上。如果输入和输出的大小不同，那就可以使用零填充或映射（通过 1×1 卷积）来得到匹配的大小。
回到我们的思想实验，这能大大简化我们对恒等层的构建。直觉上就能知道，比起从头开始学习一个恒等变换，学会使 F(x) 为 0 并使输出仍为 x 要容易得多。一般来说，ResNet 会给层一个「参考」点 x，以 x 为基础开始学习。
在此之前，深度神经网络常常会有梯度消失问题的困扰，因为 ResNet 的梯度信号可以直接通过捷径连接回到更早的层，而且它们的表现依然良好。
ResNet本质上就干了一件事：降低数据中信息的冗余度。具体说来，就是对非冗余信息采用了线性激活（通过skip connection获得无冗余的identity部分），然后对冗余信息采用了非线性激活（通过ReLU对identity之外的其余部分进行信息提取/过滤，提取出的有用信息即是残差）。其中，提取identity这一步，就是ResNet思想的核心。
优点 一方面是残差网络更好的拟合分类函数以获得更高的分类精度，另一方面是残差网络如何解决网络在层数加深时优化训练上的难题。
1.残差网络拟合函数的优越性 首先从万能近似定理（Universal Approximation Theorem）入手。这个定理表明，一个前馈神经网络（feedforward neural network）如果具有线性输出层，同时至少存在一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，那么只要给予这个网络足够数量的隐藏单元，它就可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的波莱尔可测函数(Borel Measurable Function)。 万能近似定理意味着我们在构建网络来学习什么函数的时候，我们知道一定存在一个多层感知机（Multilayer Perceptron Model，MLP）能够表示这个函数。然而，我们不能保证训练算法能够学得这个函数。因为即使多层感知机能够表示该函数，学习也可能会失败，可能的原因有两种。
（1）用于训练的优化算法可能找不到用于期望函数的参数值。 （2）训练算法可能由于过拟合而选择了错误的函数。
第二种过拟合情况不在我们的讨论范围之内，因此我们聚焦在前一种情况，为何残差网络相比简单的多层网络能更好的拟合分类函数，即找到期望函数的参数值。 对于普通的不带短连接的神经网络来说，存在这样一个命题。 事实上对于高维函数，这一特点依然适用。因此，当函数的输入维度非常高时，这一做法就变的非常有意义。尽管在高维空间这一特点很难被可视化，但是这个理论给了一个很合理的启发，就是原则上，带短连接的网络的拟合高维函数的能力比普通连接的网络更强。这部分我们讨论了残差网络有能力拟合更高维的函数，但是在实际的训练过程中仍然可能存在各种各样的问题使得学习到最优的参数非常困难，因此下一小节讨论残差在训练过程中的优越性。
2.残差网络训练过程的优越性 这个部分我们讨论为什么残差能够缓解深层网络的训练问题，以及探讨可能的短连接方式和我们最终选择的残差的理由。正如本章第三部分讨论的一样，整个残差卷积神经网络是由以上的残差卷积子模块堆积而成。如上一小节所定义的，假设第$l$层的残差卷积字子模块的映射为 pytorch 实现 import torch.nn as nn import torch from torch.nn.init import kaiming_normal, constant class BasicConvResBlock(nn.Module): def __init__(self, input_dim=128, n_filters=256, kernel_size=3, padding=1, stride=1, shortcut=False, downsample=None): super(BasicConvResBlock, self).__init__() self.downsample = downsample self.shortcut = shortcut self.conv1 = nn.Conv1d(input_dim, n_filters, kernel_size=kernel_size, padding=padding, stride=stride) self.bn1 = nn.BatchNorm1d(n_filters) self.relu = nn.ReLU() self.conv2 = nn.Conv1d(n_filters, n_filters, kernel_size=kernel_size, padding=padding, stride=stride) self.bn2 = nn.BatchNorm1d(n_filters) def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.shortcut: out += residual out = self.relu(out) return out</content></entry><entry><title>cv-卷积神经网络（CNN)</title><url>/post/cv-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Ccnn/</url><categories><category>cv</category></categories><tags/><content type="html">
卷积层 卷积提取底层特征减少神经网络中参数个数
为了避免尺寸的变化可以在当前层的矩阵的边界加入全０填充（zero-padding）．否则中间的像素会多次进入卷积野而边上的进入次数少 还可以通过设置过滤器移动步长调整结果矩阵的大小 池化层 防止过拟合、下采样、降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗 实现非线性，扩大感知野 实现不变性，（平移不变性、旋转不变性和尺度不变性）
加速卷积运算 im2col FFT（不常用） 卷积层输出矩阵大小 padding = “SAME”$n_{output}=[ n_{input} ]$
padding = “VALID”$n_{\text {output}}=\left[\frac{n_{\text {input}}- \text{kernel_size} +1}{s}\right]$
池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为 $ \frac{n- \text{kernel_size} }{s} +1 $ 一个卷积核对应一个feature map</content></entry><entry><title>cv-卡号识别</title><url>/post/cv-%E5%8D%A1%E5%8F%B7%E8%AF%86%E5%88%AB/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[# 导入工具包 from imutils import contours # 也可以用自己写的方法 import numpy as np import argparse import cv2 # 设置参数 ap = argparse.ArgumentParser() ap.add_argument(&#34;-i&#34;, &#34;--image&#34;, default=&#39;./images/1.png&#39;, help=&#34;path to input image&#34;) ap.add_argument(&#34;-t&#34;, &#34;--template&#34;, default=&#39;./images/template.png&#39;, help=&#34;path to template OCR-A image&#34;) ap.add_argument(&#34;-s&#34;, &#34;--save&#34;, default=True, help=&#34;save every image&#34;) args = vars(ap.parse_args()) def cv_show(name, image): cv2.imshow(name, image) cv2.waitKey(0) cv2.destroyAllWindows() # 轮廓排序 def sort_contours(origin_counters, method=&#34;left-to-right&#34;): reverse = False order = 0 if method == &#34;right-to-left&#34; or method == &#34;bottom-to-top&#34;: reverse = True if method == &#34;top-to-bottom&#34; or method == &#34;bottom-to-top&#34;: order = 1 boxes = [cv2.boundingRect(counter) for counter in origin_counters] # x,y,h,w ordered_counters, boxes = zip(*sorted(zip(origin_counters, boxes), key=lambda b: b[1][order], reverse=reverse)) return counters, boxes # 设置图像大小 def resize(origin_image, width=None, height=None, inter=cv2.INTER_AREA): (h, w) = origin_image.shape[:2] if width is None and height is None: return origin_image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized_image = cv2.resize(origin_image, dim, interpolation=inter) return resized_image 读取模板图像,把所有数字模板放到一个数组 img = cv2.imread(args[&#34;template&#34;]) cv2.imwrite(&#39;template.jpg&#39;, img) if args[&#34;save&#34;] else cv_show(&#39;template&#39;, img) # 灰度图 ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv2.imwrite(&#39;2gray.jpg&#39;, ref) if args[&#34;save&#34;] else cv_show(&#39;gray&#39;, ref) # 二值图像 ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1] cv2.imwrite(&#39;3binary.jpg&#39;, ref) if args[&#34;save&#34;] else cv_show(&#39;binary&#39;, ref) # 画出轮廓 counters, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cv2.drawContours(img, counters, 1, (0, 0, 255), 3) cv2.imwrite(&#39;4counters.jpg&#39;, img) if args[&#34;save&#34;] else cv_show(&#39;counters&#39;, img) # 把每个数字模板分割存下来 # counters = sort_contours(counters, method=&#34;left-to-right&#34;)[0] # 排序，从左到右，从上到下 counters = contours.sort_contours(counters, method=&#34;left-to-right&#34;)[0] # 排序，从左到右，从上到下 templateDigits = [] for (i, c) in enumerate(counters): # 计算外接矩形并且resize成合适大小 (x, y, w, h) = cv2.boundingRect(c) roi = ref[y:y + h, x:x + w] templateDigits.append(cv2.resize(roi, (57, 88))) 读取卡图像预处理 # 初始化几个结构化内核，构造了两个这样的内核 - 一个矩形和一个正方形。 # 我们将使用矩形的一个用于Top-hat形态运算符，将方形一个用于闭操作。 rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3)) sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)) # 读取输入图像，预处理 image = cv2.imread(args[&#34;image&#34;]) cv2.imwrite(&#39;5image.jpg&#39;, image) if args[&#34;save&#34;] else cv_show(&#39;image&#39;, image) # 灰度处理 image = resize(image, width=300) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) cv2.imwrite(&#39;6gray2.jpg&#39;, gray) if args[&#34;save&#34;] else cv_show(&#39;gray2&#39;, gray) 边缘检测，提取出数字轮廓区域 # 礼帽操作，突出更明亮的区域 # 礼帽：噪音提取 黑帽：空洞提取 tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel) cv2.imwrite(&#39;7tophat.jpg&#39;, tophat) if args[&#34;save&#34;] else cv_show(&#39;tophat&#39;, tophat) # 计算沿x方向的渐变在计算gradX 数组中每个元素的绝对值之后 ， # 我们采取一些步骤将值缩放到范围[0-255]（因为图像当前是浮点数据类型）。 # 要做到这一点，我们计算 最小/最大归一化 最后一步是将gradX转换 为 uint8 ，其范围为[0-255] gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, # ksize=-1相当于用3*3的 ksize=-1) # **Sobel边缘检测算法**比较简单，实际应用中效率**比canny边缘检测效率要高**，但是边缘**不如Canny检测的准确**，但是很多实际应用的场合，sobel边缘却是首选，Sobel算子是**高斯平滑与微分操作的结合体，所以其抗噪声能力很强，用途较多**。尤其是效率要求较高，而对细纹理不太关心的时候。 gradX = np.absolute(gradX) (minVal, maxVal) = (np.min(gradX), np.max(gradX)) gradX = (255 * ((gradX - minVal) / (maxVal - minVal))) gradX = gradX.astype(&#34;uint8&#34;) print(np.array(gradX).shape) cv2.imwrite(&#39;8gradX.jpg&#39;, gradX) if args[&#34;save&#34;] else cv_show(&#39;gradX&#39;, gradX) # 通过闭操作（先膨胀，再腐蚀）将数字连在一起 gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel) cv2.imwrite(&#39;9close.jpg&#39;, gradX) if args[&#34;save&#34;] else cv_show(&#39;close&#39;, gradX) # THRESH_OTSU会自动寻找合适的阈值，适合双峰，需把阈值参数设置为0 thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv2.imwrite(&#39;10thresh.jpg&#39;, thresh) if args[&#34;save&#34;] else cv_show(&#39;thresh&#39;, thresh) # 再来一个闭操作 thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) # 再来一个闭操作 cv2.imwrite(&#39;11close2.jpg&#39;, thresh) if args[&#34;save&#34;] else cv_show(&#39;close2&#39;, thresh) # 计算轮廓 threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cur_img = image.copy() cv2.drawContours(cur_img, threshCnts, -1, (0, 0, 255), 3) cv2.imwrite(&#39;12cur_img.jpg&#39;, cur_img) if args[&#34;save&#34;] else cv_show(&#39;cur_img&#39;, cur_img) 基于规则筛选出4位数字卡号区域 locs = [] # 遍历轮廓 for (i, c) in enumerate(threshCnts): # 计算矩形 (x, y, w, h) = cv2.boundingRect(c) ar = w / float(h) # 选择合适的区域，因为信用卡使用了一个固定大小的字体与4组4位数字，我们可以基于高宽比 if 2.5 &lt; ar &lt; 4.0: # 轮廓可以进一步调整最小/最大宽度 if (40 &lt; w &lt; 55) and (10 &lt; h &lt; 20): # 符合的留下来 locs.append((x, y, w, h)) # 将符合的轮廓从左到右排序 locs = sorted(locs, key=lambda x: x[0]) output = [] 遍历每一个轮廓中的数字 for (i, (gX, gY, gW, gH)) in enumerate(locs): groupOutput = [] # 根据坐标在原图提取每个数组的图像 group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5] cv2.imwrite(&#39;13-{}gray.jpg&#39;.format(i), group) if args[&#34;save&#34;] else cv_show(&#39;gray&#39;, group) # 预处理，二值化 group = cv2.threshold(group, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv2.imwrite(&#39;14-{}threshold.jpg&#39;.format(i), img) if args[&#34;save&#34;] else cv_show(&#39;threshold&#39;, group) # 计算每一组的轮廓 digitCnts, hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) digitCnts = contours.sort_contours(digitCnts, method=&#34;left-to-right&#34;)[0] # 对比组中的每一个数具体是什么值 for j, c in enumerate(digitCnts): # 找到当前数的轮廓，resize成合适的的大小 (x, y, w, h) = cv2.boundingRect(c) roi = group[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) cv2.imwrite(&#39;15-{}-{}roi.jpg&#39;.format(i, j), roi) if args[&#34;save&#34;] else cv_show(&#39;roi&#39;, roi) scores = [] for digit in templateDigits: # 模板匹配 result = cv2.matchTemplate(roi, digit, cv2.TM_CCOEFF) (_, score, _, _) = cv2.minMaxLoc(result) scores.append(score) # 得到最合适的数字 groupOutput.append(str(np.argmax(scores))) # 画出来 cv2.rectangle(image, (gX - 5, gY - 5), (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1) cv2.putText(image, &#34;&#34;.join(groupOutput), (gX, gY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2) # 得到结果 output.extend(groupOutput) # 打印结果 print(&#34;Credit Card #: {}&#34;.format(&#34;&#34;.join(output))) cv2.imwrite(&#39;result.jpg&#39;, image) if args[&#34;save&#34;] else cv_show(&#39;result&#39;, image) cv2.waitKey(0) ]]></content></entry><entry><title>cv-语义分割</title><url>/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/</url><categories><category>cv</category></categories><tags/><content type="html"><![CDATA[标注工具 Labelme 支持目标检测、语义分割、实例分割等任务。今天针对分割任务的数据标注进行简单的介绍。开源项目地址： https://github.com/wkentaro/labelme 安装非常简单，直接使用pip安装即可：
pip install labelme 安装完成后在终端输入`labelme`即可启动： labelme # 建议大家按照我提供的目录格式事先准备好数据，然后在该根目录下启动labelme ├── img_data: 存放你要标注的所有图片 ├── data_annotated: 存放后续标注好的所有json文件 └── label.txt: 所有类别信息 虽然在labelme中能够在标注时添加标签，但我个人强烈建议事先创建一个label.txt标签（放在上述位置中），然后启动labelme时直接读取。标签格式如下：
__ignore__ _background_ dog cat # 每一行代表一个类型的名称，前两行是固定格式__ignore__和_background_都加上，否则后续使用作者提供的转换脚本（转换成PASCAL VOC格式和MS COCO格式）时会报错。也就是从第三行开始就是我们需要分割的目标类别。这里以分割猫狗为例。 # 在创建好标签后，启动labelme并读取标签文件（注意启动根目录），其中--labels指定了标签文件的路径。 labelme --labels label.txt # 读取标签后，我们在界面右侧能够看到Label List中已经载入了刚刚我们自己创建的标签文件，并且不同类别用不同的颜色表示。 养成良好习惯，先将保存路径设置好。 先点击左上角File，Change Output Dir设置标注结果的保存目录，这里就设置成前面说好的data_annotated。 建议将Save With Image Data取消掉，默认是选中的。如果选中，会在保存的标注结果中将图像数据也保存在.json文件中（个人觉得没必要，还占空间）。
标注目标 首先点击左侧的CreatePolygons按钮开始绘制多边形，然后用鼠标标记一个一个点把目标边界给标注出来（鼠标放置在第一个点上，点击一下会自动闭合边界）。标注后会弹出一个选择类别的选择框，选择对应类别即可。
如果标注完一个目标后想修改目标边界，可以点击工具左侧的EditPolygons按钮，然后选中要修改的目标，拖拉边界点即可进行微调。如果要在边界上新增点，把鼠标放在边界上点击鼠标右键选择Add Point to Edge即可新增边界点。如果要删除点，把鼠标放在边界点上点击鼠标右键选择Remove Selected Point即可删除边界点
标注完一张图片后，点击界面左侧的Save按钮即可保存标注结果，默认每张图片的标注信息都用一个json文件存储。
保存json文件格式 标注得到的json文件格式如下，将一张图片中的所有目标的坐标都保存在shapes列表中，列表中每个元素对应一个目标，其中label记录了该目标的类别名称。points记录了一个目标的左右坐标信息。其他信息不在赘述。根据以下信息，其实自己就可以写个脚本取读取目标信息了。
{ &#34;version&#34;: &#34;4.5.9&#34;, &#34;flags&#34;: {}, &#34;shapes&#34;: [ { &#34;label&#34;: &#34;dog&#34;, &#34;points&#34;: [ [ 108.09090909090907, 687.1818181818181 ], .... [ 538.090909090909, 668.090909090909 ], [ 534.4545454545454, 689.0 ] ], &#34;group_id&#34;: null, &#34;shape_type&#34;: &#34;polygon&#34;, &#34;flags&#34;: {} } ], &#34;imagePath&#34;: &#34;../img_data/1.jpg&#34;, &#34;imageData&#34;: null, &#34;imageHeight&#34;: 690, &#34;imageWidth&#34;: 690 } 格式转换 转换语义分割标签 原作者为了方便，也提供了一个脚本，帮我们方便的将json文件转换成PASCAL VOC的语义分割标签格式。示例项目链接：https://github.com/wkentaro/labelme/tree/master/examples/semantic_segmentation. 在该链接中有个labelme2voc.py脚本，将该脚本下载下来后，放在上述项目根目录下，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_voc是生成PASCAL VOC数据的目录。
python labelme2voc.py data_annotated data_dataset_voc --labels label.txt 执行后会生成如下目录：
data_dataset_voc/JPEGImages
data_dataset_voc/SegmentationClass
data_dataset_voc/SegmentationClassPNG
data_dataset_voc/SegmentationClassVisualization
data_dataset_voc/class_names.txt
其中JPEGImages就和之前PASCAL VOC数据讲解中说的一样，就是存储原图像文件。而SegmentationClassPNG就是语义分割需要使用的PNG标签图片。class_names.txt存储的是所有的类别信息，包括背景。
转换实例分割标签 原作者为了方便，这里提供了两个脚本，帮我们方便的将json文件转换成PASCAL VOC的实例分割标签格式以及MS COCO的实例分割标签格式。示例项目链接：https://github.com/wkentaro/labelme/tree/master/examples/instance_segmentation. 在该链接中有个labelme2voc.py脚本，将该脚本下载下来后，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_voc是生成PASCAL VOC数据的目录。
labelme2voc.py data_annotated data_dataset_voc --labels label.txt 执行后会生成如下目录：
data_dataset_voc/JPEGImages
data_dataset_voc/SegmentationClass
data_dataset_voc/SegmentationClassPNG
data_dataset_voc/SegmentationClassVisualization
data_dataset_voc/SegmentationObject
data_dataset_voc/SegmentationObjectPNG
data_dataset_voc/SegmentationObjectVisualization
data_dataset_voc/class_names.txt
除了刚刚讲的语义分割文件夹外，还生成了针对实例分割的标签文件，主要就是SegmentationObjectPNG目录：
在该链接中有个labelme2coco.py脚本，将该脚本下载下来后，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_coco是生成MS COCO数据类型的目录。
python labelme2coco.py data_annotated data_dataset_coco --labels label.txt 其中annotations.json就是MS COCO的标签数据文件，如果不了解可以看下我之前写的MS COCO介绍。
数据集 PASCAL VOC PASCAL VOC挑战赛主要包括以下几类：图像分类(Object Classification)，目标检测(Object Detection)，目标分割(Object Segmentation)，行为识别(Action Classification) 等。
该数据集有20个分类： Person:person Animal:bird,cat,cow,dog,horse,sheep Vehicle:aeroplane,bicycle,boat,bus,car,motorbike,train Indoor:bottle,chair,dining table,potted plant,sofa,tv/monitor
http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ 下载后将文件进行解压，解压后的文件目录结构如下所示：
VOCdevkit └── VOC2012 ├── Annotations 所有的图像标注信息(XML文件) ├── ImageSets │ ├── Action 人的行为动作图像信息 │ ├── Layout 人的各个部位图像信息 │ │ │ ├── Main 目标检测分类图像信息 │ │ ├── train.txt 训练集(5717) │ │ ├── val.txt 验证集(5823) │ │ └── trainval.txt 训练集+验证集(11540) │ │ │ └── Segmentation 目标分割图像信息 │ ├── train.txt 训练集(1464) │ ├── val.txt 验证集(1449) │ └── trainval.txt 训练集+验证集(2913) │ ├── JPEGImages 所有图像文件 ├── SegmentationClass 语义分割png图（基于类别） └── SegmentationObject 实例分割png图（基于目标） 如果IOU&gt;0.5则预测为正样本，检测到同一目标多个检测，第一个检测为正样本，其余视为负样本
MS COCO MS COCO是一个非常大型且常用的数据集，其中包括了目标检测，分割，图像描述等。其主要特性如下：
Object segmentation: 目标级分割 Recognition in context: 图像情景识别 Superpixel stuff segmentation: 超像素分割 330K images (&gt;200K labeled): 超过33万张图像，标注过的图像超过20万张 1.5 million object instances: 150万个对象实例 80 object categories: 80个目标类别 91 stuff categories: 91个材料类别 5 captions per image: 每张图像有5段情景描述 250,000 people with keypoints: 对25万个人进行了关键点标注 http://cocodataset.org/ MS COCO标注文件格式
官网有给出一个关于标注文件的格式说明，可以通过以下链接查看： https://cocodataset.org/#format-data 下面是使用pycocotools读取图像以及对应bbox信息的简单示例：
Linux系统安装pycocotools： pip install pycocotools Windows系统安装pycocotools： pip install pycocotools-windows import os from pycocotools.coco import COCO from PIL import Image, ImageDraw import matplotlib.pyplot as plt json_path = &#34;/data/coco2017/annotations/instances_val2017.json&#34; img_path = &#34;/data/coco2017/val2017&#34; # load coco data coco = COCO(annotation_file=json_path) # get all image index info ids = list(sorted(coco.imgs.keys())) print(&#34;number of images: {}&#34;.format(len(ids))) # get all coco class labels coco_classes = dict([(v[&#34;id&#34;], v[&#34;name&#34;]) for k, v in coco.cats.items()]) # 遍历前三张图像 for img_id in ids[:3]: # 获取对应图像id的所有annotations idx信息 ann_ids = coco.getAnnIds(imgIds=img_id) # 根据annotations idx信息获取所有标注信息 targets = coco.loadAnns(ann_ids) # get image file name path = coco.loadImgs(img_id)[0][&#39;file_name&#39;] # read image img = Image.open(os.path.join(img_path, path)).convert(&#39;RGB&#39;) draw = ImageDraw.Draw(img) # draw box to image for target in targets: x, y, w, h = target[&#34;bbox&#34;] x1, y1, x2, y2 = x, y, int(x + w), int(y + h) draw.rectangle((x1, y1, x2, y2)) draw.text((x1, y1), coco_classes[target[&#34;category_id&#34;]]) # show image plt.imshow(img) plt.show() 下面是使用pycocotools读取图像segmentation信息的简单示例：
import os import random import numpy as np from pycocotools.coco import COCO from pycocotools import mask as coco_mask from PIL import Image, ImageDraw import matplotlib.pyplot as plt random.seed(0) json_path = &#34;/data/coco2017/annotations/instances_val2017.json&#34; img_path = &#34;/data/coco2017/val2017&#34; # random pallette pallette = [0, 0, 0] + [random.randint(0, 255) for _ in range(255*3)] # load coco data coco = COCO(annotation_file=json_path) # get all image index info ids = list(sorted(coco.imgs.keys())) print(&#34;number of images: {}&#34;.format(len(ids))) # get all coco class labels coco_classes = dict([(v[&#34;id&#34;], v[&#34;name&#34;]) for k, v in coco.cats.items()]) # 遍历前三张图像 for img_id in ids[:3]: # 获取对应图像id的所有annotations idx信息 ann_ids = coco.getAnnIds(imgIds=img_id) # 根据annotations idx信息获取所有标注信息 targets = coco.loadAnns(ann_ids) # get image file name path = coco.loadImgs(img_id)[0][&#39;file_name&#39;] # read image img = Image.open(os.path.join(img_path, path)).convert(&#39;RGB&#39;) img_w, img_h = img.size masks = [] cats = [] for target in targets: cats.append(target[&#34;category_id&#34;]) # get object class id polygons = target[&#34;segmentation&#34;] # get object polygons rles = coco_mask.frPyObjects(polygons, img_h, img_w) mask = coco_mask.decode(rles) if len(mask.shape) &lt; 3: mask = mask[..., None] mask = mask.any(axis=2) masks.append(mask) cats = np.array(cats, dtype=np.int32) if masks: masks = np.stack(masks, axis=0) else: masks = np.zeros((0, height, width), dtype=np.uint8) # merge all instance masks into a single segmentation map # with its corresponding categories target = (masks * cats[:, None, None]).max(axis=0) # discard overlapping instances target[masks.sum(0) &gt; 1] = 255 target = Image.fromarray(target.astype(np.uint8)) target.putpalette(pallette) plt.imshow(target) plt.show() 验证目标检测任务mAP 根据官方文档给的预测结果格式可以看到，我们需要以列表的形式保存结果，列表中的每个元素对应一个检测目标（每个元素都是字典类型），每个目标记录了四个信息：
image_id记录该目标所属图像的id（int类型） category_id记录预测该目标的类别索引，注意这里索引是对应stuff中91个类别的索引信息（int类型） bbox记录预测该目标的边界框信息，注意对应目标的[xmin, ymin, width, height] (list[float]类型) score记录预测该目标的概率（float类型） AP@.5(IOU=0.5)；AP@.75(IOU=0.75)
mAP=(mAP.5:.05:.95) /10 （ start from 0.5 to 0.95 with a step size of 0.05十个mAP平均）
分割、检测 图像分类：图像中的气球是一个类别。 语义分割：分割出气球和背景。 目标检测：图像中有7个目标气球，并且检测出每个气球的坐标位置。 实例分割：图像中有7个不同的气球，在像素层面给出属于每个气球的像素。
高分辨率特征(较浅的卷积层)感知域较小，有利于feature map和原图进行对齐的，也就是我说的可以提供更多的位置信息。
低分辨率信息(深层的卷积层)由于感知域较大，能够学习到更加抽象一些的特征，可以提供更多的上下文信息，即强语义信息，这有利于像素的精确分类。
上采样（意义在于将小尺寸的高维度feature map恢复回去）一般包括2种方式：
Resize，如双线性插值直接对图像进行缩放（这种方法在原文中提到）
Deconvolution（反卷积），也叫Transposed Convolution(转置卷积)，可以理解为卷积的逆向操作。
FCN FCN首先将一幅RGB图像输入到卷积神经网络后，经过多次卷积以及池化过程得到一系列的特征图，然后利用反卷积层对最后一个卷积层得到的特征图进行上采样，使得上采样后特征图与原图像的大小一样，从而实现对特征图上的每个像素值进行预测的同时保留其在原图像中的空间位置信息，最后对上采样特征图进行逐像素分类，逐个像素计算softmax分类损失。
主要特点：
不含全连接层（FC）的全卷积（Fully Conv）网络。从而可适应任意尺寸输入。 引入增大数据尺寸的反卷积（Deconv）层。能够输出精细的结果。 结合不同深度层结果的跳级（skip）结构。同时确保鲁棒性和精确性。 反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和反向传播，只用颠倒卷积的前后向传播即可。如下图所示：
要注意的是，unpooling和反卷积是有几种不同的操作的。反池化很好理解，其中一种比较常用的操作是只需记住池化过程中的位置（最大值出现的地方），在unpooling的过程中，将对应位置置相应的值，其余位置置0即可。另外还有一种操作就是将所有unpooling的值全部填充下采样（pooling）的值。
经过全卷积后的结果进行反卷积，基本上就能实现语义分割了，但是得到的结果通常是比较粗糙的。
图中，image是原图像，conv1,conv2..,conv5为卷积操作，pool1,pool2,..pool5为pool操作（pool就是使得图片变为原图的1/2），注意con6-7是最后的卷积层，最右边一列是upsample后的end to end结果。必须说明的是图中nx是指对应的特征图上采样n倍（即变大n倍），并不是指有n个特征图，如32x upsampled 中的32x是图像只变大32倍，不是有32个上采样图像，又如2x conv7是指conv7的特征图变大2倍。
（1）FCN-32s过程
只需要留意第一行，网络里面有5个pool，所以conv7的特征图是原始图像1/32，可以发现最左边image的是32x32（假设以倍数计），同时我们知道在FCN中的卷积是不会改变图像大小（或者只有少量像素的减少，特征图大小基本不会小很多）。看到pool1是16x16，pool2是8x8，pool3是4x4，pool4是2x2，pool5是1x1，所以conv7对应特征图大小为1x1，然后再经过32x upsampled prediction 图片变回32x32。FCN作者在这里增加一个卷积层，卷积后的大小为输入图像的32(2^5)倍，我们简单假设这个卷积核大小也为32，这样就是需要通过反馈训练32x32个权重变量即可让图像实现end to end，完成了一个32s的upsample。FCN作者称做后卷积，他也提及可以称为反卷积。事实上在源码中卷积核的大小为64，同时没有偏置bias。还有一点就是FCN论文中最后结果都是21×*，这里的21是指FCN使用的数据集分类，总共有21类。
（2）FCN-16s过程
现在我们把1,2两行一起看，忽略32x upsampled prediction，说明FCN-16s的upsample过程。FCN作者在conv7先进行一个2x conv7操作，其实这里也只是增加1个卷积层，这次卷积后特征图的大小为conv7的2倍，可以从pool5与2x conv7中看出来。此时2x conv7与pool4的大小是一样的，FCN作者提出对pool4与2x conv7进行一个fuse操作（事实上就是将pool4与2x conv7相加，另一篇博客说是拼接，个人认为是拼接）。fuse结果进行16x upsampled prediction，与FCN-32s一样，也是增加一个卷积层，卷积后的大小为输入图像的16(2^4)倍。我们知道pool4的大小是2x2，放大16倍，就是32x32，这样最后图像大小也变为原来的大小，至此完成了一个16s的upsample。现在我们可以知道，FCN中的upsample实际是通过增加卷积层，通过bp反馈的训练方法训练卷积层达到end to end，这时卷积层的作用可以看作是pool的逆过程。
（3）FCN-8s过程
这是我们看第1行与第3行，忽略32x upsampled prediction。conv7经过一次4x upsample，即使用一个卷积层，特征图输出大小为conv7的4倍，所得4x conv7的大小为4x4。然后pool4需要一次2x upsample，变成2x pool4，大小也为4x4。再把4x conv7，2x pool4与pool3进行fuse，得到求和后的特征图。最后增加一个卷积层，使得输出图片大小为pool3的8倍，也就是8x upsampled prediction的过程，得到一个end to end的图像。实验表明FCN-8s优于FCN-16s，FCN-32s。 我们可以发现，如果继续仿照FCN作者的步骤，我们可以对pool2，pool1实现同样的方法，可以有FCN-4s，FCN-2s，最后得到end to end的输出。这里作者给出了明确的结论，超过FCN-8s之后，结果并不能继续优化。
图中，image是原图像，conv1,conv2..,conv5为卷积操作，pool1,pool2,..pool5为pool操作（pool就是使得图片变为原图的1/2），注意con6-7是最后的卷积层，最右边一列是upsample后的end to end结果。必须说明的是图中nx是指对应的特征图上采样n倍（即变大n倍），并不是指有n个特征图，如32x upsampled 中的32x是图像只变大32倍，不是有32个上采样图像，又如2x conv7是指conv7的特征图变大2倍。
（1）FCN-32s过程
只需要留意第一行，网络里面有5个pool，所以conv7的特征图是原始图像1/32，可以发现最左边image的是32x32（假设以倍数计），同时我们知道在FCN中的卷积是不会改变图像大小（或者只有少量像素的减少，特征图大小基本不会小很多）。看到pool1是16x16，pool2是8x8，pool3是4x4，pool4是2x2，pool5是1x1，所以conv7对应特征图大小为1x1，然后再经过32x upsampled prediction 图片变回32x32。FCN作者在这里增加一个卷积层，卷积后的大小为输入图像的32(2^5)倍，我们简单假设这个卷积核大小也为32，这样就是需要通过反馈训练32x32个权重变量即可让图像实现end to end，完成了一个32s的upsample。FCN作者称做后卷积，他也提及可以称为反卷积。事实上在源码中卷积核的大小为64，同时没有偏置bias。还有一点就是FCN论文中最后结果都是21×*，这里的21是指FCN使用的数据集分类，总共有21类。
（2）FCN-16s过程
现在我们把1,2两行一起看，忽略32x upsampled prediction，说明FCN-16s的upsample过程。FCN作者在conv7先进行一个2x conv7操作，其实这里也只是增加1个卷积层，这次卷积后特征图的大小为conv7的2倍，可以从pool5与2x conv7中看出来。此时2x conv7与pool4的大小是一样的，FCN作者提出对pool4与2x conv7进行一个fuse操作（事实上就是将pool4与2x conv7相加，另一篇博客说是拼接，个人认为是拼接）。fuse结果进行16x upsampled prediction，与FCN-32s一样，也是增加一个卷积层，卷积后的大小为输入图像的16(2^4)倍。我们知道pool4的大小是2x2，放大16倍，就是32x32，这样最后图像大小也变为原来的大小，至此完成了一个16s的upsample。现在我们可以知道，FCN中的upsample实际是通过增加卷积层，通过bp反馈的训练方法训练卷积层达到end to end，这时卷积层的作用可以看作是pool的逆过程。
（3）FCN-8s过程
这是我们看第1行与第3行，忽略32x upsampled prediction。conv7经过一次4x upsample，即使用一个卷积层，特征图输出大小为conv7的4倍，所得4x conv7的大小为4x4。然后pool4需要一次2x upsample，变成2x pool4，大小也为4x4。再把4x conv7，2x pool4与pool3进行fuse，得到求和后的特征图。最后增加一个卷积层，使得输出图片大小为pool3的8倍，也就是8x upsampled prediction的过程，得到一个end to end的图像。实验表明FCN-8s优于FCN-16s，FCN-32s。 我们可以发现，如果继续仿照FCN作者的步骤，我们可以对pool2，pool1实现同样的方法，可以有FCN-4s，FCN-2s，最后得到end to end的输出。这里作者给出了明确的结论，超过FCN-8s之后，结果并不能继续优化。
结合上述的FCN的全卷积与upsample，在upsample最后加上softmax，就可以对不同类别的大小概率进行估计，实现end to end。最后输出的图是一个概率估计，对应像素点的值越大，其像素为该类的结果也越大。FCN的核心贡献在于提出使用卷积层通过学习让图片实现end to end分类。
事实上，FCN有一些短处，例如使用了较浅层的特征，因为fuse操作会加上较上层的pool特征值，导致高维特征不能很好得以使用，同时也因为使用较上层的pool特征值，导致FCN对图像大小变化有所要求，如果测试集的图像远大于或小于训练集的图像，FCN的效果就会变差。
SegNet Segnet是用于进行像素级别图像分割的全卷积网络，分割的核心组件是一个encoder 网络，及其相对应的decoder网络，后接一个象素级别的分类网络。
encoder网络：其结构与VGG16网络的前13层卷积层的结构相似。
decoder网络：作用是将由encoder的到的低分辨率的feature maps 进行映射得到与输入图像feature map相同的分辨率进而进行像素级别的分类。
最终解码器的输出被送入soft-max分类器以独立的为每个像素产生类别概率。
Segnet的亮点：decoder利用与之对应的encoder阶段中进行max-pooling时的pooling index 进行非线性上采样，这样做的好处是上采样阶段就不需要进行学习。 上采样后得到的feature maps 是非常稀疏的，因此，需要进一步选择合适的卷积核进行卷积得到dense featuremaps 。
UNet 首先进行Conv+Pooling下采样； 然后反卷积进行上采样，crop之前的低层feature map，进行融合； 再次上采样。 重复这个过程，直到获得输出388x388x2的feature map， 最后经过softmax获得output segment map。总体来说与FCN思路非常类似。 UNet的encoder下采样4次，一共下采样16倍，对称地，其decoder也相应上采样4次，将encoder得到的高级语义特征图恢复到原图片的分辨率。
它采用了与FCN不同的特征融合方式：
FCN采用的是逐点相加，对应tensorflow的tf.add()函数 U-Net采用的是channel维度拼接融合，对应tensorflow的tf.concat()函数 Unet结构特点
UNet相比于FCN和Deeplab等，共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合，从而可以进行多尺度预测和DeepSupervision。4次上采样也使得分割图恢复边缘等信息更加精细。
为什么适用于医学图像？
\1. 因为医学图像边界模糊、梯度复杂，需要较多的高分辨率信息。高分辨率用于精准分割。 \2. 人体内部结构相对固定，分割目标在人体图像中的分布很具有规律，语义简单明确，低分辨率信息能够提供这一信息，用于目标物体的识别。
UNet结合了低分辨率信息（提供物体类别识别依据）和高分辨率信息（提供精准分割定位依据），完美适用于医学图像分割。
DeepLab 基于全卷积对称语义分割模型得到的分割结果比较粗糙，忽略了像素与像素之间的空间一致性关系。于是Google提出了一种新的扩张卷积语义分割模型，考虑了像素与像素之间的空间一致性关系，可以在不增加数量的情况下增加感受野。
Deeplabv1是由深度卷积网路和概率图模型级联而成的语义分割模型，由于深度卷积网路在重复最大池化和下采样的过程中会丢失很多的细节信息，所以采用扩张卷积算法增加感受野以获得更多上下文信息。考虑到深度卷积网路在图像标记任务中的空间不敏感性限制了它的定位精度，采用了完全连接条件随机场（Conditional Random Field， CRF）来提高模型捕获细节的能力。 Deeplabv2予以分割模型增加了ASPP（Atrous spatial pyramid pooling）结构，利用多个不同采样率的扩张卷积提取特征，再将特征融合以捕获不同大小的上下文信息。 Deeplabv3语义分割模型，在ASPP中加入了全局平均池化，同时在平行扩张卷积后添加批量归一化，有效地捕获了全局语义信息。 DeepLabV3+语义分割模型在Deeplabv3的基础上增加了编-解码模块和Xception主干网路，增加编解码模块主要是为了恢复原始的像素信息，使得分割的细节信息能够更好的保留，同时编码丰富的上下文信息。增加Xception主干网络是为了采用深度卷积进一步提高算法的精度和速度。在inception结构中，先对输入进行1*1卷积，之后将通道分组，分别使用不同的3*3卷积提取特征，最后将各组结果串联在一起作为输出。 主要特点：
在多尺度上为分割对象进行带洞空间金字塔池化（ASPP） 通过使用DCNNs（空洞卷积）提升了目标边界的定位 降低了由DCNN的不变性导致的定位准确率 RefineNet RefineNet采用了通过细化中间激活映射并分层地将其链接到结合多尺度激活，同时防止锐度损失。网络由独立的RefineNet模块组成，每个模块对应于ResNet。
每个RefineNet模块由三个主要模块组成，即剩余卷积单元（RCU），多分辨率融合（MRF）和链剩余池（CRP）。RCU块由一个自适应块组成卷积集，微调预训练的ResNet权重对于分割问题。MRF层融合不同的激活物使用卷积上采样层来创建更高的分辨率地图。最后，在CRP层池中使用多种大小的内核用于从较大的图像区域捕获背景上下文。
主要特点：
提出一种多路径refinement网络，称为RefineNet。这种网络可以使用各个层级的features，使得语义分割更为精准。 RefineNet中所有部分都利用resdiual connections (identity mappings)，使得梯度更容易短向或者长向前传，使端对端的训练变得更加容易和高效。 提出了一种叫做chained residual pooling的模块，它可以从一个大的图像区域捕捉背景上下文信息。 PSPNet 深度卷积神经网络的每一层特征对语义分割都有影响，如何将高层特征的语义信息与底层识别的边界与轮廓信息结合起来是一个具有挑战性的问题。
金字塔场景稀疏网络语义分割模型（Pyramid Scene Parsing Network，PSP）首先结合预训练网络 ResNet和扩张网络来提取图像的特征，得到原图像 1/8 大小的特征图，然后，采用金字塔池化模块将特征图同时通过四个并行的池化层得到四个不同大小的输出，将四个不同大小的输出分别进行上采样，还原到原特征图大小，最后与之前的特征图进行连接后经过卷积层得到最后的预测分割图像。
PSPNet为像素级场景解析提供了有效的全局上下文先验 金字塔池化模块可以收集具有层级的信息，比全局池化更有代表性 在计算量方面，我们的PSPNet并没有比原来的空洞卷积FCN网络有很大的增加 在端到端学习中，全局金字塔池化模块和局部FCN特征可以被同时训练 主要特点：
金字塔场景解析网络是建立在FCN之上的基于像素级分类网络。将大小不同的内核集中在一起激活地图的不同区域创建空间池金字塔。 特性映射来自网络被转换成不同分辨率的激活，并经过多尺度处理池层，稍后向上采样并与原始层连接进行分割的feature map。 学习的过程利用辅助分类器进一步优化了像ResNet这样的深度网络。不同类型的池模块侧重于激活的不同区域地图。 基于全卷积的GAN语义分割模型 生成对抗网络模型（Generative Adversarial Nets，GAN）同时训练生成器 G 和判别器 D，判别器用来预测给定样本是来自于真实数据还是来自于生成模型。
利用对抗训练方法训练语义分割模型，将传统的多类交叉熵损失与对抗网络相结合，首先对对抗网络进行预训练，然后使用对抗性损失来微调分割网络，如下图所示。左边的分割网络将 RGB 图像作为输入，并产生每个像素的类别预测。右边的对抗网络将标签图作为输入并生成类标签（1 代表真实标注，0 代表合成标签）。
基于全卷积语义分割模型对比 名称 优点 缺点 FCN 可以接受任意大小的图像输入；避免了采用像素块带来的重复存储和计算的问题 得到的结果不太精确，对图像的细节不敏感，没有考虑像素与像素之间的关系，缺乏空间一致性 SegNet 使用去池化对特征图进行上采样，在分割中保持细节的完整性；去掉全连接层，拥有较少的参数 当对低分辨率的特征图进行去池化时，会忽略邻近像素的信息 Deconvnet 对分割的细节处理要强于 FCN，位于低层的filter 能捕获目标的形状信息，位于高层的 filter能够捕获特定类别的细节信息，分割效果更好 对细节的处理难度较大 U-net 简单地将编码器的特征图拼接至每个阶段解码器的上采样特征图，形成了一个梯形结构；采用跳跃连接架构，允许解码器学习在编码器池化中丢失的相关性 在卷积过程中没有加pad，导致在每一次卷积后，特征长度就会减少两个像素，导致网络最后的输出与输入大小不一样 DeepLab 使用了空洞卷积；全连接条件随机场 得到的预测结果只有原始输入的 1/8 大小 RefineNet 带有解码器模块的编码器-解码器结构；所有组件遵循残差连接的设计方式 带有解码器模块的编码器-解码器结构；所有组件遵循残差连接的设计方式 PSPNet 提出金字塔模块来聚合背景信息；使用了附加损失 采用四种不同的金字塔池化模块，对细节的处理要求较高 GCN 提出了带有大维度卷积核的编码器-解码器结构 计算复杂，具有较多的结构参数 DeepLabV3 ASPP 采用了Multigrid；在原有的网络基础上增加了几个 block；提出了ASPP，加入了 BN 不能捕捉图像大范围信息，图像层的特征整合只存在于 ASPP中 GAN 提出将分割网络作为判别器，GAN 扩展训练数据，提升训练效果；将判别器改造为 FCN，从将判别每一个样本的真假变为每一个像素的真假 没有比较与全监督+半监督精调模型的实验结果，只体现了在本文中所提创新点起到了一定的作用，但并没有体现有效的程度 图像增强 # OpenCV # 水平翻转 plt.imshow(cv2.flip(img, 0) # 水平翻转 plt.imshow(cv2.flip(img, 0) # 随机裁剪 x, y = np.random.randint(0, 256), np.random.randint(0, 256) plt.imshow(img[x:x+256, y:y+256]) # albumentations # 水平翻转 augments = A.HorizontalFlip(p=1)(image=img, mask=mask) img_aug, mask_aug = augments[&#39;image&#39;], augments[&#39;mask&#39;] # 随机裁剪 augments = A.RandomCrop(p=1, height=256, width=256)(image=img, mask=mask) img_aug, mask_aug = augments[&#39;image&#39;], augments[&#39;mask&#39;] # 旋转 augments = A.ShiftScaleRotate(p=1)(image=img, mask=mask) img_aug, mask_aug = augments[&#39;image&#39;], augments[&#39;mask&#39;] Compose transforms：转换类的数组，list类型 bbox_params：用于 bounding boxes 转换的参数，BboxPoarams 类型 keypoint_params：用于 keypoints 转换的参数， KeypointParams 类型 additional_targets：key新target 名字，value 为旧 target 名字的 dict，如 {&#39;image2&#39;: &#39;image&#39;}，dict 类型 p：使用这些变换的概率，默认值为 1.0 组合与随机选择（Compose &amp; OneOf） image4 = Compose([ RandomRotate90(), # 翻转 Flip(), Transpose(), OneOf([ # 高斯噪点 IAAAdditiveGaussianNoise(), GaussNoise(), ], p=0.2), OneOf([ # 模糊相关操作 MotionBlur(p=.2), MedianBlur(blur_limit=3, p=0.1), Blur(blur_limit=3, p=0.1), ], p=0.2), ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2), OneOf([ # 畸变相关操作 OpticalDistortion(p=0.3), GridDistortion(p=.1), IAAPiecewiseAffine(p=0.3), ], p=0.2), OneOf([ # 锐化、浮雕等操作 CLAHE(clip_limit=2), IAASharpen(), IAAEmboss(), RandomBrightnessContrast(), ], p=0.3), HueSaturationValue(p=0.3), ], p=1.0) augments = image4(image=img, mask=mask) img_aug, mask_aug = augments[&#39;image&#39;], augments[&#39;mask&#39;] ]]></content></entry><entry><title>linux-Hadoop环境</title><url>/post/linux-hadoop%E7%8E%AF%E5%A2%83/</url><categories><category>linux</category></categories><tags/><content type="html"><![CDATA[wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.4.3.tgz
tar -xf mongodb-linux-x86_64-rhel62-3.4.3.tgz -C ~/
mv mongodb-linux-x86_64-rhel62-3.4.3/ /usr/local/mongodb
// 在安装目录下创建 data 文件夹用于存放数据和日志 mkdir /usr/local/mongodb/data/ // 在 data 文件夹下创建 db 文件夹，用于存放数据 mkdir /usr/local/mongodb/data/db/ // 在 data 文件夹下创建 logs 文件夹，用于存放日志 mkdir /usr/local/mongodb/data/logs/ // 在 logs 文件夹下创建 log 文件 touch /usr/local/mongodb/data/logs/mongodb.log // 在 data 文件夹下创建 mongodb.conf 配置文件 touch /usr/local/mongodb/data/mongodb.conf // 在 mongodb.conf 文件中输入如下内容 sudo vim ./data/mongodb.conf
#端口号 port = 27017 #数据目录 dbpath = /usr/local/mongodb/data/db #日志目录 logpath = /usr/local/mongodb/data/logs/mongodb.log #设置后台运行 fork = true #日志输出方式 logappend = true #开启认证 #auth = true 完成 MongoDB 的安装后，启动 MongoDB 服务器： // 启动 MongoDB 服务器 sudo /usr/local/mongodb/bin/mongod -config /usr/local/mongodb/data/mongodb.conf // 访问 MongoDB 服务器 /usr/local/mongodb/bin/mongo // 停止 MongoDB 服务器 sudo /usr/local/mongodb/bin/mongod -shutdown -config /usr/local/mongodb/data/mongodb.conf
Redis（单节点）环境配置 // 通过 WGET 下载 REDIS 的源码 wget http://download.redis.io/releases/redis-4.0.2.tar.gz // 将源代码解压到安装目录 tar -xf redis-4.0.2.tar.gz -C ~/ // 进入 Redis 源代码目录，编译安装 cd redis-4.0.2/ // 安装 GCC sudo apt-get install gcc // 编译源代码 make MALLOC=libc // 编译安装 sudo make install // 创建配置文件 sudo cp ~/redis-4.0.2/redis.conf /etc/ // 修改配置文件中以下内容 sudo vim /etc/redis.conf
daemonize yes #37 行 #是否以后台 daemon 方式运行，默认不是后台运行 pidfile /var/run/redis/redis.pid #41 行 #redis 的 PID 文件路径（可选） bind 0.0.0.0 #64 行 #绑定主机 IP，默认值为 127.0.0.1，我们是跨机器运行，所以需要更改 logfile /var/log/redis/redis.log #104 行 #定义 log 文件位置，模式 log信息定向到 stdout，输出到/dev/null（可选） dir &#39;/usr/local/rdbfile&#39; #188 行 #本地数据库存放路径，默认为./，编译安装默认存在在/usr/local/bin 下（可选） 在安装完 Redis 之后，启动 Redis // 启动 Redis 服务器 redis-server /etc/redis.conf // 连接 Redis 服务器 redis-cli // 停止 Redis 服务器 redis-cli shutdown 2.3 ElasticSearch（单节点）环境配置 // 通过 Wget 下载 ElasticSearch 安装包 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.2.tar.gz 修改 Linux 配置参数： // 修改文件数配置，在文件末尾添加如下配置 sudo vim /etc/security/limits.conf * soft nofile 65536 * hard nofile 131072 * soft nproc 2048 * hard nproc 4096 // 修改* soft nproc 1024 为 * soft nproc 2048 ```sudo vim /etc/security/limits.d/90-nproc.conf ``` * soft nproc 2048 #将该条目修改成 2048 // 在文件末尾添加： sudo vim /etc/sysctl.conf vm.max_map_count=655360 // 在文件末尾添加： sudo sysctl -p 配置 ElasticSearch： // 解压 ElasticSearch 到安装目录 tar -xf elasticsearch-5.6.2.tar.gz -C ./cluster/ // 进入 ElasticSearch 安装目录 cd ./cluster/elasticsearch-5.6.2/ // 创建 ElasticSearch 数据文件夹 data mkdir ./cluster/elasticsearch-5.6.2/data/ // 创建 ElasticSearch 日志文件夹 logs mkdir ./cluster/elasticsearch-5.6.2/logs/ // 修改 ElasticSearch 配置文件 sudo vim ./cluster/elasticsearch-5.6.2/config/elasticsearch.yml
cluster.name: es-cluster #设置集群的名称 node.name: es-node #修改当前节点的名称 path.data: /home/bigdata/cluster/elasticsearch-5.6.2/data #修改数据路径 path.logs: /home/bigdata/cluster/elasticsearch-5.6.2/logs #修改日志路径 bootstrap.memory_lock: false#设置 ES 节点允许内存交换 bootstrap.system_call_filter: false#禁用系统调用过滤器 network.host: linux#设置当前主机名称 discovery.zen.ping.unicast.hosts: [&#34;linux&#34;]#设置集群的主机列表 完成 ElasticSearch 的配置后： // 启动 ElasticSearch 服务 ./cluster/elasticsearch-5.6.2/bin/elasticsearch -d // 访问 ElasticSearch 服务
curl http://localhost:9200/ { &#34;name&#34; : &#34;es-node&#34;, &#34;cluster_name&#34; : &#34;es-cluster&#34;, &#34;cluster_uuid&#34; : &#34;VUjWSShBS8KM_EPJdIer6g&#34;, &#34;version&#34; : { &#34;number&#34; : &#34;5.6.2&#34;, &#34;build_hash&#34; : &#34;57e20f3&#34;, &#34;build_date&#34; : &#34;2017-09-23T13:16:45.703Z&#34;, &#34;build_snapshot&#34; : false, &#34;lucene_version&#34; : &#34;6.6.1&#34; }, &#34;tagline&#34; : &#34;You Know, for Search&#34; } // 停止 ElasticSearch 服务
jps 8514 Elasticsearch 6131 GradleDaemon 8908 Jps kill -9 8514 2.4 Azkaban（单节点）环境配置 2.4.1 安装 Git // 安装 GIT sudo apt-install install git // 通过 git 下载 Azkaban 源代码 git clone https://github.com/azkaban/azkaban.git // 进入 azkaban 目录 cd azkaban/ // 切换到 3.36.0 版本 git checkout -b 3.36.0 2.4.2 编译 Azkaban 详细请参照：https://github.com/azkaban/azkaban // 安装编译环境 sudo apt-get install gcc sudo apt-get install -y gcc-c++*
// 执行编译命令 ./gradlew clean build 2.4.3 部署 Azkaban Solo // 将编译好的 azkaban 中的 azkaban-solo-server-3.36.0.tar.gz 拷贝到根目录 cp ./azkaban-solo-server/build/distributions/azkaban-solo-server-3.36.0.tar.gz ~/ // 解压 azkaban-solo-server-3.36.0.tar.gz 到安装目录 tar -xf azkaban-solo-server-3.36.0.tar.gz -C ./cluster // 启动 Azkaban Solo 单节点服务 bin/azkaban-solo-start.sh // 访问 azkaban 服务，通过浏览器代开 http://10.36.34.100:8081，通过用户名：azkaban，密码 azkaban 登录。
// 关闭 Azkaban 服务 bin/azkaban-solo-shutdown.sh 2.5 Spark（单节点）环境配置 // 通过 wget 下载 zookeeper 安装包 wget https://d3kbcqa49mib13.cloudfront.net/spark-2.1.1-bin-hadoop2.7.tgz // 将 spark 解压到安装目录 tar -xf spark-2.1.1-bin-hadoop2.7.tgz -C ./cluster
// 进入 spark 安装目录 cd ./cluster/spark-2.1.1-bin-hadoop2.7/ // 复制 slave 配置文件 cp ./conf/slaves.template ./conf/slaves
// 修改 slave 配置文件 vim ./conf/slaves 在文件最后将本机主机名进行添加
linux # // 复制 Spark-Env 配置文件 cp ./conf/spark-env.sh.template ./conf/spark-env.sh SPARK_MASTER_HOST=linux #添加 spark master 的主机名 SPARK_MASTER_PORT=7077 #添加 spark master 的端口号 安装完成之后，启动 Spark // 启动 Spark 集群 sbin/start-all.sh // 访问 Spark 集群，浏览器访问 http://linux:8080 // 关闭 Spark 集群 sbin/stop-all.sh 2.6 Zookeeper（单节点）环境配置 // 通过 wget 下载 zookeeper 安装包 wget http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz // 将 zookeeper 解压到安装目录 tar -xf zookeeper-3.4.10.tar.gz -C ./cluster // 进入 zookeeper 安装目录 cd ./cluster/zookeeper-3.4.10/ // 创建 data 数据目录 mkdir data/ // 复制 zookeeper 配置文件 cp ./conf/zoo_sample.cfg ./conf/zoo.cfg // 修改 zookeeper 配置文件 vim conf/zoo.cfg
dataDir=/home/bigdata/cluster/zookeeper-3.4.10/data #将数据目录地址修 改为创建的目录 // 启动 Zookeeper 服务 bin/zkServer.sh start // 查看 Zookeeper 服务状态 bin/zkServer.sh status
ZooKeeper JMX enabled by default Using config: /home/bigdata/cluster/zookeeper-3.4.10/bin/../conf/zoo.cfg Mode: standalone
// 关闭 Zookeeper 服务 bin/zkServer.sh stop 2.7 Flume-ng（单节点）环境配置 // 通过 wget 下载 zookeeper 安装包 wget http://archive.apache.org/dist/flume/1.8.0/apache-flume-1.8.0-bin.tar.gz // 将 zookeeper 解压到安装目录 tar –xf apache-flume-1.8.0-bin.tar.gz –C ./cluster // 等待项目部署时使用
2.8 Kafka（单节点）环境配置 // 通过 wget 下载 zookeeper 安装包 wget http://archive.apache.org/dist/kafka/0.10.2.1/kafka_2.12-0.10.2.1.tgz // 将 kafka 解压到安装目录 tar -xf kafka_2.12-0.10.2.1.tgz -C ./cluster // 进入 kafka 安装目录 cd ./cluster/kafka_2.12-0.10.2.1/ // 修改 kafka 配置文件 vim config/server.properties
host.name=linux #修改主机名 port=9092 #修改服务端口号 zookeeper.connect=linux:2181#修改 Zookeeper 服务器地址 // 启动 kafka 服务 !!! 启动之前需要启动 Zookeeper 服务 bin/kafka-server-start.sh -daemon ./config/server.properties // 关闭 kafka 服务 bin/kafka-server-stop.sh // 创建 topic bin/kafka-topics.sh --create --zookeeper linux:2181 --replication-factor 1 --partitions 1 --topic recommender // kafka-console-producer bin/kafka-console-producer.sh --broker-list linux:9092 --topic recommender // kafka-console-consumer bin/kafka-console-consumer.sh --bootstrap-server linux:9092 --topic recommender
]]></content></entry><entry><title>ml-Attention机制</title><url>/post/ml-attention%E6%9C%BA%E5%88%B6/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[ https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py 用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：
时间片 t 的计算依赖 t-1 时刻的计算结果，这样限制了模型的并行能力； 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM依旧无能为力。 Attention 通用形式 如果将下游任务抽象成查询（query），就可以归纳出注意力机制的通用形式，即将源文本看成是键-值&lt;Key,Value&gt;对序列。给定Target中的某个元素Query（解码器隐层向量 $s_{t-1}$ ）用$K=（k_1，…，k_N）$和$V=（v_1，…，v_N）$分别表示键序列和值序列，用 $Q=（q_1，…，q_M）$表示查询序列，那么针对查询$q_t$的注意力可以被描述为键-值对序列在该查询上的映射。如图2所示，计算过程可分为三步：
计算查询 $q_t$和每个键 $k_i$的注意力得分 $e_{ti}$，常用的计算方法包括点积、缩放点积、拼接以及相加等，如公式（1）所示； 使用 Softmax 等函数对注意力得分做归一化处理，得到每个键的权重$ α_{ti}$，如公式（2）所示； 将权重$ α_{ti}$和其对应的值$ v_i$加权求和作为注意力输出，如公式（3）所示模型输出的注意力是源文本序列基于查询 $q_t$的表示，不同的查询会给源文本序列带来不同的权重分布。 分类 关注范围 注意力 关注范围 全局注意力 全部元素 局部注意力 以对齐位置为中心的窗口 硬注意力 一个元素 稀疏注意力 稀疏分布的部分元素 结构注意力 结构上相关的一系列元素 全局注意力 局部注意力 硬注意力 稀疏注意力 结构注意力 组合方式 层级注意力 双向注意力 多头注意力 自注意力 自注意力机制 在self-attention中，每个单词有3个不同的向量，它们分别是Query向量 $Q$，Key向量$K$和Value向量$V$。它们是通过3个不同的权值矩阵由嵌入向量$X$乘以三个不同的权值矩阵$W_Q,W_K,W_V$ 得到，其中三个矩阵的尺寸也是相同的。
Attention的计算方法，整个过程可以分成7步：
将输入单词转化成嵌入向量； 根据嵌入向量得到 三个向量$Q,K,V$ ； 为每个向量计算一个attention score：$Q*K$； 为了梯度的稳定，Transformer使用了score归一化，即除以 $\sqrt{d_k}$ ； 对score施以softmax激活函数； softmax点乘Value值$V$ ，得到加权的每个输入向量的评分 weighted values $V$ ； 相加之后得到最终的输出结果$Z=\sum(v)$。 class AttentionLayer(Layer): &#34;&#34;&#34; # Input shape 3D tensor with shape: `(samples, steps, features)`. # Output shape 3D tensor with shape: `(samples, steps, output_dim)`. &#34;&#34;&#34; def __init__(self, output_dim, **kwargs): self.output_dim = output_dim super(AttentionLayer, self).__init__(**kwargs) def build(self, input_shape): # 为该层创建一个可训练的权重 #inputs.shape = (batch_size, time_steps, seq_len) self.kernel = self.add_weight(name=&#39;kernel&#39;, shape=(3, input_shape[2], self.output_dim), initializer=&#39;uniform&#39;, trainable=True) super(AttentionLayer, self).build(input_shape) # 一定要在最后调用它 def call(self, x): WQ = K.dot(x, self.kernel[0]) # (None, input_shape[1]，input_shape[2]) (input_shape[2], output_dim) (None, input_shape[1]，output_dim) WK = K.dot(x, self.kernel[1]) # (None, input_shape[1]，output_dim) WV = K.dot(x, self.kernel[2]) # (None, input_shape[1]，output_dim) score = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1])) / (input_shape[0]**0.5) # (None, input_shape[1], input_shape[1]) alpha = K.softmax(score) V = K.batch_dot(alpha, WV) # (None, input_shape[1], input_shape[1]) (None, input_shape[1]，output_dim) (None, input_shape[1]，output_dim) return V 总结 优点：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的**关键是将任意两个单词的距离是1，**这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。
缺点：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。+
给定一个在每个时间步产生隐藏状态$h_t$的模型，基于注意的模型计算一个“上下文”向量$c_t$作为状态序列h的加权平均值
$c_{t}=\sum_{j=1}^{T} \alpha_{t j} h_{j}$
式中，$T$是输入序列中的时间步总数，$α_{tj}$是针对每个状态$h_j$在每个时间步$t$处计算的权重。然后使用这些上下文向量来计算新的状态序列$s$，其中$s_t$依赖于$s_{t−1}$、$c_t$和$t−1$处的模型输出。然后通过以下公式计算权重$α_{tj}$： $e_{t j}=a\left(s_{t-1}, h_{j}\right), \alpha_{t j}=\frac{\exp \left(e_{t j}\right)}{\sum_{k=1}^{T} \exp \left(e_{t k}\right)}$
其中，$a$是一个学习函数，可以认为是给定$h_j$值和先前状态$s_{t−1}$计算$h_j$的标量重要性值。该公式允许新的状态序列$s$更直接地访问整个状态序列$h$。
import numpy import keras import tensorflow as tf from keras import backend as K from keras import activations from keras.engine.topology import Layer from keras.preprocessing import sequence from keras.models import Sequential from keras.models import Model from keras.layers import Input, Dense, Embedding, LSTM, Bidirectional K.clear_session() tf.compat.v1.disable_eager_execution() class AttentionLayer(Layer): &#34;&#34;&#34; # Input shape 3D tensor with shape: `(samples, steps, hidden_size)`. # Output shape 2D tensor with shape: `(samples, hidden_size)`. &#34;&#34;&#34; def __init__(self, attention_size=None, **kwargs): self.attention_size = attention_size super(AttentionLayer, self).__init__(**kwargs) def get_config(self): config = super().get_config() config[&#39;attention_size&#39;] = self.attention_size return config def build(self, input_shape): assert len(input_shape) == 3 self.time_steps = input_shape[1] hidden_size = input_shape[2] if self.attention_size is None: self.attention_size = hidden_size self.W = self.add_weight(name=&#39;att_weight&#39;, shape=(hidden_size, self.attention_size), initializer=&#39;uniform&#39;, trainable=True) self.b = self.add_weight(name=&#39;att_bias&#39;, shape=(self.attention_size,), initializer=&#39;uniform&#39;, trainable=True) self.V = self.add_weight(name=&#39;att_var&#39;, shape=(self.attention_size,), initializer=&#39;uniform&#39;, trainable=True) super(AttentionLayer, self).build(input_shape) def call(self, inputs): self.V = K.reshape(self.V, (-1, 1)) # (attention_size，1) score = K.dot(K.tanh(K.dot(inputs, self.W) + self.b), self.V) # (None, 30, hidden_size) (hidden_size, attention_size) (None, 30, attention_size) alpha = K.softmax(score, axis=1) # // (None, 30, attention_size) (attention_size，1) (None, 30, 1) outputs = K.sum(alpha * inputs, axis=1) # (None, 30, 1) (None, 30, hidden_size) (None, hidden_size) return outputs def compute_output_shape(self, input_shape): return input_shape[0], input_shape[2] ]]></content></entry><entry><title>ml-gpu版pytorch安装</title><url>/post/ml-gpu%E7%89%88pytorch%E5%AE%89%E8%A3%85/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[驱动安装 [NVIDIA 驱动程序下载]( 官方 GeForce 驱动程序 | NVIDIA )
查看显卡型号 lspci | grep -i nvidia 安装驱动 sudo bash NVIDIA-Linux-x86_64-455.23.04.run 查看显卡信息 nvidia-smi 卸载显卡驱动重新安装 命令行界面 Ctrl+Alt+F1
sudo apt-get --purge remove nvidia* sudo apt autoremove # To remove CUDA Toolkit: sudo apt-get --purge remove &#34;*cublas*&#34; &#34;cuda*&#34; # To remove NVIDIA Drivers: sudo apt-get --purge remove &#34;*nvidia*&#34; 安装cuda 驱动和cuda对应版本 cuda_download wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda-repo-ubuntu2004-11-1-local_11.1.0-455.23.05-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2004-11-1-local_11.1.0-455.23.05-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu2004-11-1-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda 查看cuda版本
nvcc -V 不显示
首先，查看cuda的bin目录下是否有nvcc： ls /usr/local/cuda/bin 如果存在，直接将cuda路径加入系统路径即可： vim ~/.bashrc进入配置文件； 添加以下两行： export PATH=/usr/local/cuda/bin:$PATH export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 然后更新配置文件： source ~/.bashrc
conda 安装cuda
conda install cudatoolkit=10.1 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/linux-64/ 安装cudnn cuda和cudnn对应版本 cudnn_download 安装cuDNN比较简单，解压后把相应的文件拷贝到对应的CUDA目录下即可
tar -xvf cudnn-8.0-linux-x64-v5.1.tgz sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* conda安装cudnn
conda install cudnn=7.6.5 -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/linux-64/
安装anaconda 在 清华大学开源软件镜像站 下载
ls
bash Anaconda3-5.3.1-Linux-x86_64.sh
一路回车
卸载 rm -rf /home/txp/anaconda3
打开终端并输入：
sudo gedit ~/.bashrc
在.bashrc文件末尾删除之前添加的路径：
export PATH=/home/lq/anaconda3/bin:$PATH
保存并关闭文件
source ~/.bashrc
关闭终端，然后再重启一个新的终端
安装pytorch conda create -n pytorch 3070环境安装 CUDA11.1和Cudnn8.0.4从官网安装 使用conda install pytorch torchvision cudatoolkit=11 -c pytorch-nightly来安装pytorch 使用tensorflow的话 可以先conda install cudatoolkit=11 然后再pip install tensorflow-gpu==1.15 先装好tf就无法安装cudatoolkit11，conda自带的是cudatoolkit10； 先装好cudatoolkit11，再用conda装tf可能会报错，用pip不会
pytorch安装命令 conda install pytorch torchvision cudatoolkit=10.1
测试是否安装好
import torch print(torch.__version__) print(torch.cuda.is_available()) 手动下载安装具体步骤 安装tensorflow 1、创建虚拟环境： conda create -n tensorflow 复制环境 conda create -n tensorflow_new &ndash;clone Tensorflow_old 删除 conda remove -n rcnn &ndash;all 所有环境 删除没有用的包 conda clean -p 删除tar包
conda clean -t 2、激活虚拟环境： source activate tensorflow PS:如果要退出：输入 source deactivate tensorflow 3、下面我们在虚拟环境里安装
安装CPU版本：
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple/ --upgrade tensorflow # CPU版本 pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple/ --upgrade tensorflow-gpu # GPU版本 更换源 阿里云 http://mirrors.aliyun.com/pypi/simple/ 中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/ 豆瓣(douban) http://pypi.douban.com/simple/ 清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/ 中国科学技术大学 http://pypi.mirrors.ustc.edu.cn/simple/ pip install gensim -i https://pypi.douban.com/simple 添加Anaconda的清华镜像
# 清华源 conda config --add channels https://pypi.tuna.tsinghua.edu.cn/simple # 设置搜索时显示通道地址 conda config --set show_channel_urls yes # 阿里源 conda config --add channels https://mirrors.aliyun.com/pypi/simple/ #豆瓣源 conda config --add channels http://pypi.douban.com/simple/ #中科大源 conda config --add channels https://pypi.mirrors.ustc.edu.cn/simple/ #显示镜像源 conda config --show-sources #删除镜像源 conda config --remove channels https://pypi.mirrors.ustc.edu.cn/simple/ #删除清华源改回默认源 conda config --remove-key channels 或修改.condarc文件 channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - defaults show_channel_urls: true pip永久提速
打开cmd输入set命令查看用户目录USERPROFILE； 目录中创建一个pip目录,建一个文件 pip.ini 在 pip.ini 文件输入： [global] index-url = https://pypi.douban.com/simple [install] trusted-host = pypi.douban.com 12345 Mac 和 Linux 配置
打开terminal 输入命令： mkdir .pip vim .pip/pip.conf （这两步是在home目录下新建文件: .pip/pip.conf）
[global］ index-url = https://pypi.doubanio.com/simple/ timeout = 1000 【install】 use-mirrors = true mirrors = https://pypi.doubanio.com// 123456 按ESC退出插入模式后，直接输入 :wq 回车，这样就会保存并退出刚才创建的文件和输入的内容了。
]]></content></entry><entry><title>ml-infoGAN</title><url>/post/ml-infogan/</url><categories><category>ml</category></categories><tags/><content type="html">
InfoGAN的发布时间应该在是DCGAN之后没多久，可以算是在大部分的GAN模型的前面的。时间上看，InfoGAN应该是在Semi-GAN和Cat-GAN之后提出，在ImprovedGAN和ACGAN之前提出。从算法分类上看，InfoGAN属于半监督模型，但是不同于一般的半监督模型，比如，SemiGAN，CatGAN, ImprovedGAN，ACGAN等。后面的这些模型添加半监督的思路，主要是想要将GAN中D扩张为一个可以分类图像label，而不是单纯的分是否是bogus data（即，是否来自于G）。
Semi-GAN：D输出变成K+1。（1为原来的fake or not的判断， K为分类器的目标分类类数） CatGAN：D的输出变成K。结合信息熵，认为概率在每个类上越接近等概率，表示data来自于G。当然，越是集中在某个类别上，这样就可以描述具体的类别了。 ImprovedGAN：D的输出变成K。做两层的softmax。一层是在D上做，这里只是将D当做一个分类器来看待。之后，再假设有还有一个类别，即fake，K+1。由于其他K个类别数字都在变，因此假设最后一个类别数值固定也可以再加一层softmax完成。 ACGAN：将D分解，D的卷积层作为特征挖掘的层（一般也是这么认为的）。之后，对于这样的特征再做不同的映射。一个将特征映射到K上（分类器）， 一个是将特征映射到0/1上（判别器）。（判别器本质上也是分类器，这里主要是为了区别说明） 但是，会注意到，其实，无论怎么改，大家在半监督的GAN上的挖掘都是停留在D上。而忽视了G（当然也是G上不太好做文章的原因）。 一般来说，G的输入只有z 。GAN的训练方式，是将一个随机变量，通过博弈的方式，让z在G上具有意义。也就是使得没有特定信息的变量z，在通过G的映射之后，变得具有某种含义。这种含义使得z的变化会影响到G的生成效果。InfoGAN的操作，就是尝试添加其他的输入，使得这参数也具有意义。
既然要让输入的噪声向量z带有一定的语义信息，那就人为的为它添加上一些限制，于是作者把G的输入看成两部分：
一部分就是噪声z，可以将它看成是不可压缩的噪声向量。 另一部分是若干个离散的和连续的latent variables（潜变量）所拼接而成的向量c，用于代表生成数据的不同语意信息。 以MNIST数据集为例，可以用一个离散的随机变量（0-9，用于表示生成数字的具体数值）和两个连续的随机变量（假设用于表示笔划的粗细与倾斜程度）。所以此时的c由一个离散的向量（长度为10）、两个连续的向量（长度为1）拼接而成，即c长度为12。 机器学习中的各种熵 a) 自信息：事件提供的信息量，与概率成反比 b) 信息熵，自信息关于概率的期望，反映不确定度 c) 联合熵，两个事件间的不确定度 d) 条件熵，已知X下，Y的不确定度 同时条件熵和联合熵，信息熵的关系如下： e) 交叉熵，衡量两个分布的差异程度 f) 相对熵，KL散度，交叉熵和KL散度成正相关 g) 互信息，已知一个变量后，另一个变量减小了多少不确定度，本文重点 互信息式中，$H$表示计算熵值，所以$I(X;Y)$是两个熵值的差。$H(X|Y)$衡量的是“给定随机变量的情况下，随机变量$X$的不确定性”。从公式中可以看出，若$X$和$Y$是独立的，此时$H(X)=H(X|Y)$，得到$I(X;Y)=0$，为最小值。若$X$和$Y$有非常强的关联时，即已知$Y$时，$X$没有不确定性，则$H(X|Y)=0$, $I(X;Y)$达到最大值。所以为了让$G(z,c)$和$c$之间产生尽量明确的语义信息，必须要让它们二者的互信息足够的大，所以我们对GAN的损失函数添加一个正则项，就可以改写为： **注意$I(c;G(z,c))$属于G的损失函数的一部分，所以这里为负号，即让该项越大越好，使得G的损失函数变小。**其中 $\lambda$ 为平衡两个损失函数的权重。但是，在计算$I(c;G(z,c))$的过程中，需要知道后验概率分布$P(c|x)$ ，而这个分布在实际中是很难获取的，因此作者在解决这个问题时采用了变分推理的思想，引入变分分布 $Q(c|x)$来逼近$P(c|x)$ 故$ L_I (G, Q) $是互信息的一个下界。作者指出，用蒙特卡罗模拟（Monte Carlo simulation）去逼近$ L_I (G, Q)$ 是较为方便的，这样我们的优化问题就可以表示为：
从上图可以清晰的看出，虽然在设计InfoGAN时的数学推导比较复杂，但是网络架构还是非常简单明了的。G和D的网络结构和DCGAN保持一致，均由CNN构成。在此基础上，改动的地方主要有：
1.G的输入不仅仅是噪声向量z了，而是z和具有语意信息的浅变量c进行拼接后的向量输入给G。 2.D的输出在原先的基础上添加了一个新的输出分支Q，Q和D共享全部分卷积层，然后各自通过不同的全连接层输出不同的内容：Q的输出对应于$X_{fake}$的c的概率分布，D则仍然判别真伪。
InfoGAN的训练流程
假设batch_size=m，数据集为MNIST，则根据作者的方法，不可压缩噪声向量的长度为62，离散潜变量的个数为1，取值范围为[0, 9]，代表0-9共10个数字，连续浅变量的个数为2，代表了生成数字的倾斜程度和笔划粗细，最好服从[-2, 2]上的均匀分布，因为这样能够显式的通过改变其在[-2,2]上的数值观察到生成数据相应的变化，便于实验，所以此时输入变量的长度为62+10+2=74。
则在每一个epoch中：
先训练判别器k（比如3）次： 1. 从噪声分布（比如高斯分布）中随机采样出m个噪声向量： 2.从真实样本x中随机采样出m个样本： 3. 用梯度下降法使损失函数real_loss：$logD(x^{(i)})$与1之间的二分类交叉熵减小（因为最后判别器最后一层的激活函数为sigmoid，所以要与0或者1做二分类交叉熵，这也是为什么损失函数要取log的原因）。 4.用梯度下降法使损失函数fake_loss： $logD(z^{(i)})$与0之间的二分类交叉熵减小。 5. 所以判别器的总损失函数d_loss: 即让d_loss减小。注意在训练判别器的时候生成器中的所有参数要固定住，即不参加训练。 再训练生成器1次： 1. 从噪声分布中随机采样出m个噪声向量： 2. 从离散随机分布中随机采样m个长度为10、one-hot编码格式的向量： 3. 从两个连续随机分布中各随机采样m个长度为1的向量： 4. 将上面的所有向量进行concat操作，得到长度为74的向量，共m个，并记录每个向量所在的位置，便于计算损失函数。 5. 此时g_loss由三部分组成：一个是 $logD(z^{(i)})$与1之间的二分类交叉熵、一个是Q分支输出的离散浅变量的预测值和相应的输入部分的交叉熵以及Q分支输出的连续浅变量的预测值和输入部分的互信息，并为这三部分乘上适当的平衡因子，其中互信息项的系数是负的。 6. 用梯度下降法使越小越好。注意在训练生成器的时候判别器中的所有参数要固定住，即不参加训练。
直到所有epoch执行完毕，训练结束。
（四）总结
1.G的输入不再是一个单一的噪声向量，而是噪声向量与潜变量的拼接。 2.对于潜变量来说，G和D组成的大网络就好比是一个AutoEncoder，不同之处只是将信息编码在了图像中，而非向量，最后通过D解码还原回。 3.D的输出由原先的单一分支变为两个不同的分支。 4.从信息熵的角度对噪声向量和潜变量的关系完成建模，并通过数学推导以及实验的方式证明了该方法确实有效。 5.通过潜变量，使得G生成的数据具有一定的可解释性。
在实现中，$D(x)、G(z, c) $和 $Q(x)$ 分别用一个 CNN (Convolutional Neural Networks)、DCNN (DeConv Neural Networks) 、CNN来实现。
同时，潜码 c 也包含两部分：一部分是类别，服从$ Cat(K = N,p = 1/N)$，其中 N 为类别数量；另一部分是连续的与生成数据有关的参数，服从$ Unif(−1,1)$ 的分布。
在此应指出，$Q(c|x) $可以表示为一个神经网络 Q(x) 的输出。对于输入随机变量 z 和类别潜码 c，实际的$ L_I(G, Q) $可以表示为： 其中$ · $表示内积（inner product），$c $是一个选择计算哪个$ log $的参数，例如 $c_i = 1$ 而 $c_j = 0(∀j = 1,2,···,i − 1,i + 1,···,n)$，那么 z 这时候计算出的$ L_I(G,Q)$ 就等于$ log(Q(z,c)i)$。这里我们可以消去 $H(c)$，因为$ c $的分布是固定的，即优化目标与 $H(c) $无关： 而对于参数潜码，我们假设它符合正态分布，神经网络$ Q(x)$ 则输出其预测出的该潜码的均值和标准差， 我们知道，对于均值$ μ$，标准差$ σ $的随机变量，其概率密度函数为： 要计算参数潜码 c 的 ，就是要计算 $log p(c)$，即： 设$ Q(x) $输出的参数潜码$ c$ 的均值$ μ$，标准差 $σ$ 分别为$ Q(x)μ$ 和 $Q(x)σ$，那么对于参数潜码$ c$： 同样的，我们可以消去 $H(c)$，因为 $c $的分布是固定的，那么： 读完论文，我们发现，对于类别潜码，这个$ L_I $本质上是$ x$ 与$ G(z, c) $之间的 KL 散度：
也就是说：
而$ min DKL(c||Q(G(z, c)))$ 意味着减小$ c$ 与$ Q(G(z, c)) $的差别。
▲ 图7. 普通GAN和InfoGAN的LI在训练过程中的比较
如果我们不考虑 $Q(x)σ $的影响，$L_I$ 的优化过程： 而 也意味着减小$ c $与 $Q(G(z, c))μ$ 的差。
再纵观整个模型，我们会发现这一对$ L_I $优化的过程，实质上是以 G 为编码器（Encoder）， Q 为解码器（Decoder），生成的图像作为我们要编码的码（code），训练一个自编码器（Autoencoder），也就是说，作者口中的信息论优化问题，本质上是无监督训练问题。</content></entry><entry><title>ml-Jupyter-Notebook</title><url>/post/ml-jupyter-notebook/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[安装 jupyter conda install ipython conda install jupyter 换主题 pip install jupyterthemes 查看可用的 Jupyter 主题 jt -l 更换 Jupyter 主题 jt -t onedork -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -T -m 10 -t 主题 -f(字体) -fs(字体大小) -cellw(占屏比或宽度) -ofs(输出段的字号) -T(显示工具栏) -T(显示自己主机名) 恢复 Jupyter 默认风格 jt -r 画图 from jupyterthemes import jtplot jtplot.style() 安装插件 pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install --user ###### Table of Contents 通过 TOC 链接你可以定位到页面中的任何位置。 ###### Autopep8 获得简洁代码 ###### variable inspector 跟踪你的工作空间 ###### ExecuteTime 显示单元格的运行时间和耗时 ###### Collapsible headings 收起/放下Notebook中的某些内容 ###### Code folding 折叠代码。 ###### Notify 在任务处理完后及时向你发送通知。 #　无法导包
在jupyter中添加tensorflow核
conda install -n tomding ipykernel python -m ipykernel install --user --name tomding --display-name tomding conda install -n pytorch ipykernel python -m ipykernel install --user --name pytorch --display-name pytorch 远程 内网 1.生成配置文件 jupyter notebook --generate-config 2.创建远程登录密码 jupyter notebook password 注意，此方法生成的密码文件为/root/.jupyter/jupyter_notebook_config.json （如果是以root用户登录，其他用户路径同上说明），此为json文件，而且json文件里的密码的优先级要高于配置文件（jupyter_notebook_config.py）里的密码设置。 3.修改配置文件 vim ~/.jupyter/jupyter_notebook_config.py c.NotebookApp.ip=&#39;*&#39; c.NotebookApp.password = &#39;密码&#39; c.NotebookApp.open_browser = False c.NotebookApp.port =8888 #可自行指定一个端口, 访问时使用该端口 c.NotebookApp.allow_remote_access = True 4.启动 jupyter notebook 5.远程登录 输入http://服务器的 IP 地址:8888，输入密码登录。 ]]></content></entry><entry><title>ml-keras</title><url>/post/ml-keras/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[tensorboard writer=tf.summary.FileWriter(&#39;/path/to/logs&#39;, tf.get_default_graph()) writer.close() 在上面程序的8、9行中，创建一个writer，将tensorboard summary写入文件夹/path/to/logs， 然后运行上面的程序，在程序定义的日志文件夹/path/to/logs目录下，生成了一个新的日志文件events.out.tfevents.1524711020.bdi-172， tensorboard –logdir /path/to/logs
https://blog.csdn.net/fendouaini/article/details/80344591 https://blog.csdn.net/fendouaini/article/details/80368770 cb.append(keras.callbacks.TensorBoard( log_dir=os.path.join(logs_path, &#39;TensorBoard&#39;), histogram_freq=0, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq=&#39;epoch&#39;)) 该类在keras.callbacks模块中。它的参数列表如下：
log_dir: 用来保存被 TensorBoard 分析的日志文件的文件名。 histogram_freq:对于模型中各个层计算激活值和模型权重直方图的频率（训练轮数中。 该参数用于设置tensorboard面板中的histograms和distributions面板如果设置成 0 ，直方图不会被计算。对于直方图可视化的验证数据（或分离数据）一定要明确的指出。 write_graph: 是否在 TensorBoard 中可视化图。 如果 write_graph 被设置为 True。 write_grads: 是否在 TensorBoard 中可视化梯度值直方图。 histogram_freq 必须要大于 0 。 batch_size: 用以直方图计算的传入神经元网络输入批的大小。 write_images: 是否在 TensorBoard 中将模型权重以图片可视化，如果设置为True，日志文件会变得非常大。 embeddings_freq: 被选中的嵌入层会被保存的频率（在训练轮中）。 embeddings_layer_names: 一个列表，会被监测层的名字。 如果是 None 或空列表，那么所有的嵌入层都会被监测。 embeddings_metadata: 一个字典，对应层的名字到保存有这个嵌入层元数据文件的名字。 查看 详情 关于元数据的数据格式。 以防同样的元数据被用于所用的嵌入层，字符串可以被传入。 embeddings_data: 要嵌入在 embeddings_layer_names 指定的层的数据。 Numpy 数组（如果模型有单个输入）或 Numpy 数组列表（如果模型有多个输入）。 Learn ore about embeddings。 update_freq: &lsquo;batch&rsquo; 或 &rsquo;epoch&rsquo; 或 整数。当使用 &lsquo;batch&rsquo; 时，在每个 batch 之后将损失和评估值写入到 TensorBoard 中。同样的情况应用到 &rsquo;epoch&rsquo; 中。如果使用整数，例如 10000，这个回调会在每 10000 个样本之后将损失和评估值写入到 TensorBoard 中。注意，频繁地写入到 TensorBoard 会减缓你的训练。 自己定义在tensorboard中显示acc和loss import numpy as np import tensorflow as tf from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation,Conv2D,MaxPool2D,Flatten from keras.optimizers import SGD from keras.datasets import mnist from keras.utils import to_categorical from keras.callbacks import TensorBoard def create_model(): model = Sequential() model.add(Conv2D(32, (5,5), activation=&#39;relu&#39;, input_shape=[28, 28, 1])) model.add(Conv2D(64, (5,5), activation=&#39;relu&#39;)) model.add(MaxPool2D(pool_size=(2,2))) #池化层 model.add(Flatten()) #平铺层 model.add(Dropout(0.5)) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(10, activation=&#39;softmax&#39;)) return model def compile_model(model): model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#34;adam&#34;,metrics=[&#39;acc&#39;]) return model def train_model(model,x_train,y_train,x_val,y_val,batch_size=128,epochs=10): train_loss = tf.placeholder(tf.float32, [],name=&#39;train_loss&#39;) train_acc = tf.placeholder(tf.float32, [],name=&#39;train_acc&#39;) val_loss = tf.placeholder(tf.float32, [],name=&#39;val_loss&#39;) val_acc = tf.placeholder(tf.float32, [],name=&#39;val_acc&#39;) #可视化训练集、验证集的loss、acc、四个指标，均是标量scalers tf.summary.scalar(&#34;train_loss&#34;, train_loss) tf.summary.scalar(&#34;train_acc&#34;, train_acc) tf.summary.scalar(&#34;val_loss&#34;, val_loss) tf.summary.scalar(&#34;val_acc&#34;, val_acc) merge=tf.summary.merge_all() batches=int(len(x_train)/batch_size) #没一个epoch要训练多少次才能训练完样本 with tf.Session() as sess: logdir = &#39;./logs&#39; writer = tf.summary.FileWriter(logdir, sess.graph) for epoch in range(epochs): #用keras的train_on_batch方法进行训练 print(F&#34;正在训练第 {epoch+1} 个 epoch&#34;) for i in range(batches): #每次训练128组数据 train_loss_,train_acc_ = model.train_on_batch(x_train[i*128:(i+1)*128:1,...],y_train[i*128:(i+1)*128:1,...]) #验证集只需要每一个epoch完成之后再验证即可 val_loss_,val_acc_ = model.test_on_batch(x_val,y_val) summary=sess.run(merge,feed_dict={train_loss:train_loss_,train_acc:train_acc_,val_loss:val_loss_,val_acc:val_acc_}) writer.add_summary(summary,global_step=epoch) if __name__==&#34;__main__&#34;: (x_train,y_train),(x_test,y_test) = mnist.load_data() #数据我已经下载好了 print(np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test)) #(60000, 28, 28) (60000,) (10000, 28, 28) (10000,) x_train=np.expand_dims(x_train,axis=3) x_test=np.expand_dims(x_test,axis=3) y_train=to_categorical(y_train,num_classes=10) y_test=to_categorical(y_test,num_classes=10) print(np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test)) #(60000, 28, 28, 1) (60000, 10) (10000, 28, 28, 1) (10000, 10) x_train_=x_train[1:50000:1,...] #重新将训练数据分成训练集50000组 x_val_=x_train[50000:60000:1,...] #重新将训练数据分成测试集10000组 y_train_=y_train[1:50000:1,...] y_val_=y_train[50000:60000:1,...] print(np.shape(x_train_),np.shape(y_train_),np.shape(x_val_),np.shape(y_val_),np.shape(x_test),np.shape(y_test)) #(49999, 28, 28, 1) (49999, 10) (10000, 28, 28, 1) (10000, 10) (10000, 28, 28, 1) (10000, 10) model=create_model() #创建模型 model=compile_model(model) #编译模型 train_model(model,x_train_,y_train_,x_val_,y_val_) # 可视化 模型可视化
from keras.utils import plot_model keras.utils.plot_model(model, to_file=&#39;model.png&#39;, show_shapes=&#39;Ture&#39;, dpi=200) 训练历史可视化
import matplotlib.pyplot as plt history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1) # 绘制训练 &amp; 验证的准确率值 plt.plot(history.history[&#39;acc&#39;]) 学习率调整 import keras.backend as K from keras.callbacks import LearningRateScheduler def lr_scheduler(epoch): initial_lrate = 0.1 drop = 0.5 epochs_drop = 10.0 lrate = initial_lrate * math.pow(drop, math.floor((1+e )/epochs_drop)) return lrate ------------------------------------------------------------------------- # 每隔2个epoch，学习率减小为原来的1/10 if epoch % 2 == 0 and epoch != 0: lr = K.get_value(model.optimizer.lr) K.set_value(model.optimizer.lr, lr * 0.1) print(&#34;lr changed to {}&#34;.format(lr * 0.1)) return K.get_value(model.optimizer.lr) reduce_lr = keras.callbacks.LearningRateScheduler(lr_scheduler) model.fit(train_x, train_y, batch_size=32, epochs=300, callbacks=[reduce_lr]) from keras.callbacks import ReduceLROnPlateau reduce_lr = ReduceLROnPlateau(monitor=&#39;val_loss&#39;, patience=2, mode=&#39;auto&#39;) model.fit(train_x, train_y, batch_size=32, epochs=300, validation_split=0.1, callbacks=[reduce_lr]) keras.callbacks.ReduceLROnPlateau(monitor=&lsquo;val_loss&rsquo;, factor=0.1, patience=10, verbose=0,, min_delta=0.0001,mode=&lsquo;auto&rsquo;, epsilon=0.0001, cooldown=0, min_lr=0) monitor：被监测的量 factor：每次减少学习率的因子，学习率将以lr = lr*factor的形式被减少 patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发 min_delta: 增大或减小的阈值。 mode：‘auto’，‘min’，‘max’之一，在min模式下，如果检测值触发学习率减少。在max模式下，当检测值不再上升则触发学习率减少。 epsilon：阈值，用来确定是否进入检测值的“平原区” cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作会经过cooldown个epoch才会重新计算被监控的指标没有提高(或者减少)的轮次(即patience).设置这个参数是因为减少学习率时, 模型的损失函数可能不在最优解附近,而训练至最优解附近需要一定轮次, 如果不设置则会导致学习率在远离最优解时接连衰减导致训练陷入僵局 min_lr：学习率的下限
tf.keras.layers中网络配置： activation：设置层的激活函数。此参数由内置函数的名称指定，或指定为可调用对象。默认情况下，系统不会应用任何激活函数。
kernel_initializer 和 bias_initializer：创建层权重（核和偏差）的初始化方案。此参数是一个名称或可调用对象，默认为 &ldquo;Glorot uniform&rdquo; 初始化器。
random_uniform：初始化权重为（-0.05，0.05）之间的均匀随机的微小数值。换句话说，给定区间里的任何值都可能作为权重。 random_normal：根据高斯分布初始化权重，平均值为0，标准差为0.05。如果你不熟悉高斯分布，可以回想一下对称钟形曲线。 zero：所有权重初始化为0。
kernel_regularizer 和 bias_regularizer：应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。
我们也可以得到网络的变量、权重矩阵、偏置等 print(layer.variables) # 包含了权重和偏置 print(layer.kernel, layer.bias) # 也可以分别取出权重和偏置
layers.Dense(32, activation=&#39;sigmoid&#39;) layers.Dense(32, activation=tf.sigmoid) layers.Dense(32, kernel_initializer=&#39;orthogonal&#39;) layers.Dense(32, kernel_initializer=tf.keras.initializers.glorot_normal) layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01)) layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l1(0.01)) metric评估 1)accuracy 如我们有6个样本，其真实标签y_true为[0, 1, 3, 3, 4, 2]，但被一个模型预测为了[0, 1, 3, 4, 4, 4]，即y_pred=[0, 1, 3, 4, 4, 4]，那么该模型的accuracy=4/6=66.67%。 2)binary_accuracy 它适用于2分类的情况。从上图中可以看到binary_accuracy的计算除了y_true和y_pred外，还有一个threshold参数，该参数默认为0.5。比如有6个样本，其y_true为[0, 0, 0, 1, 1, 0]，y_pred为[0.2, 0.3, 0.6, 0.7, 0.8, 0.1].具体计算方法为：1）将y_pred中的每个预测值和threshold对比，大于threshold的设为1，小于等于threshold的设为0，得到y_pred_new=[0, 0, 1, 1, 1, 0]；2）将y_true和y_pred_new代入到2.1中计算得到最终的binary_accuracy=5/6=87.5%。 3)categorical_accuracy **针对的是y_true为onehot标签，y_pred为向量的情况。**比如有4个样本，其y_true为[[0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0]]，y_pred为[[0.1, 0.6, 0.3], [0.2, 0.7, 0.1], [0.3, 0.6, 0.1], [0.9, 0, 0.1]]。具体计算方法为：1）将y_true转为非onehot的形式，即y_true_new=[2, 1, 1, 0]；2）根据y_pred中的每个样本预测的分数得到y_pred_new=[1, 1, 1, 0]；3）将y_true_new和y_pred_new代入到2.1中计算得到最终的categorical_accuracy=75%。 4)sparse_categorical_accuracy **和categorical_accuracy功能一样，只是其y_true为非onehot的形式。**比如有4个样本，其y_true为[2， 1， 1， 0]，y_pred为[[0.1, 0.6, 0.3], [0.2, 0.7, 0.1], [0.3, 0.6, 0.1], [0.9, 0, 0.1]]。具体计算方法为：1）根据y_pred中的每个样本预测的分数得到y_pred_new=[1, 1, 1, 0]；2）将y_true和y_pred_new代入到2.1中计算得到最终的categorical_accuracy=75%。 5)top_k_categorical_accuracy **在categorical_accuracy的基础上加上top_k。**categorical_accuracy要求样本在真值类别上的预测分数是在所有类别上预测分数的最大值，才算预测对，而top_k_categorical_accuracy只要求样本在真值类别上的预测分数排在其在所有类别上的预测分数的前k名就行。比如有4个样本，其y_true为[[0, 0, 1], [0, 1, 0], [0, 1, 0], [1, 0, 0]]，y_pred为[[0.3, 0.6, 0.1], [0.5, 0.4, 0.1], [0.3, 0.6, 0.1], [0.9, 0, 0.1]]。具体计算方法为：1）将y_true转为非onehot的形式，即y_true_new=[2, 1, 1, 0]；2）计算y_pred的top_k的label，比如k=2时，y_pred_new = [[0, 1], [0, 1], [0, 1], [0, 2]]；3）根据每个样本的真实标签是否在预测标签的top_k内来统计准确率，上述4个样本为例，2不在[0, 1]内，1在[0, 1]内，1在[0, 1]内，0在[0, 2]内，4个样本总共预测对了3个，因此k=2时top_k_categorical_accuracy=75%。说明一下，Keras中计算top_k_categorical_accuracy时默认的k值为5。 6)sparse_top_k_categorical_accuracy **和top_k_categorical_accuracy功能一样，只是其y_true为非onehot的形式。**比如有4个样本，其y_true为[2， 1， 1， 0]，y_pred为[[0.3, 0.6, 0.1], [0.5, 0.4, 0.1], [0.3, 0.6, 0.1], [0.9, 0, 0.1]]。计算步骤如下：1）计算y_pred的top_k的label，比如k=2时，y_pred_new = [[0, 1], [0, 1], [0, 1], [0, 2]]；2）根据每个样本的真实标签是否在预测标签的top_k内来统计准确率，上述4个样本为例，2不在[0, 1]内，1在[0, 1]内，1在[0, 1]内，0在[0, 2]内，4个样本总共预测对了3个，因此k=2时top_k_categorical_accuracy=75%。 ROC,AUC import tensorflow as tf from sklearn.metrics import roc_auc_score def auroc(y_true, y_pred): return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double) # Build Model... model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;, auroc]) -------------------------------------------------------------------------------- class RocAucEvaluation(Callback): def __init__(self, validation_data=(), interval=1): super(Callback, self).__init__() self.interval = interval self.x_val,self.y_val = validation_data def on_epoch_end(self, epoch, log={}): if epoch % self.interval == 0: y_pred = self.model.predict_proba(self.x_val, verbose=0) score = roc_auc_score(self.y_val, y_pred) print(&#39;\n ROC_AUC - epoch:%d - score:%.6f \n&#39; % (epoch+1, score)) x_train, x_val, y_train, y_val = train_test_split(x_train_nn, y_train_nn, train_size=0.9, random_state=233) RocAuc = RocAucEvaluation(validation_data=(y_train,y_label), interval=1) hist = model.fit(x_train, x_label, batch_size=batch_size, epochs=epochs, validation_data=(y_train, y_label), callbacks=[RocAuc], verbose=2) 训练 对.fit的调用在这里做出两个主要假设： 我们的整个训练集可以放入RAM 没有数据增强（即不需要Keras生成器）
真实世界的数据集通常太大而无法放入内存中 它们也往往具有挑战性，要求我们执行数据增强以避免过拟合并增加我们的模型的泛化能力 在这些情况下，我们需要利用Keras的.fit_generator函数：
# initialize the number of epochs and batch size EPOCHS = 100 BS = 32 # construct the training image generator for data augmentation aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=&#34;nearest&#34;) # train the network H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS) 对于寻求对Keras模型进行精细控制（ finest-grained control）的深度学习实践者，您可能希望使用.train_on_batch例如数据迭代过程非常复杂并且需要自定义代码。
模型评估 print(model.evaluate(x_test,y_test))
y = model.predict_classes(x_test) print(accuracy_score(y_test,y))
构建高级模型 模型子类化 通过对 tf.keras.Model 进行子类化并定义您自己的前向传播来构建完全可自定义的模型。在 init 方法中创建层并将它们设置为类实例的属性。在 call 方法中定义前向传播
class MyModel(tf.keras.Model): def __init__(self, num_classes=10): super(MyModel, self).__init__(name=&#39;my_model&#39;) self.num_classes = num_classes self.layer1 = layers.Dense(32, activation=&#39;relu&#39;) self.layer2 = layers.Dense(num_classes, activation=&#39;softmax&#39;) def call(self, inputs): h1 = self.layer1(inputs) out = self.layer2(h1) return out def compute_output_shape(self, input_shape): shape = tf.TensorShape(input_shape).as_list() shape[-1] = self.num_classes return tf.TensorShape(shape) model = MyModel(num_classes=10) 自定义层 通过对 tf.keras.layers.Layer 进行子类化并实现以下方法来创建自定义层：
__init__()函数，你可以在其中执行所有与输入无关的初始化 build： 可以获得输入张量的形状， 创建层的权重。使用 add_weight 方法添加权重。 call： 构建网络结构， 定义前向传播。 compute_output_shape：指定在给定输入形状的情况下如何计算层的输出形状。 或者，可以通过实现 get_config 方法和 from_config 类方法序列化层。 class MyLayer(layers.Layer): def __init__(self, output_dim, **kwargs): super(MyLayer, self).__init__(**kwargs) self.output_dim = output_dim def build(self, input_shape): shape = tf.TensorShape((input_shape[1], self.output_dim)) self.kernel = self.add_weight(name=&#39;kernel1&#39;, shape=shape, initializer=&#39;uniform&#39;, trainable=True) super(MyLayer, self).build(input_shape) def call(self, inputs): return tf.matmul(inputs, self.kernel) def compute_output_shape(self, input_shape): shape = tf.TensorShape(input_shape).as_list() shape[-1] = self.output_dim return tf.TensorShape(shape) def get_config(self): base_config = super(MyLayer, self).get_config() base_config[&#39;output_dim&#39;] = self.output_dim return base_config @classmethod def from_config(cls, config): return cls(**config) model = tf.keras.Sequential([ MyLayer(10), layers.Activation(&#39;softmax&#39;)]) 损失函数 • mean_ squared_ error 或mse 。 • mean_absolute_error 或mae 。 • mean_ absolute_percentage_error 或mape 。 • mean_squared_logarithmic_error 或msle 。 • squared_hinge 。 • hinge 。 • binary_ crossentropy 。 • categorical_ crossentropy 。 • sparse_ categorical_ crossentrop 。 • kullback_lei bier_ divergence 。 • poisson 。 • cosine_proximity 。
注意：当使用categorical_crossentropy 作为目标函数时，标签应该为多类模式，即one-hot 形式编码的向量，而不是单个数值。用户可以使用工具中的to_ categorical 函数完成该转换.
from keras.utils.np_utils import to_categorical int_labels= [1,2,3] categorical_labels=to_categorical(int_labels, num classes=None) print(categorical_labels) 优化器函数 选定了整个深度网络的损失函数，紧接着需要考虑的就是优化器的选择。因为有了训练目标，剩下最重要的就是达成该目标的方法
保存和恢复 权重保存 model.save_weights(&#39;./model.h5&#39;) model.load_weights(&#39;./model.h5&#39;) 保存网络结构 这样导出的模型并未包含训练好的参数
# 序列化成json import json with open(&#39;model_struct.json&#39;, &#39;w&#39;) as ff: json_config = am.model.to_json() ff.write(json_string) # 保存模型信息 with open(&#39;model_struct.json&#39;, &#39;r&#39;) as ff: json_config = ff.read() reinitialized_model = keras.models.model_from_json(json_config) new_prediction = reinitialized_model.predict(x_test) # 其他形式 config = model.get_config() reinitialized_model = keras.Model.from_config(config) new_prediction = reinitialized_model.predict(x_test) # 保持为yaml格式 #需要提前安装pyyaml yaml_str = model.to_yaml() print(yaml_str) fresh_model = tf.keras.models.model_from_yaml(yaml_str) 保存整个模型 内容包括：架构;权重（在训练期间学到的）;训练配置（你传递给编译的），如果有的话;优化器及其状态（如果有的话）（这使您可以从中断的地方重新启动训练）
model.save(&#39;all_model.h5&#39;) new_model = keras.models.load_model(&#39;the_save_model.h5&#39;) new_prediction = new_model.predict(x_test) np.testing.assert_allclose(predictions, new_prediction, atol=1e-6) # 预测结果一样 # 保存为SavedModel文件 keras.experimental.export_saved_model(model, &#39;saved_model&#39;) new_model = keras.experimental.load_from_saved_model(&#39;saved_model&#39;) checkpoint checkpoint_path = os.path.join(os.path.join(logs_path, &#39;Checkpoint&#39;), &#39;weights-improvement-{epoch:02d}-{loss:.2f}.hdf5&#39;) cb.append(keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=&#39;loss&#39;, verbose=0, save_best_only=True, save_weights_only=False, mode=&#39;auto&#39;)) # 恢复至最近的checkpoint latest=tf.train.latest_checkpoint(checkpoint_dir) model = create_model() model.load_weights(latest) 模型集成 import numpy as np from tensorflow.keras.wrappers.scikit_learn import KerasClassifier from sklearn.ensemble import VotingClassifier from sklearn.metrics import accuracy_score def mlp_model(): model = keras.Sequential([ layers.Dense(64, activation=&#39;relu&#39;, input_shape=(784,)), layers.Dropout(0.2), layers.Dense(64, activation=&#39;relu&#39;), layers.Dropout(0.2), layers.Dense(64, activation=&#39;relu&#39;), layers.Dropout(0.2), layers.Dense(10, activation=&#39;softmax&#39;) ]) model.compile(optimizer=keras.optimizers.SGD(), loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[&#39;accuracy&#39;]) return model # 下面是使用投票的方法进行模型集成 model1 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0) model2 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0) model3 = KerasClassifier(build_fn=mlp_model, epochs=100, verbose=0) ensemble_clf = VotingClassifier(estimators=[(&#39;model1&#39;, model1), (&#39;model2&#39;, model2), (&#39;model3&#39;, model3)], voting=&#39;soft&#39;) ensemble_clf.fit(x_train, y_train) y_pred = ensemble_clf.predict(x_test) print(&#39;acc: &#39;, accuracy_score(y_pred, y_test)) MLP样例 import tensorflow as tf import numpy as np from tensorflow import keras from tensorflow.keras import layers print(tf.__version__) print(tf.keras.__version__) # 生成数据 train_x = np.random.random((1000, 72)) train_y = np.random.random((1000, 10)) val_x = np.random.random((200, 72)) val_y = np.random.random((200, 10)) test_x = np.random.random((1000, 72)) test_y = np.random.random((1000, 10)) dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y)) dataset = dataset.batch(32).repeat() val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y)) val_dataset = val_dataset.batch(32).repeat() test_data = tf.data.Dataset.from_tensor_slices((test_x, test_y)) test_data = test_data.batch(32).repeat() # 模型堆叠 model = tf.keras.Sequential([ layers.Dense(32, activation=&#39;relu&#39;, input_shape=(72,)), layers.BatchNormalization(), layers.Dropout(0.2), layers.Dense(32, activation=&#39;relu&#39;), layers.BatchNormalization(), layers.Dropout(0.2), layers.Dense(10, activation=&#39;softmax&#39;)]) model.compile(optimizer=keras.optimizers.SGD(0.1), loss=tf.keras.losses.categorical_crossentropy, metrics=[tf.keras.metrics.categorical_accuracy]) model.summary() # 网络图 # !sudo apt-get install graphvizf # keras.utils.plot_model(model, &#39;model_info.png&#39;, show_shapes=True) history = model.fit(train_data, epochs=10, steps_per_epoch=30) # 画出学习曲线 # print(history.history.keys()) plt.plot(history.history[&#39;categorical_accuracy&#39;]) plt.plot(history.history[&#39;loss&#39;]) plt.legend([&#39;categorical_accuracy&#39;, &#39;loss&#39;], loc=&#39;upper left&#39;) plt.show() # 评估与预测 result = model.predict(test_data, steps=30) print(result) loss, accuracy = model.evaluate(test_data, steps=30) print(&#39;test loss:&#39;, loss) print(&#39;test accuracy:&#39;, accuracy) 多输入与多输出网络 import numpy as np # 载入输入数据 title_data = np.random.randint(num_words, size=(1280, 10)) body_data = np.random.randint(num_words, size=(1280, 100)) tag_data = np.random.randint(2, size=(1280, num_tags)).astype(&#39;float32&#39;) # 标签 priority_label = np.random.random(size=(1280, 1)) department_label = np.random.randint(2, size=(1280, num_departments)) # 超参 num_words = 2000 num_tags = 12 num_departments = 4 # 输入 title_input = keras.Input(shape=(None,), name=&#39;title&#39;) body_input = keras.Input(shape=(None,), name=&#39;body&#39;) tag_input = keras.Input(shape=(num_tags,), name=&#39;tag&#39;) # 嵌入层 title_feat = layers.Embedding(num_words, 64)(title_input) body_feat = layers.Embedding(num_words, 64)(body_input) # 特征提取层 title_feat = layers.Embedding(num_words, 64)(title_input) body_feat = layers.LSTM(32)(body_feat) features = layers.concatenate([title_feat,body_feat, tag_input]) # 分类层 priority_pred = layers.Dense(1, activation=&#39;sigmoid&#39;, name=&#39;priority&#39;)(features) department_pred = layers.Dense(num_departments, activation=&#39;softmax&#39;, name=&#39;department&#39;)(features) # 构建模型 model = keras.Model(inputs=[body_input, title_input, tag_input], outputs=[priority_pred, department_pred]) model.summary() keras.utils.plot_model(model, &#39;multi_model.png&#39;, show_shapes=True) model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss={&#39;priority&#39;: &#39;binary_crossentropy&#39;, &#39;department&#39;: &#39;categorical_crossentropy&#39;}, loss_weights=[1., 0.2]) # 训练 history = model.fit( {&#39;title&#39;: title_data, &#39;body&#39;:body_data, &#39;tag&#39;:tag_data}, {&#39;priority&#39;:priority_label, &#39;department&#39;:department_label}, batch_size=32, epochs=5) layers.Conv2D((filters,kernel_size,strides=(1, 1),padding='valid',data_format=None,dilation_rate=(1,1),activation=None,use_bias=True,kernel_initializer='glorot_uniform',bias_initializer='zeros',kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,kernel_constraint=None,bias_constraint=None,**kwargs)
layers.MaxPooling2D(pool_size=(2, 2),strides=None,padding='valid',data_format=None,**kwargs)
小型残差网络 inputs = keras.Input(shape=(32,32,3), name=&#39;img&#39;) h1 = layers.Conv2D(32, 3, activation=&#39;relu&#39;)(inputs) h1 = layers.Conv2D(64, 3, activation=&#39;relu&#39;)(h1) block1_out = layers.MaxPooling2D(3)(h1) h2 = layers.Conv2D(64, 3, activation=&#39;relu&#39;, padding=&#39;same&#39;)(block1_out) h2 = layers.Conv2D(64, 3, activation=&#39;relu&#39;, padding=&#39;same&#39;)(h2) block2_out = layers.add([h2, block1_out]) h3 = layers.Conv2D(64, 3, activation=&#39;relu&#39;, padding=&#39;same&#39;)(block2_out) h3 = layers.Conv2D(64, 3, activation=&#39;relu&#39;, padding=&#39;same&#39;)(h3) block3_out = layers.add([h3, block2_out]) h4 = layers.Conv2D(64, 3, activation=&#39;relu&#39;)(block3_out) h4 = layers.GlobalMaxPool2D()(h4) h4 = layers.Dense(256, activation=&#39;relu&#39;)(h4) h4 = layers.Dropout(0.5)(h4) outputs = layers.Dense(10, activation=&#39;softmax&#39;)(h4) model = keras.Model(inputs, outputs, name=&#39;small resnet&#39;) model.summary() keras.utils.plot_model(model, &#39;small_resnet_model.png&#39;, show_shapes=True) (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() x_train = x_train.astype(&#39;float32&#39;) / 255 x_test = y_train.astype(&#39;float32&#39;) / 255 y_train = keras.utils.to_categorical(y_train, 10) y_test = keras.utils.to_categorical(y_test, 10) model.compile(optimizer=keras.optimizers.RMSprop(1e-3), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;acc&#39;]) model.fit(x_train, y_train, batch_size=64, epochs=1, validation_split=0.2) #model.predict(x_test, batch_size=32) 封装被sklearn调用 from keras.models import Sequential from keras.layers import Dense from keras.wrappers.scikit_learn import KerasClassifier from sklearn.cross_validation import StratifiedKFold from sklearn.cross_validation import cross_val_score import numpy import pandas # Function to create model, required for KerasClassifier def create_model(): model = Sequential() model.add(Dense(12, input_dim=8, init=&#39;uniform&#39;, activation=&#39;relu&#39;)) model.add(Dense(8, init=&#39;uniform&#39;, activation=&#39;relu&#39;)) model.add(Dense(1, init=&#39;uniform&#39;, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=	[&#39;accuracy&#39;]) return model # fix random seed for reproducibility seed = 7 numpy.random.seed(seed) # load pima indians dataset dataset = numpy.loadtxt(&#34;pima-indians-diabetes.csv&#34;, delimiter=&#34;,&#34;) # split into input (X) and output (Y) variables X = dataset[:,0:8] Y = dataset[:,8] # Keras的KerasClassifier和KerasRegressor两个类接受build_fn参数，传入编译好的模型。我们加入nb_epoch=150和batch_size=10这两个参数这两个参数会传入模型的fit()方法。 model = KerasClassifier(build_fn=create_model, nb_epoch=150, batch_size=10) # 用scikit-learn的StratifiedKFold类进行10折交叉验证，测试模型在未知数据的性能，并使用cross_val_score()函数检测模型，打印结果。 # StratifiedKFold用法类似Kfold，但是他是分层采样，确保训练集，测试集中各类别样本的比例与原始数据集中相同 kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed) results = cross_val_score(model, X, Y, cv=kfold, , verbose=0, cv=5, n_jobs=-1, scoring=make_scorer(mean_absolute_error)) print(&#34;Results: %.2f%% (%.2f%%)&#34; % (results.mean()*100, results.std()*100)) # 使用网格搜索调整深度学习模型的参数 model = KerasClassifier(build_fn=create_model) param_grid = {&#39;optimizers&#39;: [&#39;rmsprop&#39;, &#39;adam&#39;], &#39;kernel_initializer&#39;: [&#39;glorot_uniform&#39;, &#39;normal&#39;, &#39;uniform&#39;], &#39;use_bias&#39;: [&#39;True&#39;, &#39;False&#39;], &#39;epochs&#39;: np.array([50, 100, 150]), &#39;batch_size&#39;: np.array([5, 10, 20])} # GridSearchCV会对每组参数（2×3×3×3）进行训练，进行3折交叉检验。 grid = GridSearchCV(estimator=model, param_grid=param_grid) grid_result = grid.fit(X, Y) # summarize results print(&#34;Best: %f using %s&#34; % (grid_result.best_score_, grid_result.best_params_)) for params, mean_score, scores in grid_result.grid_scores_: print(&#34;%f (%f) with: %r&#34; % (scores.mean(), scores.std(), params)) # Best: 0.751302 using {&#39;init&#39;: &#39;uniform&#39;, &#39;optimizer&#39;: &#39;rmsprop&#39;, &#39;nb_epoch&#39;: 150, &#39;batch_size&#39;: 5} #0.653646 (0.031948) with: {&#39;init&#39;: &#39;glorot_uniform&#39;, &#39;optimizer&#39;: &#39;rmsprop&#39;, &#39;nb_epoch&#39;: 50, &#39;batch_size&#39;: 5} #0.665365 (0.004872) with: {&#39;init&#39;: &#39;glorot_uniform&#39;, &#39;optimizer&#39;: &#39;adam&#39;, &#39;nb_epoch&#39;: 50, &#39;batch_size&#39;: 5} # 0.683594 (0.037603) with: {&#39;init&#39;: &#39;glorot_uniform&#39;, &#39;optimizer&#39;: &#39;rmsprop&#39;, &#39;nb_epoch&#39;: 100, &#39;batch_size&#39;: 5} 卷积 、 池化 二维卷积： keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Conv2D输入：(samples, rows, cols, channels) kernel：尺寸(k, k)， 数量filters 输出：（samples, new_rows, new_cols, filters)， new_rows=(rows-k+2padding)/strides+1 备注：实际计算时，kernel维度为(k,k,channels)，会包含所有channels维度，因此若filters=1，即只有一个卷积核，则输出为(samples, new_rows,new_cols,1) ####一维卷积： keras.layers.Conv1D(filters, kernel_size, strides=1, padding='valid', data_format='channels_last', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None) Conv1D输入：(batch_size, steps, input_dim) 行向量代表单个时间步，单个时间步包含特征维度input_dim 列向量代表单个特征维度，单个特征维度包含时间步长steps kernel：尺寸k， 数量filters 输出：(batch_size, new_steps, filters)，new_steps=(steps-k+2padding)/strides+1 备注：实际计算时，kernel维度为(k,input_dim)，会包含所有input_dim（这里的input_dim与Conv2D中的channels类似） ####一维最大池化 keras.layers.MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last') 输入为 3D 张量，尺寸为： (batch_size, steps, features) 输出为 3D 张量，尺寸为： (batch_size, downsampled_steps, features) Maxpooling1D(3,2))池化核大小为3，步长为2，(8-3＋1)/2=3, 注意:若model.add(Maxpooling1D(2))，则池化核大小为2，步长也为2。
学习 https://www.jianshu.com/p/3a8b310227e6 &#34;&#34;&#34; 训练 &#34;&#34;&#34; from keras.datasets import mnist from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Conv2D, MaxPool2D, Flatten, Dropout, Dense from keras.losses import categorical_crossentropy from keras.optimizers import Adadelta (x_train, y_train), (x_test, y_test) = mnist.load_data() # 将原始的特征矩阵做数据处理形成模型需要的数据 x_train = x_train.reshape(-1, 28, 28, 1) # 对数据进行归一化处理 x_train = x_train.astype(&#39;float32&#39;) x_train /= 255 # 对标签one-hot处理 y_train = to_categorical(y_train, 10) x_test = x_test.reshape(-1, 28, 28, 1) x_test = x_test.astype(&#39;float32&#39;) x_test /= 255 y_test = to_categorical(y_test, 10) model = Sequential() model.add(Conv2D(32, (5,5), activation=&#39;relu&#39;, input_shape=[28, 28, 1])) model.add(Conv2D(64, (5,5), activation=&#39;relu&#39;)) model.add(MaxPool2D(pool_size=(2,2))) model.add(Flatten()) model.add(Dropout(0.5)) model.add(Dense(128, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=categorical_crossentropy, optimizer=Adadelta(), metrics=[&#39;accuracy&#39;]) batch_size = 100 epochs = 1 model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs) &#34;&#34;&#34; 预测 &#34;&#34;&#34; # verbose：日志显示 fit 中默认为 1 # 0 为不在标准输出流输出日志信息 1 为输出进度条记录, ,2 为每个epoch输出一行记录 # evaluate 中默认为0 # 0 为不在标准输出流输出日志信息 1 为输出进度条记录 loss, accuracy = model.evaluate(x_train, y_train, verbose=1) print(&#39;train data loss:%.4f accuracy:%.4f&#39; %(loss, accuracy)) loss, accuracy = model.evaluate(x_test, y_test, verbose=1) print(&#39;test data loss:%.4f accuracy:%.4f&#39; %(loss, accuracy)) &#34;&#34;&#34; 画出预测结果 &#34;&#34;&#34; import math import matplotlib.pyplot as plt import numpy as np import random def draw(position, image, title, isTrue): # 设置子图位置 plt.subplot(*position) plt.imshow(image.reshape(-1, 28), cmap=&#39;gray_r&#39;) plt.axis(&#39;off&#39;) if not isTrue: plt.title(title, color=&#39;red&#39;) else: plt.title(title) def show_result(batch_size, test_X, test_y): selected_index = random.sample(range(len(test_y)), k=batch_size) images = test_X[selected_index] labels = test_y[selected_index] predict_labels = model.predict(images) image_numbers = images.shape[0] row_number = math.ceil(image_numbers ** 0.5) column_number = row_number # 设置图片大小 plt.figure(figsize=(row_number+8, column_number+8)) for i in range(row_number): for j in range(column_number): index = i * column_number + j if index &lt; image_numbers: position = (row_number, column_number, index+1) image = images[index] actual = np.argmax(labels[index]) predict = np.argmax(predict_labels[index]) isTrue = actual==predict title = &#39;actual:%d\npredict:%d&#39; %(actual,predict) draw(position, image, title, isTrue) show_result(100, x_test, y_test) plt.show() ]]></content></entry><entry><title>ml-keras-utils-plot_model报错</title><url>/post/ml-keras-utils-plot_model%E6%8A%A5%E9%94%99/</url><categories><category>ml</category></categories><tags/><content type="html">OSError: pydot failed to call GraphViz.Please install GraphViz ( https://www.graphviz.org/ ) and ensure that its executables are in the $PATH. 或 Failed to import pydot. You must install pydot and graphviz
win10 ####1.安装 GraphViz 下载msi文件安装 与python关联 pip install graphviz ####2.添加环境变量 ######用户变量Path添加： C:\programfile\graphviz\bin(这个为你安装的graphviz路径下的bin路径) ######系统变量Path添加： C:\programfile\graphviz\bin\dot.exe ######命令行输入 dot -version查看路径是否为安装目录 ######3.安装pydot pip install pydot pip install pydot_ng
修改py文件 找到E:\anaconda3\envs\tensorflow\Lib\site-packages\pydot.py 修改return '.bat' if is_anacoda() else '.exe' 为 return '.bat' if not is_anacoda() else '.exe'</content></entry><entry><title>ml-pytorch</title><url>/post/ml-pytorch/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[1.安装 https://pytorch.org pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple #　２.基础
创建一个 5x3 矩阵, 但是未初始化: torch.empty(5, 3)
创建一个随机初始化的矩阵: torch.rand(5, 3)
创建一个0填充的矩阵，数据类型为long torch.zeros(5, 3, dtype=torch.long)
创建tensor并使用现有数据初始化: torch.tensor([5.5, 3])
改变张量的维度和大小 torch.view
根据现有的张量创建张量。 这些方法将重用输入张量的属性，例如， dtype，除非设置新的值进行覆盖 x = x.new_ones(5, 3, dtype=torch.double) # new_* 方法来创建对象
tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64)
x = torch.randn_like(x, dtype=torch.float) # 覆盖 dtype!
tensor([[ 0.5691, -2.0126, -0.4064], [-0.0863, 0.4692, -1.1209], [-1.1177, -0.5764, -0.5363], [-0.4390, 0.6688, 0.0889], [ 1.3334, -1.1600, 1.8457]])
tensor/NumPy 转换 a = torch.ones(5)
tensor([1., 1., 1., 1., 1.])
b = a.numpy()
[1. 1. 1. 1. 1.]
a.add_(1) print(a) print(b) tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]
import numpy as np a = np.ones(5) b = torch.from_numpy(a) np.add(a, 1, out=a) print(a) print(b) [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
CUDA 张量 # is_available 函数判断是否有cuda可以使用 # ``torch.device``将张量移动到指定的设备中 x = torch.randn(1) if torch.cuda.is_available(): device = torch.device(&#34;cuda&#34;) # a CUDA 设备对象 y = torch.ones_like(x, device=device) # 直接从GPU创建张量 x = x.to(device) # 或者直接使用``.to(&#34;cuda&#34;)``将张量移动到cuda中 z = x + y print(z) print(z.to(&#34;cpu&#34;, torch.double)) # ``.to`` 也会对变量的类型做更改 tensor([0.7632], device=&lsquo;cuda:0&rsquo;) tensor([0.7632], dtype=torch.float64)
Autograd: 自动求导机制 每个张量都有一个.grad_fn属性，这个属性引用了一个创建了Tensor的Function（除非这个张量是用户手动创建的，即，这个张量的 grad_fn 是 None）。.requires_grad_( &hellip; ) 可以改变现有张量的 requires_grad属性。
a = torch.randn(2, 2) print(a.requires_grad) a.requires_grad_(True) print(a.requires_grad) False True
x = torch.ones(2, 2, requires_grad=True) print(x) tensor([[1., 1.], [1., 1.]], requires_grad=True)
y = x + 2 print(y) tensor([[3., 3.], [3., 3.]], grad_fn=)
z = y * y * 3 out = z.mean() print(z, out) tensor([[27., 27.],[27., 27.]], grad_fn=) tensor(27., grad_fn=)
反向传播 因为 out是一个纯量（scalar），out.backward() 等于out.backward(torch.tensor(1))。 out.backward()
print gradients d(out)/dx
print(x.grad)
tensor([[4.5000, 4.5000],[4.5000, 4.5000]])
得到 $o = \frac{1}{4}\sum_i z_i$,$z_i = 3(x_i+2)^2$ and $z_i\bigr\rvert_{x_i=1} = 27$.
因此,$\frac{\partial o}{\partial x_i} = \frac{3}{2}(x_i+2)$, hence$\frac{\partial o}{\partial x_i}\bigr\rvert_{x_i=1} = \frac{9}{2} = 4.5$.
如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:
print(x.requires_grad) print((x ** 2).requires_grad) with torch.no_grad(): print((x ** 2).requires_grad) True True False
#　３.数据集 定义一个数据集
from torch.utils.data import Dataset import pandas as pd class BulldozerDataset(Dataset): def __init__(self, csv_file): self.df=pd.read_csv(csv_file) # 该方法返回数据集的总长度 def __len__(self): return len(self.df) # 该方法定义用索引(0 到 len(self))获取一条数据或一个样本 def __getitem__(self, idx): return self.df.iloc[idx].SalePrice ds_demo= BulldozerDataset(&#39;median_benchmark.csv&#39;) #实现了 __len__ 方法所以可以直接使用len获取数据总数 len(ds_demo) #用索引可以直接访问对应的数据, 对应 __getitem__ 方法 ds_demo[0] DataLoader为我们提供了对Dataset的读取操作
#batch_size, shuffle, num_workers(加载数据的时候使用几个子进程) dl = torch.utils.data.DataLoader(ds_demo, batch_size=10, shuffle=True, num_workers=0) # DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据 # idata=iter(dl) # print(next(idata)) for i, data in enumerate(dl): print(i,data) torchvision 是PyTorch中专门用来处理图像的库 torchvision.datasets 可以理解为PyTorch团队自定义的dataset，不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用：
MNIST- COCO- Captions- Detection- LSUN- ImageFolder- Imagenet-12- CIFAR- STL10- SVHN- PhotoTour AlexNet- VGG- ResNet- SqueezeNet- DenseNet import torchvision.datasets as datasets trainset = datasets.MNIST(root=&#39;./data&#39;, # 加载的目录 train=True, # 表示是否加载数据库的训练集，false的时候加载测试集 download=True, # 表示是否自动下载 MNIST 数据集 transform=None) # 表示是否需要对数据进行预处理，none为不进行预处理 import torchvision.models as models resnet18 = models.resnet18(pretrained=True) torchvision.transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强 from torchvision import transforms as transforms transform = transforms.Compose([ transforms.RandomCrop(32, padding=4), #先四周填充0，在把图像随机裁剪成32*32 transforms.RandomHorizontalFlip(), #图像一半的概率翻转，一半的概率不翻转 transforms.RandomRotation((-45,45)), #随机旋转 transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.229, 0.224, 0.225)), #R,G,B每层的归一化用到的均值和方差 ]) 4.反向传播 当我们调用 loss.backward()时,整张计算图都会 根据loss进行微分， 而且图中所有设置为requires_grad=True的张量 将会拥有一个随着梯度累积的.grad 张量。 relu -&gt; linear -&gt; MSELoss -&gt; loss
print(loss.grad_fn) # MSELoss print(loss.grad_fn.next_functions[0][0]) # Linear print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU &lt;MseLossBackward object at 0x7f3b49fe2470&gt; &lt;AddmmBackward object at 0x7f3bb05f17f0&gt; &lt;AccumulateGrad object at 0x7f3b4a3c34e0&gt;
现在，我们将调用loss.backward()，并查看conv1层的偏差（bias）项在反向传播前后的梯度。
net.zero_grad() # 清除梯度 print(&#39;conv1.bias.grad before backward&#39;) print(net.conv1.bias.grad) loss.backward() print(&#39;conv1.bias.grad after backward&#39;) print(net.conv1.bias.grad) conv1.bias.grad before backward tensor([0., 0., 0., 0., 0., 0.]) conv1.bias.grad after backward tensor([ 0.0051, 0.0042, 0.0026, 0.0152, -0.0040, -0.0036])
5.损失函数(Loss Function) 损失函数（loss function）是用来估量模型的预测值与真实值的不一致程度，它是一个非负实值函数,损失函数越小，模型的鲁棒性就越好。
nn.L1Loss: 输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的
$ loss(x,y)=1/n\sum|x_i-y_i| $
nn.NLLLoss: 用于多分类的负对数似然损失函数
$ loss(x, class) = -x[class]$
NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了
$ loss(x, class) = -weights[class] * x[class] $
nn.MSELoss: 均方损失函数 ，输入x和目标y之间均方差
$ loss(x,y)=1/n\sum(x_i-y_i)^2 $
nn.CrossEntropyLoss: 多分类用的交叉熵损失函数，LogSoftMax和NLLLoss集成到一个类中，会调用nn.NLLLoss函数,我们可以理解为CrossEntropyLoss()=log_softmax() + NLLLoss()
$ \begin{aligned} loss(x, class) &amp;= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &amp;= -x[class] + log(\sum_j exp(x[j])) \end{aligned} $
因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：
$ loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j]))) $
所以一般多分类的情况会使用这个损失函数
nn.BCELoss: 计算 x 与 y 之间的二进制交叉熵。
$ loss(o,t)=-\frac{1}{n}\sum_i(t[i]* log(o[i])+(1-t[i])* log(1-o[i])) $
与NLLLoss类似，也可以添加权重参数：
$ loss(o,t)=-\frac{1}{n}\sum_iweights[i]* (t[i]* log(o[i])+(1-t[i])* log(1-o[i])) $
用的时候需要在该层前面加上 Sigmoid 函数。
6.优化算法 torch.optim.SGD 随机梯度下降算法,带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：
#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明 #如果设置了momentum，就是带有动量的SGD，可以不设置 optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) torch.optim.RMSprop 除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快
# 我们的课程基本不会使用到RMSprop所以这里只给一个实例 optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99) torch.optim.Adam Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法
# 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法 optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08) 7.正则化 L1正则化 损失函数基础上加上权重参数的绝对值
$ L=E_{in}+\lambda{\sum_j} \left|w_j\right|$
L2正则化 损失函数基础上加上权重参数的平方和
$ L=E_{in}+\lambda{\sum_j} w^2_j$
需要说明的是：l1 相比于 l2 会更容易获得稀疏解
8.多GPU数据并行 model = Model(input_size, output_size) if torch.cuda.device_count() &gt; 1: print(&#34;Let&#39;s use&#34;, torch.cuda.device_count(), &#34;GPUs!&#34;) # DataParallel会自动的划分数据，并将作业发送到多个GPU上的多个模型。 并在每个模型完成作业后，收集合并结果并返回。 model = nn.DataParallel(model) model.to(device) for data in rand_loader: # 请注意，只调用data.to(device)并没有复制张量到GPU上，而是返回了一个copy。所以你需要把它赋值给一个新的张量并在GPU上使用这个张量。 input = data.to(device) output = model(input) print(&#34;Outside: input size&#34;, input.size(), &#34;output_size&#34;, output.size()) 9.CNN 卷积层输出矩阵大小 ensorFlow里面的padding只有两个选项也就是valid和same pytorch里面的padding么有这两个选项，它是数字0,1,2,3等等，默认是0 所以输出的h和w的计算方式也是稍微有一点点不同的：tf中的输出大小是和原来的大小成倍数关系，不能任意的输出大小；而nn输出大小可以通过padding进行改变
Conv2d（in_channels, out_channels, kernel_size, stride=1,padding=0, dilation=1, groups=1,bias=True, padding_mode='zeros')
池化层的输出大小公式也与卷积层一样，由于没有进行填充，所以p=0，可以简化为 $ \frac{n-f}{s} +1 $ 通过减少卷积层之间的连接，降低运算复杂程度 import torch.nn.functional as F F.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False))
LeNet-5 官网 卷积神经网路的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件
用卷积提取空间特征；由空间平均得到子样本；用 tanh 或 sigmoid 得到非线性； 用 multi-layer neural network（MLP）作为最终分类器；层层之间用稀疏的连接矩阵，以避免大的计算成本。
&#39;&#39;&#39; C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5 * 5 S2层是pooling层，下采样（区域:2 * 2 ）降低网络训练参数及模型的过拟合程度。 C3层是第二个卷积层，使用16个卷积核，核大小:5 * 5 提取特征 S4层也是一个pooling层，区域:2*2 C5层是最后一个卷积层，卷积核大小:5 * 5 卷积核种类:120 最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率 &#39;&#39;&#39; import torch.nn as nn class LeNet5(nn.Module): def __init__(self): super(LeNet5, self).__init__() # 1 input image channel, 6 output channels, 5x5 square convolution # kernel self.conv1 = nn.Conv2d(1, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) # an affine operation: y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) # 这里论文上写的是conv,官方教程用了线性层 self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # If the size is a square you can only specify a single number x = F.max_pool2d(F.relu(self.conv2(x)), 2) x = x.view(-1, self.num_flat_features(x)) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features net = LeNet5() print(net) AlexNet 2012，Alex Krizhevsky 可以算作LeNet的一个更深和更广的版本，可以用来学习更复杂的对象 . 论文 用rectified linear units（ReLU）得到非线性； 使用 dropout 技巧在训练期间有选择性地忽略单个神经元，来减缓模型的过拟合； 重叠最大池，避免平均池的平均效果； 虽然 AlexNet只有8层，但是它有60M以上的参数总量，Alexnet有一个特殊的计算层，LRN层，做的事是对当前层的输出结果做平滑处理，这里就不做详细介绍了， Alexnet的每一阶段（含一次卷积主要计算的算作一层）可以分为8层：
con - relu - pooling - LRN ： 要注意的是input层是227*227，而不是paper里面的224，这里可以算一下，主要是227可以整除后面的conv1计算，224不整除。如果一定要用224可以通过自动补边实现，不过在input就补边感觉没有意义，补得也是0，这就是我们上面说的公式的重要性。 conv - relu - pool - LRN ： group=2，这个属性强行把前面结果的feature map分开，卷积部分分成两部分做 conv - relu conv-relu conv - relu - pool fc - relu - dropout ： dropout层，在alexnet中是说在训练的以1/2概率使得隐藏层的某些neuron的输出为0，这样就丢到了一半节点的输出，BP的时候也不更新这些节点，防止过拟合。 fc - relu - dropout fc - softmax
import torchvision model = torchvision.models.alexnet(pretrained=False) #我们不下载预训练权重 print(model) VGG 2015，牛津的 VGG。 论文 每个卷积层中使用更小的 3×3 filters，并将它们组合成卷积序列 多个3×3卷积序列可以模拟更大的接收场的效果 每次的图像像素缩小一倍，卷积核的数量增加一倍
VGG清一色用小卷积核，结合作者和自己的观点，这里整理出小卷积核比用大卷积核的优势： 根据作者的观点，input8 -&gt; 3层conv3x3后，output=2，等同于1层conv7x7的结果； input=8 -&gt; 2层conv3x3后，output=2，等同于2层conv5x5的结果 卷积层的参数减少。相比5x5、7x7和11x11的大卷积核，3x3明显地减少了参数量 通过卷积和池化层后，图像的分辨率降低为原来的一半，但是图像的特征增加一倍，这是一个十分规整的操作: 分辨率由输入的224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7, 特征从原始的RGB3个通道-&gt; 64 -&gt;128 -&gt; 256 -&gt; 512
import torchvision model = torchvision.models.vgg16(pretrained=False) #我们不下载预训练权重 print(model) GoogLeNet (Inception) 2014，Google Christian Szegedy 论文 使用1×1卷积块（NiN）来减少特征数量，这通常被称为“瓶颈”，可以减少深层神经网络的计算负担。 每个池化层之前，增加 feature maps，增加每一层的宽度来增多特征的组合性 googlenet最大的特点就是包含若干个inception模块，所以有时候也称作 inception net googlenet虽然层数要比VGG多很多，但是由于inception的设计，计算速度方面要快很多。
#　Tensorboard 安装 pip install tensorboard tensorboard --logdir logs 即可启动，默认的端口是 6006,在浏览器中打开 http://localhost:6006/ 即可看到web页面。
MNIST数据集手写数字识别 import argparse # Python 命令行解析工具 import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torchvision import datasets, transforms class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1,) self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,) self.dropout1 = nn.Dropout2d(0.25) self.dropout2 = nn.Dropout2d(0.5) self.fc1 = nn.Linear(9216, 128) self.fc2 = nn.Linear(128, 10) def forward(self, x): x = self.conv1(x) x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, 2) x = self.dropout1(x) x = torch.flatten(x, 1) x = self.fc1(x) x = F.relu(x) x = self.dropout2(x) x = self.fc2(x) output = F.log_softmax(x, dim=1) return output def train(args, model, device, train_loader, optimizer, epoch): # 如果模型中有Batch Normalization和Dropout，需要在训练时添加model.train()，在测试时添加model.eval()。 # Batch Normalization在train时不仅使用了当前batch的均值和方差，也使用了历史batch统计上的均值和方差， # 并做一个加权平均 （momentum参数）。在test时，由于此时batchsize不一定一致，因此不再使用当前batch的 # 均值和方差，仅使用历史训练时的统计值。 # Dropout在train时随机选择神经元而predict要使用全部神经元并且要乘一个补偿系数 model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) &#34;&#34;&#34; pytorch中CrossEntropyLoss是通过两个步骤计算出来的: 第一步是计算log softmax，第二步是计算cross entropy（或者说是negative log likehood）， CrossEntropyLoss不需要在网络的最后一层添加softmax和log层，直接输出全连接层即可。 而NLLLoss则需要在定义网络的时候在最后一层添加log_softmax层(softmax和log层) 总而言之：CrossEntropyLoss() = log_softmax() + NLLLoss() nn.CrossEntropyLoss() &#34;&#34;&#34; loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % args.log_interval == 0: print(&#39;Train_Epoch:{} [{}/{} ({:.2f}%)] \t loss:{:.6f}&#39;.format(epoch, batch_idx*len(data), len(train_loader), 100.0*batch_idx/len(train_loader), loss.item())) if args.dry_run: break def test(model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): # for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) # 默认情况下size_average=False，是mini-batchloss的平均值，然而，如果size_average=False，则是mini-batchloss的总和。 test_loss += F.nll_loss(output, target, reduction=&#39;sum&#39;).item() # sum up batch loss pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print(&#39;\n Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#39;.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) def main(): # Training settings parser = argparse.ArgumentParser(description=&#39;PyTorch MNIST Example&#39;) parser.add_argument(&#39;-batch_size&#39;, type=int, default=64, metavar=&#39;N&#39;, help=&#39;input batch size for training (default: 64)&#39;) parser.add_argument(&#39;-test_batch_size&#39;, type=int, default=1000, metavar=&#39;N&#39;, help=&#39;input batch size for testing (default: 1000)&#39;) parser.add_argument(&#39;-epochs&#39;, type=int, default=10, metavar=&#39;N&#39;, help=&#39;number of epochs to train (default: 10)&#39;) parser.add_argument(&#39;-lr&#39;, type=float, default=0.01, metavar=&#39;LR&#39;, help=&#39;learning rate (default: 0.01)&#39;) parser.add_argument(&#39;-momentum&#39;, type=float, default=0.5, metavar=&#39;M&#39;, help=&#39;SGD momentum (default: 0.5)&#39;) parser.add_argument(&#39;-gamma&#39;, type=float, default=0.7, metavar=&#39;M&#39;, help=&#39;Learning rate step gamma (default: 0.7)&#39;) parser.add_argument(&#39;-no_cuda&#39;, action=&#39;store_true&#39;, default=False, help=&#39;disables CUDA training&#39;) parser.add_argument(&#39;-dry_run&#39;, action=&#39;store_true&#39;, default=False, help=&#39;quickly check a single pass&#39;) parser.add_argument(&#39;-seed&#39;, type=int, default=1, metavar=&#39;S&#39;, help=&#39;random seed (default: 1)&#39;) parser.add_argument(&#39;-log_interval&#39;, type=int, default=100, metavar=&#39;N&#39;, help=&#39;how many batches to wait before logging training status&#39;) parser.add_argument(&#39;-save_model&#39;, action=&#39;store_true&#39;, default=False, help=&#39;For Saving the current Model&#39;) args = parser.parse_args(args=[]) torch.manual_seed(args.seed) # #为CPU设置种子用于生成随机数，以使得结果是确定的 # torch.cuda.manual_seed(args.seed)为当前GPU设置随机种子；如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。 kwargs = {&#39;batch_size&#39;: args.batch_size} use_cuda = not args.no_cuda and torch.cuda.is_available() if use_cuda: kwargs.update({&#39;num_workers&#39;: 1, &#39;pin_memory&#39;: True, &#39;shuffle&#39;: True},) transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(&#39;./data&#39;, train=True, download=True, transform=transform,) test_dataset = datasets.MNIST(&#39;./data&#39;, train=False, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, **kwargs) test_loader = torch.utils.data.DataLoader(test_dataset, **kwargs) device = torch.device(&#34;cuda&#34; if use_cuda else &#34;cpu&#34;) model = Net().to(device) optimizer = optim.Adadelta(model.parameters(), lr=args.lr) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.gamma) for epoch in range(1, args.epochs+1): train(args, model, device, train_loader, optimizer, epoch) test(model, device, test_loader) # 训练完成，保存状态字典到linear.pkl # torch.save(model.state_dict(), &#39;./linear.pkl&#39;) # model.load_state_dict(torch.load(&#39;linear.pth&#39;)) if args.save_model: torch.save(model.state_dict(), &#34;mnist_cnn.pt&#34;) if __name__ == &#39;__main__&#39;: main() ]]></content></entry><entry><title>ml-Seq2Seq-模型及-Attention-机制</title><url>/post/ml-seq2seq-%E6%A8%A1%E5%9E%8B%E5%8F%8A-attention-%E6%9C%BA%E5%88%B6/</url><categories><category>ml</category></categories><tags/><content type="html">什么是 Seq2Seq ？ Seq2Seq 任务指的是输入和输出都是序列的任务。例如说英语翻译成中文。
Seq2Seq任务最常见的是使用Encoder+Decoder的模式。在 Encoder 中，将可变长度的序列转变为固定长度的向量表达，Decoder 将这个固定长度的向量转换为可变长度的目标的信号序列。* Encoder-Decoder 有很多弊端
训练速度慢，计算无法并行化（根本原因是encoder和decoder阶段中的RNN/LSTM/GRU的结构，由于decoder实际上是一个语言模型，因此其时间复杂度为O(n)）； Encoder 将输入编码为固定大小状态向量（hidden state）的过程实际上是一个“信息有损压缩”的过程。如果信息量越大，那么这个转化向量的过程对信息造成的损失就越大。 随着 sequence length的增加，意味着时间维度上的序列很长，RNN 模型也会出现梯度弥散。 基础的模型连接 Encoder 和 Decoder 模块的组件仅仅是一个固定大小的状态向量，这使得Decoder无法直接去关注到输入信息的更多细节。 基本的Attention原理。 单纯的Encoder-Decoder 框架并不能有效的聚焦到输入目标上，这使得像 seq2seq 的模型在独自使用时并不能发挥最大功效。比如说在上图中，编码器将输入编码成上下文变量 C，在解码时每一个输出 Y 都会不加区分的使用这个 C 进行解码。而注意力模型要做的事就是根据序列的每个时间步将编码器编码为不同 C，在解码时，结合每个不同的 C 进行解码输出，这样得到的结果会更加准确，如下所示：</content></entry><entry><title>ml-sklearn</title><url>/post/ml-sklearn/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[ 中文手册 英文手册 在Sklearn当中有三大模型：Transformer 转换器、Estimator 估计器、Pipeline 管道
估计器 (estimator) 可以基于数据集对一些参数进行估计的对象都被称为估计器。 **预测器 (predictor) **在估计器上做了一个延展，延展出预测的功能。 **转换器 (transformer) **也是一种估计器，两者都带拟合功能，但估计器做完拟合来预测，而转换器做完拟合来转换。
估计器都有 fit() 方法，预测器都有 predict() 和 score() （返回的是分类准确率）方法，言外之意不是每个预测器都有 predict_proba() 和 decision_function() （返回的是每个样例在每个类下的分数值）方法
管道将Transformer、Estimator 组合起来成为一个大模型。 Transformer放在管道前几个模型中，而Estimator 只能放到管道的最后一个模型中。
###　多分类和多标签的 multiclass
多输出的 multioutput 选择模型的 model_selection KFold交叉采样：将训练/测试数据集划分n_splits个互斥子集，每次只用其中一个子集当做测试集，剩下的（n_splits-1）作为训练集，进行n_splits次实验并得到n_splits个结果。 注：对于不能均等分的数据集，前n_samples%n_spllits子集拥有n_samples//n_spllits+1个样本，其余子集都只有n_samples//n_spllits个样本。（例10行数据分3份，只有一份可分4行，其他均为3行）
StratifiedKFold分层采样，用于交叉验证：与KFold最大的差异在于，StratifiedKFold方法是根据标签中不同类别占比1：1来进行拆分数据的。
流水线的 pipeline 1.监督学习 1.1. 线性模型 1.2. 线性和二次判别分析 1.3. 内核岭回归 1.4. 支持向量机 1.5. 随机梯度下降 1.6. 最近邻 1.7. 高斯过程 1.8. 交叉分解 1.9. 朴素贝叶斯 1.10. 决策树 1.11. 集成方法 AdaBoostClassifier: 逐步提升分类器
AdaBoostRegressor: 逐步提升回归器
BaggingClassifier: 装袋分类器
BaggingRegressor: 装袋回归器
GradientBoostingClassifier: 梯度提升分类器
GradientBoostingRegressor: 梯度提升回归器
RandomForestRegressor: 随机森林回归
RandomForestClassifier: 随机森林分类器(同质估计器)
VotingClassifier: 投票分类器(异质估计器)
VotingRegressor: 投票回归器
1.12. 多类和多标签算法 1.13. 特征选择 1.14. 半监督学习 1.15. 等式回归 1.16. 概率校准 1.17. 神经网络模型（监督的） 2.无监督的学习 2.1. 高斯混合模型 2.2. 流形学习 2.3. 聚类 2.4. 双聚类 2.5. 分解组件中的信号（矩阵分解问题） 2.6. 协方差估计 2.7. 异常值检测 2.8. 密度估算 2.9. 神经网络模型（无监督） 3.模型选择和评估 3.1. 交叉验证：评估估算器性能 3.2. 调整估算器的超参数 sklearn.model_selection.GridSearchCV(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=‘2*n_jobs’, error_score=’raise’, return_train_score=’warn’)
estimator：所使用的分类器 param_grid：值为字典或者列表，即需要最优化的参数的取值 scoring :准确度评价标准，默认None,如果是None，则使用estimator的误差估计函数。scoring参数选择如下： cv :交叉验证参数，默认None，也可以是yield训练/测试数据的生成器。 refit :默认为True,即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。 iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。 verbose：，0：不输出训练过程，1：偶尔输出，&gt;1：对每个子模型都输出。 n_jobs: 并行数-1：跟CPU核数一致, 1:默认值。 pre_dispatch：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次 方法：
fit（X, y=None, groups=None, **fit_params）：与所有参数组合运行。 get_params（[deep]）：获取此分类器的参数。 inverse_transform（Xt）使用找到的最佳参数在分类器上调用inverse_transform。 predict（X）调用使用最佳找到的参数对估计量进行预测，X：可索引，长度为n_samples； score（X, y=None）返回给定数据上的分数，X： [n_samples，n_features]输入数据，其中n_samples是样本的数量，n_features是要素的数量。y： [n_samples]或[n_samples，n_output]，可选，相对于X进行分类或回归; 无无监督学习。 属性：
cv_results_ :将键作为列标题和值作为列的字典，可将其导入到pandas DataFrame中。 best_estimator_ : estimator或dict；由搜索选择的估算器，即在左侧数据上给出最高分数（或者如果指定最小损失）的估算器。 如果refit = False，则不可用。 best_params_ : 在保持数据上给出最佳结果的参数设置。对于多度量评估，只有在指定了重新指定的情况下才会出现。 best_score_ : best_estimator的平均交叉验证分数，对于多度量评估，只有在指定了重新指定的情况下才会出现。 n_splits：交叉验证拆分的数量 3.3. 评分：量化预测的质量 准确率
from sklearn import metrics metrics.accuracy_score(y_test, RF.predict(X_test)) 3.4. 模型持久性 3.5. 验证曲线：绘制分数以评估模型 4.检查 4.1. 部分依赖图 4.2. 排列特征的重要性 5.可视化 5.1. 可用的绘图实用程序 6.数据集转换 6.1. Pipeline（管道）和 复合估计器 Pipeline from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler # evaluate baseline model with standardized dataset numpy.random.seed(seed) estimators = [] # 标准化的数据传到分类器 estimators.append((&#39;standardize&#39;, StandardScaler())) estimators.append((&#39;mlp&#39;, KerasClassifier(build_fn=create_baseline, nb_epoch=100, batch_size=5, verbose=0))) pipeline = Pipeline(estimators) kfold = StratifiedKFold(y=encoded_Y, n_folds=10, shuffle=True, random_state=seed) results = cross_val_score(pipeline, X, encoded_Y, cv=kfold) print(&#34;Standardized: %.2f%% (%.2f%%)&#34; % (results.mean()*100, results.std()*100)) 6.2. 特征提取 6.3. 预处理数据 onehot OneHotEncoder(n_values=’auto’, categorical_features=’all’, dtype=&lt;class ‘numpy.float64’&gt;, sparse=True, handle_unknown=’error’)
n_values=’auto’表示每个特征使用几维的数值来表示。 categorical_features = 'all'指定了对哪些特征进行编码，默认对所有类别都进行编码。 dtype=&lt;class ‘numpy.float64’&gt; 表示编码数值格式，默认是浮点型。 sparse=True 表示编码的格式，默认为 True，即为稀疏的格式，指定 False 则就不用 toarray() 了 handle_unknown=’error’其值可以指定为 &ldquo;error&rdquo; 或者 &ldquo;ignore&rdquo;，即如果碰到未知的类别，是返回一个错误还是忽略它。
from sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder() enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) # 如果不加 toarray() 的话，输出的是稀疏的存储格式，即索引加值的形式， # 也可以通过参数指定 sparse = False 来达到同样的效果 ans = enc.transform([[0, 1, 3]]).toarray() print(ans) # 输出 [[ 1. 0. 0. 1. 0. 0. 0. 0. 1.]] 把每一行当作一个样本，每一列当作一个特征
第一个特征有两个取值 0 或者 1，那么 one-hot 就会使用两位来表示这个特征，[1,0] 表示 0， [0,1] 表示 1， 第二列 [0,1,2,0]，它有三种值，那么 one-hot 就会使用三位来表示这个特征，[1,0,0] 表示 0， [0,1,0] 表示 1，[0,0,1] 表示 2， 第三列 [3,0,1,2]，它有四种值，那么 one-hot 就会使用四位来表示这个特征，[1,0,0,0] 表示 0， [0,1,0,0] 表示 1，[0,0,1,0] 表示 2，[0,0,0,1] 表示 3
enc = OneHotEncoder(n_values = [2, 3, 4]) enc.fit([[0, 0, 3], [1, 1, 0]]) ans = enc.transform([[0, 2, 3]]).toarray() print(ans) # 输出 [[ 1. 0. 0. 0. 1. 0. 0. 0. 1.]] 注意到训练样本中第二个特征列没有类别 2，但是结果中依然将类别 2 给编码了出来，这就是自己指定维数的作用了（我们使用 3 位来表示第二个特征，自然包括了类别 2），第三列特征同样如此。这也告诫我们，如果训练样本中有丢失的分类特征值，我们就必须显示地设置参数 n_values 了，这样防止编码出错。
from sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder(categorical_features = [0,2]) # 等价于 [True, False, True] enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) ans = enc.transform([[0, 2, 3]]).toarray() print(ans) # 输出 [[ 1. 0. 0. 0. 0. 1. 2.]] 输出结果中前两位 [1,0] 表示 0，中间四位 [0,0,0,1] 表示对第三个特征 3 编码，第二个特征 2 没有进行编码，就放在最后一位。
from sklearn.preprocessing import OneHotEncoder enc = OneHotEncoder(sparse = False) ans = enc.fit_transform([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) print(ans) # 输出 [[ 1. 0. 1. ..., 0. 0. 1.] # [ 0. 1. 0. ..., 0. 0. 0.] # [ 1. 0. 0. ..., 1. 0. 0.] # [ 0. 1. 1. ..., 0. 1. 0.]] 类别转换为数字 # encode class values as integers encoder = LabelEncoder() encoder.fit(Y) encoded_Y = encoder.transform(Y) 练集测试集划分 X_train,X_test, y_train, y_test =sklearn.model_selection.train_test_split(train_data,train_target,test_size=0.4, random_state=0,stratify=y_train)
6.4. 缺失值的插补 6.5. 无监督降维 6.6. 随机投影 6.7. 内核近似 6.8. 成对度量，亲和力和内核 6.9. 转换预测目标（y） 7.数据集加载实用程序 打包好的数据：对于小数据集，用 sklearn.datasets.load_*
分流下载数据：对于大数据集，用 sklearn.datasets.fetch_*
随机创建数据：为了快速展示，用 sklearn.datasets.make_*
7.1. 通用数据集API 1.分类 手写数字数据集：
from sklearn.datasets import load_digits digits = load_digits(n_class=5) # 只加载0-4 print(digits.data.shape, digits.target.shape, digits.target_names) (901, 64) (901,) [0 1 2 3 4 5 6 7 8 9]
线鸢尾花数据集：
from sklearn.datasets import load_iris iris = load_iris() print(iris.data.shape, iris.target.shape, iris.feature_names, iris.target_names) (150, 4) (150,) [&lsquo;sepal length (cm)&rsquo;, &lsquo;sepal width (cm)&rsquo;, &lsquo;petal length (cm)&rsquo;, &lsquo;petal width (cm)&rsquo;] [&lsquo;setosa&rsquo; &lsquo;versicolor&rsquo; &lsquo;virginica&rsquo;]
乳腺癌数据集：
from sklearn.datasets import load_breast_cancer bc = load_breast_cancer() print(bc.data.shape, bc.target.shape, bc.feature_names, bc.target_names) (569, 30) (569,) [&lsquo;mean radius&rsquo; &lsquo;mean texture&rsquo; &hellip;][&lsquo;malignant&rsquo; &lsquo;benign&rsquo;]
2.回归 糖尿病数据集：
from sklearn.datasets import load_diabetes diabetes = load_diabetes() print(diabetes.data.shape, diabetes.target.shape, diabetes.feature_names) (442, 10) (442,) [&lsquo;age&rsquo;, &lsquo;sex&rsquo;, &lsquo;bmi&rsquo;, &lsquo;bp&rsquo;, &lsquo;s1&rsquo;, &lsquo;s2&rsquo;, &lsquo;s3&rsquo;, &lsquo;s4&rsquo;, &lsquo;s5&rsquo;, &lsquo;s6&rsquo;] Average blood pressure
波士顿房价数据集：
from sklearn.datasets import load_boston boston = load_boston() print(boston.data.shape, boston.target.shape, boston.feature_names) (506, 13) (506,) [&lsquo;CRIM&rsquo; &lsquo;ZN&rsquo; &lsquo;INDUS&rsquo; &lsquo;CHAS&rsquo; &lsquo;NOX&rsquo; &lsquo;RM&rsquo; &lsquo;AGE&rsquo; &lsquo;DIS&rsquo; &lsquo;RAD&rsquo; &lsquo;TAX&rsquo; &lsquo;PTRATIO&rsquo; &lsquo;B&rsquo; &lsquo;LSTAT&rsquo;]
3.多标签回归 体能训练数据集：
from sklearn.datasets import load_linnerud linnerud = load_linnerud() print(linnerud.data.shape, linnerud.target.shape, linnerud.target_names, linnerud.feature_names) (20, 3) (20, 3) [&lsquo;Weight&rsquo;, &lsquo;Waist&rsquo;, &lsquo;Pulse&rsquo;] [&lsquo;Chins&rsquo;, &lsquo;Situps&rsquo;, &lsquo;Jumps&rsquo;]
7.2. 玩具数据集 7.3. 现实世界的数据集 7.4. 生成数据集 单标签 make_blobs：多类单标签数据集，为每个类分配一个或多个正太分布的点集，对于中心和各簇的标准偏差提供了更好的控制，可用于演示聚类
make_classification：多类单标签数据集，为每个类分配一个或多个正太分布的点集，引入相关的，冗余的和未知的噪音特征；将高斯集群的每类复杂化；在特征空间上进行线性变换
make_gaussian_quantiles：将single Gaussian cluster （单高斯簇）分成近乎相等大小的同心超球面分离。
make_hastie_10_2：产生类似的二进制、10维问题。
make_moons/make_moons：生成二维分类数据集时可以帮助确定算法（如质心聚类或线性分类），包括可以选择性加入高斯噪声。它们有利于可视化。用球面决策边界对高斯数据生成二值分类。
多标签 make_multilabel_classification：生成多个标签的随机样本。
二分聚类 make_biclusters：Generate an array with constant block diagonal structure for biclustering。
make_checkerboard：Generate an array with block checkerboard structure for biclustering。
回归生成器 make_regression：产生的回归目标作为一个可选择的稀疏线性组合的具有噪声的随机的特征。它的信息特征可能是不相关的或低秩（少数特征占大多数的方差）。
make_sparse_uncorrelated: 产生目标为一个有四个固定系数的线性组合。
make_friedman1: 与多项式和正弦相关变换相联系。
make_friedman2: 包括特征相乘与交互。
make_friedman3: 类似与对目标的反正切变换。
7.5. 加载其他数据集 8.使用scikit-learn 8.1计算. 计算扩展的策略：更大的数据 8.2. 计算性能 8.3. 并行性，资源管理和配置 ]]></content></entry><entry><title>ml-Tensorflow官方debug--tfdbg</title><url>/post/ml-tensorflow%E5%AE%98%E6%96%B9debug--tfdbg/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[example code：
import numpy as np import tensorflow as tf from tensorflow.python import debug as tf_debug xs = np.linspace(-0.5, 0.49, 100) x = tf.placeholder(tf.float32, shape=[None], name=&#34;x&#34;) y = tf.placeholder(tf.float32, shape=[None], name=&#34;y&#34;) k = tf.Variable([0.0], name=&#34;k&#34;) y_hat = tf.multiply(k, x, name=&#34;y_hat&#34;) sse = tf.reduce_sum((y - y_hat) * (y - y_hat), name=&#34;sse&#34;) train_op = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(sse) sess = tf.Session() sess.run(tf.global_variables_initializer()) sess = tf_debug.LocalCLIDebugWrapperSession(sess,ui_type=&#34;readline&#34;) for _ in range(10): sess.run(y_hat,feed_dict={x:xs,y:10*xs}) sess.run(train_op, feed_dict={x: xs, y: 42 * xs}) guidance：
第一步，启动Python调试， $&gt; python tfdbgdemo.py –debug ... TTTTTT FFFF DDD BBBB GGG TT F D D B B G TT FFF D D BBBB G GG TT F D D B B G G TT F DDD BBBB GGG TensorFlow version: 1.10.0 ====================================== Session.run() call #1: Fetch(es): y_hat:0 Feed dict: x:0 y:0 ====================================== Select one of the following commands to proceed ----&gt; run: Execute the run() call with debug tensor-watching run -n: Execute the run() call without debug tensor-watching run -t &lt;T&gt;: Execute run() calls (T - 1) times without debugging, then execute run() once more with debugging and drop back to the CLI run -f &lt;filter_name&gt;: Keep executing run() calls until a dumped tensor passes a given, registered filter (conditional breakpoint mode) Registered filter(s): * has_inf_or_nan invoke_stepper: Use the node-stepper interface, which allows you to interactively step through nodes involved in the graph run() call and inspect/modify their values For more details, see help.. 第二步，运行程序 tfdbg&gt; run run-end: run #1: 1 fetch (y_hat:0); 2 feeds 3 dumped tensor(s): t (ms) Size (B) Op type Tensor name [0.000] 170 VariableV2 k:0 [33.354] 180 Identity k/read:0 [55.531] 576 Mul y_hat:0 第三步，打印想看的变量 tfdbg&gt; pt y_hat:0 Tensor &#34;y_hat:0:DebugIdentity&#34;: dtype: float32 shape: (100,) array([-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32) 运行时可能出现的问题 No module named _curses 首先这个问题产生的 根本原因 是 curses 库不支持 windows。所以我们在下载完成python后（python 是自带 curses 库的），虽然在 python目录\Lib 中可以看到 curses 库，但其实我们是不能使用的。会产生如上的错误。在提示的文件 init 文件中也确实可以找到 from _curses import * 这句话。
要解决这个问题，我们就需要使用一个 unofficial curses（非官方curses库）来代替 python 自带的curses库。也就是 whl 包。
用我自己的例子，我下载的是 python3.5.1 版本，在 https://link.jianshu.com/?t=http://www.lfd.uci.edu/~gohlke/pythonlibs/#curses python whl包下载 中找到 curses ，然后下载与自己python版本对应的 whl 包（如我的就是 curses-2.2-cp35-cp35m-win_amd64.whl），我是windows10-64bit，需要下载 amd64 的版本，但是具体下载哪个版本可以测试一下，只要经测试发现使用那个版本时在安装的时候会报一个环境不支持的错误就对了。
## tfgdb常用命令总结 tfgdb使用步进制的方法运行，其实和gdb，pdb很像，每调用一次run函数就会停止并输出当前的tensor节点的值，以方便我们进行观察和调试。在界面中可以直接使用鼠标点击界面上方的菜单栏进行命令的执行，比如选中一个节点后，点击list_inputs就会显示该节点的输入列表。下面总结一下常用的几个命令的含义：
run：运行一次sess.run()。-t可以运行很多次。-n运行结束。-f运行到filter条件出现，比如nan，inf等
list_inputs：打印节点的输入信息，简写为li。常用的包括-d和-r两个参数，分别用来限制显示的层数和方式。
list_outputs：打印节点的输出信息，简写为lo。后面常用的参数跟list_input一样。
list_tensors：展示所有的Tensor信息，简单可以理解为主界面，简写为lt。可以使用-f，-n，-t操作进行限制输出tensor所满足的条件。-s，-r用来控制显示信息排序或者倒序。
node_info：打印节点信息，简写为ni。
print_tensor：打印Tensor的值，简写为pt。
/ : 查找。譬如：/inf就是查找当前打印的值中的inf，并将其高亮显示。
此外还可以看一下官网上面给出的常用命令组合，如下图所示： ]]></content></entry><entry><title>ml-插值</title><url>/post/ml-%E6%8F%92%E5%80%BC/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[一维插值 插值不同于拟合。插值函数经过样本点，拟合函数一般基于最小二乘法尽量靠近所有样本点穿过。常见插值方法有拉格朗日插值法、分段插值法、样条插值法。
拉格朗日插值多项式：当节点数n较大时，拉格朗日插值多项式的次数较高，可能出现不一致的收敛情况，而且计算复杂。随着样点增加，高次插值会带来误差的震动现象称为龙格现象。 分段插值：虽然收敛，但光滑性较差。 样条插值:样条插值是使用一种名为样条的特殊分段多项式进行插值的形式。由于样条插值可以使用低阶多项式样条实现较小的插值误差，这样就避免了使用高阶多项式所出现的龙格现象，所以样条插值得到了流行。
# -*-coding:utf-8 -*- import numpy as np from scipy import interpolate import pylab as plt x=np.linspace(0,10,11) #x=[ 0. 1. 2. 3. 4. 5. 6. 7. 8. 9. 10.] y=np.sin(x) xnew=np.linspace(0,10,101) plt.plot(x,y,&#34;ro&#34;,label=&#39;origin&#39;) for kind in [&#34;nearest&#34;,&#34;zero&#34;,&#34;slinear&#34;,&#34;quadratic&#34;,&#34;cubic&#34;]: #&#34;nearest&#34;,&#34;zero&#34;为阶梯插值 #slinear 线性插值 #&#34;quadratic&#34;,&#34;cubic&#34; 为2阶、3阶B样条曲线插值 f=interpolate.interp1d(x,y,kind=kind) # ‘slinear’, ‘quadratic’ and ‘cubic’ refer to a spline interpolation of first, second or third order) ynew=f(xnew) plt.plot(xnew,ynew,label=str(kind)) plt.legend(loc=&#39;best&#39;) plt.show() 二维插值 方法与一维数据插值类似，为二维样条插值。
# -*- coding: utf-8 -*- import numpy as np from scipy import interpolate import matplotlib.pyplot as plt def func(x, y): return (x+y)*np.exp(-5.0*(x**2 + y**2)) # np.mgrid[1:5:0.1, 1:3:0.1]表示1：5切片间隔为0.1，1：3切片间隔为0.1 # np.mgrid[1:5:4j, 1:3:3j]表示1：5切片均匀取数，取4个，1：3切片均匀取数，取3个 y, x = np.mgrid[-1:1:15j, -1:1:15j] # (15, 15) (15, 15) fvals = func(x, y) # 计算每个网格点上的函数值 # (15, 15) # 三次样条二维插值 newfunc = interpolate.interp2d(x, y, fvals, kind=&#39;cubic&#39;) # 计算100*100的网格上的插值 xnew = np.linspace(-1, 1, 100) # 100 ynew = np.linspace(-1, 1, 100) fnew = newfunc(xnew, ynew) # 仅仅是y值 (100, 100) # 为了更明显地比较插值前后的区别，使用关键字参数interpolation=&#39;nearest&#39;关闭imshow()内置的插值运算。 plt.subplot(121) im1 = plt.imshow(fvals, extent=[-1, 1, -1, 1], cmap=plt.cm.hot, interpolation=&#39;nearest&#39;, origin=&#34;lower&#34;) # extent=[-1,1,-1,1]为x,y范围 plt.colorbar(im1) plt.subplot(122) im2 = plt.imshow(fnew, extent=[-1, 1, -1, 1], cmap=plt.cm.hot, interpolation=&#39;nearest&#39;, origin=&#34;lower&#34;) plt.colorbar(im2) plt.show() 二维插值的三维展示方法 # -*- coding: utf-8 -*- import numpy as np from scipy import interpolate import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D def func(x, y): return (x+y)*np.exp(-5.0*(x**2 + y**2)) # X-Y轴分为20*20的网格 x = np.linspace(-1, 1, 2) y = np.linspace(-1, 1, 2) print(x, y) # [-1. 1.] x, y = np.meshgrid(x, y) # (20, 20) (20, 20) print(x, y) # [[-1. 1.] # [-1. 1.]] # # [[-1. -1.] # [ 1. 1.]] fvals = func(x, y) # (20, 20) fig = Axes3D(plt.figure(figsize=(9, 6))) ax = plt.subplot(1, 2, 1, projection=&#39;3d&#39;) # rstride:行之间的跨度 cstride:列之间的跨度 # rcount:设置间隔个数，默认50个，ccount:列的间隔个数 不能与上面两个参数同时出现 surf = ax.plot_surface(x, y, fvals, rstride=2, cstride=2, cmap=plt.cm.coolwarm, linewidth=0.5, antialiased=True) ax.set_xlabel(&#39;x&#39;) ax.set_ylabel(&#39;y&#39;) ax.set_zlabel(&#39;f(x, y)&#39;) plt.colorbar(surf, shrink=0.5, aspect=5) # 二维插值 newfunc = interpolate.interp2d(x, y, fvals, kind=&#39;cubic&#39;) # 计算100*100的网格上的插值 xnew = np.linspace(-1, 1, 100) ynew = np.linspace(-1, 1, 100) fnew = newfunc(xnew, ynew) # 100*100 xnew, ynew = np.meshgrid(xnew, ynew) # 100*100 100*100 ax2 = plt.subplot(1, 2, 2, projection=&#39;3d&#39;) surf2 = ax2.plot_surface(xnew, ynew, fnew, rstride=2, cstride=2, cmap=plt.cm.coolwarm, linewidth=0.5, antialiased=True) ax2.set_xlabel(&#39;xnew&#39;) ax2.set_ylabel(&#39;ynew&#39;) ax2.set_zlabel(&#39;fnew(x, y)&#39;) plt.colorbar(surf2, shrink=0.5, aspect=5) plt.show() s
]]></content></entry><entry><title>ml-迁移学习(Transfer-Learning)</title><url>/post/ml-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0transfer-learning/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[训练复杂的卷积神经网络需要非常多的标注数据。
所谓迁移学习，就是讲一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。
根据论文DeCAF:A Deep Convolutional Activation Feature for Generic Visual Recognition中的结论，可以保留训练好的Inception-v3模型中所有卷积层的参数，只是替换最后一层全连接层。在最后这一层全连接之前的网络层称之为瓶颈层（bottleneck）。
**将新的图像通过训练好的卷积神经网络直到瓶颈处的过程可以看成事对图像进行特征提取的过程。**在训练好的Inception-v3模型中，因为将瓶颈处的输出再通过一个单层的全连接层神经网络可以很好地区分1000种类别的图像，所以有理由认为瓶颈处输出的节点向量可以被作为任何图像的一个更加精简且表达能力更强的特征向量。
蓝色点，卷积层间有联合依赖和适应性，不能破坏 输入数据文件夹包含了5个子文件夹，每一个子文件的名称为一种花的名称，代表不同的类别。平均每一种花有734张图片，每一张图片都是RGB色彩模式的，大小也不相同。和之前的样例不同，在这一节中给出的程序将直接处理没有整理过的图像数据。
import glob import os.path import random import numpy as np import tensorflow as tf from tensorflow.python.platform import gfile # 模型和样本路径的设置 BOTTLENECK_TENSOR_SIZE = 2048 BOTTLENECK_TENSOR_NAME = &#39;pool_3/_reshape:0&#39; JPEG_DATA_TENSOR_NAME = &#39;DecodeJpeg/contents:0&#39; MODEL_DIR = &#39;../../datasets/inception_dec_2015&#39; MODEL_FILE= &#39;tensorflow_inception_graph.pb&#39; CACHE_DIR = &#39;../../datasets/bottleneck&#39; INPUT_DATA = &#39;../../datasets/flower_photos&#39; VALIDATION_PERCENTAGE = 10 TEST_PERCENTAGE = 10 # 神经网络参数的设置 LEARNING_RATE = 0.01 STEPS = 4000 BATCH = 100 # 把样本中所有的图片列表并按训练、验证、测试数据分开 def create_image_lists(testing_percentage, validation_percentage): result = {} sub_dirs = [x[0] for x in os.walk(INPUT_DATA)] is_root_dir = True for sub_dir in sub_dirs: if is_root_dir: is_root_dir = False continue extensions = [&#39;jpg&#39;, &#39;jpeg&#39;, &#39;JPG&#39;, &#39;JPEG&#39;] file_list = [] dir_name = os.path.basename(sub_dir) for extension in extensions: file_glob = os.path.join(INPUT_DATA, dir_name, &#39;*.&#39; + extension) file_list.extend(glob.glob(file_glob)) if not file_list: continue label_name = dir_name.lower() # 初始化 training_images = [] testing_images = [] validation_images = [] for file_name in file_list: base_name = os.path.basename(file_name) # 随机划分数据 chance = np.random.randint(100) if chance &lt; validation_percentage: validation_images.append(base_name) elif chance &lt; (testing_percentage + validation_percentage): testing_images.append(base_name) else: training_images.append(base_name) result[label_name] = { &#39;dir&#39;: dir_name, &#39;training&#39;: training_images, &#39;testing&#39;: testing_images, &#39;validation&#39;: validation_images, } return result # 定义函数通过cache/input路径、类别名称、图片编号和所属数据集获取一张图片的bottleneck/input地址。 def get_image_path(image_lists, image_dir, label_name, index, category): label_lists = image_lists[label_name] category_list = label_lists[category] mod_index = index % len(category_list) base_name = category_list[mod_index] sub_dir = label_lists[&#39;dir&#39;] full_path = os.path.join(image_dir, sub_dir, base_name) return full_path # 定义函数获取Inception-v3模型处理之后的特征向量的文件地址。 def get_bottleneck_path(image_lists, label_name, index, category): return get_image_path(image_lists, CACHE_DIR, label_name, index, category) + &#39;.txt&#39; # 定义函数使用加载的训练好的Inception-v3模型处理一张图片，得到这个图片的特征向量。 def process_image_to_bottleneck(sess, image_data, image_data_tensor, bottleneck_tensor): bottleneck_values = sess.run(bottleneck_tensor, {image_data_tensor: image_data}) bottleneck_values = np.squeeze(bottleneck_values) return bottleneck_values # 定义函数先试图寻找已经计算且保存下来的特征向量，如果找不到则先计算这个特征向量，然后保存到文件。 def get_or_create_bottleneck(sess, image_lists, label_name, index, category, jpeg_data_tensor, bottleneck_tensor): # 得到子文件夹图像列表 label_lists = image_lists[label_name] sub_dir = label_lists[&#39;dir&#39;] sub_dir_path = os.path.join(CACHE_DIR, sub_dir) # 如果cache文件夹中中不存在该子目录则新建 if not os.path.exists(sub_dir_path): os.makedirs(sub_dir_path) # 获取Inception-v3模型处理之后的特征向量的文件地址 bottleneck_path = get_bottleneck_path(image_lists, label_name, index, category) # 如果未处理过将原始图片数据送入模型进行处理;并将每个数据用逗号分开 if not os.path.exists(bottleneck_path): image_path = get_image_path(image_lists, INPUT_DATA, label_name, index, category) image_data = gfile.FastGFile(image_path, &#39;rb&#39;).read() bottleneck_values = process_image_to_bottleneck(sess, image_data, jpeg_data_tensor, bottleneck_tensor) bottleneck_string = &#39;,&#39;.join(str(x) for x in bottleneck_values) with open(bottleneck_path, &#39;w&#39;) as bottleneck_file: bottleneck_file.write(bottleneck_string) # 如果处理过直接读取 else: with open(bottleneck_path, &#39;r&#39;) as bottleneck_file: bottleneck_string = bottleneck_file.read() bottleneck_values = [float(x) for x in bottleneck_string.split(&#39;,&#39;)] return bottleneck_values # 随机获取数据。 def get_random_cached_bottlenecks(sess, image_lists, n_classes, how_many, category, jpeg_data_tensor, bottleneck_tensor): &#34;&#34;&#34; label is subdir_name.lower() train,validate获取随机一个batch的图片,test获得全部图片 &#34;&#34;&#34; bottlenecks = [] ground_truths = [] if category == &#39;testing&#39;: label_name_list = list(image_lists.keys()) for label_index, label_name in enumerate(label_name_list): for image_index, _ in enumerate(image_lists[label_name][category]): makedata() else: for _ in range(how_many): label_index = random.randrange(n_classes) label_name = list(image_lists.keys())[label_index] image_index = random.randrange(65536) makedata() def makedata(): bottleneck = get_or_create_bottleneck(sess, image_lists, label_name, image_index, category, jpeg_data_tensor, bottleneck_tensor) ground_truth = np.zeros(n_classes, dtype=np.float32) ground_truth[label_index] = 1.0 bottlenecks.append(bottleneck) ground_truths.append(ground_truth) return bottlenecks, ground_truths # 定义主函数。 def main(_): # 整理好的数据 image_lists = create_image_lists(TEST_PERCENTAGE, VALIDATION_PERCENTAGE) # 子文件夹的数量 n_classes = len(image_lists.keys()) # 读取已经训练好的Inception-v3模型。 # tf.gfile.FastGFile(path,decodestyle) 函数功能：实现对图片的读取。 函数参数：(1)path：图片所在路径 (2)decodestyle:图片的解码方式。(‘r’:UTF-8编码; ‘rb’:非UTF-8编码) with gfile.FastGFile(os.path.join(MODEL_DIR, MODEL_FILE), &#39;rb&#39;) as f: graph_def = tf.GraphDef() graph_def.ParseFromString(f.read()) # 得到的与return_element中的名称相对应的操作和/或张量对象的列表 bottleneck_tensor, jpeg_data_tensor = tf.import_graph_def( graph_def, return_elements=[BOTTLENECK_TENSOR_NAME, JPEG_DATA_TENSOR_NAME]) # 定义新的神经网络输入 bottleneck_input = tf.placeholder(tf.float32, [None, BOTTLENECK_TENSOR_SIZE], name=&#39;BottleneckInputPlaceholder&#39;) ground_truth_input = tf.placeholder(tf.float32, [None, n_classes], name=&#39;GroundTruthInput&#39;) # 定义一层全链接层 with tf.name_scope(&#39;final_training_ops&#39;): weights = tf.Variable(tf.truncated_normal([BOTTLENECK_TENSOR_SIZE, n_classes], stddev=0.001)) biases = tf.Variable(tf.zeros([n_classes])) logits = tf.matmul(bottleneck_input, weights) + biases final_tensor = tf.nn.softmax(logits) # 定义交叉熵损失函数。 cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=ground_truth_input) cross_entropy_mean = tf.reduce_mean(cross_entropy) train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy_mean) # 计算正确率。 with tf.name_scope(&#39;evaluation&#39;): correct_prediction = tf.equal(tf.argmax(final_tensor, 1), tf.argmax(ground_truth_input, 1)) evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) with tf.Session() as sess: init = tf.global_variables_initializer() sess.run(init) # 训练过程。 for i in range(STEPS): train_bottlenecks, train_ground_truth = get_random_cached_bottlenecks( sess, image_lists, n_classes, BATCH, &#39;training&#39;, jpeg_data_tensor, bottleneck_tensor) sess.run(train_step, feed_dict={bottleneck_input: train_bottlenecks, ground_truth_input: train_ground_truth}) if i % 100 == 0 or i + 1 == STEPS: validation_bottlenecks, validation_ground_truth = get_random_cached_bottlenecks( sess, image_lists, n_classes, BATCH, &#39;validation&#39;, jpeg_data_tensor, bottleneck_tensor) validation_accuracy = sess.run(evaluation_step, feed_dict={ bottleneck_input: validation_bottlenecks, ground_truth_input: validation_ground_truth}) print(&#39;After %d steps: Validation accuracy on random sampled %d examples = %.1f%%&#39; % (i, BATCH, validation_accuracy * 100)) # 在最后的测试数据上测试正确率。 test_bottlenecks, test_ground_truth = get_random_cached_bottlenecks( sess, image_lists, n_classes, BATCH, &#39;testing&#39;, jpeg_data_tensor, bottleneck_tensor) test_accuracy = sess.run(evaluation_step, feed_dict={ bottleneck_input: test_bottlenecks, ground_truth_input: test_ground_truth}) print(&#39;Final test accuracy = %.1f%%&#39; % (test_accuracy * 100)) if __name__ == &#39;__main__&#39;: main(_) ### 1. 定义需要使用到的常量 import glob import os.path import numpy as np import tensorflow as tf from tensorflow.python.platform import gfile # 原始输入数据的目录，这个目录下有5个子目录，每个子目录底下保存这属于该 # 类别的所有图片。 INPUT_DATA = &#39;../../datasets/flower_photos&#39; # 输出文件地址。我们将整理后的图片数据通过numpy的格式保存。 OUTPUT_FILE = &#39;../../datasets/flower_processed_data.npy&#39; # 测试数据和验证数据比例。 VALIDATION_PERCENTAGE = 10 TEST_PERCENTAGE = 10 ### 2. 定义数据处理过程 # 读取数据并将数据分割成训练数据、验证数据和测试数据。 def create_image_lists(sess, testing_percentage, validation_percentage): sub_dirs = [x[0] for x in os.walk(INPUT_DATA)] is_root_dir = True # 初始化各个数据集。 training_images = [] training_labels = [] testing_images = [] testing_labels = [] validation_images = [] validation_labels = [] current_label = 0 # 读取所有的子目录。 for sub_dir in sub_dirs: if is_root_dir: is_root_dir = False continue # 获取一个子目录中所有的图片文件。 extensions = [&#39;jpg&#39;, &#39;jpeg&#39;, &#39;JPG&#39;, &#39;JPEG&#39;] file_list = [] dir_name = os.path.basename(sub_dir) for extension in extensions: file_glob = os.path.join(INPUT_DATA, dir_name, &#39;*.&#39; + extension) file_list.extend(glob.glob(file_glob)) if not file_list: continue print(&#34;processing:&#34;, dir_name) i = 0 # 处理图片数据。 for file_name in file_list: i += 1 # 读取并解析图片，将图片转化为299*299以方便inception-v3模型来处理。 image_raw_data = gfile.FastGFile(file_name, &#39;rb&#39;).read() image = tf.image.decode_jpeg(image_raw_data) if image.dtype != tf.float32: image = tf.image.convert_image_dtype(image, dtype=tf.float32) image = tf.image.resize_images(image, [299, 299]) image_value = sess.run(image) # 随机划分数据聚。 chance = np.random.randint(100) if chance &lt; validation_percentage: validation_images.append(image_value) validation_labels.append(current_label) elif chance &lt; (testing_percentage + validation_percentage): testing_images.append(image_value) testing_labels.append(current_label) else: training_images.append(image_value) training_labels.append(current_label) if i % 200 == 0: print(i, &#34;images processed.&#34;) current_label += 1 # 将训练数据随机打乱以获得更好的训练效果。 state = np.random.get_state() np.random.shuffle(training_images) np.random.set_state(state) np.random.shuffle(training_labels) return np.asarray([training_images, training_labels, validation_images, validation_labels, testing_images, testing_labels]) ### 3. 运行数据处理过程 with tf.Session() as sess: processed_data = create_image_lists(sess, TEST_PERCENTAGE, VALIDATION_PERCENTAGE) # 通过numpy格式保存处理后的数据。 np.save(OUTPUT_FILE, processed_data) ### 1. 定义训练过程中将要使用到的常量。 **因为GitHub无法保存大于100M的文件，所以在运行时需要先自行从Google下载inception_v3.ckpt文件。** import glob import os.path import numpy as np import tensorflow as tf from tensorflow.python.platform import gfile import tensorflow.contrib.slim as slim # 加载通过TensorFlow-Slim定义好的inception_v3模型。 import tensorflow.contrib.slim.python.slim.nets.inception_v3 as inception_v3 # 处理好之后的数据文件。 INPUT_DATA = &#39;../../datasets/flower_processed_data.npy&#39; # 保存训练好的模型的路径。 TRAIN_FILE = &#39;train_dir/model&#39; # 谷歌提供的训练好的模型文件地址。因为GitHub无法保存大于100M的文件，所以 # 在运行时需要先自行从Google下载inception_v3.ckpt文件。 CKPT_FILE = &#39;../../datasets/inception_v3.ckpt&#39; # 定义训练中使用的参数。 LEARNING_RATE = 0.0001 STEPS = 300 BATCH = 32 N_CLASSES = 5 # 不需要从谷歌训练好的模型中加载的参数。 CHECKPOINT_EXCLUDE_SCOPES = &#39;InceptionV3/Logits,InceptionV3/AuxLogits&#39; # 需要训练的网络层参数明层，在fine-tuning的过程中就是最后的全联接层。 TRAINABLE_SCOPES=&#39;InceptionV3/Logits,InceptionV3/AuxLogit&#39; ### 2. 获取所有需要从谷歌训练好的模型中加载的参数。 def get_tuned_variables(): # 不需要从谷歌训练好的模型中加载的参数。 exclusions = [scope.strip() for scope in CHECKPOINT_EXCLUDE_SCOPES.split(&#39;,&#39;)] variables_to_restore = [] # 枚举inception-v3模型中所有的参数，然后判断是否需要从加载列表中移除。 for var in slim.get_model_variables(): excluded = False for exclusion in exclusions: if var.op.name.startswith(exclusion): excluded = True break if not excluded: variables_to_restore.append(var) return variables_to_restore ### 3. 获取所有需要训练的变量列表。 def get_trainable_variables(): scopes = [scope.strip() for scope in TRAINABLE_SCOPES.split(&#39;,&#39;)] variables_to_train = [] # 枚举所有需要训练的参数前缀，并通过这些前缀找到所有需要训练的参数。 for scope in scopes: variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope) variables_to_train.extend(variables) return variables_to_train ### 4. 定义训练过程。 def main(): # 加载预处理好的数据。 processed_data = np.load(INPUT_DATA) training_images = processed_data[0] n_training_example = len(training_images) training_labels = processed_data[1] validation_images = processed_data[2] validation_labels = processed_data[3] testing_images = processed_data[4] testing_labels = processed_data[5] print(&#34;%d training examples, %d validation examples and %d testing examples.&#34; % ( n_training_example, len(validation_labels), len(testing_labels))) # 定义inception-v3的输入，images为输入图片，labels为每一张图片对应的标签。 images = tf.placeholder(tf.float32, [None, 299, 299, 3], name=&#39;input_images&#39;) labels = tf.placeholder(tf.int64, [None], name=&#39;labels&#39;) # 定义inception-v3模型。因为谷歌给出的只有模型参数取值，所以这里 # 需要在这个代码中定义inception-v3的模型结构。虽然理论上需要区分训练和 # 测试中使用到的模型，也就是说在测试时应该使用is_training=False，但是 # 因为预先训练好的inception-v3模型中使用的batch normalization参数与 # 新的数据会有出入，所以这里直接使用同一个模型来做测试。 with slim.arg_scope(inception_v3.inception_v3_arg_scope()): logits, _ = inception_v3.inception_v3( images, num_classes=N_CLASSES, is_training=True) trainable_variables = get_trainable_variables() # 定义损失函数和训练过程。 tf.losses.softmax_cross_entropy(tf.one_hot(labels, N_CLASSES), logits, weights=1.0) total_loss = tf.losses.get_total_loss() train_step = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(total_loss) # 计算正确率。 with tf.name_scope(&#39;evaluation&#39;): correct_prediction = tf.equal(tf.argmax(logits, 1), labels) evaluation_step = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # 定义加载Google训练好的Inception-v3模型的Saver。 load_fn = slim.assign_from_checkpoint_fn( CKPT_FILE, get_tuned_variables(), ignore_missing_vars=True) # 定义保存新模型的Saver。 saver = tf.train.Saver() with tf.Session() as sess: # 初始化没有加载进来的变量，一定要在模型加载之前否则会重新赋值 init = tf.global_variables_initializer() sess.run(init) # 加载谷歌已经训练好的模型。 print(&#39;Loading tuned variables from %s&#39; % CKPT_FILE) load_fn(sess) start = 0 end = BATCH for i in range(STEPS): _, loss = sess.run([train_step, total_loss], feed_dict={ images: training_images[start:end], labels: training_labels[start:end]}) if i % 30 == 0 or i + 1 == STEPS: saver.save(sess, TRAIN_FILE, global_step=i) validation_accuracy = sess.run(evaluation_step, feed_dict={ images: validation_images, labels: validation_labels}) print(&#39;Step %d: Training loss is %.1f Validation accuracy = %.1f%%&#39; % ( i, loss, validation_accuracy * 100.0)) start = end if start == n_training_example: start = 0 end = start + BATCH if end &gt; n_training_example: end = n_training_example # 在最后的测试数据上测试正确率。 test_accuracy = sess.run(evaluation_step, feed_dict={ images: testing_images, labels: testing_labels}) print(&#39;Final test accuracy = %.1f%%&#39; % (test_accuracy * 100)) ### 5. 运行训练过程。 if __name__ == &#39;__main__&#39;: main() 迁移学习，是指利用数据、任务、或模型之间的相似性，将在旧领域学习过的模型，应用于新领域的一种学习过程。
迁移学习最权威的综述文章是香港科技大学杨强教授团队的A survey on transfer learning[Pan and Yang, 2010]。
大数据与少标注之间的矛盾。 尽管我们可以获取到海量的数据，这些数据往往是很初级的原始形态，很少有数据被加以正确的人工标注。数据的标注是一个耗时且昂贵的操作，目前为止，尚未有行之有效的方式来解决这一问题。这给机器学习和深度学习的模型训练和更新带来了挑战。反过来说，特定的领域，因为没有足够的标定数据用来学习，使得这些领域一直不能很好的发展。
迁移数据标注，利用迁移学习的思想，我们可以寻找一些与目标数据相近的有标注的数据，从而利用这 些数据来构建模型，增加我们目标数据的标注。
大数据与弱计算之间的矛盾。 绝大多数普通用户是不可能具有这些强计算能力的。这就引发了大数据和弱计算之间的矛盾。
模型迁移，利用迁移学习的思想，我们可以将那些大公司在大数据上训练好的模型，迁移到我们的任务中。针对于我们的任务进行微调，从而我们也可以拥有在大数据上训练好的模型。
普适化模型与个性化需求之间的矛盾。 目前的情况是，我们对于每一个通用的任务都构建了一个通用的模型。这个模型可以解决绝大多数的公共问题。但是具体到每个个体、每个需求，都存在其唯一性和特异性，一个普适化的通用模型根本无法满足。
自适应学习，我们利用迁移学习的思想，进行自适应的学习。考虑到不同用户之间的相似性和差异性，我们对普适化模型进行灵活的调整，以便完成我们的任务。
特定应用的需求。 比如推荐系统的冷启动问题。一个新的推荐系统，没有足够 的用户数据，如何进行精准的推荐? 一个崭新的图片标注系统，没有足够的标签，如何进行 精准的服务？现实世界中的应用驱动着我们去开发更加便捷更加高效的机器学习方法来加 以解决。
为了满足特定领域应用的需求，我们可以利用上述介绍过的手段，从数据和模型方法上 进行迁移学习。
比较项目 传统机器学习 迁移学习 数据分布 训练和测试数据服从相同的分布 训练和测试数据服从不同的分布 数据标注 需要足够的数据标注来训练模型 不需要足够的数据标注 模型 每个任务分别建模模型 可以在不同任务之间迁移 按照目标领域有无标签，迁移学习可以分为以下三个大类： 监督迁移学习(Supervised Transfer Learning) 半监督迁移学习(Semi-Supervised Transfer Learning) 无监督迁移学习(Unsupervised Transfer Learning) 按学习方法的分类形式，最早在迁移学习领域的权威综述文章[Pan and Yang, 2010] 给出定义。它将迁移学习方法分为以下四个大类： 基于样本的迁移学习方法(Instance based Transfer Learning)，简单来说就是通过权重重用，对源域和目标域的样例进行迁移。就是说直接对不同的样本赋予不同权重，比如说相似的样本，我就给它高权重，这样我就完成了迁移，非常简单非常非常直接。 基于特征的迁移学习方法(Feature based Transfer Learning)，，就是更进一步对特征进行变换。意思是说，假设源域和目标域的特征原来不在一个空间，或者说它们在原来那个空间上不相似，那我们就想办法把它们变换到一个空间里面，那这些特征不就相似了？这个思路也非常直接。这个方法是用得非常多的，一直在研究，目前是感觉是研究最热的。 基于模型的迁移学习方法(Model based Transfer Learning)，就是说构建参数共享的模型。这个主要就是在神经网络里面用的特别多，因为神经网络的结构可以直接进行迁移。比如说神经网络最经典的finetune 就是模型参数迁移的很好的体现。 基于关系的迁移学习方法(Relation based Transfer Learning)，，这个方法用的比较少，这个主要就是说挖掘和利用关系进行类比迁移。比如老师上课、学生听课就可以类比为公司开会的场景。 按照特征的属性进行分类，也是一种常用的分类方法。这在最近的迁移学习综述[Weiss et al., 2016] 中给出。按照特征属性，迁移学习可以分为两个大类： 同构迁移学习(Homogeneous Transfer Learning) 异构迁移学习(Heterogeneous Transfer Learning) 如果特征语义和维度都相同，那么就是同构；反之，如果特征完全不相同，那么就是异构。举个例子来说，不同图片的迁移，就可以认为是同构；而图片到文本的迁移，则是异构的。
时间序列行为识别(Activity Recognition) 主要通过佩戴在用户身体上的传感器，研究用户的行 为。行为数据是一种时间序列数据。不同用户、不同环境、不同位置、不同设备，都会导致 时间序列数据的分布发生变化。此时，也需要进行迁移学习。图12展示了同一用户不同位 置的信号差异性。在这个领域，华盛顿州立大学的Diane Cook 等人在2013 年发表的关于 迁移学习在行为识别领域的综述文章[Cook et al., 2013] 是很好的参考资料。
领域(Domain): 是进行学习的主体。领域主要由两部分构成：数据和生成这些数据的概率分布。通常我们用花体D 来表示一个domain，用大写斜体P 来表示一个概率分布。
任务(Task): 是学习的目标。任务主要由两部分组成：标签和标签对应的函数。通常我们用花体Y 来表示一个标签空间，用f(*) 来表示一个学习函数。
找到相似性(不变量)，是进行迁移学习的核心。有了这种相似性后，下一步工作就是，如何度量和利用这种相似性。度量工作的目标有两点：一是很好地度量两个领域的相似性，不仅定性地告诉我们它们是否相似，更定量地给出相似程度。二是以度量为准则，通过我们所要采用的学习手段，增大两个领域之间的相似性，从而完成迁移学习。
深度迁移学习 由于深度学习直接对原始数据进行学习，所以其对比非深度方法还有两个优势：自动化 地提取更具表现力的特征，以及满足了实际应用中的端到端(End-to-End) 需求。.
深度迁移学习方法(BA、DDC、DAN) 对比传统迁移学习方法(TCA、GFK 等)，在精度上具有无可匹敌的优势。
深度神经网络前面几层都学习到的是通用的特征（general feature）；随着网络层次的加深，后面的网络更偏重于学习任务特定的特征（specific feature）。这非常好理解，我们也都很好接受。那么问题来了：如何得知哪些层能够学习到general feature，哪些层能够学习到specific feature。更进一步：如果应用于迁移学习，如何决定该迁移哪些层、固定哪些层？
#一个典型的迁移学习过程是这样的。首先通过transfer learning对新的数据集进行训练，训练过一定epoch之后，改用fine tune方法继续训练，同时降低学习率。这样做是因为如果一开始就采用fine tune方法的话，网络还没有适应新的数据，那么在进行参数更新的时候，比较大的梯度可能会导致原本训练的比较好的参数被污染，反而导致效果下降。借助setup_to_transfer_learning与setup_to_fine_tune这两个函数，我们先只训练模型的顶层，再训练模型的大多数层，进而在提高模型训练效果的同时，降低训练时间。
例子 第一种即所谓的transfer learning，迁移训练时，移掉最顶层，比如ImageNet训练任务的顶层就是一个1000输出的全连接层，换上新的顶层，比如输出为10的全连接层，然后**训练的时候，只训练最后两层，即原网络的倒数第二层和新换的全连接输出层。可以说transfer learning将底层的网络当做了一个特征提取器来使用。
第二种叫做fine tune，和transfer learning一样，换一个新的顶层，但是这一次在训练的过程中，所有的（或大部分）其它层都会经过训练。也就是底层的权重也会随着训练进行调整。
模型的预训练权重将下载到~/.keras/models/并在载入模型时自动载入。模型的官方下载路径：https://github.com/fchollet/deep-learning-models/releases
notop：指模型不包含最后的3个全连接层。用来做fine-tuning专用，专门开源了这类模型。
keras.applications. mobilenet. MobileNet ( include_top=True, # 是否保留顶层的3个全连接网络 pop函数，去掉最后一层。1old_model.layers.pop() weights=&#39;imagenet&#39;, # None代表随机初始化，即不加载预训练权重。&#39;imagenet&#39;代表加载预训练权重 input_tensor=None, # 可填入Keras tensor作为模型的图像输出tensor input_shape=None, # 可选，仅当include_top=False有效，应为长为3的tuple，指明输入图片的shape，图片的宽高必须大于71，如(150,150,3) pooling=None, # 当include_top=False时，该参数指定了池化方式。None代表不池化，最后一个卷积层的输出为4D张量。‘avg’代表全局平均池化，‘max’代表全局最大值池化。 classes=1000 # 可选，图片分类的类别数，仅当include_top=True并且不加载预训练权重时可用。) from keras.applications.mobilenet import MobileNet ##方式（1） base_model = MobileNet(weights=&#39;imagenet&#39;,include_top=False) ##方式（2） base_model = MobileNet(weights=&#39;G:\mobilenet_1_0_128_tf_no_top.h5&#39;) x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(1024,activation=&#39;relu&#39;)(x) #we add dense layers so that the model can learn more complex functions and classify for better results. x = Dense(1024,activation=&#39;relu&#39;)(x) #dense layer 2 x = Dense(512,activation=&#39;relu&#39;)(x) #dense layer 3 preds = Dense(3,activation=&#39;softmax&#39;)(x) #final layer with softmax activation model = Model(inputs=base_model.input,outputs=preds) model.summary() print(len(model.layers)) for layer in model.layers[:20]: layer.trainable=False for layer in model.layers[20:]: layer.trainable=True from keras.preprocessing.image import img_to_array, array_to_img def preprocess_input_new(x): img = preprocess_input(img_to_array(x)) return array_to_img(img) # train_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip = True,fill_mode = &#34;nearest&#34;,zoom_range = 0.3,width_shift_range = 0.3,height_shift_range=0.3,rotation_range=30) train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input_new) #included in our dependencies train_generator=train_datagen.flow_from_directory(&#39;./train/&#39;, # data folder target_size=(224,224), color_mode=&#39;rgb&#39;, batch_size=32, class_mode=&#39;categorical&#39;, shuffle=True) test_generator = gen.flow_from_directory(&#34;test&#34;, image_size, shuffle=False, batch_size=batch_size, class_mode=None) # 测试集由于没有label，生成test_generator的函数需加参数class_mode=None。 # 早停法是一种被广泛使用的方法，在很多案例上都比正则化的方法要好。是在训练中计算模型在验证集上的表现，当模型在验证集上的表现开始下降的时候，停止训练，这样就能避免继续训练导致过拟合的问题。其主要步骤如下： # 1. 将原始的训练数据集划分成训练集和验证集 # 2. 只在训练集上进行训练，并每隔一个周期计算模型在验证集上的误差 # 3. 当模型在验证集上（权重的更新低于某个阈值；预测的错误率低于某个阈值；达到一定的迭代次数），则停止训练 # 4. 使用上一次迭代结果中的参数作为模型的最终参数 checkpoint = ModelCheckpoint(&#34;vgg16_1.h5&#34;, monitor=&#39;val_acc&#39;, verbose=1, save_best_only=True, save_weights_only=False, mode=&#39;auto&#39;, period=1) early = EarlyStopping(monitor=&#39;val_acc&#39;, min_delta=0, patience=10, verbose=1, mode=&#39;auto&#39;) model.compile(optimizer=&#39;Adam&#39;,loss=&#39;categorical_crossentropy&#39;,metrics=[&#39;accuracy&#39;]) step_size_train=train_generator.n//train_generator.batch_size model.fit_generator(generator=train_generator, steps_per_epoch=step_size_train, epochs=5， validation_data = validation_generator, callbacks = [checkpoint, early]) 借助setup_to_transfer_learning与setup_to_fine_tune这两个函数，我们先只训练模型的顶层，再训练模型的大多数层，进而在提高模型训练效果的同时，降低训练时间。 def setup_to_transfer_learning(model,base_model): # 这个函数将base_model的所有层都设置为不可训练，顶层默认为可训练。 for layer in base_model.layers: layer.trainable = False model.compile(optimizer=&#39;adam&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) def setup_to_fine_tune(model,base_model): # 这个函数将base_model中的前几层设置为不可训练，后面的所有层都设置为可训练。具体应确定到第几层，还需通过模型结构与不断调试来确定。 GAP_LAYER = 17 for layer in base_model.layers[:GAP_LAYER+1]: layer.trainable = False for layer in base_model.layers[GAP_LAYER+1:]: layer.trainable = True model.compile(optimizer=Adagrad(lr=0.0001), loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) setup_to_transfer_learning(model,base_model) history_tl = model.fit_generator(generator=train_generator, steps_per_epoch=800, epochs=2, validation_data=val_generator, validation_steps=12, class_weight=&#39;auto&#39;) model.save(&#39;./flowers17_iv3_tl.h5&#39;) setup_to_fine_tune(model,base_model) history_ft = model.fit_generator(generator=train_generator, steps_per_epoch=800, epochs=2, validation_data=val_generator, validation_steps=1, class_weight=&#39;auto&#39;) model.save(&#39;./flowers17_iv3_ft.h5&#39;) （2）加载权重到不同的网络结构
#old model model = Sequential() model.add(Dense(2, input_dim=3, name=&#34;dense_1&#34;)) model.add(Dense(3, name=&#34;dense_2&#34;)) model.save_weights(fname) # new model model = Sequential() model.add(Dense(2, input_dim=3, name=&#34;dense_1&#34;)) # will be loaded model.add(Dense(10, name=&#34;new_dense&#34;)) # will not be loaded # load weights from first model; will only affect the first layer, dense_1. model.load_weights(fname, by_name=True) layer_dict = dict([(layer.name, layer) for layer in model.layers]) import h5py weights_path = &#39;vgg19_weights.h5&#39; # (&#39;https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5) f = h5py.File(weights_path) list(f[&#34;model_weights&#34;].keys()) layer_names = [layer.name for layer in model.layers] for i in layer_dict.keys(): weight_names = f[&#34;model_weights&#34;][i].attrs[&#34;weight_names&#34;] weights = [f[&#34;model_weights&#34;][i][j] for j in weight_names] index = layer_names.index(i) model.layers[index].set_weights(weights) 新数据集较小，和原数据集相似，如果我们尝试训练整个网络，容易导致过拟合。由于新数据和原数据相似，因此我们期望卷积网络中的高层特征和新数据集相关。因此，建议冻结所有卷积层，只训练分类器（比如，线性分类器）：
for layer in model.layers:
layer.trainable = False新数据集较大，和原数据集相似，由于我们有更多数据，我们更有自信，如果尝试对整个网络进行精细调整，不会导致过拟合。
for layer in model.layers: layer.trainable = True # 其实默认值就是True 新数据集很小，但和原数据很不一样，由于数据集很小，我们大概想要从靠前的层提取特征，然后在此之上训练一个分类器：（假定你对h5py有所了解）
新数据集很大，和原数据很不一样，由于你有一个很大的数据集，你可以设计你自己的网络，或者使用现有的网络。你可以基于随机初始化权重或预训练网络权重初始化训练网络。一般选择后者。
keras 样例由imagenet花到猫狗 # 基于VGG16迁移学习 # 通过Keras的ImageDataGenerator加载数据集 # 加载VGG16模型但是不包括输出层 input_tensor = keras.Input(shape=(64, 64, 3)) vgg_model = keras.applications.VGG16(weights=&#39;imagenet&#39;, include_top=False, input_tensor=input_tensor) layer_dict = dict([(layer.name, layer) for layer in vgg_model.layers]) print(len(layer_dict)) vgg_model.summary() num_classes = 3 data_generator = keras.preprocessing.image.ImageDataGenerator( rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True) train_generator = data_generator.flow_from_directory( &#39;/content/drive/My Drive/transferlearndata/train&#39;, target_size=(64, 64), batch_size=4, shuffle=True, class_mode=&#39;categorical&#39;) print(train_generator.classes) print(train_generator.class_indices) data_generator = keras.preprocessing.image.ImageDataGenerator(rescale=1./255) validation_generator = data_generator.flow_from_directory( &#39;/content/drive/My Drive/transferlearndata/validate&#39;, target_size=(64, 64), batch_size=4, class_mode=&#39;categorical&#39;) # 构建迁移学习网络使用VGG6的前面两个权重block， # 依赖block2_pool的输出，输入张量（64x64x3） # 构建网络的层 x = vgg_model.output x = keras.layers.BatchNormalization()(x) x = keras.layers.Flatten()(x) x = keras.layers.Dense(4096, activation=&#39;relu&#39;)(x) x = keras.layers.Dropout(0.25)(x) x = keras.layers.Dense(num_classes, activation=tf.nn.softmax)(x) my_model = keras.Model(inputs=vgg_model.input, outputs=x) my_model.summary() # 是否fine-tuning整个网络或者几层 for layer in my_model.layers[:-10]: layer.trainable = False # 编译与训练 my_model.compile( loss=&#39;categorical_crossentropy&#39;, optimizer=keras.optimizers.Adam(0.0001), metrics=[&#39;accuracy&#39;]) my_model.fit_generator(train_generator, epochs=10, validation_data=validation_generator) # 保存整个模型 my_model.save(&#34;my_transfer_vgg.h5&#34;) # 加载与使用 flower_dict = [&#39;cats&#39;, &#39;dogs&#39;, &#39;horses&#39;] new_model = keras.models.load_model(&#34;my_transfer_vgg.h5&#34;) root_dir = &#34;/content/drive/My Drive/transferlearndata/test&#34; for file in os.listdir(root_dir): src = cv2.imread(os.path.join(root_dir, file)) img = cv2.resize(src, (64, 64)) img = np.expand_dims(img, 0) result = new_model.predict(img) index = np.argmax(result) print(result.shape, index, flower_dict[index]) cv2.putText(src, flower_dict[index],(50, 50), cv2.FONT_HERSHEY_PLAIN, 2.0, (0, 0, 255), 2, 8) cv2.imshow(&#34;input&#34;, src) cv2.waitKey(0) cv2.destroyAllWindows() ]]></content></entry><entry><title>ml-神经网络</title><url>/post/ml-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url><categories><category>ml</category></categories><tags/><content type="html">反向传播(Back Propagation) 用样本的特征$x$，计算出神经网络中每个隐藏层节点的输出$a_i$以及输出层每个节点的输出$y_i$然后，我们按照下面的方法计算出每个节点的误差项$\delta_i$
对于输出层节点i， $$ \delta_{i}=y_{i}\left(1-y_{i}\right)\left(t_{i}-y_{i}\right) $$ 例如，节点8 $\delta_{8}=y_{1}\left(1-y_{1}\right)\left(t_{1}-y_{1}\right)$
对于隐藏层节点 $\delta_{i}=a_{i}\left(1-a_{i}\right) \sum_{k \in \text {outputs}} w_{k i} \delta_{k}$ 其中，$a_i$是节点的输出值，$w_{ki}$是节点到它的下一层节点k的连接的权重，$\delta_k$是节点i的下一层节点k的误差项。 例如，对于隐藏层节点4来说，计算方法如下：$\delta_{4}=a_{4}\left(1-a_{4}\right)\left(w_{84} \delta_{8}+w_{94} \delta_{9}\right)$
最后，更新每个连接上的权值： $w_{j i} \leftarrow w_{j i}+\eta \delta_{j} x_{j i}$ 其中，$w_{ji}$是节点i到节点j的权重，$\eta$是一个成为学习速率的常数，$\delta_j$是节点j的误差项，$x_{ji}$是节点i传递给节点j的输入。例如，权重$w_{84}$的更新方法如下： $w_{84} \leftarrow w_{84}+\eta \delta_{8} a_{4}$
激活函数 如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数） 激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题； 在神经网络中，我们可以经常看到对于某一个隐藏层的节点，该节点的激活之计算一般分为两步： （1） 输入该节点的值后先进行一个线性变换，计算出值 （2）再进行一个非线性变换，也就是经过一个非线性激活函数
常用的激活函数包括：sigmoid函数、tanh函数、ReLU函数。
sigmoid函数： 当目标是解决一个二分类问题，可在输出层使用sigmoid函数进行二分类。
该函数数将取值为(−∞,+∞) 的数映射到(0,1)之间，其公式以及函数图如下所示： 缺点
1.当值非常大或者非常小是sigmoid的导数会趋近为0，则会导致梯度消失 2.函数的输出不是以0位均值，不便于下一层的计算。这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：???????????如x&amp;gt;0, f=wTx+b 那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。 3.解析式中含有幂运算，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间
tanh函数 tanh读作Hyperbolic Tangent，它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。 该函数数将取值为(−∞,+∞) 的数映射到(-1,1)之间，其公式以及函数图如下所示： ReLU函数 ReLU函数又称为修正线性单元, 是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下： 优点： 1. 当输入大于0时，不存在梯度消失的问题2. 由于ReLU函数只有线性关系，所以计算速度要快很多 缺点：1.当输入小于0时，梯度为0，会产生梯度消失问题。
ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点： 1） 解决了gradient vanishing问题 (在正区间) 2）计算速度非常快，只需要判断输入是否大于0 3）收敛速度远快于sigmoid和tanh
ReLU也有几个需要特别注意的问题： 1）ReLU的输出不是zero-centered 2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。
尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！
#　深度学习中的正则化（参数范数惩罚：L1正则化、L2正则化；数据集增强；噪声添加；early stop；Dropout层）、正则化的介绍。 #　深度模型中的优化：参数初始化策略；自适应学习率算法（梯度下降、AdaGrad、RMSProp、Adam；优化算法的选择）；batch norm层（提出背景、解决什么问题、层在训练和测试阶段的计算公式）；layer norm层。 https://www.jianshu.com/p/01a5ca060f07 #　FastText的原理。 #　利用FastText模型进行文本分类。 https://blog.csdn.net/yyy430/article/details/88419694#FastText%E4%BB%8B%E7%BB%8D https://github.com/nicken/nlp_study_on_datawhale/blob/master/task_3/task-03.md</content></entry><entry><title>ml-生成对抗网络(GAN)</title><url>/post/ml-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9Cgan/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[Goodfellow的Generative Adversarial Networks https://arxiv.org/abs/1406.2661 **本质上， GAN 目标是训练出一个好的生成模型，来模拟训练集中的数据。**不同的是， 一般的生成模型，必须先初始化一个“假设分布”，即后验分布，通过各种抽样方法抽样这个后验分布，就能知道这个分布与真实分布之间究竟有多大差异。这里的差异就要通过构造损失函数（ loss function ）来估算。知道了这个差异后，就能不断调优一开始的“假设分布”，从而不断逼近真实分布。限制玻尔兹曼机CRBM)就是这种生成模型的一种。然而，对抗网络可以学习自己的损失函数，无须精心设计和建构一个损失函数，却能达成无监督学习。 生成网络负责生成，辨别网络负责分辨生成的质量，然后不断的生成与辨别，最后达到效果。通过这种方式，损失函数被蕴含在判别器中了。我们不再需要思考损失函数应该如何设定
虽然，省去复杂的后验推断过程是GANs相对其他生成模型的优势。但是，早期的GANs 有许多问题，最主要的一项通病是GANs 不稳定“有时候它永远不会开始学习，或者生成我们认为合格的输出。DCGAN 方法对CNN 使用和修改的核心建议如下：
在判别器中用带步长的卷积层（ strided convolutions ）取代的池化层（ pooling layers ） 。在生成器中用小步幅卷积C fractional strided convolutions ）取代的池化层（ pooling layers ） ，达到学习上采样的效果。 在判别器和生成器中都采用Batch Normalization 批标准化。 对于较深的网络，移除全连接层。 在生成器中除了最后输出层，其他每一层输出使用ReLU 激活函数。在最后一层输出，可以使用Tanh 或Sigmoid 等两端饱和的激活函数。 在判别器中的每一层使用LeakyReLU 激活函数。 以图片为例，在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此D(G(z)) = 0.5。但是实际训练的时候这个状态一般是不可达的。上面的过程使用数学公式来表达：
$$ \min {G} \max {D} V(D, G)=\mathbb{E}{\boldsymbol{x} \sim p{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}{\boldsymbol{z} \sim p{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))] $$ 分析这个公式：
$x$表示真实输入 $z$表示输入G网络的噪声 $G(z)$表示G网络生成 $D(x)$表示D网络判断真实图片是否真实的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。 $D(G(z))$是D网络判断G生成的图片的是否真实的概率 G希望$D(G(z))$尽可能得大，这时$V(D, G)$会变小。因此我们看到式子的最前面的记号是$min_G$ D希望$D(x)$越大，$D(G(z))$越小。这时$V(D,G)$会变大。因此式子对于D来说是求最大$max_D$ 需要注意的是，整个GAN的整个过程都是无监督的 这里，给的真图是没有经过人工标注的，而系统里的D并不知道来的图片是什么玩意儿，它只需要分辨真假。G也不知道自己生成的是什么，反正就是学真图片的样子骗D。 正由于GAN的无监督，在生成过程中，G就会按照自己的意思天马行空生成一些“诡异”的图片，可怕的是D还能给一个很高的分数。
用keras实现GAN识别MNIST import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import sys from tensorflow import keras from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import * import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import sys from tensorflow import keras from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import * class GAN(): def __init__(self): self.img_rows = 28 self.img_cols = 28 self.channels = 1 self.img_shape = (self.img_rows, self.img_cols, self.channels) self.latent_dim = 100 # -----------------------创建编译 discriminator 训练D---------------------------- self.discriminator = self.build_discriminator() self.discriminator.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(0.0002, 0.5), metrics=[&#39;accuracy&#39;]) # ------------------------创建编译 DG 联合模型 训练G,inputs=z, outputs=label-------- z = Input(shape=(self.latent_dim,)) self.generator = self.build_generator() img = self.generator(z) # DG 联合模型在训练时discriminator不需训练 # 只会关闭self.combined中discriminator的训练，之前的discriminator已经compile了， # 不影响discriminator单独训练。 self.discriminator.trainable = False label = self.discriminator(img) self.combined = Model(inputs=z, outputs=label) self.combined.summary() self.combined.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(0.0002, 0.5)) # 输入长为100的噪声，返回28*28图像 def build_generator(self): noises = Input(shape=(self.latent_dim,)) l = Dense(256, input_dim=self.latent_dim)(noises) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(512)(l) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(1024)(l) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(np.prod(self.img_shape), activation=&#39;tanh&#39;)(l) imgs = Reshape(self.img_shape)(l) return Model(inputs=noises, outputs=imgs) # 输入28*28图像，返回0/1标签 def build_discriminator(self): imgs = Input(shape=self.img_shape) l = Flatten(input_shape=self.img_shape)(imgs) l = Dense(512)(l) l = LeakyReLU(alpha=0.2)(l) l = Dense(256)(l) l = LeakyReLU(alpha=0.2)(l) labels = Dense(1, activation=&#39;sigmoid&#39;)(l) return Model(inputs=imgs, outputs=labels) def train(self, epochs, batch_size=128, sample_interval=50): # 加载数据 (x_train, y_train), (x_test, y_test) = mnist.load_data() # 归一化 -1 to 1 x_train = x_train/127.5-1. # (60000, 28, 28) ---&gt; (60000, 28, 28, 1) x_train = np.expand_dims(x_train, axis=3) # 生成照片的标签真1假0 real_label = np.ones((batch_size, 1)) fake_label = np.zeros((batch_size, 1)) for epoch in range(epochs): &#34;&#34;&#34; GAN的训练在同一轮梯度反传的过程中可以细分为2步，先训练D在训练G； 注意不是等所有的D训练好以后，才开始训练G， 因为D的训练也需要上一轮梯度反传中G的输出值作为输入。 &#34;&#34;&#34; # ---------------------训练 Discriminator--------------------- &#34;&#34;&#34; 当训练D的时候，上一轮G产生的新图片，和真实图片，直接拼接在一起，作为x。 然后根据，按顺序摆放0和1假图对应0，真图对应1。 然后就可以通过，x输入生成一个score（从0到1之间的数），通过score和y组成的损失函数， 就可以进行梯度反传了。 &#34;&#34;&#34; # 随机选择一个batch 照片训练 index = np.random.randint(0, x_train.shape[0], batch_size) real_imgs = x_train[index] # 生成一个batch假照片 noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) fake_imgs = self.generator.predict(noise) # 开始训练 d_loss_real = self.discriminator.train_on_batch(real_imgs, real_label) # train_on_batch 返回compile里的 loss and metrics d_loss_fake = self.discriminator.train_on_batch(fake_imgs, fake_label) # d_loss_fake [0.7735841, 0.0625] d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # d_loss [0.7576531 0.21875 ] D_loss Acc # ---------------------训练 DG模型--------------------- &#34;&#34;&#34; 当训练G的时候， 需要把G和D当作一个整体。这个整体(下面简称DG系统)的输出仍然是score。 输入一组随机向量，就可以在G生成一张图，通过D对生成的这张图进行打分， 这就是DG系统的前向过程。 score=1就是DG系统需要优化的目标， score和y=1之间的差异可以组成损失函数，然后可以反向传播梯度。 注意，这里的D的参数是不可训练的。这样就能保证G的训练是符合D的打分标准的。 这就好比：如果你参加考试，你别指望能改变老师的评分标准&#34;&#34;&#34; noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) g_loss = self.combined.train_on_batch(noise, real_label) # g_loss 0.63344437 if epoch % 100 == 0: print (&#34;epoch:{}， D_Acc:{}，D_loss:{}，G_loss:{}&#34; .format( epoch, 100*d_loss[1], d_loss[0], g_loss)) if epoch % sample_interval == 0: self.show_images(epoch) self.combined.save(&#39;all_model.h5&#39;) def show_images(self, epoch): row, column = 5, 5 noise = np.random.normal(0, 1, (row*column, self.latent_dim)) fake_imgs = self.generator.predict(noise) # Rescale images 0 - 1 fake_imgs = 0.5*fake_imgs+0.5 fig, axs = plt.subplots(row, column) counter = 0 for i in range(row): for j in range(column): axs[i,j].imshow(fake_imgs[counter, :, :, 0], cmap=&#39;gray&#39;) axs[i,j].axis(&#39;off&#39;) counter += 1 # fig.savefig(&#34;images/{}.png&#34;.format(epoch)) plt.show() plt.close() if __name__ == &#39;__main__&#39;: gan = GAN() gan.train(epochs=10000, batch_size=1024, sample_interval=1000) Tensorflow实现GAN识别MNIST import tensorflow as tf import numpy as np import matplotlib.pyplot as plt import sys from tensorflow import keras from tensorflow.keras.datasets import mnist from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import * class GAN(): def __init__(self): self.img_rows = 28 self.img_cols = 28 self.channels = 1 self.img_shape = (self.img_rows, self.img_cols, self.channels) self.latent_dim = 100 # -----------------------创建编译 discriminator 训练D---------------------------- self.discriminator = self.build_discriminator() self.discriminator.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(0.0002, 0.5), metrics=[&#39;accuracy&#39;]) # ------------------------创建编译 DG 联合模型 训练G,inputs=z, outputs=label-------- z = Input(shape=(self.latent_dim,)) self.generator = self.build_generator() img = self.generator(z) # DG 联合模型在训练时discriminator不需训练 # 只会关闭self.combined中discriminator的训练，之前的discriminator已经compile了， # 不影响discriminator单独训练。 self.discriminator.trainable = False label = self.discriminator(img) self.combined = Model(inputs=z, outputs=label) self.combined.summary() self.combined.compile(loss=&#39;binary_crossentropy&#39;, optimizer=Adam(0.0002, 0.5)) # 输入长为100的噪声，返回28*28图像 def build_generator(self): noises = Input(shape=(self.latent_dim,)) l = Dense(256, input_dim=self.latent_dim)(noises) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(512)(l) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(1024)(l) l = LeakyReLU(alpha=0.2)(l) l = BatchNormalization(momentum=0.8)(l) l = Dense(np.prod(self.img_shape), activation=&#39;tanh&#39;)(l) imgs = Reshape(self.img_shape)(l) return Model(inputs=noises, outputs=imgs) # 输入28*28图像，返回0/1标签 def build_discriminator(self): imgs = Input(shape=self.img_shape) l = Flatten(input_shape=self.img_shape)(imgs) l = Dense(512)(l) l = LeakyReLU(alpha=0.2)(l) l = Dense(256)(l) l = LeakyReLU(alpha=0.2)(l) labels = Dense(1, activation=&#39;sigmoid&#39;)(l) return Model(inputs=imgs, outputs=labels) def train(self, epochs, batch_size=128, sample_interval=50): # 加载数据 (x_train, y_train), (x_test, y_test) = mnist.load_data() # 归一化 -1 to 1 x_train = x_train/127.5-1. # (60000, 28, 28) ---&gt; (60000, 28, 28, 1) x_train = np.expand_dims(x_train, axis=3) # 生成照片的标签真1假0 real_label = np.ones((batch_size, 1)) fake_label = np.zeros((batch_size, 1)) for epoch in range(epochs): &#34;&#34;&#34; GAN的训练在同一轮梯度反传的过程中可以细分为2步，先训练D在训练G； 注意不是等所有的D训练好以后，才开始训练G， 因为D的训练也需要上一轮梯度反传中G的输出值作为输入。 &#34;&#34;&#34; # ---------------------训练 Discriminator--------------------- &#34;&#34;&#34; 当训练D的时候，上一轮G产生的新图片，和真实图片，直接拼接在一起，作为x。 然后根据，按顺序摆放0和1假图对应0，真图对应1。 然后就可以通过，x输入生成一个score（从0到1之间的数），通过score和y组成的损失函数， 就可以进行梯度反传了。 &#34;&#34;&#34; # 随机选择一个batch 照片训练 index = np.random.randint(0, x_train.shape[0], batch_size) real_imgs = x_train[index] # 生成一个batch假照片 noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) fake_imgs = self.generator.predict(noise) # 开始训练 d_loss_real = self.discriminator.train_on_batch(real_imgs, real_label) # train_on_batch 返回compile里的 loss and metrics d_loss_fake = self.discriminator.train_on_batch(fake_imgs, fake_label) # d_loss_fake [0.7735841, 0.0625] d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) # d_loss [0.7576531 0.21875 ] D_loss Acc # ---------------------训练 DG模型--------------------- &#34;&#34;&#34; 当训练G的时候， 需要把G和D当作一个整体。这个整体(下面简称DG系统)的输出仍然是score。 输入一组随机向量，就可以在G生成一张图，通过D对生成的这张图进行打分， 这就是DG系统的前向过程。 score=1就是DG系统需要优化的目标， score和y=1之间的差异可以组成损失函数，然后可以反向传播梯度。 注意，这里的D的参数是不可训练的。这样就能保证G的训练是符合D的打分标准的。 这就好比：如果你参加考试，你别指望能改变老师的评分标准&#34;&#34;&#34; noise = np.random.normal(0, 1, (batch_size, self.latent_dim)) g_loss = self.combined.train_on_batch(noise, real_label) # g_loss 0.63344437 if epoch % 100 == 0: print (&#34;epoch:{}， D_Acc:{}，D_loss:{}，G_loss:{}&#34; .format( epoch, 100*d_loss[1], d_loss[0], g_loss)) if epoch % sample_interval == 0: self.show_images(epoch) self.combined.save(&#39;all_model.h5&#39;) def show_images(self, epoch): row, column = 5, 5 noise = np.random.normal(0, 1, (row*column, self.latent_dim)) fake_imgs = self.generator.predict(noise) # Rescale images 0 - 1 fake_imgs = 0.5*fake_imgs+0.5 fig, axs = plt.subplots(row, column) counter = 0 for i in range(row): for j in range(column): axs[i,j].imshow(fake_imgs[counter, :, :, 0], cmap=&#39;gray&#39;) axs[i,j].axis(&#39;off&#39;) counter += 1 # fig.savefig(&#34;images/{}.png&#34;.format(epoch)) plt.show() plt.close() if __name__ == &#39;__main__&#39;: gan = GAN() gan.train(epochs=10000, batch_size=1024, sample_interval=1000) 参考： https://blog.csdn.net/leviopku/article/details/81292192 ]]></content></entry><entry><title>ml-循环神经网络(RNN)</title><url>/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[RNN 神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。
基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。
RNN存在的问题 梯度消失的问题 RNN网络的激活函数一般选用双曲正切，而不是sigmod函数，（RNN的激活函数除了双曲正切，RELU函数也用的非常多）原因在于RNN网络在求解时涉及时间序列上的大量求导运算，使用sigmod函数容易出现梯度消失，且sigmod的导数形式较为复杂。事实上，即使使用双曲正切函数，传统的RNN网络依然存在梯度消失问题。 无论是梯度消失还是梯度爆炸，都是源于网络结构太深，造成网络权重不稳定，从本质上来讲是因为梯度反向传播中的连乘效应，类似于：$0.99^{100}=0.36$，于是梯度越来越小，开始消失，另一种极端情况就是$1.1^{100}=13780$。 长期依赖的问题还有一个问题是无法“记忆”长时间序列上的信息，这个bug直到LSTM上引入了单元状态后才算较好地解决 LSTM 下面来了解一下LSTM（long short-term memory）。长短期记忆网络是RNN的一种变体，RNN由于梯度消失的原因只能有短期记忆，LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题。 $f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)$ $i_{t}=\sigma\left(W_{i} \cdot \left[h_{t-1}, x_{t}\right]+b_{i}\right)$ $o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)$ $\tilde{C}{t}=\tanh \left(W{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)$ $C_{t}=f_{t} ⊙C_{t-1}+i_{t} ⊙ \tilde{C}$ $h_{t}=o_{t} ⊙ \tanh \left(C_{t}\right)$
遗忘门：主要控制是否遗忘上一层的记忆细胞状态， 输入分别是当前时间步序列数据，上一时间步的隐藏状态，进行矩阵相乘，经sigmoid激活后，获得一个值域在[0, 1]的输出F，再跟上一层记忆细胞进行对应元素相乘，输出F中越接近0，代表需要遗忘上层记忆细胞的元素。 候选记忆细胞：这里的区别在于将sigmoid函数换成tanh激活函数，因此输出的值域在[-1, 1]。 输入门：与遗忘门类似，也是经过sigmoid激活后，获得一个值域在[0, 1]的输出。它用于控制当前输入X经过候选记忆细胞如何流入当前时间步的记忆细胞。 如果输入门输出接近为0，而遗忘门接近为1，则当前记忆细胞一直保存过去状态 输出门：也是通过sigmoid激活，获得一个值域在[0,1]的输出。主要控制记忆细胞到下一时间步隐藏状态的信息流动
其中，四个蓝色的小矩形就是普通神经网络的隐藏层结构，其中第一、二和四的激活函数是sigmoid，第三个的激活函数是tanh。t时刻的输入X和t-1时刻的输出h(t-1)进行拼接，然后输入cell中，其实可以这样理解，我们的输入X(t)分别feed进了四个小蓝矩形中，每个小黄矩形中进行的运算和正常的神经网络的计算一样（矩阵乘法），有关记忆的部分完全由各种门结构来控制（就是0和1），同时在输入时不仅仅有原始的数据集，同时还加入了上一个数据的输出结果，也就是h(t-1)，那么讲来LSTM和正常的神经网络类似，只是在输入和输出上加入了一点东西。cell中可以大体分为两条横线，上面的横线用来控制长时记忆，下面的横线用来控制短时记忆。 不过LSTM依旧不能解决梯度“爆炸”的问题。但似乎梯度爆炸相对于梯度消失，问题没有那么严重。一般靠裁剪后的优化算法即可解决，比如gradient clipping（如果梯度的范数大于某个给定值，将梯度同比收缩）。 梯度剪裁的方法一般有两种： 1.一种是当梯度的某个维度绝对值大于某个上限的时候，就剪裁为上限。 2.另一种是梯度的L2范数大于上限后，让梯度除以范数，避免过大。
LSTM是一种拥有三个“门”的特殊网络结构，包括遗忘门、输入门、输出门。所谓“门”结构就是一个使用sigmoid神经网络和一个按位做乘法的操作，这两个操作合在一起就是一个“门”结构。
LSTM与GRU的区别 LSTM与GRU二者结构十分相似，不同在于：
新的记忆都是根据之前状态及输入进行计算，但是GRU中有一个重置门控制之前状态的进入量，而在LSTM里没有类似门； 产生新的状态方式不同，LSTM有两个不同的门，分别是遗忘门(forget gate)和输入门(input gate)，而GRU只有一种更新门(update gate)； LSTM对新产生的状态可以通过输出门(output gate)进行调节，而GRU对输出无任何调节。 GRU的优点更加简单，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。 LSTM更加强大和灵活，因为它有三个门而不是两个。 GRU结构 GRU是LSTM网络的一种效果很好的变体，它较LSTM网络的结构更加简单，而且效果也很好，因此也是当前非常流形的一种网络。GRU既然是LSTM的变体，因此也是可以解决RNN网络中的长依赖问题。 在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在GRU模型中只有两个门：分别是更新门和重置门。具体结构如下图所示：
$$R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r))$$
$Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z))$
$\tilde{H}t=tanh(X_tW{xh}+(R_t⊙H_{t-1})W_{hh}+b_h))$
更新⻔可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，
重置⻔可以⽤来丢弃与预测⽆关的历史信息。
我们对⻔控循环单元的设计稍作总结：
重置⻔有助于捕捉时间序列⾥短期的依赖关系； 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。 BILSTM 如果能像访问过去的上下文信息一样，访问未来的上下文，这样对于许多序列标注任务是非常有益的。例如，在最特殊字符分类的时候，如果能像知道这个字母之前的字母一样，知道将要来的字母，这将非常有帮助。
双向循环神经网络（BRNN）的基本思想是提出每一个训练序列向前和向后分别是两个循环神经网络（RNN），而且这两个都连接着一个输出层。这个结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的。
tf.nn.rnn_cell.BasicLSTMCell() 来实现BILSTM 深层循环神经网络（DRNN） tf.rnn_cell.MultiRNNCell(lstm * number_of_layer) 来构建DRNN，其中number_of_layer表示了有多少层 tf.nn.rnn_cell.DropoutWrapper 来实现dropout功能 tensorflow lstm预测正弦函数 import numpy as np import tensorflow as tf from tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat from tensorflow.python.ops import array_ops as array_ops_ import matplotlib.pyplot as plt learn = tf.contrib.learn #### 1. 设置神经网络的参数。 HIDDEN_SIZE = 30 NUM_LAYERS = 2 TIMESTEPS = 10 TRAINING_STEPS = 3000 BATCH_SIZE = 32 TRAINING_EXAMPLES = 10000 TESTING_EXAMPLES = 1000 SAMPLE_GAP = 0.01 #### 2. 定义生成正弦数据的函数。 def generate_data(seq): X = [] y = [] for i in range(len(seq) - TIMESTEPS - 1): X.append([seq[i: i + TIMESTEPS]]) y.append([seq[i + TIMESTEPS]]) return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32) #### 3. 定义lstm模型。 def lstm_model(X, y):/;;;p lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE) cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS) x_ = tf.unpack(X, axis=1) output, _ = tf.nn.rnn(cell, x_, dtype=tf.float32) output = output[-1] # 通过无激活函数的全联接层计算线性回归，并将数据压缩成一维数组的结构。 predictions = tf.contrib.layers.fully_connected(output, 1, None) predictions = array_ops_.squeeze(predictions, squeeze_dims=[1]) loss = tf.contrib.losses.mean_squared_error(predictions, y) train_op = tf.contrib.layers.optimize_loss( loss, tf.contrib.framework.get_global_step(), optimizer=&#34;Adagrad&#34;, learning_rate=0.1) return predictions, loss, train_op #### 4. 进行训练。 # 封装之前定义的lstm。 regressor = SKCompat(learn.Estimator(model_fn=lstm_model,model_dir=&#34;Models/model_2&#34;)) # 生成数据。 test_start = TRAINING_EXAMPLES * SAMPLE_GAP test_end = (TRAINING_EXAMPLES + TESTING_EXAMPLES) * SAMPLE_GAP train_X, train_y = generate_data(np.sin(np.linspace( 0, test_start, TRAINING_EXAMPLES, dtype=np.float32))) test_X, test_y = generate_data(np.sin(np.linspace( test_start, test_end, TESTING_EXAMPLES, dtype=np.float32))) # 拟合数据。 regressor.fit(train_X, train_y, batch_size=BATCH_SIZE, steps=TRAINING_STEPS) # 计算预测值。 predicted = [[pred] for pred in regressor.predict(test_X)] # 计算MSE。 rmse = np.sqrt(((predicted - test_y) ** 2).mean(axis=0)) print (&#34;Mean Square Error is: %f&#34; % rmse[0]) #### 5. 画出预测值和真实值的曲线。 plot_predicted, = plt.plot(predicted, label=&#39;predicted&#39;) plot_test, = plt.plot(test_y, label=&#39;real_sin&#39;) plt.legend([plot_predicted, plot_test],[&#39;predicted&#39;, &#39;real_sin&#39;]) plt.show() ]]></content></entry><entry><title>ml-自动机器学习</title><url>/post/ml-%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url><categories><category>ml</category></categories><tags/><content type="html"><![CDATA[AutoKeras是一个开源的，基于 Keras 的新型 AutoML 库。AutoKeras 是一个用于自动化机器学习的开源软件库，提供自动搜索深度学习模型的架构和超参数的功能。AutoKeras 采用的架构搜索方法是一种结合了贝叶斯优化的神经架构搜索。它主要关注于降低架构搜索所需要的计算力，并提高搜索结果在各种任务上的性能。
官方网站：https://autokeras.com/ 项目github：https://github.com/jhfjhfj1/autokeras
TensorFlow版本：https://github.com/melodyguan/enas PyTorch 版本：https://github.com/carpedm20/ENAS-pytorch
!pip install autokeras
from tensorflow.keras.datasets import mnist import autokeras as ak from keras.models import load_model from keras.utils import plot_model MODEL_DIR = &#39;my_model.h5&#39; MODEL_PNG = &#39;model.png&#39; IMAGE_SIZE = 28 # 获取本地图片，转换成numpy格式 (train_data, train_labels), (test_data, test_labels) = mnist.load_data() # 数据进行格式转换 train_data = train_data.astype(&#39;float32&#39;) / 255 test_data = test_data.astype(&#39;float32&#39;) / 255 print(&#34;train data shape:&#34;, train_data.shape) # 使用图片识别器 clf = ak.ImageClassifier() # 给其训练数据和标签，训练的最长时间可以设定，假设为1分钟，autokers会不断找寻最优的网络模型 clf.fit(train_data, train_labels, epochs=10) # 给出评估结果 accuracy = clf.evaluate(test_data, test_labels, batch_size=32) print(&#34;accuracy:&#34;, accuracy) y = clf.predict(test_data, batch_size=32) print(&#34;predict:&#34;, y) # 导出我们生成的模型 clf.export_keras_model(MODEL_DIR) # 加载模型 model = load_model(MODEL_DIR) # 将模型导出成可视化图片 plot_model(model, to_file=MODEL_PNG) https://blog.csdn.net/lvsolo/article/details/103445431?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522158626893719724848310208%2522%252C%2522scm%2522%253A%252220140713.130056874..%2522%257D&amp;request_id=158626893719724848310208&amp;biz_id=0&amp;utm_source=distribute.pc_search_result.none-task-blog-blog_SOOPENSEARCH-6 ]]></content></entry><entry><title>面试-leetcode 热题100</title><url>/post/%E9%9D%A2%E8%AF%95-leetcode-%E7%83%AD%E9%A2%98100/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[leetcode 热题100 第一周，链表、栈、队列 第一天：链表（周三） 1、 链表的基础知识：单链表 2、 反转链表（ LeetCode 206 ） 3、 相交链表（ LeetCode 160 ） 4、 合并两个有序链表 （ LeetCode 21 ） 5、 分隔链表 （ LeetCode 86 ） 6、 环形链表 II （ LeetCode 142 ） 7、 反转链表 II （ LeetCode 92 ） 8、 复制带随机指针的链表（ LeetCode 138 ） 第二天：直播 + 栈（周六） 1、 链表算法题直播答疑 2、 栈的基础知识 3、 有效的括号（ LeetCode 20 ） 4、 基本计算器（ LeetCode 224 ） 5、 最小栈（ LeetCode 155 ） 6、 验证栈序列（ LeetCode 946 ） 7、 每日温度（ LeetCode 739 ） 8、 接雨水（ LeetCode 42 ） 第三天：队列（周日） 1、 队列的基础知识 2、优先队列基础知识
3、 用栈实现队列 （ LeetCode 232 ） 4、 滑动窗口最大值（ LeetCode 239 ） 5、设计循环双端队列（ LeetCode 641 ）
补充内容 以下内容比较简单，学完队列还有余力的时候可以快速过一遍下面这些链表题。
1、 移除链表元素（ LeetCode 203 ） 2、 K 个一组翻转链表（ LeetCode 25 ） 3、 回文链表（ LeetCode 234 ） 4、 奇偶链表（ LeetCode 328 ） 5、 从尾到头打印链表（ 剑指Offer 06 ） 6、 链表中倒数第 k 个节点（ 剑指Offer 22 ） 第二周，递归、排序、贪心 第一天：直播 + 递归 + 简单排序算法（周三） 1、 递归基础知识 2、 冒泡排序基础知识 3、 选择排序基础知识 4、 插入排序基础知识 5、 合并两个有序数组( LeetCode 88 ) 6、 颜色分类( LeetCode 75 ) 7、 部分排序 （面试题 16） 8、 归并排序 基础知识(明天上传)
9、计算右侧小于当前元素的个数 （ LeetCode 315 ）(明天上传)
10、合并 K 个升序链表（ LeetCode 23 ）(明天上传)
第二天：复杂排序算法（周六） 1、栈 + 队列问题答疑
2、快速排序基础知识
3、二叉堆基础知识
4、堆排序基础知识
5、计数排序基础知识
6、基数排序基础知识
7、桶排序基础知识
第三天：贪心算法（周日） 1、贪心算法基础知识
2、分发饼干（ LeetCode 455 ）
3、柠檬水找零（ LeetCode 860 ）
4、用最少数量的箭引爆气球（ LeetCode 452 ）
5、移掉 K 位数字（ LeetCode 402 ）
6、跳跃游戏（ LeetCode 55 ）
7、摆动序列（ LeetCode 376 ）
8、买卖股票的最佳时机 II（ LeetCode 122 ）
第三周，搜索算法、回溯算法、位运算、二分查找 第一天：搜索 + 回溯（周三） 1、搜索基础知识
2、回溯基础知识
3、子集（ LeetCode 78 ）
4、组合总和 II（ LeetCode 40 ）
5、括号生成（ LeetCode 22 ）
6、N 皇后（ LeetCode 51 ）
7、火柴拼正方形（ LeetCode 473 ）
8、岛屿数量（ LeetCode 200 ）
9、接雨水 II（ LeetCode 407 ）
第二天：直播 + 位运算（周六） 1、排序算法问题、贪心算法问题答疑
2、位运算基础知识
3、丢失的数字（ LeetCode 268 ）
4、2 的幂（ LeetCode 231 ）
5、比特位计数（ LeetCode 338 ）
6、位 1 的个数（ LeetCode 268 ）
7、4 的幂（ LeetCode 342 ）
8、只出现一次的数字（ LeetCode 136 ）
第三天：二分查找（周日） 1、二分查找基础知识
2、搜索插入位置（ LeetCode 35 ）
3、在排序数组中查找元素的第一个和最后一个位置（ LeetCode 34 ）
4、搜索旋转排序数组（ LeetCode 33 ）
5、搜索二维矩阵（ LeetCode 74 ）
6、寻找两个正序数组的中位数（ LeetCode 4 ）
第四周，二叉树、二叉查找树、线段树 第一天：二叉树（周三） 1、二叉树基础知识
2、二叉树的前序遍历（ LeetCode 144 ）
3、二叉树的中序遍历（ LeetCode 94 ）
4、二叉树的后序遍历（ LeetCode 145 ）
5、二叉树的层序遍历（ LeetCode 102 ）
6、二叉树的锯齿形层序遍历（ LeetCode 103 ）
7、从前序与中序遍历序列构造二叉树（ LeetCode 105 ）
8、路径总和 II（ LeetCode 113 ）
9、二叉树的最近公共祖先（ LeetCode 236 ）
10、二叉树的右视图（ LeetCode 199 ）
11、二叉树展开为链表（ LeetCode 114 ）
第二天：直播 + 二叉查找树（周六） 1、回溯算法问题、位运算问题、二分查找问题直播答疑
2、二叉查找树基础知识
3、将有序数组转换为二叉搜索树（ LeetCode 108 ）
4、把二叉搜索树转换为累加树（ LeetCode 538 ）
5、删除二叉搜索树中的节点（ LeetCode 450 ）
6、 序列化 和反序列化二叉搜索树（ LeetCode 449 ）
7、计算右侧小于当前元素的个数（ LeetCode 315 ）
第三天：线段树（周日） 1、线段树基础知识
2、区域和检索 - 数组可修改（ LeetCode 307 ）
3、天际线问题（ LeetCode 218 ）
第五周、动态规划、图算法 第一天：动态规划（周三） 1、动态规划基础知识
2、爬楼梯（ LeetCode 70 ）
3、打家劫舍（ LeetCode 198 ）
4、 三角形 最小路径和（ LeetCode 120 ）
5、最大子序和（ LeetCode 53 ）
6、零钱兑换（ LeetCode 322 ）
7、最小路径和（ LeetCode 64 ）
8、编辑距离（ LeetCode 72 ）
9、地下城游戏（ LeetCode 174 ）
第二天：二叉树直播 + 动态规划（周六） 1、二叉树算法问题直播答疑
2、买卖股票的最佳时机（ LeetCode 121 ）
3、买卖股票的最佳时机II（ LeetCode 122 ）
4、买卖股票的最佳时机III（ LeetCode 123 ）
5、买卖股票的最佳时机IV（ LeetCode 188 ）
6、最长递增子序列（ LeetCode 300 ）
7、分割等和子集（ LeetCode 416 ）
8、完全平方数（ LeetCode 279 ）
第三天：图算法（周日） 1、图基础知识
2、省份数量（ LeetCode 547 ）
3、课程表（ LeetCode 547 ）
1. 两数之和 2. 两数相加 难度中等4480
给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。
如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。
您可以假设除了数字 0 之外，这两个数都不会以 0 开头。
示例：
输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4) 输出：7 -&gt; 0 -&gt; 8 原因：342 + 465 = 807 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode addTwoNumbers(ListNode l1, ListNode l2) { ListNode root = new ListNode(0); ListNode cursor = root; int c = 0; while(l1!=null || l2!=null || c!=0){ int val1 = l1==null? 0 : l1.val; int val2 = l2==null? 0 : l2.val; int sum = val1 + val2 + c; c = sum / 10; ListNode node = new ListNode(sum%10); cursor.next = node; cursor = node; if (l1 != null) l1=l1.next; if (l2 != null) l2=l2.next; } return root.next; } } 3. 无重复字符的最长子串 难度中等3815
给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。
示例 1:
输入: &#34;abcabcbb&#34; 输出: 3 解释: 因为无重复字符的最长子串是 &#34;abc&#34;，所以其长度为 3。 示例 2:
输入: &#34;bbbbb&#34; 输出: 1 解释: 因为无重复字符的最长子串是 &#34;b&#34;，所以其长度为 1。 示例 3:
输入: &#34;pwwkew&#34; 输出: 3 解释: 因为无重复字符的最长子串是 &#34;wke&#34;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&#34;pwke&#34; 是一个子序列，不是子串。 思路：
class Solution { public int lengthOfLongestSubstring(String s) { int n=s.length(), ans=0; Map&lt;Character, Integer&gt; map = new HashMap&lt;&gt;(); for(int j=0, i=0; j&lt;n; j++) { if (map.containsKey(s.charAt(j))) { i = Math.max(map.get(s.charAt(j)), i); } ans = Math.max(ans, j-i+1); map.put(s.charAt(j), j+1); } return ans; } } 使用滑动窗口：
class Solution { public int lengthOfLongestSubstring(String s) { int res = 0; if(s==null || s.length()&lt;1) return res; if(s.length()==1) return 1; for(int i=0; i&lt;s.length()-1; i++){ Set&lt;Character&gt; set = new HashSet&lt;&gt;(); set.add(s.charAt(i)); for(int j=i+1 ; j&lt;s.length(); j++){ if(!set.contains(s.charAt(j))){ set.add(s.charAt(j)); }else{ break; } } res = Math.max(res, set.size()); } return res; } } 4. 寻找两个正序数组的中位数 难度困难2787
给定两个大小为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。
请你找出这两个正序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。
你可以假设 nums1 和 nums2 不会同时为空。
示例 1:
nums1 = [1, 3] nums2 = [2] 则中位数是 2.0 示例 2:
nums1 = [1, 2] nums2 = [3, 4] 则中位数是 (2 + 3)/2 = 2.5 class Solution { public double findMedianSortedArrays(int[] nums1, int[] nums2) { int m = nums1.length; int n = nums2.length; int left = (m + n + 1) / 2;//注意 int right = (m + n + 2) / 2; return (findKth(nums1, 0, nums2, 0, left) + findKth(nums1, 0, nums2, 0, right)) / 2.0; } //i: nums1的起始位置 j: nums2的起始位置 public int findKth(int[] nums1, int i, int[] nums2, int j, int k){//寻找第K小 if( i &gt;= nums1.length) return nums2[j + k - 1];//nums1为空数组 if( j &gt;= nums2.length) return nums1[i + k - 1];//nums2为空数组 if(k == 1){ return Math.min(nums1[i], nums2[j]); } int midVal1 = (i + k / 2 - 1 &lt; nums1.length) ? nums1[i + k / 2 - 1] : Integer.MAX_VALUE; int midVal2 = (j + k / 2 - 1 &lt; nums2.length) ? nums2[j + k / 2 - 1] : Integer.MAX_VALUE; if(midVal1 &lt; midVal2){ return findKth(nums1, i + k / 2, nums2, j , k - k / 2); }else{ return findKth(nums1, i, nums2, j + k / 2 , k - k / 2); } } } 5. 最长回文子串 难度中等2333
给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。
示例 1：
输入: &#34;babad&#34; 输出: &#34;bab&#34; 注意: &#34;aba&#34; 也是一个有效答案。 示例 2：
输入: &#34;cbbd&#34; 输出: &#34;bb&#34; class Solution { public String longestPalindrome(String s) { if(s==null || s.length()==0) return &#34;&#34;; int len = s.length(); int[][] dp = new int[len][len]; int start = 0; int longest = 1; for (int i=0; i&lt;len; i++){ dp[i][i] = 1; if(i&lt;len-1){ if (s.charAt(i)==s.charAt(i+1)){ dp[i][i+1] = 1; start = i; longest = 2; } } } for(int l=3; l&lt;=len; l++){ for (int i=0; i+l-1&lt;len; i++){ int j=i+l-1; if(s.charAt(i)==s.charAt(j) &amp;&amp; dp[i+1][j-1]==1){ dp[i][j] = 1; start = i; longest = l; } } } return s.substring(start, start+longest); } } class Solution { public: string longestPalindrome(string s) { int n = s.size(); vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n)); string ans; for (int l = 0; l &lt; n; ++l) { for (int i = 0; i + l &lt; n; ++i) { int j = i + l; if (l == 0) { dp[i][j] = 1; } else if (l == 1) { dp[i][j] = (s[i] == s[j]); } else { dp[i][j] = (s[i] == s[j] &amp;&amp; dp[i + 1][j - 1]); } if (dp[i][j] &amp;&amp; l + 1 &gt; ans.size()) { ans = s.substr(i, l + 1); } } } return ans; } }; 10. 正则表达式匹配 难度困难1316
给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 '.' 和 '*' 的正则表达式匹配。
&#39;.&#39; 匹配任意单个字符 &#39;*&#39; 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。
说明:
s 可能为空，且只包含从 a-z 的小写字母。 p 可能为空，且只包含从 a-z 的小写字母，以及字符 . 和 *。 示例 1:
输入: s = &#34;aa&#34; p = &#34;a&#34; 输出: false 解释: &#34;a&#34; 无法匹配 &#34;aa&#34; 整个字符串。 示例 2:
输入: s = &#34;aa&#34; p = &#34;a*&#34; 输出: true 解释: 因为 &#39;*&#39; 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 &#39;a&#39;。因此，字符串 &#34;aa&#34; 可被视为 &#39;a&#39; 重复了一次。 示例 3:
输入: s = &#34;ab&#34; p = &#34;.*&#34; 输出: true 解释: &#34;.*&#34; 表示可匹配零个或多个（&#39;*&#39;）任意字符（&#39;.&#39;）。 示例 4:
输入: s = &#34;aab&#34; p = &#34;c*a*b&#34; 输出: true 解释: 因为 &#39;*&#39; 表示零个或多个，这里 &#39;c&#39; 为 0 个, &#39;a&#39; 被重复一次。因此可以匹配字符串 &#34;aab&#34;。 示例 5:
输入: s = &#34;mississippi&#34; p = &#34;mis*is*p*.&#34; 输出: false class Solution { public boolean isMatch(String s, String p) { int slen = s.length(); int plen = p.length(); boolean[][] dp = new boolean[slen+1][plen+1]; dp[0][0] = true; if(plen != 0) dp[0][1] = false; for(int i=0; i&lt;= slen; i++){ for(int j=1; j&lt;=plen; j++){ if(p.charAt(j-1)==&#39;*&#39;){ dp[i][j] = dp[i][j-2]; if(macthes(s, p, i, j-1)){ dp[i][j] = dp[i][j] || dp[i-1][j]; } }else{ if (macthes(s, p, i, j)){ dp[i][j] = dp[i-1][j-1]; } } } } return dp[slen][plen]; } private boolean macthes(String s, String p, int i, int j){ if(i==0) return false; if(p.charAt(j-1)==&#39;.&#39;) return true; return s.charAt(i-1)==p.charAt(j-1); } } 11. 盛最多水的容器 难度中等1548
给你 n 个非负整数 a1，a2，&hellip;，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。
**说明：**你不能倾斜容器，且 n 的值至少为 2。
图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。
示例：
输入：[1,8,6,2,5,4,8,3,7] 输出：49 class Solution { public int maxArea(int[] height) { if(height.length &lt;= 1) return 0; int i=0, j=height.length-1, re=0; while(i&lt;j){ int h = Math.min(height[i], height[j]); re = Math.max(re, h*(j-i)); if(height[i]&lt;height[j]) i++; else j--; } return re; } } 15. 三数之和 难度中等2300
给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 *a，b，c ，*使得 a + b + c = 0 ？请你找出所有满足条件且不重复的三元组。
**注意：**答案中不可以包含重复的三元组。
示例：
给定数组 nums = [-1, 0, 1, 2, -1, -4]， 满足要求的三元组集合为： [ [-1, 0, 1], [-1, -1, 2] ] class Solution { public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) { List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;&gt;(); int len = nums.length; if(len&lt;3 || nums==null) return ans; Arrays.sort(nums); for(int i=0; i&lt;len; i++){ if(nums[i]&gt;0) break; if(i&gt;0 &amp;&amp; nums[i]==nums[i-1]) continue;//去除i重复 int l=i+1; int r=len-1; while(l&lt;r){ int sum = nums[i] + nums[l] + nums[r]; if(sum==0){ ans.add(Arrays.asList(nums[i], nums[l], nums[r])); while(l&lt;r &amp;&amp; nums[l]==nums[l+1]) l++;//去除l重复 while(l&lt;r &amp;&amp; nums[r]==nums[r-1]) r--; l++; r--; }else if(sum&lt;0) l++;//左移 else if(sum&gt;0) r--;//右移 } } return ans; } } 17. 电话号码的字母组合 难度中等760
给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。
给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。
示例:
输入：&#34;23&#34; 输出：[&#34;ad&#34;, &#34;ae&#34;, &#34;af&#34;, &#34;bd&#34;, &#34;be&#34;, &#34;bf&#34;, &#34;cd&#34;, &#34;ce&#34;, &#34;cf&#34;]. 说明: 尽管上面的答案是按字典序排列的，但是你可以任意选择答案输出的顺序。
class Solution { Map&lt;String, String&gt; map = new HashMap&lt;&gt;(){{ put(&#34;2&#34;, &#34;abc&#34;); put(&#34;3&#34;, &#34;def&#34;); put(&#34;4&#34;, &#34;ghi&#34;); put(&#34;5&#34;, &#34;jkl&#34;); put(&#34;6&#34;, &#34;mno&#34;); put(&#34;7&#34;, &#34;pqrs&#34;); put(&#34;8&#34;, &#34;tuv&#34;); put(&#34;9&#34;, &#34;wxyz&#34;); }}; List&lt;String&gt; ans = new ArrayList&lt;&gt;(); public void backtrack(String combination, String next_digits){ if(next_digits.length()==0){ ans.add(combination); }else{ String digit = next_digits.substring(0, 1); String letters = map.get(digit); for (int i = 0; i &lt; letters.length(); i++) { String letter = letters.substring(i, i+1); backtrack(combination+letter, next_digits.substring(1)); } } } public List&lt;String&gt; letterCombinations(String digits) { if(digits.length() != 0){ backtrack(&#34;&#34;, digits); } return ans; } } 19. 删除链表的倒数第N个节点 难度中等861
给定一个链表，删除链表的倒数第 n 个节点，并且返回链表的头结点。
示例：
给定一个链表: 1-&gt;2-&gt;3-&gt;4-&gt;5, 和 n = 2. 当删除了倒数第二个节点后，链表变为 1-&gt;2-&gt;3-&gt;5. 说明：
给定的 n 保证是有效的。
进阶：
你能尝试使用一趟扫描实现吗？
/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode first = head; int len = 0; while(first!=null){ len++; first=first.next; } len-=n; first = dummy; while(len&gt;0){ len--; first = first.next; } first.next = first.next.next; return dummy.next; } } 一趟扫描：
/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode fast = head; ListNode slow = dummy; for(int i=0; i&lt;n; i++){ fast = fast.next; } while(fast!=null){ slow = slow.next; fast = fast.next; } slow.next = slow.next.next; return dummy.next; } } 20. 有效的括号 难度简单1627
给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断字符串是否有效。
有效字符串需满足：
左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。
示例 1:
输入: &#34;()&#34; 输出: true 示例 2:
输入: &#34;()[]{}&#34; 输出: true 示例 3:
输入: &#34;(]&#34; 输出: false 示例 4:
输入: &#34;([)]&#34; 输出: false 示例 5:
输入: &#34;{[]}&#34; 输出: true class Solution { public boolean isValid(String s) { if(s.length()==0) return true; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for(int i=0; i&lt;s.length(); i++){ if(stack.isEmpty()){ stack.push(s.charAt(i)); }else{ if(macth(stack.peek(), s.charAt(i))) stack.pop(); else stack.push(s.charAt(i)); } } return stack.isEmpty(); } private boolean macth(char left, char right){ if ((left==&#39;(&#39; &amp;&amp; right==&#39;)&#39;) ) return true; if ((left==&#39;[&#39; &amp;&amp; right==&#39;]&#39;) ) return true; if ((left==&#39;{&#39; &amp;&amp; right==&#39;}&#39;) ) return true; return false; } } 21. 合并两个有序链表 难度简单1115
将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。
示例：
输入：1-&gt;2-&gt;4, 1-&gt;3-&gt;4 输出：1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode mergeTwoLists(ListNode l1, ListNode l2) { if(l1==null) return l2; if(l2==null) return l1; if(l1.val &lt; l2.val){ l1.next = mergeTwoLists(l1.next, l2); return l1; }else{ l2.next = mergeTwoLists(l1, l2.next); return l2; } } } 22. 括号生成 难度中等1109
数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。
示例：
输入：n = 3 输出：[ &#34;((()))&#34;, &#34;(()())&#34;, &#34;(())()&#34;, &#34;()(())&#34;, &#34;()()()&#34; ] class Solution { public List&lt;String&gt; generateParenthesis(int n) { List&lt;String&gt; ans = new ArrayList&lt;&gt;(); backtrack(ans, &#34;&#34;, 0, 0, n); return ans; } public void backtrack(List&lt;String&gt; ans, String cur, int open, int close, int max){ if(open==max &amp;&amp; close==max){ ans.add(cur); return; } if(open&lt;close) return; if(open&lt;max) backtrack(ans, cur+&#34;(&#34;, open+1, close, max); if(close&lt;max) backtrack(ans, cur+&#34;)&#34;, open, close+1, max); } } 23. 合并K个排序链表 难度困难732
合并 k 个排序链表，返回合并后的排序链表。请分析和描述算法的复杂度。
示例:
输入: [ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6 ] 输出: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode mergeKLists(ListNode[] lists) { ListNode sum = new ListNode(Integer.MIN_VALUE); for(int i=0; i&lt;lists.length; i++){ sum = mergeTwo(sum, lists[i]); } return sum.next; } public ListNode mergeTwo(ListNode root1, ListNode root2){ if(root1==null) return root2; if(root2==null) return root1; if(root1.val&lt;root2.val){ root1.next = mergeTwo(root1.next, root2); return root1; }else{ root2.next = mergeTwo(root1, root2.next); return root2; } } } 6.21 update
/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode mergeKLists(ListNode[] lists) { ListNode dummy = new ListNode(Integer.MIN_VALUE); for(ListNode node : lists){ dummy = mergeTwo(dummy, node); } return dummy.next; } private ListNode mergeTwo(ListNode l1, ListNode l2){ if(l1==null) return l2; if(l2==null) return l1; if(l1.val &lt; l2.val){ l1.next = mergeTwo(l1.next, l2); return l1; }else{ l2.next = mergeTwo(l1, l2.next); return l2; } } } 31. 下一个排列 难度中等530
实现获取下一个排列的函数，算法需要将给定数字序列重新排列成字典序中下一个更大的排列。
如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。
必须** 原地 **修改，只允许使用额外常数空间。
以下是一些例子，输入位于左侧列，其相应输出位于右侧列。 1,2,3 → 1,3,2 3,2,1 → 1,2,3 1,1,5 → 1,5,1
import java.util.Arrays; //leetcode submit region begin(Prohibit modification and deletion) class Solution { public void nextPermutation(int[] nums) { for (int i = nums.length-1; i &gt;=0; i--) { for (int j = nums.length-1; j &gt; i ; j--) { if(nums[i]&lt;nums[j]){ swap(nums, i, j); Arrays.sort(nums, i+1, nums.length); return; } } } Arrays.sort(nums); } public void swap(int[] a, int i, int j){ int t = a[i]; a[i] = a[j]; a[j] = t; } } //leetcode submit region end(Prohibit modification and deletion) 32. 最长有效括号 难度困难699
给定一个只包含 '(' 和 ')' 的字符串，找出最长的包含有效括号的子串的长度。
示例 1:
输入: &#34;(()&#34; 输出: 2 解释: 最长有效括号子串为 &#34;()&#34; 示例 2:
输入: &#34;)()())&#34; 输出: 4 解释: 最长有效括号子串为 &#34;()()&#34; import java.util.Stack; class Solution { public int longestValidParentheses(String s) { if(s.length()==0 || s==null) return 0; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); stack.add(-1); int max = 0; for (int i = 0; i &lt; s.length(); i++) { if(s.charAt(i)==&#39;(&#39;){ stack.add(i); } else { stack.pop(); if(stack.isEmpty()){ stack.push(i); }else { max = max &gt; (i-stack.peek()) ? max : i - stack.peek(); } } } return max; } } x 动态规划版：
class Solution { public int longestValidParentheses(String s) { int maxans=0; int[] dp = new int[s.length()]; for(int i=1; i&lt;s.length(); i++){ if(s.charAt(i)==&#39;)&#39;){ if(s.charAt(i-1)==&#39;(&#39;){ dp[i] = (i&gt;=2 ? dp[i-2] : 0) + 2; }else if((i-dp[i-1])&gt;0 &amp;&amp; s.charAt(i-dp[i-1]-1)==&#39;(&#39;){ //dp[i-1]是指前一个符号已经匹配的数量。 dp[i] = dp[i-1] + (i-dp[i-1]&gt;=2 ? dp[i- dp[i-1] - 2] : 0) + 2; } } maxans = Math.max(maxans, dp[i]); } return maxans; } } 33. 搜索旋转排序数组 难度中等788
假设按照升序排序的数组在预先未知的某个点上进行了旋转。
( 例如，数组 [0,1,2,4,5,6,7] 可能变为 [4,5,6,7,0,1,2] )。
搜索一个给定的目标值，如果数组中存在这个目标值，则返回它的索引，否则返回 -1 。
你可以假设数组中不存在重复的元素。
你的算法时间复杂度必须是 O(log n) 级别。
示例 1:
输入: nums = [4,5,6,7,0,1,2], target = 0 输出: 4 示例 2:
输入: nums = [4,5,6,7,0,1,2], target = 3 输出: -1 class Solution { public int search(int[] nums, int target) { int lo=0, hi=nums.length-1, mid=0; while(lo &lt;= hi){ mid = lo + (hi-lo)/2; if(nums[mid]==target) return mid; if(nums[lo]&lt;= nums[mid]){ if(nums[lo]&lt;=target &amp;&amp; target&lt;=nums[mid]){ hi = mid-1; }else { lo = mid+1; } }else{ if(nums[mid]&lt;=target &amp;&amp; target &lt;= nums[hi]){ lo = mid+1; }else{ hi = mid-1; } } } return -1; } } 34. 在排序数组中查找元素的第一个和最后一个位置 难度中等466
给定一个按照升序排列的整数数组 nums，和一个目标值 target。找出给定目标值在数组中的开始位置和结束位置。
你的算法时间复杂度必须是 O(log n) 级别。
如果数组中不存在目标值，返回 [-1, -1]。
示例 1:
输入: nums = [5,7,7,8,8,10], target = 8 输出: [3,4] 示例 2:
输入: nums = [5,7,7,8,8,10], target = 6 输出: [-1,-1] class Solution { public int[] searchRange(int[] nums, int target) { int lo=0, hi=nums.length-1; while(lo &lt;= hi){ int mid = lo + (hi-lo)/2; if(nums[mid]&gt;target) hi=mid-1; if(nums[mid]&lt;target) lo=mid+1; if (nums[mid] == target){ lo = mid; hi = mid; while(hi+1&lt;nums.length &amp;&amp; nums[hi+1]==target) hi++; while (lo-1&gt;=0 &amp;&amp; nums[lo-1]==target) lo--; return new int[]{lo, hi}; } } return new int[]{-1, -1};//记住返回值的形式，怎么不创建对象返回答案。 } } 39. 组合总和 难度中等726
给定一个无重复元素的数组 candidates 和一个目标数 target ，找出 candidates 中所有可以使数字和为 target 的组合。
candidates 中的数字可以无限制重复被选取。
说明：
所有数字（包括 target）都是正整数。 解集不能包含重复的组合。 示例 1:
输入: candidates = [2,3,6,7], target = 7, 所求解集为: [ [7], [2,2,3] ] 示例 2:
输入: candidates = [2,3,5], target = 8, 所求解集为: [ [2,2,2,2], [2,3,3], [3,5] ] import java.util.ArrayList; import java.util.List; class Solution { List&lt;List&lt;Integer&gt;&gt; lists = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) { if(candidates==null || candidates.length==0 || target&lt;0) return lists; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); backtrack(0, candidates, target, list); return lists; } public void backtrack(int start, int[] candidates, int target, List&lt;Integer&gt; list){ if(target&lt; 0) return; if(target==0){ lists.add(new ArrayList&lt;&gt;(list)); }else { for (int i = start; i &lt; candidates.length; i++) { list.add(candidates[i]); backtrack(i, candidates, target-candidates[i], list); list.remove(list.size()-1); } } } } 42.接雨水 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。
上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 感谢 Marcos 贡献此图。
示例:
输入: [0,1,0,2,1,0,1,3,2,1,2,1] 输出: 6
来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/trapping-rain-water 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。
class Solution { public int trap(int[] height) { if(height==null || height.length==0) return 0; int size = height.length; int[] leftmax = new int[size]; int[] rightmax = new int[size]; int ans = 0; leftmax[0] = height[0]; for (int i = 1; i &lt; size; i++) { leftmax[i] = Math.max(height[i], leftmax[i-1]); } rightmax[size-1] = height[size-1]; for (int i = size-2; i &gt;= 0 ; i--) { rightmax[i] = Math.max(rightmax[i+1], height[i]); } for (int i = 1; i &lt; size-1; i++) { ans += Math.min(leftmax[i], rightmax[i])-height[i]; } return ans; } } 46.全排列 给定一个 没有重复 数字的序列，返回其所有可能的全排列。
示例:
输入: [1,2,3] 输出: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1] ]
来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/permutations 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。
这是一道经典的回溯法题目，请记住代码流程
class Solution { List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) { if(nums==null || nums.length&lt;1) return ans; int[] visited = new int[nums.length]; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); backtrack(nums, visited, list); return ans; } private void backtrack(int[] nums, int[] visited, List&lt;Integer&gt; list){ if(list.size()==nums.length){ ans.add(new ArrayList&lt;&gt;(list)); return; } for(int i=0; i&lt;nums.length; i++){ if(visited[i]==1) continue; visited[i] = 1; list.add(nums[i]); backtrack(nums, visited, list); visited[i] = 0; list.remove(list.size()-1); } } } 48.旋转图像 给定一个 n × n 的二维矩阵表示一个图像。
将图像顺时针旋转 90 度。
说明：
你必须在原地旋转图像，这意味着你需要直接修改输入的二维矩阵。请不要使用另一个矩阵来旋转图像。
示例 1:
给定 matrix = [ [1,2,3], [4,5,6], [7,8,9] ],
原地旋转输入矩阵，使其变为: [ [7,4,1], [8,5,2], [9,6,3] ] 示例 2:
给定 matrix = [ [ 5, 1, 9,11], [ 2, 4, 8,10], [13, 3, 6, 7], [15,14,12,16] ],
原地旋转输入矩阵，使其变为: [ [15,13, 2, 5], [14, 3, 4, 1], [12, 6, 8, 9], [16, 7,10,11] ]
来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/rotate-image 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。
先转置，再交换列
import java.util.Arrays; class Solution { public static void main(String[] args) { int[][] a = new int[][]{{1,2 ,3}, {4, 5, 6}, {7, 8, 9}}; rotate(a); System.out.println(Arrays.deepToString(a)); } public static void rotate(int[][] matrix) { int n = matrix.length; for (int i=0; i&lt;n; i++){//转置 for (int j = i; j&lt; n; j++){ int tmp = matrix[i][j]; matrix[i][j] = matrix[j][i]; matrix[j][i] = tmp; } } for (int i=0; i&lt;n; i++){//交换对称列 for (int j=0; j&lt;n/2; j++){ int tmp = matrix[i][j]; matrix[i][j] = matrix[i][n-j-1]; matrix[i][n-j-1] = tmp; } } } } 49. 字母异位词分组 难度中等346
给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。
示例:
输入: [&#34;eat&#34;, &#34;tea&#34;, &#34;tan&#34;, &#34;ate&#34;, &#34;nat&#34;, &#34;bat&#34;] 输出: [ [&#34;ate&#34;,&#34;eat&#34;,&#34;tea&#34;], [&#34;nat&#34;,&#34;tan&#34;], [&#34;bat&#34;] ] 说明：
所有输入均为小写字母。 不考虑答案输出的顺序。 使用map，将排序好字符的str作为key, 值为 list, 加入。
import java.util.*; class Solution { public List&lt;List&lt;String&gt;&gt; groupAnagrams(String[] strs) { if(strs.length == 0) { return new ArrayList&lt;&gt;(); } Map&lt;String, List&gt; map = new HashMap&lt;&gt;(); for (String s: strs){ char[] ca = s.toCharArray(); Arrays.sort(ca); String key = String.valueOf(ca); if(!map.containsKey(key)){ map.put(key, new ArrayList()); } map.get(key).add(s); } return new ArrayList(map.values()); } } 53. 最大子序和 难度简单2092
给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。
示例:
输入: [-2,1,-3,4,-1,2,1,-5,4], 输出: 6 解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。 进阶:
如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的分治法求解。
class Solution { public int maxSubArray(int[] nums) { int[] dp = new int[nums.length]; dp[0] = nums[0]; int max = dp[0]; for(int i=1; i&lt;nums.length; i++){ dp[i] = Math.max(dp[i-1]+nums[i], nums[i]); max = Math.max(max, dp[i]); } return max; } } 55. 跳跃游戏 难度中等671
给定一个非负整数数组，你最初位于数组的第一个位置。
数组中的每个元素代表你在该位置可以跳跃的最大长度。
判断你是否能够到达最后一个位置。
示例 1:
输入: [2,3,1,1,4] 输出: true 解释: 我们可以先跳 1 步，从位置 0 到达 位置 1, 然后再从位置 1 跳 3 步到达最后一个位置。 示例 2:
输入: [3,2,1,0,4] 输出: false 解释: 无论怎样，你总会到达索引为 3 的位置。但该位置的最大跳跃长度是 0 ， 所以你永远不可能到达最后一个位置。 class Solution { public boolean canJump(int[] nums) { int k=0; for (int i = 0; i &lt; nums.length; i++) { if(i&gt;k) return false;//判断能否达到当前位置 k = Math.max(k, i+nums[i]);//能够跳的最远位置 } return true; } } 56. 合并区间 难度中等432
给出一个区间的集合，请合并所有重叠的区间。
示例 1:
输入: [[1,3],[2,6],[8,10],[15,18]] 输出: [[1,6],[8,10],[15,18]] 解释: 区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2:
输入: [[1,4],[4,5]] 输出: [[1,5]] 解释: 区间 [1,4] 和 [4,5] 可被视为重叠区间。 import java.util.ArrayList; import java.util.Arrays; import java.util.List; class Solution { public int[][] merge(int[][] intervals) { int len = intervals.length; if(len &lt;=1|| intervals==null) return intervals; Arrays.sort(intervals, (v1, v2)-&gt;v1[0]-v2[0]);//注意排序方式实现，升序 List&lt;int[]&gt; res = new ArrayList&lt;&gt;(); res.add(intervals[0]); for (int i = 1; i &lt; intervals.length; i++) { int[] cur = intervals[i]; int[] last = res.get(res.size()-1); if(cur[0]&gt;last[1]) { res.add(cur); }else { last[1] = Math.max(cur[1], last[1]); } } return res.toArray(new int[res.size()][]);//注意写法 } } 62. 不同路径 难度中等568
一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。
机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。
问总共有多少条不同的路径？
例如，上图是一个7 x 3 的网格。有多少可能的路径？
示例 1:
输入: m = 3, n = 2 输出: 3 解释: 从左上角开始，总共有 3 条路径可以到达右下角。 1. 向右 -&gt; 向右 -&gt; 向下 2. 向右 -&gt; 向下 -&gt; 向右 3. 向下 -&gt; 向右 -&gt; 向右 示例 2:
输入: m = 7, n = 3 输出: 28 提示：
1 &lt;= m, n &lt;= 100 题目数据保证答案小于等于 2 * 10 ^ 9 class Solution { public int uniquePaths(int m, int n) { if(m==0 || n==0){ return 0; } int[][] dp = new int[m][n]; for(int i=0; i&lt;m; i++){ dp[i][0] = 1; } for(int i=0; i&lt;n; i++){ dp[0][i] = 1; } for(int i=1; i&lt;m; i++){ for (int j=1; j&lt;n; j++){ dp[i][j] = dp[i-1][j] + dp[i][j-1]; } } return dp[m-1][n-1]; } } 一维dp
class Solution { public int uniquePaths(int m, int n) { int[] dp = new int[n]; for(int i=0; i&lt;n; i++){ dp[i] = 1; } for(int i=1; i&lt;m; i++){ dp[0] = 1; for(int j=1; j&lt;n; j++){ dp[j] = dp[j-1]+dp[j]; } } return dp[n-1]; } } 64. 最小路径和 难度中等491
给定一个包含非负整数的 m x n 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。
**说明：**每次只能向下或者向右移动一步。
示例:
输入: [ [1,3,1], [1,5,1], [4,2,1] ] 输出: 7 解释: 因为路径 1→3→1→1→1 的总和最小。 class Solution { public int minPathSum(int[][] grid) { int m=grid.length; int n=grid[0].length; if(m&lt;=0 || n&lt;=0) return 0; int[][] dp = new int[m][n]; dp[0][0] = grid[0][0]; for(int i=1; i&lt;n; i++){ dp[0][i] = dp[0][i-1]+grid[0][i]; } for(int i=1; i&lt;m; i++){ dp[i][0] = dp[i-1][0] + grid[i][0]; } for(int i=1; i&lt;m; i++){ for (int j=1; j&lt;n; j++){ dp[i][j] = Math.min(dp[i-1][j], dp[i][j-1])+grid[i][j]; } } return dp[m-1][n-1]; } } 70. 爬楼梯 难度简单1086
假设你正在爬楼梯。需要 n 阶你才能到达楼顶。
每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？
**注意：**给定 n 是一个正整数。
示例 1：
输入： 2 输出： 2 解释： 有两种方法可以爬到楼顶。 1. 1 阶 + 1 阶 2. 2 阶 示例 2：
输入： 3 输出： 3 解释： 有三种方法可以爬到楼顶。 1. 1 阶 + 1 阶 + 1 阶 2. 1 阶 + 2 阶 3. 2 阶 + 1 阶 class Solution { public int climbStairs(int n) { if(n&lt;=1) return n; int[] dp = new int[n+1]; dp[0] = 0; dp[1] = 1; dp[2] = 2; for(int i=3; i&lt;=n; i++){ dp[i] = dp[i-1]+dp[i-2]; } return dp[n]; } } 72. 编辑距离 难度困难882
给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。
你可以对一个单词进行如下三种操作：
插入一个字符 删除一个字符 替换一个字符 示例 1：
输入：word1 = &#34;horse&#34;, word2 = &#34;ros&#34; 输出：3 解释： horse -&gt; rorse (将 &#39;h&#39; 替换为 &#39;r&#39;) rorse -&gt; rose (删除 &#39;r&#39;) rose -&gt; ros (删除 &#39;e&#39;) 示例 2：
输入：word1 = &#34;intention&#34;, word2 = &#34;execution&#34; 输出：5 解释： intention -&gt; inention (删除 &#39;t&#39;) inention -&gt; enention (将 &#39;i&#39; 替换为 &#39;e&#39;) enention -&gt; exention (将 &#39;n&#39; 替换为 &#39;x&#39;) exention -&gt; exection (将 &#39;n&#39; 替换为 &#39;c&#39;) exection -&gt; execution (插入 &#39;u&#39;) class Solution { public int minDistance(String word1, String word2) { int len1 = word1.length(); int len2 = word2.length(); int[][] dp = new int[len1+1][len2+1]; for (int i = 1; i &lt;= len1; i++) { dp[i][0] = dp[i-1][0]+1; } for (int i = 1; i &lt;= len2; i++) { dp[0][i] = dp[0][i-1]+1; } for (int i = 1; i &lt;= len1; i++) { for (int j = 1; j &lt;= len2; j++) { if(word1.charAt(i-1)==word2.charAt(j-1)) dp[i][j] = dp[i-1][j-1]; else { dp[i][j] = Math.min(dp[i-1][j-1], Math.min(dp[i][j-1], dp[i-1][j]))+1; } } } return dp[len1][len2]; } } 75. 颜色分类 难度中等433
给定一个包含红色、白色和蓝色，一共 n 个元素的数组，** 原地 **对它们进行排序，使得相同颜色的元素相邻，并按照红色、白色、蓝色顺序排列。
此题中，我们使用整数 0、 1 和 2 分别表示红色、白色和蓝色。
注意: 不能使用代码库中的排序函数来解决这道题。
示例:
输入: [2,0,2,1,1,0] 输出: [0,0,1,1,2,2] 进阶：
一个直观的解决方案是使用计数排序的两趟扫描算法。 首先，迭代计算出0、1 和 2 元素的个数，然后按照0、1、2的排序，重写当前数组。 你能想出一个仅使用常数空间的一趟扫描算法吗？ 使用三路排序，当前值是0，与left交换，是1则++，是2则与right交换。
class Solution { public void sortColors(int[] nums) { if(nums.length &lt; 2) return; int left = -1; int cur = 0; int right = nums.length-1; while (cur&lt;=right){ if(nums[cur]==0){ left++; swap(nums, left, cur); cur++; }else if (nums[cur]==1){ cur++; }else { swap(nums, right, cur); right--; } } } public void swap(int[] nums, int i1, int i2){ int tmp = nums[i1]; nums[i1] = nums[i2]; nums[i2] = tmp; } } 76. 最小覆盖子串 难度困难565
给你一个字符串 S、一个字符串 T，请在字符串 S 里面找出：包含 T 所有字符的最小子串。
示例：
输入: S = &#34;ADOBECODEBANC&#34;, T = &#34;ABC&#34; 输出: &#34;BANC&#34; 说明：
如果 S 中不存这样的子串，则返回空字符串 &quot;&quot;。 如果 S 中存在这样的子串，我们保证它是唯一的答案。 使用滑动窗口来解决
class Solution { public String minWindow(String s, String t) { if(s.length()==0 || s==null || t.length()==0 || t==null) return &#34;&#34;; int[] map = new int[128]; for(char c : t.toCharArray()){ map[c]++; } int start=0, end=0; int left=0, right=0; int matches = 0; int minLen = s.length()+1; while (right &lt; s.length()){ char rightc = s.charAt(right); map[rightc]--; if(map[rightc]&gt;=0) matches++; right++;//不要忘记！ while(matches==t.length()){ int size = right-left; if(size &lt; minLen){ minLen = size; start = left; end = right; } char leftc = s.charAt(left); map[leftc]++;//不在t中出现的字符要移出窗口 //最终达到的最大值map[leftc]==0 if(map[leftc]&gt;0) matches--; left++;//不要忘记！ } } return s.substring(start, end); } } 78. 子集 难度中等582
给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。
**说明：**解集不能包含重复的子集。
示例:
输入: nums = [1,2,3] 输出: [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] 使用回溯法
class Solution { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) { if(nums==null || nums.length==0) return res; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); process(list, nums, 0); return res; } private void process(List&lt;Integer&gt; list, int[] nums, int start){ res.add(new ArrayList(list)); for (int i=start; i&lt;nums.length; i++){ list.add(nums[i]); process(list, nums, i+1); list.remove(list.size()-1); } } } 79. 单词搜索 难度中等423
给定一个二维网格和一个单词，找出该单词是否存在于网格中。
单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。
示例:
board = [ [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;E&#39;], [&#39;S&#39;,&#39;F&#39;,&#39;C&#39;,&#39;S&#39;], [&#39;A&#39;,&#39;D&#39;,&#39;E&#39;,&#39;E&#39;] ] 给定 word = &#34;ABCCED&#34;, 返回 true 给定 word = &#34;SEE&#34;, 返回 true 给定 word = &#34;ABCB&#34;, 返回 false 提示：
board 和 word 中只包含大写和小写英文字母。 1 &lt;= board.length &lt;= 200 1 &lt;= board[i].length &lt;= 200 1 &lt;= word.length &lt;= 10^3 回溯法。
class Solution { boolean hasFind=false; char[] words; boolean[][] visited; int row, col; int[][] directions = new int[][]{{1, 0}, {0, 1}, {-1, 0}, {0, -1}}; public boolean exist(char[][] board, String word) { words = word.toCharArray(); row = board.length; col = board[0].length; visited = new boolean[row][col]; if(row*col&lt;word.length()) return false; for(int i=0; i&lt;row; i++){ for (int j=0; j&lt;col; j++){ if (board[i][j]==words[0]){ backtrack(board, words, 1, i, j); if(hasFind) return true; } } } return false; } public void backtrack(char[][] board, char[] words, int curindex, int i, int j){ if (hasFind) return; if (curindex == words.length) { hasFind = true; return; } visited[i][j] = true; for(int[] direction : directions){ int x = i+direction[0]; int y = j+direction[1]; if(validIndex(x, y) &amp;&amp; !visited[x][y] &amp;&amp; board[x][y]==words[curindex]){ backtrack(board, words, curindex+1, x, y); } } visited[i][j] = false; } public boolean validIndex(int x, int y){ return x&gt;=0 &amp;&amp; y&gt;=0 &amp;&amp; x&lt;row &amp;&amp; y&lt;col; } } 84. 柱状图中最大的矩形 难度困难715
给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。
求在该柱状图中，能够勾勒出来的矩形的最大面积。
以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 [2,1,5,6,2,3]。
图中阴影部分为所能勾勒出的最大矩形面积，其面积为 10 个单位。
示例:
输入: [2,1,5,6,2,3] 输出: 10 暴力 class Solution { public int largestRectangleArea(int[] heights) { int area=0, n = heights.length; for(int i=0; i&lt;n; i++){ int w = 1, h = heights[i], j = i; while(--j &gt;= 0 &amp;&amp; heights[j]&gt;=h){ w++; } j = i; while(++j &lt; n &amp;&amp; heights[j]&gt;=h){ w++; } area = Math.max(area, w*h); } return area; } } 单调栈 关键在于找到左右边界
单调栈 单调栈分为单调递增栈和单调递减栈
单调递增栈即栈内元素保持单调递增的栈 同理单调递减栈即栈内元素保持单调递减的栈 操作规则（下面都以单调递增栈为例） 21. 如果新的元素比栈顶元素大，就入栈 22. 如果新的元素较小，那就一直把栈内元素弹出来，直到栈顶比新元素小
加入这样一个规则之后，会有什么效果 31. 栈内的元素是递增的 32. 当元素出栈时，说明这个新元素是出栈元素向后找第一个比其小的元素
举个例子，配合下图，现在索引在 6 ，栈里是 1 5 6 。 接下来新元素是 2 ，那么 6 需要出栈。 当 6 出栈时，右边 2 代表是 6 右边第一个比 6 小的元素。
当元素出栈后，说明新栈顶元素是出栈元素向前找第一个比其小的元素 当 6 出栈时，5 成为新的栈顶，那么 5 就是 6 左边第一个比 6 小的元素。
代码模板
C++ stack st; for(int i = 0; i &lt; nums.size(); i++) { while(!st.empty() &amp;&amp; st.top() &gt; nums[i]) { st.pop(); } st.push(nums[i]); }
作者：ikaruga 链接：https://leetcode-cn.com/problems/largest-rectangle-in-histogram/solution/84-by-ikaruga/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
class Solution { public int largestRectangleArea(int[] heights) { int[] tmp = new int[heights.length+2]; System.arraycopy(heights, 0, tmp, 1, heights.length); Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); int area=0; for (int i=0; i&lt;tmp.length; i++){ while (!stack.isEmpty() &amp;&amp; tmp[i]&lt;tmp[stack.peek()]){ int h = tmp[stack.pop()]; area = Math.max(area, (i-stack.peek()-1)*h); } stack.push(i); } return area; } } 85. 最大矩形 难度困难469
给定一个仅包含 0 和 1 的二维二进制矩阵，找出只包含 1 的最大矩形，并返回其面积。
示例:
输入: [ [&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;,&#34;0&#34;], [&#34;1&#34;,&#34;0&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;], [&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;,&#34;1&#34;], [&#34;1&#34;,&#34;0&#34;,&#34;0&#34;,&#34;1&#34;,&#34;0&#34;] ] 输出: 6 每一层看作是柱状图，可以套用84题柱状图的最大面积。
第一层柱状图的高度[&ldquo;1&rdquo;,&ldquo;0&rdquo;,&ldquo;1&rdquo;,&ldquo;0&rdquo;,&ldquo;0&rdquo;]，最大面积为1；
第二层柱状图的高度[&ldquo;2&rdquo;,&ldquo;0&rdquo;,&ldquo;2&rdquo;,&ldquo;1&rdquo;,&ldquo;1&rdquo;]，最大面积为3；
第三层柱状图的高度[&ldquo;3&rdquo;,&ldquo;1&rdquo;,&ldquo;3&rdquo;,&ldquo;2&rdquo;,&ldquo;2&rdquo;]，最大面积为6；
第四层柱状图的高度[&ldquo;4&rdquo;,&ldquo;0&rdquo;,&ldquo;0&rdquo;,&ldquo;3&rdquo;,&ldquo;0&rdquo;]，最大面积为4；
class Solution { public int maximalRectangle(char[][] matrix) { if (matrix.length == 0) return 0; int maxarea = 0; int[] dp = new int[matrix[0].length]; for(int i=0; i&lt;matrix.length; i++){ for(int j=0; j&lt;matrix[0].length; j++){ dp[j] = matrix[i][j]==&#39;1&#39; ? dp[j]+1 : 0;//*********!!! } maxarea = Math.max(maxarea, largestRectangleArea(dp)); } return maxarea; } public int largestRectangleArea(int[] heights) { int[] tmp = new int[heights.length+2]; System.arraycopy(heights, 0, tmp, 1, heights.length); Deque&lt;Integer&gt; stack = new ArrayDeque&lt;&gt;(); int area=0; for(int i=0; i&lt;tmp.length; i++){ while(!stack.isEmpty() &amp;&amp; tmp[i]&lt;tmp[stack.peek()]){ int h = tmp[stack.pop()]; area = Math.max(area, (i-stack.peek()-1)*h); } stack.push(i); } return area; } } 94. 二叉树的中序遍历 难度中等514
给定一个二叉树，返回它的中序 遍历。
示例:
输入: [1,null,2,3] 1 \ 2 / 3 输出: [1,3,2] 进阶: 递归算法很简单，你可以通过迭代算法完成吗？
递归版本 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); helper(root, list); return list; } private void helper(TreeNode root, List&lt;Integer&gt; list){ if(root!=null){ if(root.left != null) helper(root.left, list); list.add(root.val); if(root.right != null) helper(root.right, list); } } } 迭代版本 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public List&lt;Integer&gt; inorderTraversal(TreeNode root) { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); while(root!=null || !stack.isEmpty()){ if(root != null){ stack.push(root); root = root.left; }else{ root = stack.pop(); list.add(root.val); root = root.right; } } return list; } } 96. 不同的二叉搜索树 难度中等527
给定一个整数 n，求以 1 &hellip; n 为节点组成的二叉搜索树有多少种？
示例:
输入: 3 输出: 5 解释: 给定 n = 3, 一共有 5 种不同结构的二叉搜索树: 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 思路 标签：动态规划 假设n个节点存在二叉排序树的个数是G(n)，令f(i)为以i为根的二叉搜索树的个数，则 G(n) = f(1) + f(2) + f(3) + f(4) + &hellip; + f(n)
G(n)=f(1)+f(2)+f(3)+f(4)+&hellip;+f(n)
当i为根节点时，其左子树节点个数为i-1个，右子树节点为n-i，则 f(i) = G(i-1)*G(n-i)
f(i)=G(i−1)∗G(n−i)
综合两个公式可以得到 卡特兰数 公式 G(n) = G(0)G(n-1)+G(1)(n-2)+&hellip;+G(n-1)*G(0)
G(n)=G(0)∗G(n−1)+G(1)∗(n−2)+&hellip;+G(n−1)∗G(0)
作者：guanpengchn 链接：https://leetcode-cn.com/problems/unique-binary-search-trees/solution/hua-jie-suan-fa-96-bu-tong-de-er-cha-sou-suo-shu-b/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
class Solution { public int numTrees(int n) { int[] dp = new int[n+1]; dp[0] = 1; dp[1] = 1; for (int i=2; i&lt;n+1; i++){ for (int j=1; j&lt;i+1; j++){ dp[i] += dp[j-1]*dp[i-j]; } } return dp[n]; } } 98. 验证二叉搜索树 难度中等596
给定一个二叉树，判断其是否是一个有效的二叉搜索树。
假设一个二叉搜索树具有如下特征：
节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 示例 1:
输入: 2 / \ 1 3 输出: true 示例 2:
输入: 5 / \ 1 4 / \ 3 6 输出: false 解释: 输入为: [5,1,4,null,null,3,6]。 根节点的值为 5 ，但是其右子节点值为 4 。 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { long pre = Long.MIN_VALUE;//按照系统输入 应该为long public boolean isValidBST(TreeNode root) { if(root==null) return true; if(!isValidBST(root.left)) return false; if(root.val &lt;= pre) return false; pre = root.val; return isValidBST(root.right); } } 101. 对称二叉树 难度简单825
给定一个二叉树，检查它是否是镜像对称的。
例如，二叉树 [1,2,2,3,4,4,3] 是对称的。
1 / \ 2 2 / \ / \ 3 4 4 3 但是下面这个 [1,2,2,null,3,null,3] 则不是镜像对称的:
1 / \ 2 2 \ \ 3 3 进阶：
你可以运用递归和迭代两种方法解决这个问题吗？
/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public boolean isSymmetric(TreeNode root) { return isMirror(root, root); } public boolean isMirror(TreeNode r1, TreeNode r2){ if(r1==null &amp;&amp; r2==null) return true; if(r1==null || r2==null) return false; return (r1.val==r2.val) &amp;&amp; isMirror(r1.left, r2.right) &amp;&amp; isMirror(r1.right, r2.left); } } 102. 二叉树的层序遍历 难度中等517
给你一个二叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。
示例： 二叉树：[3,9,20,null,null,15,7],
3 / \ 9 20 / \ 15 7 返回其层次遍历结果：
[ [3], [9,20], [15,7] ] // class Solution { public List&lt;List&lt;Integer&gt;&gt; levelOrder(TreeNode root) { List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); if(root == null){ return res; }else{ queue.add(root); } while(!queue.isEmpty()){ int n = queue.size(); List&lt;Integer&gt; level = new ArrayList&lt;&gt;(); for (int i=0;i&lt;n; i++){ TreeNode node = queue.poll(); level.add(node.val); if(node.left!=null) { queue.add(node.left); } if(node.right!=null){ queue.add(node.right); } } res.add(level); } return res; } } 104. 二叉树的最大深度 难度简单549
给定一个二叉树，找出其最大深度。
二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。
说明: 叶子节点是指没有子节点的节点。
示例： 给定二叉树 [3,9,20,null,null,15,7]，
3 / \ 9 20 / \ 15 7 返回它的最大深度 3 。
递归 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public int maxDepth(TreeNode root) { if(root==null) return 0; int leftd = maxDepth(root.left); int rightd = maxDepth(root.right); return Math.max(leftd, rightd)+1; } } 迭代 BFS /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public int maxDepth(TreeNode root) { if(root == null) return 0; Queue&lt;TreeNode&gt; queue = new ArrayDeque&lt;&gt;(); queue.add(root); int max = 0; while(!queue.isEmpty()){ max++; int n = queue.size(); for (int i=0; i&lt;n; i++){ TreeNode node = queue.poll(); if(node.left != null) queue.add(node.left); if(node.right != null) queue.add(node.right); } } return max; } } DFS 105. 从前序与中序遍历序列构造二叉树 难度中等515
根据一棵树的前序遍历与中序遍历构造二叉树。
注意: 你可以假设树中没有重复的元素。
例如，给出
前序遍历 preorder = [3,9,20,15,7] 中序遍历 inorder = [9,3,15,20,7] 返回如下的二叉树：
3 / \ 9 20 / \ 15 7 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode buildTree(int[] preorder, int[] inorder) { HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for(int i=0; i&lt;inorder.length; i++){ map.put(inorder[i], i); } return build(preorder, map, 0, preorder.length-1, 0); } public TreeNode build(int[] preorder, HashMap&lt;Integer, Integer&gt; map, int preStart, int preEnd, int inStart){ if(preEnd&lt;preStart) return null; TreeNode root = new TreeNode(preorder[preStart]); int rootIndex = map.get(root.val); int len = rootIndex-inStart; root.left = build(preorder, map, preStart+1, preStart+len, inStart); root.right = build(preorder, map, preStart+len+1, preEnd, rootIndex+1); return root; } } 114. 二叉树展开为链表 难度中等357
给定一个二叉树， 原地 将它展开为一个单链表。
例如，给定二叉树
1 / \ 2 5 / \ \ 3 4 6 将其展开为：
1 \ 2 \ 3 \ 4 \ 5 \ 6 采用后序遍历
/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public void flatten(TreeNode root) { if(root==null) return ; flatten(root.left); flatten(root.right); TreeNode tmp = root.right; root.right = root.left; root.left = null; while(root.right!=null) root=root.right; root.right=tmp; } } 121. 买卖股票的最佳时机 难度简单1028
给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。
如果你最多只允许完成一笔交易（即买入和卖出一支股票一次），设计一个算法来计算你所能获取的最大利润。
注意：你不能在买入股票前卖出股票。
示例 1:
输入: [7,1,5,3,6,4] 输出: 5 解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 示例 2:
输入: [7,6,4,3,1] 输出: 0 解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 class Solution { public int maxProfit(int[] prices) { int len = prices.length; if(len &lt;= 1) return 0; int min = prices[0], max = 0; for(int i=0; i&lt;len; i++){ max = Math.max(max, prices[i]-min); min = Math.min(min, prices[i]); } return max; } } 124. 二叉树中的最大路径和 难度困难446
给定一个非空二叉树，返回其最大路径和。
本题中，路径被定义为一条从树中任意节点出发，达到任意节点的序列。该路径至少包含一个节点，且不一定经过根节点。
示例 1:
输入: [1,2,3] 1 / \ 2 3 输出: 6 示例 2:
输入: [-10,9,20,null,null,15,7] -10 / \ 9 20 / \ 15 7 输出: 42 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ /* 求每个节点最大贡献值的贡献值 */ class Solution { int max = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) { getMax(root); return max; } public int getMax(TreeNode root){ if(root == null) return 0; int left = Math.max(0, getMax(root.left)); int right = Math.max(0, getMax(root.right)); int sum = root.val + left + right; max = Math.max(max, sum); return root.val + Math.max(left, right); } } 128. 最长连续序列 难度困难346
给定一个未排序的整数数组，找出最长连续序列的长度。
要求算法的时间复杂度为 O(n)。
示例:
输入: [100, 4, 200, 1, 3, 2] 输出: 4 解释: 最长连续序列是 [1, 2, 3, 4]。它的长度为 4。 class Solution { public int longestConsecutive(int[] nums) { if(nums==null || nums.length==0) return 0; TreeSet&lt;Integer&gt; set = new TreeSet&lt;&gt;(); for(int i=0; i&lt;nums.length; i++){ set.add(nums[i]); } int init = set.first()-1; int cnt = 0; int max = 0; for(Integer num : set){ if(num-1==init){ cnt++; }else{ max = Math.max(max, cnt); cnt=1; } init = num; } max = Math.max(cnt, max); return max; } } 136. 只出现一次的数字 难度简单1296
给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。
说明：
你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？
示例 1:
输入: [2,2,1] 输出: 1 示例 2:
输入: [4,1,2,1,2] 输出: 4 class Solution { public int singleNumber(int[] nums) { int ans = 0; for(int i=0; i&lt;nums.length; i++){ ans ^= nums[i]; } return ans; } } 139. 单词拆分 难度中等440
给定一个非空字符串 s 和一个包含非空单词列表的字典 wordDict，判定 s 是否可以被空格拆分为一个或多个在字典中出现的单词。
说明：
拆分时可以重复使用字典中的单词。 你可以假设字典中没有重复的单词。 示例 1：
输入: s = &#34;leetcode&#34;, wordDict = [&#34;leet&#34;, &#34;code&#34;] 输出: true 解释: 返回 true 因为 &#34;leetcode&#34; 可以被拆分成 &#34;leet code&#34;。 示例 2：
输入: s = &#34;applepenapple&#34;, wordDict = [&#34;apple&#34;, &#34;pen&#34;] 输出: true 解释: 返回 true 因为 &#34;applepenapple&#34; 可以被拆分成 &#34;apple pen apple&#34;。 注意你可以重复使用字典中的单词。 示例 3：
输入: s = &#34;catsandog&#34;, wordDict = [&#34;cats&#34;, &#34;dog&#34;, &#34;sand&#34;, &#34;and&#34;, &#34;cat&#34;] 输出: false class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { int n = s.length(); //参考背包问题 boolean[] dp = new boolean[n+1]; dp[0] = true; //dp[i]表示 s 中以 i - 1 结尾的字符串是否可被 wordDict 拆分 for (int i=1; i&lt;=n; i++){ for (int j=0; j&lt;i; j++){ if(dp[j] &amp;&amp; wordDict.contains(s.substring(j, i))){ dp[i] = true; break; } } } return dp[n]; } } 141. 环形链表 难度简单623
给定一个链表，判断链表中是否有环。
为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。
示例 1：
输入：head = [3,2,0,-4], pos = 1 输出：true 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：
输入：head = [1,2], pos = 0 输出：true 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：
输入：head = [1], pos = -1 输出：false 解释：链表中没有环。 进阶：
你能用 O(1)（即，常量）内存解决此问题吗？
/** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */ public class Solution { public boolean hasCycle1(ListNode head) { Set&lt;ListNode&gt; set = new HashSet&lt;&gt;(); while(head!=null){ if(set.contains(head)) return true; else set.add(head); head=head.next; } return false; } public boolean hasCycle(ListNode head) {//O(1) if(head==null || head.next==null){ return false; } ListNode fast = head.next; ListNode slow = head; while(slow != fast){ if(fast==null || fast.next==null) return false; slow = slow.next; fast = fast.next.next; } return true; } } 142. 环形链表 II 难度中等490
给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。
为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。
**说明：**不允许修改给定的链表。
示例 1：
输入：head = [3,2,0,-4], pos = 1 输出：tail connects to node index 1 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2：
输入：head = [1,2], pos = 0 输出：tail connects to node index 0 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3：
输入：head = [1], pos = -1 输出：no cycle 解释：链表中没有环。 相遇点不是环的入口！
进阶： 你是否可以不用额外空间解决此题？
/** * Definition for singly-linked list. * class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */ public class Solution { public ListNode detectCycle(ListNode head) { Set&lt;ListNode&gt; set = new HashSet&lt;&gt;(); ListNode tmp = null; while(head!=null){ if(set.contains(head)){ tmp = head; break; } set.add(head); head=head.next; } return tmp; } } //常量内存 public class Solution { public ListNode detectCycle(ListNode head) { ListNode inter = intercept(head); if(inter == null) return null; ListNode ptr1 = head; while(ptr1 != inter){ ptr1 = ptr1.next; inter = inter.next; } return ptr1; } public ListNode intercept(ListNode head){ ListNode fast, slow; slow = fast = head; while(fast!=null &amp;&amp; fast.next!=null){ slow = slow.next; fast = fast.next.next; if(slow == fast){ return fast; } } return null; } } 146. LRU缓存机制 难度中等650
运用你所掌握的数据结构，设计和实现一个 LRU (最近最少使用) 缓存机制 。它应该支持以下操作： 获取数据 get 和 写入数据 put 。
获取数据 get(key) - 如果关键字 (key) 存在于缓存中，则获取关键字的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字/值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。
进阶:
你是否可以在 O(1) 时间复杂度内完成这两种操作？
class LRUCache { private int cap; private Map&lt;Integer, Integer&gt; map = new LinkedHashMap&lt;&gt;(); public LRUCache(int capacity) { this.cap = capacity; } public int get(int key) { if(map.keySet().contains(key)){ int value = map.get(key); map.remove(key); map.put(key, value); return value; } return -1; } public void put(int key, int value) { if(map.keySet().contains(key)){ map.remove(key); }else if(map.size()==cap){ Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; iter = map.entrySet().iterator(); iter.next(); iter.remove(); } map.put(key, value); } } /** * Your LRUCache object will be instantiated and called as such: * LRUCache obj = new LRUCache(capacity); * int param_1 = obj.get(key); * obj.put(key,value); */ 148. 排序链表 难度中等560
在 O(n log n) 时间复杂度和常数级空间复杂度下，对链表进行排序。
示例 1:
输入: 4-&gt;2-&gt;1-&gt;3 输出: 1-&gt;2-&gt;3-&gt;4 示例 2:
输入: -1-&gt;5-&gt;3-&gt;4-&gt;0 输出: -1-&gt;0-&gt;3-&gt;4-&gt;5 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode sortList(ListNode head) { if(head == null) return null; if(head.next == null) return head; ListNode p1= head; ListNode p2= head; while(p2.next!=null &amp;&amp; p2.next.next!=null){ p1 = p1.next; p2 = p2.next.next; } ListNode tail = p1; p1 = p1.next; tail.next = null; ListNode l = sortList(head); ListNode r = sortList(p1); return merge(l, r); } public ListNode merge(ListNode left, ListNode right){ ListNode pre = new ListNode(0); ListNode cur = pre; while(left!=null &amp;&amp; right!=null){ if(left.val &lt;= right.val){ cur.next = left; cur = cur.next; left = left.next; }else{ cur.next = right; cur = cur.next; right = right.next; } } if(left != null) cur.next = left; if(right != null) cur.next = right; return pre.next; } } 152. 乘积最大子数组 难度中等602
给你一个整数数组 nums ，请你找出数组中乘积最大的连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。
示例 1:
输入: [2,3,-2,4] 输出: 6 解释: 子数组 [2,3] 有最大乘积 6。 示例 2:
输入: [-2,0,-1] 输出: 0 解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 思路 标签：动态规划 遍历数组时计算当前最大值，不断更新 令imax为当前最大值，则当前最大值为 imax = max(imax * nums[i], nums[i]) 由于存在负数，那么会导致最大的变最小的，最小的变最大的。因此还需要维护当前最小值imin，imin = min(imin * nums[i], nums[i]) 当负数出现时则imax与imin进行交换再进行下一步计算 时间复杂度：O(n)O(n)
作者：guanpengchn 链接：https://leetcode-cn.com/problems/maximum-product-subarray/solution/hua-jie-suan-fa-152-cheng-ji-zui-da-zi-xu-lie-by-g/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
class Solution { public int maxProduct(int[] nums) { int max=Integer.MIN_VALUE, imax=1, imin=1; for(int i=0; i&lt;nums.length; i++){ if(nums[i]&lt;0){ int tmp = imax; imax = imin; imin = tmp; } imax = Math.max(imax*nums[i], nums[i]); imin = Math.min(imin*nums[i], nums[i]); max = Math.max(max, imax); } return max; } } 155. 最小栈 难度简单547
设计一个支持 push ，pop ，top 操作，并能在常数时间内检索到最小元素的栈。
push(x) —— 将元素 x 推入栈中。 pop() —— 删除栈顶的元素。 top() —— 获取栈顶元素。 getMin() —— 检索栈中的最小元素。 示例:
输入： [&#34;MinStack&#34;,&#34;push&#34;,&#34;push&#34;,&#34;push&#34;,&#34;getMin&#34;,&#34;pop&#34;,&#34;top&#34;,&#34;getMin&#34;] [[],[-2],[0],[-3],[],[],[],[]] 输出： [null,null,null,null,-3,null,0,-2] 解释： MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --&gt; 返回 -3. minStack.pop(); minStack.top(); --&gt; 返回 0. minStack.getMin(); --&gt; 返回 -2. 提示：
pop、top 和 getMin 操作总是在 非空栈 上调用。 思路：元素入栈两次（最小，两次都是入栈值，否则再入栈顶值），弹出两次。
class MinStack { /** initialize your data structure here. */ private Stack&lt;Integer&gt; stack; public MinStack() { stack = new Stack&lt;&gt;(); } public void push(int x) { if(stack.isEmpty()){ stack.push(x); stack.push(x); }else{ int tmp = stack.peek(); stack.push(x); if(tmp&lt;x){ stack.push(tmp); }else{ stack.push(x); } } } public void pop() { stack.pop(); stack.pop(); } public int top() { return stack.get(stack.size()-2); } public int getMin() { return stack.peek(); } } /** * Your MinStack object will be instantiated and called as such: * MinStack obj = new MinStack(); * obj.push(x); * obj.pop(); * int param_3 = obj.top(); * int param_4 = obj.getMin(); */ 160. 相交链表 难度简单674
编写一个程序，找到两个单链表相交的起始节点。
如下面的两个链表**：**
在节点 c1 开始相交。
示例 1：
输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,0,1,8,4,5], skipA = 2, skipB = 3 输出：Reference of the node with value = 8 输入解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,0,1,8,4,5]。在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2：
输入：intersectVal = 2, listA = [0,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1 输出：Reference of the node with value = 2 输入解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。从各自的表头开始算起，链表 A 为 [0,9,1,2,4]，链表 B 为 [3,2,4]。在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3：
输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null 输入解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 解释：这两个链表不相交，因此返回 null。 注意：
如果两个链表没有交点，返回 null. 在返回结果后，两个链表仍须保持原有的结构。 可假定整个链表结构中没有循环。 程序尽量满足 O(n) 时间复杂度，且仅用 O(1) 内存。 /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { * val = x; * next = null; * } * } */ public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode pa = headA; ListNode pb = headB; while(pa != pb){ pa = pa==null?headB:pa.next; pb = pb==null?headA:pb.next; } return pa; } } 169. 多数元素 难度简单616
给定一个大小为 n 的数组，找到其中的多数元素。多数元素是指在数组中出现次数大于 ⌊ n/2 ⌋ 的元素。
你可以假设数组是非空的，并且给定的数组总是存在多数元素。
示例 1:
输入: [3,2,3] 输出: 3 示例 2:
输入: [2,2,1,1,1,2,2] 输出: 2 class Solution { public int majorityElement(int[] nums) { Arrays.sort(nums); return nums[nums.length/2]; } } 198. 打家劫舍 难度简单868
你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。
给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。
示例 1：
输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2：
输入：[2,7,9,3,1] 输出：12 解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 提示：
0 &lt;= nums.length &lt;= 100 0 &lt;= nums[i] &lt;= 400 class Solution { public int rob(int[] nums) { if(nums.length==0) return 0; int[] dp = new int[nums.length]; dp[0] = nums[0]; if(nums.length&lt;2) return nums[0]; dp[1] = nums[0]&gt;nums[1]? nums[0]:nums[1]; for(int i=2; i&lt;nums.length; i++){ dp[i] = (nums[i]+dp[i-2]) &gt; dp[i-1] ? nums[i]+dp[i-2] : dp[i-1]; } return dp[nums.length-1]; } } 200. 岛屿数量 难度中等586
给你一个由 '1'（陆地）和 '0'（水）组成的的二维网格，请你计算网格中岛屿的数量。
岛屿总是被水包围，并且每座岛屿只能由水平方向或竖直方向上相邻的陆地连接形成。
此外，你可以假设该网格的四条边均被水包围。
示例 1:
输入: 11110 11010 11000 00000 输出: 1 示例 2:
输入: 11000 11000 00100 00011 输出: 3 解释: 每座岛屿只能由水平和/或竖直方向上相邻的陆地连接而成。 思路：遍历岛这个二维数组，如果当前数为1，则进入感染函数并将岛个数+1 感染函数：其实就是一个递归标注的过程，它会将所有相连的1都标注成2。为什么要标注？这样就避免了遍历过程中的重复计数的情况，一个岛所有的1都变成了2后，遍历的时候就不会重复遍历了。建议没想明白的同学画个图看看。 class Solution { public int numIslands(char[][] grid) { int num = 0; for(int i=0; i&lt;grid.length; i++){ for (int j=0; j&lt;grid[0].length; j++){ if(grid[i][j]==&#39;1&#39;){ infect(grid, i, j); num++; } } } return num; } public void infect(char[][] grid, int i, int j){ if(i&lt;0 || i&gt;=grid.length || j&lt;0 || j&gt;=grid[0].length || grid[i][j]!=&#39;1&#39;){ return ; } grid[i][j] = &#39;2&#39;; infect(grid, i-1, j); infect(grid, i+1, j); infect(grid, i, j-1); infect(grid, i, j+1); } } 206. 反转链表 难度简单987
反转一个单链表。
示例:
输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL 输出: 5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 进阶: 你可以迭代或递归地反转链表。你能否用两种方法解决这道题？
/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } */ class Solution { public ListNode reverseList(ListNode head) { if(head==null || head.next==null) return head; ListNode newList = reverse(head.next); head.next.next = head; head.next = null; return newList; } } class Solution { public ListNode reverseList(ListNode head) { ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode nextNode = cur.next; cur.next = pre; pre = cur; cur = nextNode; } return pre; } } 207. 课程表 难度中等351
你这个学期必须选修 numCourse 门课程，记为 0 到 numCourse-1 。
在选修某些课程之前需要一些先修课程。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示他们：[0,1]
给定课程总量以及它们的先决条件，请你判断是否可能完成所有课程的学习？
示例 1:
输入: 2, [[1,0]] 输出: true 解释: 总共有 2 门课程。学习课程 1 之前，你需要完成课程 0。所以这是可能的。 示例 2:
输入: 2, [[1,0],[0,1]] 输出: false 解释: 总共有 2 门课程。学习课程 1 之前，你需要先完成课程 0；并且学习课程 0 之前，你还应先完成课程 1。这是不可能的。 提示：
输入的先决条件是由 边缘列表 表示的图形，而不是 邻接矩阵 。详情请参见 图的表示法 。 你可以假定输入的先决条件中没有重复的边。 1 &lt;= numCourses &lt;= 10^5 class Solution { public boolean canFinish(int numCourses, int[][] prerequisites) { List&lt;List&lt;Integer&gt;&gt; list = new ArrayList&lt;&gt;(); for(int i = 0; i &lt; numCourses; i++){ list.add(new ArrayList&lt;&gt;()); } for(int[] pair : prerequisites){ list.get(pair[0]).add(pair[1]); } int[] flags = new int[numCourses]; for(int i = 0; i &lt; numCourses; i++){ if(!dfs(list, flags, i)) return false; } return true; } private boolean dfs(List&lt;List&lt;Integer&gt;&gt; list, int[] flags, int i){ if(flags[i] == 1) return false; if(flags[i] == -1) return true; flags[i] = 1; for(int j : list.get(i)){ if(!dfs(list, flags, j)) return false; } flags[i] = -1; return true; } } 作者：Utopiahiker 链接：https://leetcode-cn.com/problems/course-schedule/solution/li-yong-huan-de-xing-zhi-by-utopiahiker/ 来源：力扣（LeetCode） 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 208. 实现 Trie (前缀树) 难度中等320
实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。
示例:
Trie trie = new Trie(); trie.insert(&#34;apple&#34;); trie.search(&#34;apple&#34;); // 返回 true trie.search(&#34;app&#34;); // 返回 false trie.startsWith(&#34;app&#34;); // 返回 true trie.insert(&#34;app&#34;); trie.search(&#34;app&#34;); // 返回 true 说明:
你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。 class Trie { private List&lt;String&gt; words; /** Initialize your data structure here. */ public Trie() { words = new ArrayList&lt;&gt;(); } /** Inserts a word into the trie. */ public void insert(String word) { words.add(word); } /** Returns if the word is in the trie. */ public boolean search(String word) { if(words.contains(word)) return true; return false; } /** Returns if there is any word in the trie that starts with the given prefix. */ public boolean startsWith(String prefix) { for (int i=0; i&lt;words.size(); i++){ if (words.get(i).startsWith(prefix)){ return true; } } return false; } } /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */ 215. 数组中的第K个最大元素 难度中等517
在未排序的数组中找到第 k 个最大的元素。请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。
示例 1:
输入: [3,2,1,5,6,4] 和 k = 2 输出: 5 示例 2:
输入: [3,2,3,1,2,4,5,5,6] 和 k = 4 输出: 4 说明:
你可以假设 k 总是有效的，且 1 ≤ k ≤ 数组的长度。
//时间复杂度 NlogK class Solution { public int findKthLargest(int[] nums, int k) { PriorityQueue&lt;Integer&gt; heap = new PriorityQueue&lt;&gt;((n1, n2)-&gt;n1-n2); for(int n : nums){ heap.add(n); if(heap.size()&gt;k) heap.poll(); } return heap.poll(); } } //时间复杂度 nlogn 221. 最大正方形 难度中等450
在一个由 0 和 1 组成的二维矩阵内，找到只包含 1 的最大正方形，并返回其面积。
示例:
输入: 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 输出: 4 class Solution { public int maximalSquare(char[][] matrix) { if(matrix.length &lt; 1 || matrix[0].length &lt; 1) return 0; int rows = matrix.length; int cols = matrix[0].length; int[][] dp = new int[rows+1][cols+1]; int max = 0; for(int i=1; i&lt;=rows; i++){ for (int j=1; j &lt;=cols; j++){ if(matrix[i-1][j-1]==&#39;1&#39;){ dp[i][j] = Math.min(Math.min(dp[i-1][j], dp[i][j-1]), dp[i-1][j-1])+1; max = Math.max(max, dp[i][j]); } } } return max*max; } } 226. 翻转二叉树 翻转一棵二叉树。
示例：
输入：
4 / \ 2 7 / \ / \ 1 3 6 9 输出：
4 / \ 7 2 / \ / \ 9 6 3 1 难度简单465
翻转一棵二叉树。
示例：
输入：
4 / \ 2 7 / \ / \ 1 3 6 9 输出：
4 / \ 7 2 / \ / \ 9 6 3 1 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode invertTree(TreeNode root) { if(root==null) return root; TreeNode left = invertTree(root.left); TreeNode right = invertTree(root.right); root.left = right; root.right = left; return root; } } 234. 回文链表 难度简单522
请判断一个链表是否为回文链表。
示例 1:
输入: 1-&gt;2 输出: false 示例 2:
输入: 1-&gt;2-&gt;2-&gt;1 输出: true 进阶： 你能否用 O(n) 时间复杂度和 O(1) 空间复杂度解决此题？
class Solution { public boolean isPalindrome(ListNode head) { // 要实现 O(n) 的时间复杂度和 O(1) 的空间复杂度，需要翻转后半部分 if (head == null || head.next == null) { return true; } ListNode fast = head; ListNode slow = head; // 根据快慢指针，找到链表的中点 while(fast.next != null &amp;&amp; fast.next.next != null) { fast = fast.next.next; slow = slow.next; } slow = reverse(slow.next); while(slow != null) { if (head.val != slow.val) { return false; } head = head.next; slow = slow.next; } return true; } private ListNode reverse(ListNode head){ // 递归到最后一个节点，返回新的新的头结点 if (head.next == null) { return head; } ListNode newHead = reverse(head.next); head.next.next = head; head.next = null; return newHead; } } 236. 二叉树的最近公共祖先 难度中等598
给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。
百度百科 中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。”
例如，给定如下二叉树: root = [3,5,1,6,2,0,8,null,null,7,4]
示例 1:
输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出: 3 解释: 节点 5 和节点 1 的最近公共祖先是节点 3。 示例 2:
输入: root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 输出: 5 解释: 节点 5 和节点 4 的最近公共祖先是节点 5。因为根据定义最近公共祖先节点可以为节点本身。 说明:
所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉树中。 注意p,q必然存在树内, 且所有节点的值唯一!!! 递归思想, 对以root为根的(子)树进行查找p和q, 如果root == null || p || q 直接返回root 表示对于当前树的查找已经完毕, 否则对左右子树进行查找, 根据左右子树的返回值判断: 1. 左右子树的返回值都不为null, 由于值唯一左右子树的返回值就是p和q, 此时root为LCA 2. 如果左右子树返回值只有一个不为null, 说明只有p和q存在于左或右子树中, 最先找到的那个节点为LCA 3. 左右子树返回值均为null, p和q均不在树中, 返回null /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root==null || root==p || root==q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left == null &amp;&amp; right==null) return null; else if(left!=null &amp;&amp; right!=null) return root; else return left == null ? right : left; } } 238. 除自身以外数组的乘积 难度中等486
给你一个长度为 n 的整数数组 nums，其中 n &gt; 1，返回输出数组 output ，其中 output[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。
示例:
输入: [1,2,3,4] 输出: [24,12,8,6] **提示：**题目数据保证数组之中任意元素的全部前缀元素和后缀（甚至是整个数组）的乘积都在 32 位整数范围内。
说明: 请**不要使用除法，**且在 O(n) 时间复杂度内完成此题。
进阶： 你可以在常数空间复杂度内完成这个题目吗？（ 出于对空间复杂度分析的目的，输出数组不被视为额外空间。）
class Solution { public int[] productExceptSelf(int[] nums) { int left=1, right=1; int[] res = new int[nums.length]; Arrays.fill(res, 1); int n = nums.length; for(int i=0; i&lt;n; i++){ res[i] *= left; left*=nums[i]; res[n-i-1] *= right; right*=nums[n-i-1]; } return res; } } 239. 滑动窗口最大值 难度困难398
给定一个数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。
返回滑动窗口中的最大值。
进阶：
你能在线性时间复杂度内解决此题吗？
示例:
输入: nums = [1,3,-1,-3,5,3,6,7], 和 k = 3 输出: [3,3,5,5,6,7] 解释: 滑动窗口的位置 最大值 --------------- ----- [1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 提示：
1 &lt;= nums.length &lt;= 10^5 -10^4 &lt;= nums[i] &lt;= 10^4 1 &lt;= k &lt;= nums.length class Solution { public int[] maxSlidingWindow(int[] nums, int k) { if(nums==null || nums.length&lt;2) return nums; int len = nums.length; LinkedList&lt;Integer&gt; queue = new LinkedList&lt;&gt;(); int[] res = new int[len-k+1]; for (int i = 0; i &lt; len; i++) { while (!queue.isEmpty() &amp;&amp; nums[queue.peekLast()]&lt;=nums[i]){ queue.pollLast(); } queue.addLast(i); if(queue.peek()&lt;=i-k){ queue.poll(); } if(i+1&gt;=k){ res[i+1-k] = nums[queue.peek()]; } } return res; } } 240. 搜索二维矩阵 II 难度中等327
编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target。该矩阵具有以下特性：
每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 示例:
现有矩阵 matrix 如下：
[ [1, 4, 7, 11, 15], [2, 5, 8, 12, 19], [3, 6, 9, 16, 22], [10, 13, 14, 17, 24], [18, 21, 23, 26, 30] ] 给定 target = 5，返回 true。
给定 target = 20，返回 false。
class Solution { public boolean searchMatrix(int[][] matrix, int target) { if(matrix==null || matrix.length==0) return false; int i=0, j=matrix[0].length-1; while(i&lt;matrix.length &amp;&amp; j&gt;=0){ if(matrix[i][j]&gt;target){ j--; }else if(matrix[i][j]&lt;target){ i++; }else{return true;} } return false; } } 279. 完全平方数 难度中等452
给定正整数 n，找到若干个完全平方数（比如 1, 4, 9, 16, ...）使得它们的和等于 n。你需要让组成和的完全平方数的个数最少。
示例 1:
输入: n = 12 输出: 3 解释: 12 = 4 + 4 + 4. 示例 2:
输入: n = 13 输出: 2 解释: 13 = 4 + 9. /*这里使用动态规划来做。时间复杂度O(nlogn)，空间复杂度O(n)。代码非常精简 定义一个函数f(n)表示我们要求的解。f(n)的求解过程为： f(n) = 1 + min{ f(n-1^2), f(n-2^2), f(n-3^2), f(n-4^2), ... , f(n-k^2) //(k为满足k^2&lt;=n的最大的k) } */ class Solution { public int numSquares(int n) { int[] dp = new int[n+1]; dp[0] = 0; for(int i=1; i&lt;=n; i++){ int min = Integer.MAX_VALUE; for (int j=1; j*j&lt;=i; j++){ min = Math.min(min, dp[i-j*j]); } dp[i] = min+1; } return dp[n]; } } 399. 除法求值 难度中等163
给出方程式 A / B = k, 其中 A 和 B 均为用字符串表示的变量， k 是一个浮点型数字。根据已知方程式求解问题，并返回计算结果。如果结果不存在，则返回 -1.0。
示例 : 给定 a / b = 2.0, b / c = 3.0 问题: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? 返回 [6.0, 0.5, -1.0, 1.0, -1.0 ]
输入为: vector&lt;pair&lt;string, string&gt;&gt; equations, vector&lt;double&gt;&amp; values, vector&lt;pair&lt;string, string&gt;&gt; queries(方程式，方程式结果，问题方程式)， 其中 equations.size() == values.size()，即方程式的长度与方程式结果长度相等（程式与结果一一对应），并且结果值均为正数。以上为方程式的描述。 返回vector&lt;double&gt;类型。
基于上述例子，输入如下：
equations(方程式) = [ [&#34;a&#34;, &#34;b&#34;], [&#34;b&#34;, &#34;c&#34;] ], values(方程式结果) = [2.0, 3.0], queries(问题方程式) = [ [&#34;a&#34;, &#34;c&#34;], [&#34;b&#34;, &#34;a&#34;], [&#34;a&#34;, &#34;e&#34;], [&#34;a&#34;, &#34;a&#34;], [&#34;x&#34;, &#34;x&#34;] ]. 输入总是有效的。你可以假设除法运算中不会出现除数为0的情况，且不存在任何矛盾的结果。
283. 移动零 难度简单612
给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。
示例:
输入: [0,1,0,3,12] 输出: [1,3,12,0,0] 说明:
必须在原数组上操作，不能拷贝额外的数组。 尽量减少操作次数。 class Solution { public void moveZeroes(int[] nums) { if(nums==null || nums.length==0) return; int index = 0; for(int i=0; i&lt;nums.length; i++){ if(nums[i]!=0){ nums[index] = nums[i]; index++; } } for (int i=index; i&lt;nums.length; i++){ nums[i] = 0; } } } 287. 寻找重复数 难度中等714
给定一个包含 n + 1 个整数的数组 nums，其数字都在 1 到 n 之间（包括 1 和 n），可知至少存在一个重复的整数。假设只有一个重复的整数，找出这个重复的数。
示例 1:
输入: [1,3,4,2,2] 输出: 2 示例 2:
输入: [3,1,3,4,2] 输出: 3 说明：
不能更改原数组（假设数组是只读的）。 只能使用额外的 O(1) 的空间。 时间复杂度小于 O(n2) 。 数组中只有一个重复的数字，但它可能不止重复出现一次。 思路：
快慢指针思想, fast 和 slow 是指针, nums[slow] 表示取指针对应的元素 注意 nums 数组中的数字都是在 1 到 n 之间的(在数组中进行游走不会越界), 因为有重复数字的出现, 所以这个游走必然是成环的, 环的入口就是重复的元素, 即按照寻找链表环入口的思路来做 class Solution { public int findDuplicate(int[] nums) { int fast=0, slow=0; while(true){ fast = nums[nums[fast]]; slow = nums[slow]; if(fast == slow){ fast = 0; while(nums[slow] != nums[fast]){ fast = nums[fast]; slow = nums[slow]; } return nums[slow]; } } } } 297. 二叉树的序列化与反序列化 难度困难208
序列化是将一个数据结构或者对象转换为连续的比特位的操作，进而可以将转换后的数据存储在一个文件或者内存中，同时也可以通过网络传输到另一个计算机环境，采取相反方式重构得到原数据。
请设计一个算法来实现二叉树的序列化与反序列化。这里不限定你的序列 / 反序列化算法执行逻辑，你只需要保证一个二叉树可以被序列化为一个字符串并且将这个字符串反序列化为原始的树结构。
示例:
你可以将以下二叉树： 1 / \ 2 3 / \ 4 5 序列化为 &#34;[1,2,3,null,null,4,5]&#34; 提示: 这与 LeetCode 目前使用的方式一致，详情请参阅 LeetCode 序列化二叉树的格式 。你并非必须采取这种方式，你也可以采用其他的方法解决这个问题。
说明: 不要使用类的成员 / 全局 / 静态变量来存储状态，你的序列化和反序列化算法应该是无状态的。
public class Codec { public String rserialize(TreeNode root, String str) { if (root == null) { str += &#34;None,&#34;; } else { str += str.valueOf(root.val) + &#34;,&#34;; str = rserialize(root.left, str); str = rserialize(root.right, str); } return str; } public String serialize(TreeNode root) { return rserialize(root, &#34;&#34;); } public TreeNode rdeserialize(List&lt;String&gt; l) { if (l.get(0).equals(&#34;None&#34;)) { l.remove(0); return null; } TreeNode root = new TreeNode(Integer.valueOf(l.get(0))); l.remove(0); root.left = rdeserialize(l); root.right = rdeserialize(l); return root; } public TreeNode deserialize(String data) { String[] data_array = data.split(&#34;,&#34;); List&lt;String&gt; data_list = new LinkedList&lt;String&gt;(Arrays.asList(data_array)); return rdeserialize(data_list); } }; 300. 最长上升子序列 难度中等757收藏分享切换为英文关注反馈
给定一个无序的整数数组，找到其中最长上升子序列的长度。
示例:
输入: [10,9,2,5,3,7,101,18] 输出: 4 解释: 最长的上升子序列是 [2,3,7,101]，它的长度是 4。 说明:
可能会有多种最长上升子序列的组合，你只需要输出对应的长度即可。 你算法的时间复杂度应该为 O(n2) 。 进阶: 你能将算法的时间复杂度降低到 O(n log n) 吗?
class Solution { public int lengthOfLIS(int[] nums) { int len = nums.length; if(len&lt;2) return len; int[] dp = new int[len]; Arrays.fill(dp, 1); for(int i=1; i&lt;len; i++){ for(int j=0; j&lt;i; j++){ if(nums[j]&lt;nums[i]) dp[i] = Math.max(dp[i], dp[j]+1); } } int re = dp[0]; for(int e : dp){ re = Math.max(re, e); } return re; } } 301. 删除无效的括号 难度困难186
删除最小数量的无效括号，使得输入的字符串有效，返回所有可能的结果。
说明: 输入可能包含了除 ( 和 ) 以外的字符。
示例 1:
输入: &#34;()())()&#34; 输出: [&#34;()()()&#34;, &#34;(())()&#34;] 示例 2:
输入: &#34;(a)())()&#34; 输出: [&#34;(a)()()&#34;, &#34;(a())()&#34;] 示例 3:
输入: &#34;)(&#34; 输出: [&#34;&#34;] class Solution { public List&lt;String&gt; removeInvalidParentheses(String s) { List&lt;String&gt; res = new ArrayList&lt;&gt;(); if(s.equals(&#34;()&#34;) || s.equals(&#34;&#34;)){ res.add(s); return res; } Deque&lt;String&gt; queue = new LinkedList&lt;&gt;(); queue.offer(s); Set&lt;String&gt; set = new HashSet&lt;&gt;(); boolean isFound = false; while(!queue.isEmpty()){ String cur = queue.poll(); if(isValid(cur)){ res.add(cur); isFound = true; } if(isFound){ continue; } for(int i=0; i&lt;cur.length(); i++){ if(cur.charAt(i)==&#39;(&#39; || cur.charAt(i)==&#39;)&#39;){ String str; if(i==cur.length()-1){ str = cur.substring(0, cur.length()-1); }else{ str = cur.substring(0, i)+cur.substring(i+1); } if(set.add(str)){ queue.offer(str); } } } } if(res.isEmpty()) { res.add(&#34;&#34;); } return res; } public boolean isValid(String s){ int left = 0; for(int i=0; i&lt;s.length(); i++){ char cur = s.charAt(i); if(cur==&#39;(&#39;){ left++; }else if(cur==&#39;)&#39;){ if(left != 0){ left--; }else{ return false; } } } return left == 0; } } 309. 最佳买卖股票时机含冷冻期 难度中等342
给定一个整数数组，其中第 i 个元素代表了第 i 天的股票价格 。
设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）:
你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 示例:
输入: [1,2,3,0,2] 输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 操作：买股票、卖股票、持有（什么都不做）
状态：
第i天手中没股票 第i天手中有股票 dp[n][0] ----------第n天手中没有股票最大利润 dp[n][1] ----------第n天手中持有股票最大利润 dp[n][0]: maxProfit (a) n-1天没有股票，n天啥都不做 --- dp[n-1][0] (b) n-1持有股票，n天卖出了 --- dp[n-1][1]+price[n] dp[n][1]: maxProfitHold (a) n-1天持有股票，n天啥都不做 --- dp[n-1][1] (b) n-2天卖出了，n-1天冷冻期，n天买入了 --- dp[n-2][0]-price[n] 初始值： dp[n-1][0] = 0; oneDaybefore dp[n-2][0] = 0; twoDayBefore dp[n-1][1] = minValue; oneDayBeforeHold; class Solution { public int maxProfit(int[] prices) { if(prices==null || prices.length == 0) return 0; int oneDayBefore = 0;//dp[n-1][0]=0 int twoDayBefore = 0;//dp[n-2][0]=0 //dp[n-1][1] = min int oneDayBeforeHold = Integer.MIN_VALUE; int maxProfit = 0;//dp[n][0] int maxProfitHold = 0;//dp[n][1] for (int i=0; i&lt;prices.length; i++){ maxProfit = Math.max(oneDayBefore, oneDayBeforeHold + prices[i]); maxProfitHold = Math.max(oneDayBeforeHold, twoDayBefore - prices[i]); twoDayBefore = oneDayBefore; oneDayBefore = maxProfit; oneDayBeforeHold = maxProfitHold; } return maxProfit; } } 312. 戳气球 难度困难313
有 n 个气球，编号为0 到 n-1，每个气球上都标有一个数字，这些数字存在数组 nums 中。
现在要求你戳破所有的气球。每当你戳破一个气球 i 时，你可以获得 nums[left] * nums[i] * nums[right] 个硬币。 这里的 left 和 right 代表和 i 相邻的两个气球的序号。注意当你戳破了气球 i 后，气球 left 和气球 right 就变成了相邻的气球。
求所能获得硬币的最大数量。
说明:
你可以假设 nums[-1] = nums[n] = 1，但注意它们不是真实存在的所以并不能被戳破。 0 ≤ n ≤ 500, 0 ≤ nums[i] ≤ 100 示例:
输入: [3,1,5,8] 输出: 167 解释: nums = [3,1,5,8] --&gt; [3,5,8] --&gt; [3,8] --&gt; [8] --&gt; [] coins = 3*1*5 + 3*5*8 + 1*3*8 + 1*8*1 = 167 class Solution { public int maxCoins(int[] nums) { int[] newNums = new int[nums.length+2]; newNums[0] = 1; newNums[newNums.length-1] = 1; for(int i=0; i&lt;nums.length; i++){ newNums[i+1] = nums[i]; } int n = newNums.length; int[][] dp = new int[n][n]; for(int i=n-2; i&gt;=0; i--){ for (int j=i+2; j&lt;n; j++){ for(int k=i+1; k &lt; j; k++){ dp[i][j] = Math.max(dp[i][j], dp[i][k]+dp[k][j]+newNums[i]*newNums[k]*newNums[j]); } } } return dp[0][n-1]; } } 322. 零钱兑换 难度中等642
给定不同面额的硬币 coins 和一个总金额 amount。编写一个函数来计算可以凑成总金额所需的最少的硬币个数。如果没有任何一种硬币组合能组成总金额，返回 -1。
示例 1:
输入: coins = [1, 2, 5], amount = 11 输出: 3 解释: 11 = 5 + 5 + 1 示例 2:
输入: coins = [2], amount = 3 输出: -1 说明: 你可以认为每种硬币的数量是无限的。
dp[i]=x 表示钱为i时需要的最少金币数
dp[i] = min(dp[i], dp[i-c]+1); class Solution { public int coinChange(int[] coins, int amount) { if(coins==null || coins.length&lt;1) return -1; int[] dp = new int[amount+1]; Arrays.fill(dp, amount+1); dp[0] = 0; for(int i=1; i&lt;= amount; i++){ for(int c : coins){ if (i-c&lt;0) continue; dp[i] = Math.min(dp[i], dp[i-c]+1); } } return dp[amount]==amount+1 ? -1 : dp[amount]; } } 337. 打家劫舍 III 难度中等377
在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。
计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。
示例 1:
输入: [3,2,3,null,3,null,1] 3 / \ 2 3 \ \ 3 1 输出: 7 解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7. 示例 2:
输入: [3,4,5,1,3,null,1] 3 / \ 4 5 / \ \ 1 3 1 输出: 9 解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9. public int[] helper(TreeNode r){ if(r == null) return new int[2];//边界条件，r为null时，跳出 int[] left = helper(r.left);//对于以r.left为根的树，计算抢劫根节点(r.left)与不抢劫根节点可获得最大金额. left[0]则为不抢r.lrft可获得的最大金额,left[1]则为抢劫r.left可获得的最大金额 以下right[] 分析同理 int[] right = helper(r.right); int[] res = new int[2]; res[0] = Math.max(left[0],left[1]) + Math.max(right[0],right[1]);//计算不抢劫当前根节点可获得的最大金额(那么其左右子树可以随便抢) res[1] = r.val + left[0] + right[0];//计算若抢劫根节点可获得的最大金额(此时,其左右子树的根节点不能被抢) return res; } /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public int rob(TreeNode root) { int[] res = helper(root); return Math.max(res[0], res[1]); } public int[] helper(TreeNode root){ if(root==null) return new int[2]; int[] left = helper(root.left); int[] right = helper(root.right); int[] res = new int[2]; res[0] = Math.max(left[0], left[1]) + Math.max(right[0], right[1]); res[1] = root.val + left[0] + right[0]; return res; } } 338. 比特位计数 难度中等326
给定一个非负整数 num。对于 0 ≤ i ≤ num 范围中的每个数字 i ，计算其二进制数中的 1 的数目并将它们作为数组返回。
示例 1:
输入: 2 输出: [0,1,1] 示例 2:
输入: 5 输出: [0,1,1,2,1,2] 进阶:
给出时间复杂度为**O(n*sizeof(integer))的解答非常容易。但你可以在线性时间O(n)**内用一趟扫描做到吗？ 要求算法的空间复杂度为O(n)。 你能进一步完善解法吗？要求在C++或任何其他语言中不使用任何内置函数（如 C++ 中的 __builtin_popcount）来执行此操作。 i &amp; (i - 1)可以去掉i最右边的一个1（如果有），因此 i &amp; (i - 1）是比 i 小的，而且i &amp; (i - 1)的1的个数已经在前面算过了，所以i的1的个数就是 i &amp; (i - 1)的1的个数加上1
class Solution { public int[] countBits(int num) { int[] res = new int[num+1]; for(int i=1; i&lt;=num; i++){ res[i] = res[i&amp;(i-1)]+1; } return res; } } 347. 前 K 个高频元素 难度中等363
给定一个非空的整数数组，返回其中出现频率前 *k* 高的元素。
示例 1:
输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2] 示例 2:
输入: nums = [1], k = 1 输出: [1] 提示：
你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。 你的算法的时间复杂度必须优于 O(n log n) , n 是数组的大小。 题目数据保证答案唯一，换句话说，数组中前 k 个高频元素的集合是唯一的。 你可以按任意顺序返回答案。 使用Map和优先队列。
class Solution { public int[] topKFrequent(int[] nums, int k) { Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); for(int num : nums){ map.put(num, map.getOrDefault(num, 0)+1); } PriorityQueue&lt;Map.Entry&lt;Integer, Integer&gt;&gt; heap = new PriorityQueue&lt;&gt;((x,y)-&gt;x.getValue()-y.getValue()); for(Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()){ heap.add(entry); if(heap.size()&gt;k){ heap.poll(); } } int[] res = new int[k]; int i=0; while(!heap.isEmpty()){ res[i++] = heap.poll().getKey(); } return res; } } 394. 字符串解码 难度中等383
给定一个经过编码的字符串，返回它解码后的字符串。
编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。
你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。
此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。
示例 1：
输入：s = &#34;3[a]2[bc]&#34; 输出：&#34;aaabcbc&#34; 示例 2：
输入：s = &#34;3[a2[c]]&#34; 输出：&#34;accaccacc&#34; 示例 3：
输入：s = &#34;2[abc]3[cd]ef&#34; 输出：&#34;abcabccdcdcdef&#34; 示例 4：
输入：s = &#34;abc3[cd]xyz&#34; 输出：&#34;abccdcdcdxyz&#34; class Solution { public String decodeString(String s) { int len = s.length(); int num = 0; Deque&lt;String&gt; str_stack = new LinkedList&lt;&gt;(); Deque&lt;Integer&gt; num_stack = new LinkedList&lt;&gt;(); String cur = &#34;&#34;; String res = &#34;&#34;; for (int i=0; i&lt;len; i++){ char c = s.charAt(i); if(c&gt;=&#39;0&#39; &amp;&amp; c&lt;=&#39;9&#39;){ num = 10*num + c - &#39;0&#39;; }else if(c==&#39;[&#39;){ num_stack.push(num); str_stack.push(cur); num = 0; cur = &#34;&#34;; } else if(Character.isAlphabetic(c)){ cur+=s.substring(i,i+1); } else if(c==&#39;]&#39;){ int k = num_stack.pop(); String tmp = str_stack.pop(); for(int j=0; j&lt;k; j++){ tmp+=cur; } cur = tmp; } } res += cur; return res; } } 399. 除法求值 难度中等157
给出方程式 A / B = k, 其中 A 和 B 均为用字符串表示的变量， k 是一个浮点型数字。根据已知方程式求解问题，并返回计算结果。如果结果不存在，则返回 -1.0。
示例 : 给定 a / b = 2.0, b / c = 3.0 问题: a / c = ?, b / a = ?, a / e = ?, a / a = ?, x / x = ? 返回 [6.0, 0.5, -1.0, 1.0, -1.0 ]
输入为: vector&lt;pair&lt;string, string&gt;&gt; equations, vector&lt;double&gt;&amp; values, vector&lt;pair&lt;string, string&gt;&gt; queries(方程式，方程式结果，问题方程式)， 其中 equations.size() == values.size()，即方程式的长度与方程式结果长度相等（程式与结果一一对应），并且结果值均为正数。以上为方程式的描述。 返回vector&lt;double&gt;类型。
基于上述例子，输入如下：
equations(方程式) = [ [&#34;a&#34;, &#34;b&#34;], [&#34;b&#34;, &#34;c&#34;] ], values(方程式结果) = [2.0, 3.0], queries(问题方程式) = [ [&#34;a&#34;, &#34;c&#34;], [&#34;b&#34;, &#34;a&#34;], [&#34;a&#34;, &#34;e&#34;], [&#34;a&#34;, &#34;a&#34;], [&#34;x&#34;, &#34;x&#34;] ]. 输入总是有效的。你可以假设除法运算中不会出现除数为0的情况，且不存在任何矛盾的结果。
406. 根据身高重建队列 难度中等360
假设有打乱顺序的一群人站成一个队列。 每个人由一个整数对(h, k)表示，其中h是这个人的身高，k是排在这个人前面且身高大于或等于h的人数。 编写一个算法来重建这个队列。
注意： 总人数少于1100人。
示例
输入: [[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]] 输出: [[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]] 思路：按照身高降序、K升序排序，按照k来插入，注意comparator的写法，前面减后面升序，后面减前面降序。
class Solution { public int[][] reconstructQueue(int[][] people) { if(people.length==0 || people[0].length==0) return new int[0][0]; Arrays.sort(people, new Comparator&lt;int[]&gt;(){ public int compare(int[] o1, int[] o2){ return o1[0]==o2[0] ? o1[1]-o2[1] : o2[0]-o1[0]; } }); List&lt;int[]&gt; list = new ArrayList&lt;&gt;(); for(int[] i : people){ list.add(i[1], i); } return list.toArray(new int[list.size()][]); } } 416. 分割等和子集 难度中等301
给定一个只包含正整数的非空数组。是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。
注意:
每个数组中的元素不会超过 100 数组的大小不会超过 200 示例 1:
输入: [1, 5, 11, 5] 输出: true 解释: 数组可以分割成 [1, 5, 5] 和 [11]. 示例 2:
输入: [1, 2, 3, 5] 输出: false 解释: 数组不能分割成两个元素和相等的子集. class Solution { public boolean canPartition(int[] nums) { int n=nums.length; int sum=0; for(int e : nums){ sum+=e; } if(sum%2==1) return false; boolean[][] dp = new boolean[n+1][sum/2+1]; for(int i=0; i&lt;=n; i++){ dp[i][0] = true; } for(int i=1; i&lt;=n; i++){ for (int j=0; j&lt;=sum/2; j++){ if(j&gt;=nums[i-1]){ dp[i][j] = dp[i-1][j] || dp[i-1][j-nums[i-1]]; }else{ dp[i][j] = dp[i-1][j]; } } } return dp[n][sum/2]; } } 437. 路径总和 III 难度简单447收藏分享切换为英文关注反馈
给定一个二叉树，它的每个结点都存放着一个整数值。
找出路径和等于给定数值的路径总数。
路径不需要从根节点开始，也不需要在叶子节点结束，但是路径方向必须是向下的（只能从父节点到子节点）。
二叉树不超过1000个节点，且节点数值范围是 [-1000000,1000000] 的整数。
示例：
root = [10,5,-3,3,2,null,11,3,-2,null,1], sum = 8 10 / \ 5 -3 / \ \ 3 2 11 / \ \ 3 -2 1 返回 3。和等于 8 的路径有: 1. 5 -&gt; 3 2. 5 -&gt; 2 -&gt; 1 3. -3 -&gt; 11 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { int cnt; public int pathSum(TreeNode root, int sum) { if (root==null) return 0; sum(root, sum); pathSum(root.left, sum); pathSum(root.right, sum); return cnt; } private void sum(TreeNode root, int sum){ if(root==null) return ; sum-=root.val; if(sum==0) cnt++; sum(root.left, sum); sum(root.right, sum); } } 438. 找到字符串中所有字母异位词 难度中等287
给定一个字符串 s 和一个非空字符串 p，找到 s 中所有是 p 的字母异位词的子串，返回这些子串的起始索引。
字符串只包含小写英文字母，并且字符串 s 和 p 的长度都不超过 20100。
说明：
字母异位词指字母相同，但排列不同的字符串。 不考虑答案输出的顺序。 示例 1:
输入: s: &#34;cbaebabacd&#34; p: &#34;abc&#34; 输出: [0, 6] 解释: 起始索引等于 0 的子串是 &#34;cba&#34;, 它是 &#34;abc&#34; 的字母异位词。 起始索引等于 6 的子串是 &#34;bac&#34;, 它是 &#34;abc&#34; 的字母异位词。 示例 2:
输入: s: &#34;abab&#34; p: &#34;ab&#34; 输出: [0, 1, 2] 解释: 起始索引等于 0 的子串是 &#34;ab&#34;, 它是 &#34;ab&#34; 的字母异位词。 起始索引等于 1 的子串是 &#34;ba&#34;, 它是 &#34;ab&#34; 的字母异位词。 起始索引等于 2 的子串是 &#34;ab&#34;, 它是 &#34;ab&#34; 的字母异位词。 class Solution { public List&lt;Integer&gt; findAnagrams(String s, String p) { Map&lt;Character, Integer&gt; pmap = new HashMap&lt;&gt;(); Map&lt;Character, Integer&gt; smap = new HashMap&lt;&gt;(); for(char c : p.toCharArray()){ pmap.put(c, pmap.getOrDefault(c, 0)+1); } List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int cnt=0, left=0, right=0; int len = p.length(); while(right &lt; s.length()){ char ch = s.charAt(right); smap.put(ch, smap.getOrDefault(ch, 0)+1); if(pmap.containsKey(ch)&amp;&amp;(smap.get(ch)&lt;=pmap.get(ch))){ cnt++; } if(cnt==len) list.add(left); if(right-left+1 &gt;= len){ char left_ch = s.charAt(left); if(pmap.containsKey(left_ch)&amp;&amp;smap.get(left_ch)&lt;=pmap.get(left_ch)){ cnt--; } smap.put(left_ch, smap.getOrDefault(left_ch, 1)-1); left++; } right++; } return list; } } 448. 找到所有数组中消失的数字 难度简单364收藏分享切换为英文关注反馈
给定一个范围在 1 ≤ a[i] ≤ n ( n = 数组大小 ) 的 整型数组，数组中的元素一些出现了两次，另一些只出现一次。
找到所有在 [1, n] 范围之间没有出现在数组中的数字。
您能在不使用额外空间且时间复杂度为*O(n)*的情况下完成这个任务吗? 你可以假定返回的数组不算在额外空间内。
示例:
输入: [4,3,2,7,8,2,3,1] 输出: [5,6] class Solution { public List&lt;Integer&gt; findDisappearedNumbers(int[] nums) { List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); for(int i=0; i&lt;nums.length; i++){ if(nums[Math.abs(nums[i])-1]&gt;0){ nums[Math.abs(nums[i])-1] = -nums[Math.abs(nums[i])-1]; } } for(int i=0; i&lt; nums.length; i++){ if(nums[i]&gt;0){ res.add(i+1); } } return res; } } 461. 汉明距离 难度简单288收藏分享切换为英文关注反馈
两个整数之间的 汉明距离 指的是这两个数字对应二进制位不同的位置的数目。
给出两个整数 x 和 y，计算它们之间的汉明距离。
注意： 0 ≤ x, y &lt; 231.
示例:
输入: x = 1, y = 4 输出: 2 解释: 1 (0 0 0 1) 4 (0 1 0 0) ↑ ↑ 上面的箭头指出了对应二进制位不同的位置。 class Solution { public int hammingDistance(int x, int y) { int z = x^y; int cnt=0; while(z&gt;0){ if(z%2 == 1) cnt++; z = z&gt;&gt;1; } return cnt; } } 494. 目标和 难度中等288
给定一个非负整数数组，a1, a2, &hellip;, an, 和一个目标数，S。现在你有两个符号 + 和 -。对于数组中的任意一个整数，你都可以从 + 或 -中选择一个符号添加在前面。
返回可以使最终数组和为目标数 S 的所有添加符号的方法数。
示例：
输入：nums: [1, 1, 1, 1, 1], S: 3 输出：5 解释： -1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3 一共有5种方法让最终目标和为3。 提示：
数组非空，且长度不会超过 20 。 初始的数组的和不会超过 1000 。 保证返回的最终结果能被 32 位整数存下 class Solution { public int findTargetSumWays(int[] nums, int S) { int sum = 0; for(int e : nums){ sum+=e; } if(sum&lt;Math.abs(S) || (sum+S)%2 != 0) return 0; int target = (sum+S)/2; int[] dp = new int[target+1]; dp[0] = 1; for(int i=1; i&lt;=nums.length; i++){ for(int j=target; j&gt;=nums[i-1];j--){ dp[j] = dp[j]+dp[j-nums[i-1]]; } } return dp[target]; } } 538. 把二叉搜索树转换为累加树 难度简单263收藏分享切换为英文关注反馈
给定一个二叉搜索树（Binary Search Tree），把它转换成为累加树（Greater Tree)，使得每个节点的值是原来的节点值加上所有大于它的节点值之和。
例如：
输入: 原始二叉搜索树: 5 / \ 2 13 输出: 转换为累加树: 18 / \ 20 13 **注意：**本题和 1038: https://leetcode-cn.com/problems/binary-search-tree-to-greater-sum-tree/ 相同
中序遍历（左中右）：从小到大
反过来（右中左）：从大到小
/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { int num=0; public TreeNode convertBST(TreeNode root) { if(root != null){ convertBST(root.right); root.val = root.val + num; num = root.val;//重要 convertBST(root.left); return root; } return null; } } 543. 二叉树的直径 难度简单378收藏分享切换为英文关注反馈
给定一棵二叉树，你需要计算它的直径长度。一棵二叉树的直径长度是任意两个结点路径长度中的最大值。这条路径可能穿过也可能不穿过根结点。
示例 : 给定二叉树
1 / \ 2 3 / \ 4 5 返回 3, 它的长度是路径 [4,2,1,3] 或者 [5,2,1,3]。
**注意：**两结点之间的路径长度是以它们之间边的数目表示。
/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { int max=0; public int diameterOfBinaryTree(TreeNode root) { if(root==null) return 0; dfs(root); return max; } private int dfs(TreeNode root){ if(root.left==null &amp;&amp; root.right==null) return 0; int leftsize = root.left==null ? 0 : dfs(root.left)+1; int rightsize = root.right==null ? 0 : dfs(root.right)+1; max = Math.max(max, leftsize+rightsize); return Math.max(leftsize, rightsize); } } 560. 和为K的子数组 难度中等467
给定一个整数数组和一个整数 **k，**你需要找到该数组中和为 k 的连续的子数组的个数。
示例 1 :
输入:nums = [1,1,1], k = 2 输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。 说明 :
数组的长度为 [1, 20,000]。 数组中元素的范围是 [-1000, 1000] ，且整数 k 的范围是 [-1e7, 1e7]。 暴力法
class Solution { public int subarraySum(int[] nums, int k) { int cnt=0; for(int i=0; i&lt;nums.length; i++){ int sum=0; for(int j=i; j&lt;nums.length; j++){ sum+=nums[j]; cnt += sum==k ? 1 :0; } } return cnt; } } class Solution { public int subarraySum(int[] nums, int k) { /** 扫描一遍数组, 使用map记录出现同样的和的次数, 对每个i计算累计和sum并判断map内是否有sum-k **/ Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); map.put(0, 1); int sum = 0, ret = 0; for(int i = 0; i &lt; nums.length; ++i) { sum += nums[i]; if(map.containsKey(sum-k)) ret += map.get(sum-k); map.put(sum, map.getOrDefault(sum, 0)+1); } return ret; } } 581. 最短无序连续子数组 难度简单318收藏分享切换为英文关注反馈
给定一个整数数组，你需要寻找一个连续的子数组，如果对这个子数组进行升序排序，那么整个数组都会变为升序排序。
你找到的子数组应是最短的，请输出它的长度。
示例 1:
输入: [2, 6, 4, 8, 10, 9, 15] 输出: 5 解释: 你只需要对 [6, 4, 8, 10, 9] 进行升序排序，那么整个表都会变为升序排序。 说明 :
输入的数组长度范围在 [1, 10,000]。 输入的数组可能包含重复元素 ，所以升序的意思是**&lt;=。** 思路：
从左到右找出最后一个破坏递增的数
从右到左找出最后一个破坏递减的数
class Solution { public int findUnsortedSubarray(int[] nums) { int len = nums.length; if(len &lt;= 1) return 0; int high = 0, low = len-1; int curMax = Integer.MIN_VALUE; int curMin = Integer.MAX_VALUE; for(int i=0; i&lt;len; i++){ if(nums[i] &gt;= curMax){ curMax = nums[i]; }else{ high = i; } if(nums[len-i-1] &lt;= curMin){ curMin = nums[len-i-1]; }else{ low = len - i - 1; } } return high&gt;low ? high-low+1 : 0; } } 617. 合并二叉树 难度简单392收藏分享切换为英文关注反馈
给定两个二叉树，想象当你将它们中的一个覆盖到另一个上时，两个二叉树的一些节点便会重叠。
你需要将他们合并为一个新的二叉树。合并的规则是如果两个节点重叠，那么将他们的值相加作为节点合并后的新值，否则不为 NULL 的节点将直接作为新二叉树的节点。
示例 1:
输入: Tree 1 Tree 2 1 2 / \ / \ 3 2 1 3 / \ \ 5 4 7 输出: 合并后的树: 3 / \ 4 5 / \ \ 5 4 7 注意: 合并必须从两个树的根节点开始。
/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */ class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if(t1==null) return t2; if(t2==null) return t1; t1.val += t2.val; t1.left = subMerge(t1.left, t2.left); t1.right = subMerge(t1.right, t2.right); return t1; } public TreeNode subMerge(TreeNode t1, TreeNode t2){ if(t1==null &amp;&amp; t2==null) return null; TreeNode root = new TreeNode((t1==null ? 0 : t1.val)+(t2==null ? 0 : t2.val)); root.left = subMerge(t1==null ? null : t1.left, t2==null ? null : t2.left); root.right = subMerge(t1==null ? null : t1.right, t2==null ? null : t2.right); return root; } } 621. 任务调度器 难度中等290
给定一个用字符数组表示的 CPU 需要执行的任务列表。其中包含使用大写的 A - Z 字母表示的26 种不同种类的任务。任务可以以任意顺序执行，并且每个任务都可以在 1 个单位时间内执行完。CPU 在任何一个单位时间内都可以执行一个任务，或者在待命状态。
然而，两个相同种类的任务之间必须有长度为 n 的冷却时间，因此至少有连续 n 个单位时间内 CPU 在执行不同的任务，或者在待命状态。
你需要计算完成所有任务所需要的最短时间。
示例 ：
输入：tasks = [&#34;A&#34;,&#34;A&#34;,&#34;A&#34;,&#34;B&#34;,&#34;B&#34;,&#34;B&#34;], n = 2 输出：8 解释：A -&gt; B -&gt; (待命) -&gt; A -&gt; B -&gt; (待命) -&gt; A -&gt; B. 在本示例中，两个相同类型任务之间必须间隔长度为 n = 2 的冷却时间，而执行一个任务只需要一个单位时间，所以中间出现了（待命）状态。 提示：
任务的总个数为 [1, 10000]。 n 的取值范围为 [0, 100]。 647. 回文子串 739. 每日温度 ]]></content></entry><entry><title>面试-leetcode分类</title><url>/post/%E9%9D%A2%E8%AF%95-leetcode%E5%88%86%E7%B1%BB/</url><categories><category>面试</category></categories><tags/><content type="html"> 数据结构与算法简介、LeetCode 入门及攻略（1 天） 数据结构与算法 算法复杂度 LeetCode 入门与攻略 0001. 两数之和 1929. 数组串联 0771. 宝石与石头 数组基础（2 天） 数组基础知识 0066. 加一 0724. 寻找数组的中心下标 0189. 旋转数组 0048. 旋转图像 0054. 螺旋矩阵 0498. 对角线遍历 更多数组基础题目 数组排序（4 天） 冒泡排序 、 选择排序 、 插入排序 179. 最大数 324. 摆动排序 II 剑指 Offer 45. 把数组排成最小的数 0283. 移动零 0912. 排序数组 归并排序 、 希尔排序 0506. 相对名次 面试题 10.01. 合并排序的数组 剑指 Offer 51. 数组中的逆序对 快速排序 、 堆排序 0075. 颜色分类 0215. 数组中的第K个最大元素 剑指 Offer 40. 最小的k个数 计数排序 、 桶排序 、 基数排序 1122. 数组的相对排序 0908. 最小差值 I 0164. 最大间距 更多排序相关题目 数组二分查找（3 天） 二分查找知识 0704. 二分查找 0035. 搜索插入位置 0374. 猜数字大小 0069. Sqrt(x) 0167. 两数之和 II - 输入有序数组 1011. 在 D 天内送达包裹的能力 0278. 第一个错误的版本 0033. 搜索旋转排序数组 0153. 寻找旋转排序数组中的最小值 162. 寻找峰值 287. 寻找重复数 315. 计算右侧小于当前元素的个数 更多二分查找题目 数组双指针、滑动窗口（3 天） 双指针基础知识 0344. 反转字符串 0015. 三数之和 0080. 删除有序数组中的重复项 II 0283. 移动零 0075. 颜色分类 0088. 合并两个有序数组 395. 至少有K个重复字符的最长子串 更多双指针题目 滑动窗口基础知识 0674. 最长连续递增序列 1004. 最大连续1的个数 III 0220. 存在重复元素 III 更多滑动窗口题目 152. 乘积最大子序列 169. 求众数 189. 旋转数组 217. 存在重复元素 283. 移动零 384. 打乱数组 350. 两个数组的交集 II 334. 递增的三元子序列 240. 搜索二维矩阵 II 238. 除自身以外数组的乘积 链表 01-01 链表基础知识 链表基础知识 0707. 设计链表 0206. 反转链表 0203. 移除链表元素 0328. 奇偶链表 0234. 回文链表 0138. 复制带随机指针的链表 更多链表基础题目 01-02 链表排序 链表排序 0148. 排序链表 0021. 合并两个有序链表 0147. 对链表进行插入排序 更多链表排序题目 01-03 链表双指针 链表双指针 0141. 环形链表 0142. 环形链表 II 0019. 删除链表的倒数第 N 个结点 160. 相交链表 更多链表双指针题目 堆栈与深度优先搜索（5 天） 02-01 堆栈基础知识（2 天） 堆栈基础知识 0155. 最小栈 0020. 有效的括号 0227. 基本计算器 II 0150. 逆波兰表达式求值 0394. 字符串解码 0946. 验证栈序列 341. 扁平化嵌套列表迭代器 更多堆栈基础知识相关题目 02-02 栈与深度优先搜索（3 天） 栈与深度优先搜索 0200. 岛屿数量 0133. 克隆图 0494. 目标和 0841. 钥匙和房间 0695. 岛屿的最大面积 0130. 被围绕的区域 0417. 太平洋大西洋水流问题 更多栈与深度优先搜索题目 单调栈（1 天） 所以单调栈一般用于解决一下几种问题：
寻找左侧第一个比当前元素大的元素
从左到右遍历元素，构造单调递增栈（从栈顶到栈底递增）：一个元素左侧第一个比它大的元素就是将其「插入单调递增栈」时的栈顶元素。如果插入时的栈为空，则说明左侧不存在比当前元素大的元素。
寻找左侧第一个比当前元素小的元素
寻找右侧第一个比当前元素大的元素
寻找右侧第一个比当前元素小的元素
从左到右遍历元素，构造单调递减栈（从栈顶到栈底递减）：一个元素右侧第一个比它小的元素就是将其「弹出单调递减栈」时即将插入的元素。如果该元素没有被弹出栈，则说明右侧不存在比当前元素小的元素。
查找 「比当前元素大的元素」 就用 单调递增栈，查找 「比当前元素小的元素」 就用 单调递减栈。
0496. 下一个更大元素 I 0739. 每日温度 0316. 去除重复字母 更多单调栈题目 队列与广度优先搜索 04-01 队列基础知识 队列基础知识 0622. 设计循环队列 剑指 Offer II 041. 滑动窗口的平均值 239. 滑动窗口最大值 0225. 用队列实现栈 更多队列基础题目 04-02 队列与广度优先搜索 队列与广度优先搜索 0463. 岛屿的周长 0752. 打开转盘锁 0279. 完全平方数 0542. 01 矩阵 0322. 零钱兑换 剑指 Offer 13. 机器人的运动范围 更多队列与广度优先搜索题目 优先队列 优先队列 0215. 数组中的第K个最大元素 0347. 前 K 个高频元素 0451. 根据字符出现频率排序 295. 数据流的中位数 378. 有序矩阵中第K小的元素 更多优先队列题目 动态规划 124. 二叉树中的最大路径和 128. 最长连续序列 198. 打家劫舍 279. 完全平方数 300. 最长上升子序列 322. 零钱兑换 329. 矩阵中的最长递增路径 图论 127. 单词接龙 200. 岛屿的个数 207. 课程表 210. 课程表 II 数学 &amp;amp; 位运算 136. 只出现一次的数字 149. 直线上最多的点数 166. 分数到小数 172. 阶乘后的零 190. 颠倒二进制位 191. 位1的个数 204. 计数质数 268. 缺失数字 326. 3的幂 字符串 125. 验证回文串 131. 分割回文串 139. 单词拆分 140. 单词拆分 II 208. 实现 Trie (前缀树) 212. 单词搜索 II 242. 有效的字母异位词 387. 字符串中的第一个唯一字符 344. 反转字符串 树 230. 二叉搜索树中第K小的元素 236. 二叉树的最近公共祖先 297. 二叉树的序列化与反序列化 线段树 218. 天际线问题 哈希 / Map 171. Excel表列序号 454. 四数相加 II 380. 常数时间插入、删除和获取随机元素 模拟 134. 加油站 146. LRU缓存机制 202. 快乐数 289. 生命游戏 371. 两整数之和 412. Fizz Buzz</content></entry><entry><title>面试-leetcode面试top100</title><url>/post/%E9%9D%A2%E8%AF%95-leetcode%E9%9D%A2%E8%AF%95top100/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[汇总 (10 封私信) 互联网公司最常见的面试算法题有哪些？ - 知乎 (zhihu.com) 模拟 134. 加油站 在一条环路上有 N 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1。
如果 sum(gas) &lt; sum(cost) ，那么不可能环行一圈，这种情况下答案是 -1 。 对于加油站 i ，如果 gas[i] - cost[i] &lt; 0 ，则不可能从这个加油站出发，因为在前往 i + 1 的过程中，汽油就不够了。
class Solution: def canCompleteCircuit(self, gas, cost): n = len(gas) total_residue_tank, next_residue_tank = 0, 0 starting_station = 0 for i in range(n): total_residue_tank += gas[i] - cost[i] # 所有加的油减去所有的消耗，最后剩下的油 next_residue_tank += gas[i] - cost[i] # 到达下一站后油箱里的油 # 能否到达下一站 if next_residue_tank &lt; 0: # 不能到达下一站以下一站为起点 starting_station = i + 1 # 此时油箱里的油为空 next_residue_tank = 0 return starting_station if total_residue_tank &gt;= 0 else -1 146. LRU缓存机制 设计和实现一个 LRU (最近最少使用) 缓存机制。它应该支持以下操作： 获取数据 get 和 写入数据 put 。 获取数据 get(key) - 如果关键字 (key) 存在于缓存中，则获取关键字的值（总是正数），否则返回 -1。 写入数据 put(key, value) - 如果关键字已经存在，则变更其数据值；如果关键字不存在，则插入该组「关键字/值」。当缓存容量达到上限时，它应该在写入新数据之前删除最久未使用的数据值，从而为新的数据值留出空间。
实现一个可以存储 key-value 形式数据的数据结构，并且可以记录最近访问的 key 值。首先想到的就是用字典来存储 key-value 结构，这样对于查找操作时间复杂度就是 O(1)O(1)。
但是因为字典本身是无序的，所以我们还需要一个类似于队列的结构来记录访问的先后顺序，这个队列需要支持如下几种操作：
在末尾加入一项 去除最前端一项 将队列中某一项移到末尾
# 双向链表 class ListNode: def __init__(self, key=None, value=None): self.key = key self.value = value self.nex = None self.pre = None class LRUCache: def __init__(self, capacity: int): self.capacity = capacity self.hashmap = {} # 新建两个节点 head 和 tail self.head = ListNode() self.tail = ListNode() # 初始化链表为 head &lt;-&gt; tail self.head.nex = self.tail self.tail.pre = self.head # 因为get与put操作都可能需要将双向链表中的某个节点移到末尾，所以定义一个方法 def move_to_end(self, key): node = self.hashmap[key] # 先将哈希表key指向的节点node拎出来 node.pre.nex = node.nex node.nex.pre = node.pre # 之后将node插入到尾节点前 node.pre = self.tail.pre node.nex = self.tail self.tail.pre.nex = node self.tail.pre = node def get(self, key: int) -&gt; int: if key in self.hashmap: # 如果已经在链表中了久把它移到末尾（变成最新访问的） self.move_to_end(key) node = self.hashmap.get(key, -1) return node if node == -1 else node.value def put(self, key: int, value: int) -&gt; None: if key in self.hashmap: # 如果key本身已经在哈希表中了就不需要在链表中加入新的节点 # 但是需要更新字典该值对应节点的value self.hashmap[key].value = value self.move_to_end(key) else: if len(self.hashmap) == self.capacity: # 去掉哈希表对应项 self.hashmap.pop(self.head.nex.key) # 去掉最久没有被访问过的节点，即头节点之后的节点 self.head.nex = self.head.nex.nex self.head.nex.pre = self.head # 如果不在的话就插入到尾节点前 new = ListNode(key, value) self.hashmap[key] = new new.pre = self.tail.pre new.nex = self.tail self.tail.pre.nex = new self.tail.pre = new # Your LRUCache object will be instantiated and called as such: # obj = LRUCache(capacity) # param_1 = obj.get(key) # obj.put(key,value) 202. 快乐数 定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。如果 可以变为 1，那么这个数就是快乐数。
最终会得到 1。 最终会进入循环。 值会越来越大，最后接近无穷大。
Digits Largest Next 1 9 81 2 99 162 3 999 243 4 9999 324 13 9999999999999 1053 对于 3 位数的数字，它不可能大于 243。这意味着它要么被困在243 以下的循环内，要么跌到 1。4 位或 4 位以上的数字在每一步都会丢失一位，直到降到 3 位为止。所以我们知道，最坏的情况下，算法可能会在 243 以下的所有数字上循环，然后回到它已经到过的一个循环或者回到 11。但它不会无限期地进行下去，所以我们排除第三种选择。
# 检测单链表是否有环，用快慢指针 class Solution: def get_next(self, num): all_squre_sum = 0 while num &gt; 0: num, digit = divmod(num, 10) all_squre_sum += digit ** 2 return all_squre_sum def isHappy(self, n: int) -&gt; bool: slow_index = n fast_index = self.get_next(n) while fast_index != 1 and slow_index != fast_index: slow_index = self.get_next(slow_index) fast_index = self.get_next(self.get_next(fast_index)) return fast_index == 1 289. 生命游戏 给定一个包含 m × n 个格子的面板，每一个格子都可以看成是一个细胞。每个细胞都具有一个初始状态：1即为活细胞（live），或 0 即为死细胞（dead）。每个细胞与其八个相邻位置（水平，垂直，对角线）的细胞都遵循以下四条生存定律： 1.如果活细胞周围八个位置的活细胞数少于两个，则该位置活细胞死亡； 2.如果活细胞周围八个位置有两个或三个活细胞，则该位置活细胞仍然存活； 3.如果活细胞周围八个位置有超过三个活细胞，则该位置活细胞死亡； 4.如果死细胞周围正好有三个活细胞，则该位置死细胞复活；
根据当前状态，写一个函数来计算面板上所有细胞的下一个（一次更新后的）状态。下一个状态是通过将上述规则同时应用于当前状态下的每个细胞所形成的，其中细胞的出生和死亡是同时发生的。
如果你直接根据规则更新原始数组，那么就做不到题目中说的 同步 更新。假设你直接将更新后的细胞状态填入原始数组，那么当前轮次其他细胞状态的更新就会引用到当前轮已更新细胞的状态，但实际上每一轮更新需要依赖上一轮细胞的状态，是不能用这一轮的细胞状态来更新的。
拓展一些复合状态使其包含之前的状态。
class Solution: def gameOfLife(self, board: List[List[int]]) -&gt; None: neighbors = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)] rows, cols = len(board), len(board[0]) # 遍历面板每一个格子里的细胞,根据周围细胞改变联合状态 for r in range(rows): for c in range(cols): # 对于每一个细胞统计其八个相邻位置里的活细胞数量 live_neighbor = 0 for i in neighbors: if 0 &lt;= r+i[0] &lt; rows and 0 &lt;= c+i[1] &lt; cols and abs(board[r+i[0]][c+i[1]]) == 1: # abs() 因为后面会把1改成-1 live_neighbor += 1 # 规则 2 不造成状态变化 # 规则 1 或规则 3 活细胞，周围活的少于两个多于三个，死 if board[r][c] == 1 and (live_neighbor &gt; 3 or live_neighbor &lt; 2): board[r][c] = -1 # 规则 4 死细胞，周围活的为2或3，活 if board[r][c] == 0 and live_neighbor == 3: board[r][c] = 2 # # 遍历面板每一个格子里的细胞 # 遍历 board 得到一次更新后的状态 for r in range(rows): for c in range(cols): if board[r][c] &gt; 0: board[r][c] = 1 else: board[r][c] = 0 371. 两整数之和 正数的补码就是原码，而负数的补码=原码符号位不变，其他位按位取反后+1。
不使用运算符 + 和 - ，计算两整数 a 、b 之和。
异或的一个重要特性是无进位加法。我们来看一个例子：
a = 5 = 0101 b = 4 = 0100 a ^ b 如下： 0 1 0 1 0 1 0 0 0 0 0 1 a ^ b 得到了一个无进位加法结果，如果要得到 a + b 的最终值，我们还要找到进位的数，把这二者相加。 在位运算中，我们可以使用与操作获得进位： a = 5 = 0101 b = 4 = 0100 a &amp; b 如下： 0 1 0 1 0 1 0 0 0 1 0 0 由计算结果可见，0100 并不是我们想要的进位，1 + 1 所获得的进位应该要放置在它的更高位，即左侧位上，因此我们还要把 0100 左移一位，才是我们所要的进位结果。
总结一下：
a + b 的问题拆分为 (a 和 b 的无进位结果) + (a 和 b 的进位结果) 无进位加法使用异或运算计算得出 进位结果使用与运算和移位运算计算得出 循环此过程，直到进位为 0 在 Python 中，整数不是 32 位的，也就是说你一直循环左移并不会存在溢出的现象，这就需要我们手动对 Python 中的整数进行处理，手动模拟 32 位 INT 整型。
a=-2 ; b=3 a=1 1 1 0
b=0 0 1 1
将输入数字转化成无符号整数 a &amp;= 0xF # a = a &amp; 0b1111 = 1110 b &amp;= 0xF # b = 0011 计算无符号整数相加并的到结果 while b: carry = a &amp; b a ^= b b = carry &lt;&lt; 1 &amp; 0xF # 模拟溢出操作 过程为
0 1 2 3 carry 0010 0100 1000 a 1110 1101 1001 0001 b 0011 0100 1000 0000 最后结果为 1 是没有问题的 但是对于 -2 + -2 , 最后结果为 1110 + 1110 = 1100 (12) 会出现问题
讲结果根据范围判定,映射为有符号整型 首先有符号整数的值域应该为 [-8, 7] 对于初步运算的结果,当结果小于8直接返回就可. 对于大于 7 的结果, 可知符号位必为1. 现在的问题转化为, 如何通过位运算把负数转换出来. 假设python用的是 8bit 有符号整数,当前结果为0000 1100, 对应8bit有符号整数为12, 但结果应该为-4对应8bit有符号整数为1111 1100 通过两步转换可以得到: 1.结果 与 0b1111 异或 2.对异或结果按位取反 ~(a ^ 0xF) # 异或 - -&gt; 无进位加法 # 与，向左移动一位 - - &gt; 进位 class Solution: def getSum(self, a: int, b: int) -&gt; int: # 截取后32位为无符号整数 a &amp;= 0xFFFFFFFF b &amp;= 0xFFFFFFFF # 无符号整数加法 while b: # 模拟进位 carry = (a &amp; b) &lt;&lt; 1 # 模模拟相加 a ^= b b = carry &amp; 0xFFFFFFFF # 模拟溢出操作 # 结果映射为有符号整数 return a if a &lt; 0x80000000 else ~(a ^ 0xFFFFFFFF) 412. Fizz Buzz 写一个程序，输出从 1 到 n 数字的字符串表示。
如果 n 是3的倍数，输出“Fizz”； 如果 n 是5的倍数，输出“Buzz”； 3.如果 n 同时是3和5的倍数，输出 “FizzBuzz”。 class Solution: def fizzBuzz(self, n: int) -&gt; List[str]: result_list = [] # 映射关系放在散列表,可以对散列表添加/删除映射关系 key_value = {3: &#39;Fizz&#39;, 5: &#39;Buzz&#39;} for i in range(1, n+1): result = &#39;&#39; for k in key_value: if i % k == 0: result += key_value[k] if not result: result += str(i) result_list.append(result) return result_list 数组 376. 摆动序列 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为摆动序列。第一个差（如果存在的话）可能是正数或负数。少于两个元素的序列也是摆动序列。 例如， [1,7,4,9,2,5] 是一个摆动序列，因为差值 (6,-3,5,-7,3) 是正负交替出现的。相反, [1,4,7,2,5] 和 [1,7,4,5,5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 给定一个整数序列，返回作为摆动序列的最长子序列的长度。 通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。 class Solution: def wiggleMaxLength(self, nums: List[int]) -&gt; int: if len(nums) &lt; 2: return len(nums) up, down = 1, 1 for i in range(1, len(nums)): if nums[i] &gt; nums[i-1]: up = down+1 # 找升序，在降序的基础上加1 elif nums[i] &lt; nums[i-1]: down = up+1 # 找降序，在升序的基础上加1 else: # 如果等于剔除该元素 continue return max(up, down) 二分查找 # 返回 x 在 arr 中的索引，如果不存在返回 -1 def binarySearch(arr, l, r, x): # 基本判断 if r &gt;= l: mid = int(l + (r - l)/2) # 元素整好的中间位置 if arr[mid] == x: return mid # 元素小于中间位置的元素，只需要再比较左边的元素 elif arr[mid] &gt; x: return binarySearch(arr, l, mid-1, x) # 元素大于中间位置的元素，只需要再比较右边的元素 else: return binarySearch(arr, mid+1, r, x) else: # 不存在 return -1 # 测试数组 arr = [2, 3, 4, 10, 40] x = 10 # 函数调用 result = binarySearch(arr, 0, len(arr)-1, x) if result != -1: print(&#34;元素在数组中的索引为 %d&#34; % result) else: print(&#34;元素不在数组中&#34;) 链表 24. 反转链表 定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。
递归 定义递归函数功能，返回反转后头节点 寻找结束条件，若是节点为零或者1直接返回 寻找等价关系，reverse(head) = reverse(head.next);head.next.next=head;head.next=None # Definition for singly-linked list. class ListNode: def __init__(self, x): self.val = x self.next = None class Solution: # 如果链表是1-&gt;2-&gt;3， def reverseList(self, head: ListNode) -&gt; ListNode: # 递归终止条件是当前为空，或者下一个节点为空 if not head or not head.next: return head # head=1, head.next=2, 此时下一层返回的new_head是3 new_head = self.reverseList(head.next) # new_head指向3 head.next.next = head # 把2指向1 head.next = None # 把1指向None return new_head 双指针 next -head -pre -head
# Definition for singly-linked list. class ListNode: def __init__(self, x): self.val = x self.next = None class Solution: def reverseList(self, head: ListNode) -&gt; ListNode: if not head or not head.next: return head pre_index = next_index = None while head: # head指向当前节点，一次循环后的下个节点是否为空？为空返回空节点的上一个节点 # head-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5 head指向当前节点 next_index = head.next # next 的指向 head.next = pre_index # head 的指向 pre_index = head # pre 的指向 head = next_index # head 的指向 return pre_index 找出环的入口点： 从上面的分析知道，当fast和slow相遇时，slow还没有走完链表 假设fast已经在环内循环了n(1&lt;= n)圈 环长r 假设slow走了s步，则fast走了2s步 $2s = s + n * r $ $ s = nr $ 如果假设整个链表的长度是L 入口和相遇点的距离是x 起点到入口点的距离是a $a + x = s = n * r$ $ L = a +r$ $a + x = (n - 1) * r + r = (n - 1) * r + (L - a) $ $a = (n - 1) * r + (L -a -x) $ $a = n * r + x$
从起点到入口的距离a,相遇点到入口的距离x相等。 因此我们就可以分别用一个指针（ptr1, prt2），同时从起点、相遇点出发，每一次操作走一步，直到ptr1 == ptr2，此时的位置也就是入口点！
堆栈 哈希、队列 树、线段树 排序、二分检索、滑动窗口 排序
假设（不是一般性），某一对整数 a 和 b ，我们的比较结果是 a 应该在 b 前面，这意味着 $a⌢b&gt;b⌢a $，其中$ ⌢$ 表示连接。
如果排序结果是错的，说明存在一个 c ， b 在 c 前面且 c 在 a的前面。这产生了矛盾，因为 $a⌢b&gt;b⌢a $和$b⌢c&gt;c⌢b $意味着$ a⌢c&gt;c⌢a$ 。 换言之，我们的自定义比较方法保证了传递性，所以这样子排序是对的。
179. 最大数 给定一组非负整数 nums，重新排列它们每位数字的顺序使之组成一个最大的整数。 注意：输出结果可能非常大，所以你需要返回一个字符串而不是整数。
class sort_rule(str): def __lt__(self, y): return self+y &gt; self+x class Solution: def largestNumber(self, nums: List[int]) -&gt; str: sorted_nums = &#39;&#39;.join(sorted(map(str, nums), key=sort_rule)) return &#39;0&#39; if sorted_nums[0] == &#39;0&#39; else sorted_nums 324. 摆动排序 II class Solution: def wiggleSort(self, nums: List[int]) -&gt; None: &#34;&#34;&#34; Do not return anything, modify nums in-place instead. &#34;&#34;&#34; s = len(nums)-len(nums)//2 nums[0::2], nums[1::2] = sorted(nums)[:s][::-1], sorted(nums)[s:][::-1] # [::-1]防止连续的数连续的放进去 376. 摆动序列 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为摆动序列。第一个差（如果存在的话）可能是正数或负数。少于两个元素的序列也是摆动序列。 例如， [1,7,4,9,2,5] 是一个摆动序列，因为差值 (6,-3,5,-7,3) 是正负交替出现的。相反, [1,4,7,2,5] 和 [1,7,4,5,5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 给定一个整数序列，返回作为摆动序列的最长子序列的长度。 通过从原始序列中删除一些（也可以不删除）元素来获得子序列，剩下的元素保持其原始顺序。
class Solution: def wiggleMaxLength(self, nums: List[int]) -&gt; int: if len(nums) &lt; 2: return len(nums) up, down = 1, 1 for i in range(1, len(nums)): if nums[i] &gt; nums[i-1]: up = down+1 # 直到遇到一个升序，才在降序的基础上加1 elif nums[i] &lt; nums[i-1]: down = up+1 else: # 如果等于剔除该元素 continue return max(up, down) 动态规划 案例一、简单的一维 DP
剑指 Offer 10- II. 青蛙跳台阶问题 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 n 级的台阶总共有多少种跳法。
#####(1)定义数组元素的含义 跳上一个 i 级的台阶总共有 dp[i] 种跳法 #####(2)找出数组元素间的关系式 目的是要求 dp[n]，动态规划的题，就是把一个规模比较大的问题分成几个规模比较小的问题，然后由小的问题推导出大的问题。 所有可能的跳法的，所以有 dp[n] = dp[n-1] + dp[n-2]。 #####(3)找出初始条件 数组是不允许下标为负数的，所以对于 0、1、2，我们必须要直接给出它的数值，相当于初始值，显然，dp[0] = 1，dp[1] = 1, dp[2] = 2。
class Solution: def numWays(self, n: int) -&gt; int: dp = [1]*(n+1) # 初始条件dp[0] = 1，dp[1] = 1 for i in range(2, n+1): dp[i] = dp[i-1]+dp[i-2] return dp[-1] % 1000000007 案例二：二维数组的 DP
62. 不同路径 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。问总共有多少条不同的路径？ 步骤一、定义数组元素的含义 dp[i] [j]的含义为：当机器人从左上角走到(i, j) 这个位置时，一共有 dp[i] [j] 种路径。数组是从下标为 0 开始算起的，所以 右下角的位置是 (m-1, n - 1)dp[m-1] [n-1] 就是我们要的答案了。 注意，这个网格相当于一个二维数组，所以 dp[m-1] [n-1] 就是我们要找的答案。 步骤二：找出关系数组元素间的关系式 由于机器人可以向下走或者向右走，所以有两种方式到达 一种是从 (i-1, j) 这个位置走一步到达 一种是从(i, j - 1) 这个位置走一步到达 因为是计算所有可能的步骤，所以是把所有可能走的路径都加起来，所以关系式是 dp[i] [j] = dp[i-1] [j] + dp[i] [j-1]。 步骤三、找出初始值 初始值如下：dp[0] [0….n-1] = 1; 相当于最上面一行，机器人只能一直往左走 dp[0…m-1] [0] = 1; 相当于最左面一列，机器人只能一直往下走
class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: dp = [[1]*n for _ in range(m)] # m行n列 # 初始值dp[0] [0….n-1] = 1; dp[0…m-1] [0] = 1 for i in range(1, m): for j in range(1, n): dp[i][j] = dp[i][j-1]+dp[i-1][j] return dp[-1][-1] # 优化 class Solution: def uniquePaths(self, m: int, n: int) -&gt; int: cur = [1] * n for i in range(1, m): for j in range(1, n): cur[j] += cur[j-1] return cur[-1] 64. 最小路径和 给定一个包含非负整数的 m x n 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 输入：grid = [[1,3,1],[1,5,1],[4,2,1]] 输出：7 解释：因为路径 1→3→1→1→1 的总和最小。
class Solution: def minPathSum(self, grid: List[List[int]]) -&gt; int: m, n = len(grid), len(grid[0]) # 设置初始状态 dp = [[grid[0][0]]*n for _ in range(m)] # 转移方程 # 第一行、第一列不同 for i in range(1, m): dp[i][0] = dp[i-1][0]+grid[i][0] for j in range(1, n): dp[0][j] = dp[0][j-1]+grid[0][j] for i in range(1, m): for j in range(1, n): dp[i][j] += min(dp[i-1][j], dp[i][j-1]) return dp[-1][-1] 优化
class Solution: def minPathSum(self, grid: List[List[int]]) -&gt; int: m = len(grid) n = len(grid[0]) dp = [0]*n for i in range(m): for j in range(n): if i==0: # 第一行初始化 dp[j] = dp[j-1] + grid[i][j] else: dp[j] = min(dp[j], dp[j-1]) + grid[i][j] return dp[-1] 图论 数学、位运算 字符串 string.capitalize() 第一个字符大写 string.count(str, beg=0, end=len(string)) 返回 str 在 string 里面出现的次数，beg 、 end 指定范围 string.endswith(obj, beg=0, end=len(string)) 检查字符串是否以 obj 结束 string.find(str, beg=0, end=len(string)) 返回str开始的索引值，否则返回-1 string.index(str, beg=0, end=len(string)) 跟find()方法一样，只不过如果str不在 string中会报一个异常. string.isalnum() string 至少有一个字符并且所有字符都是字母或数字则返回 True string.isalpha() string 至少有一个字符并且所有字符都是字母则返回 True string.isdecimal() string 只包含十进制数字则返回 True string.isdigit() string 只包含数字则返回 True string.islower() string 都是小写，则返回 True string.isnumeric() string 中只包含数字字符，则返回 Tru string.isspace() string 中只包含空格，则返回 True string.istitle() string 是标题化的(见 title())则返回 True string.isupper() string 都是大写，则返回 True string.join(seq) 以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串 string.lower() 转换 string 中所有大写字符为小写. string.lstrip() 截掉 string 左边的空格 max(str) 返回字符串 str 中最大的字母。 min(str) 返回字符串 str 中最小的字母。 string.replace(str1, str2, num=string.count(str1)) 把 string 中的 str1 替换成 str2,替换不超过 num 次. string.split(str=&#34;&#34;, num=string.count(str)) 以 str 为分隔符切片 string，仅分隔 num+ 个子字符串 string.startswith(obj, beg=0,end=len(string)) 检查字符串是否是以 obj 开头，是则返回 True string.strip([obj]) 在 string 上执行 lstrip()和 rstrip() string.swapcase() 翻转 string 中的大小写 string.title() 返回&#34;标题化&#34;的 string,就是说所有单词都是以大写开始，其余字母均为小写(见 istitle()) string.translate(str, del=&#34;&#34;) 根据 str 给出的表(包含 256 个字符)转换 string 的字符,要过滤掉的字符放到 del 参数中 string.upper() 转换 string 中的小写字母为大写 【125. 验证回文串]( https://link.zhihu.com/?target=https%3A//leetcode-cn.com/problems/valid-palindrome/ ) 给定一个字符串，验证它是否是回文串，只考虑字母和数字字符，可以忽略字母的大小写。 说明：本题中，我们将空字符串定义为有效的回文串。 输入: &ldquo;A man, a plan, a canal: Panama&rdquo; 输出: true
逆字符串
去除非文字，全转成小写，和自身逆对比
class Solution: def isPalindrome(self, s: str) -&gt; bool: alnum = &#39;&#39;.join(w.lower() for w in s if w.isalnum()) return alnum == alnum[::-1] 使用双指针。初始时，左右指针分别指向两侧，随后我们不断地将这两个指针相向移动，每次移动一步，并判断这两个指针指向的字符是否相同。当这两个指针相遇时，就说明是回文串。 class Solution: def isPalindrome(self, s: str) -&gt; bool: pre = 0 nex = len(s) - 1 while pre &lt; nex: # 找到下一个字母或数字 while pre &lt; nex and not s[pre].isalnum(): # 忽略非字母数字 pre += 1 while pre &lt; nex and not s[nex].isalnum(): # 忽略非字母或数字 nex -= 1 # 判断 if pre &lt; nex: if s[pre].lower() != s[nex].lower(): # 忽略大小写 return False pre += 1 nex -= 1 return True 131. 分割回文串 给定一个字符串 s，将 s 分割成一些子串，使每个子串都是回文串。 返回 s 所有可能的分割方案。 示例: 输入: &ldquo;aab&rdquo; 输出: [ [&ldquo;aa&rdquo;,&ldquo;b&rdquo;], [&ldquo;a&rdquo;,&ldquo;a&rdquo;,&ldquo;b&rdquo;] ]
139. 单词拆分 140. 单词拆分 II 208. 实现 Trie (前缀树) 实现一个 Trie (前缀树)，包含 insert, search, 和 startsWith 这三个操作。
Trie 是一颗非典型的多叉树模型 一般的多叉树的结点是这样的：
struct TreeNode { VALUETYPE value; //结点值 TreeNode* children[NUM]; //指向孩子结点 }; 而 Trie 的结点是这样的(假设只包含&rsquo;a&rsquo;~&lsquo;z&rsquo;中的字符)：
struct TrieNode { bool isEnd; //该结点是否是一个串的结束 TrieNode* next[26]; //字母映射表 }; 这时字母映射表next 的妙用就体现了，TrieNode* next[26]中保存了对当前结点而言下一个可能出现的所有字符的链接，因此我们可以通过一个父结点来预知它所有子结点的值：
示例: Trie trie = new Trie(); trie.insert(&ldquo;apple&rdquo;); trie.search(&ldquo;apple&rdquo;); // 返回 true trie.search(&ldquo;app&rdquo;); // 返回 false trie.startsWith(&ldquo;app&rdquo;); // 返回 true trie.insert(&ldquo;app&rdquo;);
trie.search(&ldquo;app&rdquo;); // 返回 true
说明: 你可以假设所有的输入都是由小写字母 a-z 构成的。 保证所有输入均为非空字符串。
212. 单词搜索 II 给定一个二维网格 board 和一个字典中的单词列表 words，找出所有同时在二维网格和字典中出现的单词。 单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母在一个单词中不允许被重复使用。
输入: words = [&ldquo;oath&rdquo;,&ldquo;pea&rdquo;,&ldquo;eat&rdquo;,&ldquo;rain&rdquo;] board = [ [&lsquo;o&rsquo;,&lsquo;a&rsquo;,&lsquo;a&rsquo;,&rsquo;n&rsquo;], [&rsquo;e&rsquo;, &rsquo;t&rsquo;, &lsquo;a&rsquo;, &rsquo;e&rsquo;], [&lsquo;i&rsquo;, &lsquo;h&rsquo;, &lsquo;k&rsquo;, &lsquo;r&rsquo;], [&lsquo;i&rsquo;, &lsquo;f&rsquo;, &rsquo;l&rsquo;, &lsquo;v&rsquo;] ] 输出: [&ldquo;eat&rdquo;,&ldquo;oath&rdquo;]
242. 有效的字母异位词 给定两个字符串 s 和 t ，编写一个函数来判断 t 是否是 s 的字母异位词。 输入: s = &ldquo;anagram&rdquo;, t = &ldquo;nagaram&rdquo; 输出: true
逐个去除b里的a中字符
class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: if len(s) != len(t): return False s = list(s) t = list(t) for i in s: try: t.remove(i) except ValueError as e: return False return True 第一次循环哈希表记录，第二次循环删去哈希表记录，最后统计哈希表每个值是否都为0
class Solution: def isAnagram(self, s: str, t: str) -&gt; bool: if len(s) != len(t): return False word_count = {} # 建立哈希表 for i in s: if i not in word_count: word_count[i] = 1 else: word_count[i] += 1 # 清空哈希表 for i in t: if i in word_count: word_count[i] -= 1 else: return False # all([]) ture any([]) false return not any(list(word_count.values())) 387. 字符串中的第一个唯一字符 给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。
哈希表记录每个字符出现次数
class Solution: def firstUniqChar(self, s: str) -&gt; int: # count = collections.Counter(s) word_count = {} # 建立哈希表 for i in s: if i not in word_count: word_count[i] = 1 else: word_count[i] +=1 # 根据哈希表判断出现次数 for index, word in enumerate(s): if word_count[word] == 1: return index return -1 344. 反转字符串 编写一个函数，其作用是将输入的字符串反转过来。输入字符串以字符数组 char[] 的形式给出。
# s[:]=s[::-1] # 双指针 l,r=0,len(s)-1 while l&lt;r: s[l],s[r]=s[r],s[l] l+=1 r-=1 3.无重复字符的最长子串长度 示例 1: 输入: s = &ldquo;abcabcbb&rdquo; 输出: 3 解释: 因为无重复字符的最长子串是 &ldquo;abc&rdquo;，所以其长度为 3。
使用数组作为滑动窗口
class Solution: def lengthOfLongestSubstring(self, s: str) -&gt; int: if not s: return 0 substr = [] max_len = 0 for word in s: if word not in substr: # 扩展窗口 substr.append(word) else: # 从窗口中移除重复字符及之前的字符串部分 substr = substr[substr.index(word)+1:] # 扩展窗口 substr.append(word) max_len = max(max_len, len(substr)) return max_len 法1中容器的伸缩涉及内存分配,所以方法2换成位置指针省掉了内存分配
直观的滑动窗口方法需要维护数组的增删，实际上比较耗时。使用双指针（索引），记录滑动窗口起始和结束的索引值，可以减除数组增删操作，提高效率，使用指针位移以及从原数组中截取，代替原来的窗口元素增删操作
def lengthOfLongestSubstring(self, s: str) -&gt; int: # 字符串为空则返回零 if not s: return 0 max_len = 0 left_index, right_index = 0, 0 # 双指针 for word in s: # 如果字符不在滑动窗口中，则直接扩展窗口 if word not in s[left_index:right_index]: # 右指针右移一位 right_index += 1 else: # 左指针右移 word在substr中的索引 位 left_index += s[left_index:right_index].index(word) + 1 # 右指针右移一位 right_index += 1 max_len = max(right_index - left_index, max_len) return max_len Hash（字典），滑动窗口，双指针 使用字典记录任意字符最近的索引值，字典查询时间复杂度为O(1)，相比数组查询，效率更高 该算法的难点在于理解word_index[word] &gt; ignore_end_index如果不大于说明word已经被丢弃；大于说明word未被丢弃需要，更新ignore_end_index
def lengthOfLongestSubstring(self, s: str) -&gt; int: ignore_end_index = -1 # 指向子串左边一个字符，即丢弃的子串的尾部， 初始值为 -1，还没有开始移动 max_len = 0 # 记录最大的长度 word_index = {} # 滑动窗口，任意字符最后出现位置的索引 for index, word in enumerate(s): # 如果 word出现过 且 最近一次出现的索引大于ignore_end，意味着需要丢弃这个词前面的部分 # 如果不大于说明word已经被丢弃；大于说明word未被丢弃需要，更新ignore_end_index if word in word_index and word_index[word] &gt; ignore_end_index: ignore_end_index = word_index[word] # 新的子串开始 word_index[word] = index # 更新word的索引 else: # word未出现过 word_index[word] = index # 子串变长 max_len = max(max_len, index - ignore_end_index) # 更新最大长度 return max_len ]]></content></entry><entry><title>面试-ML</title><url>/post/%E9%9D%A2%E8%AF%95-ml/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[ 激活函数 需要激活函数来引入非线性因素，使得神经网络可以任意逼近任何非线性函数。
softmax函数，又称**归一化指数函数。**它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。
范数 什么是最小二乘法？ 最小二乘的核心思想&mdash;通过最小化误差的平方和，使得拟合对象无限接近目标对象.
常见的距离度量公式有那些？ Minkowski(闵可夫斯基距离) &mdash; P=1:曼哈顿距离 &mdash; P=2:欧式距离 &mdash; P无穷:切比雪夫距离
夹角余弦相似度
KL距离(相对熵)
杰卡德相似度系数(集合)
Pearson相关系数ρ(距离= 1-ρ)
8.缺失值处理 删除：缺失数据太多
不处理：针对类似XGBoost等树模型
插值：统计量：均值、众数、中位数、
补全：感知压缩补全、最邻近补全、矩阵补全、建模预测、多重插补
分箱：（缺失值一个箱）
对于有缺失值的数据在经过缺失值处理后：
(1) 数据量很小，用
(2) 数据量适中或者较大，用树模型，优先 xgboost
(3) 数据量较大，也可以用神经网络
(4) 避免使用距离度量相关的模型，如KNN和SVM
异常值处理 箱线图(没有对数据作任何限制性要求)
3-$\sigma$(Sigma)(符合正态分布）
BOX-COX转换（处理有偏分布）
长尾截断
异常值检测：聚类、k近邻、One Class SVM、Isolation Forest
关于高势集特征model，也就是类别中取值个数非常多的， 一般可以使用聚类的方式，然后独热
随机森林、朴素贝叶斯、KNN，对异常值、缺失值不敏感
数据分桶 等频分桶：区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。 等距分桶：从最小值到最大值之间,均分为 N 等份； Best-KS分桶：类似利用基尼指数进行二分类； 卡方分桶：自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。
为什么要做数据分桶呢？ 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展； 离散后的特征对异常值更具鲁棒性，如 age&gt;30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合； 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
特征选择 过滤式（filter）： 概述：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。然后再训练学习器。
方法： Relief、方差选择、相关系数、卡方检验、互信息
数值型特征，方差很小的特征可不要 分类特征,取值个数高度偏斜的那种可以先去掉
连续数据，正态分布，线性关系，用pearson相关系数是最恰当，当然用spearman相关系数也可以，效率没有pearson相关系数高。上述任一条件不满足，就用spearman相关系数，不能用pearson相关系数。
两个定序测量数据（顺序变量）之间也用spearman相关系数，不能用pearson相关系数。 Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。
卡方检验一般是检查离散变量与离散变量的相关性，当然离散变量的相关性信息增益和信息增益比也是不错的选择（可以通过决策树模型来评估来看）
互信息，利用互信息从信息熵的角度分析相关性
包裹式（wrapper）： 概述：直接把最终将要使用的学习器的性能作为特征子集的评价准则，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
方法：LVM（Las Vegas Wrapper）递归特征消除算法, 基于机器学习模型的特征排序
优缺点： 从最终学习器的性能来看，包裹式比过滤式更好； 计算开销通常比过滤式特征选择要大得多。
嵌入式（embedding）： 概述：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择。先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。
方法：LR+L1、决策树、lasso回归
使用L1正则化的模型建叫做Lasso回归（权值稀疏），使用L2正则化的模型叫做岭回归（权值小）。
学习率 α的取值要合适，太小太慢，太大震荡。 选取α的经验，从……0.001—&gt;0.01—&gt;0.1—&gt;1… 参数的更新公式为： $𝒘_{𝒏+𝟏} = 𝒘_𝒏 − 𝒍𝒆𝒂𝒓𝒏𝒊𝒏𝒈_𝒓𝒂𝒕𝒆𝛁$ ######指数衰减学习率： 学习率随着训练轮数变化而动态更新 学习率计算公式如下：$$decayed_learning_rate=LEARNING_RATE_BASE*decay_rate^{(global_step/decay_steps)}$$
什么是判别式模型和生成式模型 监督学习的任务是学习一个模型，对给定的输入预测相应的输出，这个模型的一般形式为一个决策函数或一个条件概率分布（后验概率）： $Y=f(X)/P(Y|X)) $ 决策函数：输入 X 返回 Y；其中 Y 与一个阈值比较，然后根据比较结果判定 X 的类别 条件概率分布：输入 X 返回 X 属于每个类别的概率；将其中概率最大的作为 X 所属的类别
假设我们有训练数据$(X,Y)$，$X$是属性集合，$Y$是类别标记。这时来了一个新的样本$x$，我们想要预测它的类别$y$,我们最终的目的是求得最大的条件概率$P(y|x)$作为新样本的分类。
判别式模型 (Discriminative Model)： 直接学习决策函数或者条件概率分布，直观来说，学习的是类别之间的最优分隔面，反映的是不同类数据之间的差异，无法反映训练数据本身的特性，反映的是不同类数据之间的差异。 常见判别模型有：线性回归、决策树、支持向量机SVM、k近邻、神经网络等；
生成式模型 (Generative Model)： 一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。比如说类别标签有｛猫，狗，猪｝，那首先根据猫的特征学习出一个猫的模型，再根据狗的特征学习出狗的模型，之后分别计算新样本$x$ 跟三个类别的联合概率 $P(y，x)$ ，然后根据贝叶斯公式：
$ P(y|x) = \frac{P(x, y)}{P(x)}$ 分别计算$P(y|x)$，选择三类中最大的$P(y|x)$作为样本的分类。 常见生成式模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等；
不管是生成式模型还是判别式模型，它们最终的判断依据都是条件概率$P(y|x)$，但是**生成式模型先计算了联合概率$P(y，x)$，再由贝叶斯公式计算得到条件概率。**因此，生成式模型可以体现更多数据本身的分布信息，其普适性更广。判别式模型更直接，目标性更强。
判别模型简单，准确率更高，不能反映训练数据本身的特性
由生成式模型可以产生判别式模型，但是由判别式模式没法形成生成式模型，当存在“隐变量”时，只能使用生成模型（隐变量：当我们找不到引起某一现象的原因时，就把这个在起作用，但无法确定的因素，叫“隐变量)
介绍svm，介绍xgb 从统计谈到了机器学习，从机器学习聊到了nlp的attention 讲一下核方法 讲一下svm推倒 AB测试，统计学相关 ,假设检验 解释Markov Chain（简历写了学过随机过程） 深拷贝和浅拷贝的区别 可变对象：可以修改的对象，包括列表、字典、集合
在浅拷贝时，拷贝出来的新对象的地址和原对象是不一样的，但是新对象里面的可变元素（如列表）的地址和原对象里的可变元素的地址是相同的，也就是说浅拷贝它拷贝的是浅层次的数据结构（不可变元素），对象里的可变元素作为深层次的数据结构并没有被拷贝到新地址里面去，而是和原对象里的可变元素指向同一个地址，所以在新对象或原对象里对这个可变元素做修改时，两个对象是同时改变的，但是深拷贝不会这样
赋值： 值相等，地址相等
copy浅拷贝：值相等，地址不相等
deepcopy深拷贝：值相等，地址不相等
列表和元组的区别 列表是动态数组，它们可变且可以重设长度（改变其内部元素的个数）。
元组是静态数组，它们不可变，且其内部数据一旦创建便无法改变。 但是我们可以将两个元组合并成一个新元组。
元组缓存于Python运行时环境，这意味着我们每次使用元组时无须访问内核去分配内存。
Python是一门垃圾收集语言，这意味着当一个变量不再被使用时，Python会将该变量使用的内存释放回操作系统，以供其他程序（变量）使用。然而，对于长度为1~20的元组，即使它们不在被使用，它们的空间也不会立刻还给系统，而是留待未来使用。这意味着当未来需要一个同样大小的新的元组时，我们不再需要向操作系统申请一块内存来存放数据，因为我们已经有了预留的空间。
工作表，制作一个表 仪表盘，放多个表 故事，ppt
甘特图,可视化进度
[图片上传失败&hellip;(image-e2d2b5-1602807317823)]
MODE()众数
quartile分位数
stdev 标准差（standard deviation）variance方差
异常值检测：
有哪些改善模型的思路 **数据角度 **
增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。
模型角度
模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。
调参优化角度
如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选取、学习率的策略以及如何配置正则和损失函数等等。
训练角度
在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。
]]></content></entry><entry><title>面试-mysql</title><url>/post/%E9%9D%A2%E8%AF%95-mysql/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[[TOC]
安装 win https://dev.mysql.com/downloads/mysql/ http://mirrors.sohu.com/mysql/MySQL-8.0/ 配置环境变量path 新建一个my.ini 用记事本打开
[mysqld] # 设置mysql的安装目录 basedir=D:\\software\\java\\mysql-5.7.28-winx64 # 切记此处一定要用双斜杠\\，单斜杠这里会出错。 # 设置mysql数据库的数据的存放目录 datadir=D:\\software\\java\\mysql-5.7.28-winx64\\Data # 此处同上 # 允许最大连接数 max_connections=200 # 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统 max_connect_errors=10 # 服务端使用的字符集默认为UTF8 character-set-server=utf8 # 创建新表时将使用的默认存储引擎 default-storage-engine=INNODB # 默认使用“mysql_native_password”插件认证 default_authentication_plugin=mysql_native_password [mysql] # 设置mysql客户端默认字符集 default-character-set=utf8 [client] # 设置mysql客户端连接服务端时默认使用的端口 port=3306 default-character-set=utf8 管理员权限下运行cmd
mysqld -install # 执行初始化代码（会在根目录创建data文件夹，并创建root用户） mysqld --initialize-insecure --user=mysql # 启动mysql服务 net start mysql # 关闭服务 net stop mysql 绿色版
------------在D:\mysql\创建my.ini -----------zip版本不带默认的数据库文件，需进行数据库初始化，使用CMD管理员打开并 cd d:\mysql\bin mysqld --defaults-file=d:\mysql\my.ini --initialize --console ！！！会创建默认数据库并设置默认的root密码，localhost：后面为默认的密码。 -----------初始化完成之后，需要启动mysql服务，输入一下命令: &#34;d:\mysql\bin\mysqld&#34; --console -----------现在如果关闭了窗口，mysql服务就停止了，不适用与服务器环境，所以需要将mysql作为windows服务进行开机启动。先停止mysql。 &#34;d:\mysql\bin\mysqld&#34; -u root shutdown -----------安装为windows服务，可以在install后面指定服务名，如果涉及一个服务器安装多个服务，可以在后面写mysql5或者mysql8 cd d:\mysql\bin mysqld --install -----------将d:\mysql\bin 目录增加到系统环境变量path -----------允许所有计算机远程链接mysql服务器，执行下面命令后可以从其他服务器远程mysql啦，程序也可以链接了。 use mysql update user set host = &#39;%&#39; where user = &#39;root&#39;; flush privileges; centos 下载命令： wget https://dev.mysql.com/get/mysql80-community-release-el7-2.noarch.rpm 用 yum 命令安装下载好的 rpm 包. yum -y install mysql80-community-release-el7-2.noarch.rpm 安装 MySQL 服务器. yum -y install mysql-community-server 启动 MySQL systemctl start mysqld.service 查看 MySQL 运行状态, Active 后面的状态代表启功服务后为 active (running), 停止后为 inactive (dead), 运行状态如图： systemctl status mysqld.service 重新启动 Mysql 和停止 Mysql 的命令如下： service mysqld restart #重新启动 Mysql systemctl stop mysqld.service #停止 Mysql 此时 MySQL 已经开始正常运行, 不过要想进入 MySQL 还得先找出此时 root 用户的密码, 通过如下命令可以在日志文件中找出密码： 为了加强安全性, MySQL8.0 为 root 用户随机生成了一个密码, 在 error log 中, 关于 error log 的位置, 如果安装的是 RPM 包, 则默认是/var/log/mysqld.log. 只有启动过一次 mysql 才可以查看临时密码 使用命令: grep &#39;temporary password&#39; /var/log/mysqld.log 登录 root 用户 mysql -u root -p 修改密码为&#34;123&#34;, 注意结尾要有分号, 表示语句的结束. ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123&#39;; MySQL 完整的初始密码规则可以通过如下命令查看： SHOW VARIABLES LIKE &#39;validate_password%&#39; 如果想要设置简单的密码必须要修改约束, 修改两个全局参数： validate_password_policy代表密码策略, 默认是 1：符合长度, 且必须含有数字, 小写或大写字母, 特殊字符. 设置为 0 判断密码的标准就基于密码的长度了. 一定要先修改两个参数再修改密码 set global validate_password.policy=0; validate_password_length代表密码长度, 最小值为 4 set global validate_password.length=4; 但此时还有一个问题, 就是因为安装了 Yum Repository, 以后每次 yum 操作都会自动更新, 如不需要更新, 可以把这个 yum 卸载掉： [root@localhost ~]# yum -y remove mysql80-community-release-el7-2.noarch 设置表名为大小写不敏感: 1.用root帐号登录, 使用命令 systemctl stop mysqld.service 停止MySQL数据库服务，修改vi /etc/my.cnf，在[mysqld]下面添加 lower_case_table_names=1 这里的参数 0 表示区分大小写，1 表示不区分大小写. 2.做好数据备份，然后使用下述命令删除数据库数据(删除后, 数据库将恢复到刚安装的状态) rm -rf /var/lib/mysql 3.重启数据库 ,此时数据库恢复到初始状态. service mysql start 4.重复安装时的操作, 查找临时密码 grep &#39;temporary password&#39; /var/log/mysqld.log 5.修改密码. 密码8位以上, 大小写符号数据. 如想要使用简单密码, 需按照上述安装流程中的步骤进行操作. ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;****&#39;;update user set host = &#34;%&#34; where user=&#39;root&#39;; 6.刷新MySQL的系统权限相关表 FLUSH PRIVILEGES; 此时, MySQL的表名的大小写不再敏感. 设置表名为大小写不敏感: 1.用root帐号登录后, 使用命令 systemctl stop mysqld.service 停止MySQL数据库服务，修改vi /etc/my.cnf，在[mysqld]下面添加 lower_case_table_names=1 这里的参数 0 表示区分大小写，1 表示不区分大小写. 2.做好数据备份，然后使用下述命令删除数据库数据(删除后, 数据库将恢复到刚安装的状态) rm -rf /var/lib/mysql 3.重启数据库 ,此时数据库恢复到初始状态. service mysql start 4.重复安装时的操作, 查找临时密码 grep &#39;temporary password&#39; /var/log/mysqld.log 5.修改密码. 密码8位以上, 大小写符号数据. 如想要使用简单密码, 需按照上述安装流程中的步骤进行操作. ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;****&#39;;update user set host = &#34;%&#34; where user=&#39;root&#39;; 6.刷新MySQL的系统权限相关表 FLUSH PRIVILEGES; 此时, MySQL的表名的大小写不再敏感. ubuntu sudo apt-get install mysql-server #sudo apt-get install mysql-client 配置远程连接
1.将bind-address = 127.0.0.1注释掉 sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf 2.登录进数据库： mysql -uroot -p123456 3.然后，切换到数据库mysql。SQL如下： use mysql; 4.删除匿名用户。SQL如下： delete from user where user=&#39;&#39;; 5.给root授予在任意主机（%）访问任意数据库的所有权限。SQL语句如下： create user &#39;root&#39;@&#39;%&#39; identified by &#39;123456&#39;; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39;; ALTER USER &#39;root&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123456&#39;; 6.退出数据库 exit 7.重启数据库： sudo service mysql restart 删除 mysql
sudo apt-get autoremove --purge mysql-server sudo apt-get remove mysql-server sudo apt-get autoremove mysql-server sudo apt-get remove mysql-common 清理残留数据
dpkg -l |grep ^rc|awk &#39;{print $2}&#39; |sudo xargs dpkg -P 进入mysql
sudo mysql -uroot -p 配置 MySQL 的管理员密码：
sudo mysqladmin -u root password newpassword 安装MySQL-workbench
sudo apt-get install mysql-workbench 一旦安装完成，MySQL 服务器应该自动启动。您可以在终端提示符后运行以下命令来检查 MySQL 服务器是否正在运行：
sudo netstat -tap | grep mysql 当您运行该命令时，您可以看到类似下面的行： tcp 0 0 localhost.localdomain:mysql : LISTEN - 如果服务器不能正常运行，您可以通过下列命令启动它：
sudo /etc/init.d/mysql restart 重置密码 net stop mysql，停止MySQL服务， 开启跳过密码验证登录的MySQL服务 mysqld --console --skip-grant-tables --shared-memory 再打开一个新的cmd，无密码登录MySQL mysql -u root -p 密码置为空 use mysql update user set authentication_string=&#39;&#39; where user=&#39;root&#39;; 出mysql，执行命令： quit 关闭以-console --skip-grant-tables --shared-memory 启动的MySQL服务， 打开命令框，输入：net start mysql 启动MySQL服务，一管理员的身份运行cmd。 步骤4密码已经置空，所以无密码状态登录MySQL，输入登录命令：mysql -u root -p 利用上一篇博客中更改密码的命令，成功修改密码 ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123&#39;; DBMS的种类 用来管理数据库的计算机系统称为数据库管理系统（Database Management System，DBMS）。
DBMS 主要通过数据的保存格式（数据库的种类）来进行分类，现阶段主要有以下 5 种类型.
层次数据库（Hierarchical Database，HDB）
关系数据库（Relational Database，RDB）
Oracle Database：甲骨文公司的RDBMS SQL Server：微软公司的RDBMS DB2：IBM公司的RDBMS PostgreSQL：开源的RDBMS MySQL：开源的RDBMS 如上是5种具有代表性的RDBMS，其特点是由行和列组成的二维表来管理数据，这种类型的 DBMS 称为关系数据库管理系统（Relational Database Management System，RDBMS）。
面向对象数据库（Object Oriented Database，OODB）
XML数据库（XML Database，XMLDB）
键值存储系统（Key-Value Store，KVS），举例：MongoDB
基本书写规则 SQL语句要以分号（ ; ）结尾 SQL 不区分关键字的大小写，但是插入到表中的数据是区分大小写的 win 系统默认不区分表名及字段名的大小写 linux / mac 默认严格区分表名及字段名的大小写 * 本教程已统一调整表名及字段名的为小写，以方便初学者学习使用。 注释是SQL语句中用来标识说明或者注意事项的部分。分为1行注释&quot;&ndash; &ldquo;和多行注释两种&rdquo;/* */&quot;。 数据类型的指定 数据库创建的表，所有的列都必须指定数据类型，每一列都不能存储与该列数据类型不符的数据。
四种最基本的数据类型
INTEGER 型 用来指定存储整数的列的数据类型（数字型），不能存储小数。
CHAR 型 用来存储定长字符串，当列中存储的字符串长度达不到最大长度的时候，使用半角空格进行补足，由于会浪费存储空间，所以一般不使用。
VARCHAR 型 用来存储可变长度字符串，定长字符串在字符数未达到最大长度时会用半角空格补足，但可变长字符串不同，即使字符数未达到最大长度，也不会用半角空格补足。
DATE 型 用来指定存储日期（年月日）的列的数据类型（日期型）。
约束的设置 约束是除了数据类型之外，对列中存储的数据进行限制或者追加条件的功能。
NOT NULL是非空约束，即该列必须输入数据。
PRIMARY KEY是主键约束，代表该列是唯一值，可以通过该列取出特定的行的数据。
执行顺序 在加入窗口函数的基础上SQL的执行顺序也会发生变化，具体的执行顺序如下（window就是窗口函数） DDL DDL（Data Definition Language，数据定义语言） 用来创建或者删除存储数据用的数据库以及数据库中的表等对象。DDL 包含以下几种指令。
CREATE ： 创建数据库和表等对象 DROP ： 删除数据库和表等对象 ALTER ： 修改数据库和表等对象的结构 DML DML（Data Manipulation Language，数据操纵语言） 用来查询或者变更表中的记录。DML 包含以下几种指令。
SELECT ：查询表中的数据 INSERT ：向表中插入新数据 UPDATE ：更新表中的数据 DELETE ：删除表中的数据 DCL DCL（Data Control Language，数据控制语言） 用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限操作数据库中的对象（数据库表等）进行设定。DCL 包含以下几种指令。
COMMIT ： 确认对数据库中的数据进行的变更 ROLLBACK ： 取消对数据库中的数据进行的变更 GRANT ： 赋予用户操作权限 REVOKE ： 取消用户的操作权限 增删改查库 增加(CREATE) CREATE DATABASE &lt; 数据库名称 &gt; default character utf8; 删除(DROP) DROP DATABASE 库名 ; 修改(ALTER) ALTER DATABASE 库名 default character gbk; 查询(SHOW) SHOW DATABASE 选择数据库 USE 库名 ; 增删改查表 新建表(CREATE) CREATE TABLE &lt; 表名 &gt; ( &lt; 列名 1&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 2&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 3&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 列名 4&gt; &lt; 数据类型 &gt; &lt; 该列所需约束 &gt; , &lt; 该表的约束 1&gt; , &lt; 该表的约束 2&gt; ,……); CREATE TABLE 表名 ( id int(10) unsigned NOT NULL COMMENT &#39;Id&#39;, username varchar(64) NOT NULL DEFAULT &#39;default&#39; COMMENT &#39;用户名&#39;, password varchar(64) NOT NULL DEFAULT &#39;default&#39; COMMENT &#39;密码&#39;, email varchar(64) NOT NULL DEFAULT &#39;default&#39; COMMENT &#39;邮箱&#39; ) COMMENT=&#39;用户表&#39;; 复制表
CREATE TABLE 新表 AS SELECT * FROM 旧表 删除(DROP) ALTER TABLE 语句和 DROP TABLE 语句一样，执行之后无法恢复。
DROP TABLE user; 修改表名称(ALTER RENAME) ALTER TABLE user RENAME user_new; 增删改查字段 添加字段(ALTER ADD COLUMN) ALTER TABLE user ADD COLUMN age int(3); 删除字段(ALTER DROP COLUMN) ALTER TABLE user DROP COLUMN age; 修改字段类型(ALTER MODIFY) ALTER TABLE user MODIFY 字段名 新的字段类型; mysql 设置字段 not null 变成可以null
ALTER TABLE 表名 MODIFY 字段名 VARCHAR(20) DEFAULT NULL 修改字段名称(ALTER CHANGE) ALTER TABLE user CHANGE 旧字段名 新字段名 字段类型; 显示表字段信息 DESC user; 管理数据 添加主键(ALTER ADD PRIMARY KEY) ALTER TABLE user ADD PRIMARY KEY (id); 删除主键(ALTER DROP PRIMARY KEY)
ALTER TABLE user DROP PRIMARY KEY; 查询表的大小：
use information_schema; select data_length,index_length from tables where table_schema=库 and table_name = 表 ; 添加数据(INSERT INTO) 插入完整的行
INSERT INTO user VALUES (10, &#39;root&#39;, &#39;root&#39;, &#39;xxxx@163.com&#39;); -- 多行INSERT （ DB2、SQL、SQL Server、 PostgreSQL 和 MySQL多行插入） INSERT INTO productins VALUES (&#39;0002&#39;, &#39;打孔器&#39;, &#39;办公用品&#39;, 500, 320, &#39;2009-09-11&#39;), (&#39;0003&#39;, &#39;运动T恤&#39;, &#39;衣服&#39;, 4000, 2800, NULL), (&#39;0004&#39;, &#39;菜刀&#39;, &#39;厨房用具&#39;, 3000, 2800, &#39;2009-09-20&#39;); -- Oracle中的多行INSERT INSERT ALL INTO productins VALUES (&#39;0002&#39;, &#39;打孔器&#39;, &#39;办公用品&#39;, 500, 320, &#39;2009-09-11&#39;) INTO productins VALUES (&#39;0003&#39;, &#39;运动T恤&#39;, &#39;衣服&#39;, 4000, 2800, NULL) INTO productins VALUES (&#39;0004&#39;, &#39;菜刀&#39;, &#39;厨房用具&#39;, 3000, 2800, &#39;2009-09-20&#39;) SELECT * FROM DUAL; -- DUAL是Oracle特有（安装时的必选项）的一种临时表A。因此“SELECT *FROM DUAL” 部分也只是临时性的，并没有实际意义。 插入行的一部分
INSERT INTO user(username, password, email) VALUES (&#39;admin&#39;, &#39;admin&#39;, &#39;xxxx@163.com&#39;); 插入查询出来的数据
INSERT INTO user(字段) SELECT 字段 FROM account; SELECT * INTO new_表名 FROM 表名 // MySQL 数据库不支持 SELECT ... INTO 语句，但支持 INSERT INTO ... SELECT 。 INSERT INTO p1 SELECT * FROM product; 删除(DELETE FROM) DELETE FROM 1)可以带条件删除2）只能删除表的数据，不能删除表的约束3)删除的数据可以回滚（事务）
DELETE FROM user WHERE username=&#39;robot&#39;; truncate 1）不能带条件删除 2）可以删除表的数据，也可以删除表的约束 3）不能回滚
TRUNCATE TABLE user; 总结： 1、在速度上，一般来说，drop&gt; truncate &gt; delete。 2、在使用drop和truncate时一定要注意，虽然可以恢复，但为了减少麻烦，还是要慎重。 3、如果想删除部分数据用delete，注意带上where子句，回滚段要足够大； 如果想删除表，当然用drop； 如果想保留表而将所有数据删除，如果和事务无关，用truncate即可； 如果和事务有关，或者想触发trigger，还是用delete； 如果是整理表内部的碎片，可以用truncate跟上reuse stroage，再重新导入/插入数据。
希望删除表结构时，用 drop; 希望保留表结构，但要删除所有记录时， 用 truncate; 希望保留表结构，但要删除部分记录时， 用 delete。
修改(UPDATE SET) UPDATE user SET username=&#39;robot&#39;, password=&#39;robot&#39; WHERE username = &#39;root&#39;; -- 合并后的写法 UPDATE product SET sale_price = sale_price * 10, purchase_price = purchase_price / 2 WHERE product_type = &#39;厨房用具&#39;; 查询 所有字段：
select * from 表; 指定字段：
select 字段 from 表; 指定别名：
select 字段1 as 别名 from 表; 合并列：
select (字段1+字段2) as “和” from 表; 去重：
select distinct 字段 from 表; 特殊字符 逻辑 and、 or、in、not in
select * from 表 where 条件1 and/or 条件2 IN (&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) ––使用IN 和 NOT IN 时是无法选取出NULL数据的。
比较 &gt; 、 &lt; 、&gt;= 、 &lt;= 、 =、 !=、 &lt;&gt;、 between and
select * from 表 where 字段&gt;=条件;
模糊 like 、%（替换任意个字符）、 _（替换一个字符）
SELECT * FROM student WHERE NAME LIKE '李%';
谓词 返回值为真值的函数。包括TRUE / FALSE / UNKNOWN。
谓词主要有以下几个：
LIKE
BETWEEN
IS NULL、IS NOT NULL
NULL 只能使用 IS NULL 和 IS NOT NULL 来进行判断。
IN
EXISTS
谓词的作用就是 “判断是否存在满足某种条件的记录”。如果存在这样的记录就返回真（TRUE），如果不存在就返回假（FALSE）。
判断 case when 条件 then 真的操作 else 假的操作 end
if(条件, 真的操作, 假的操作)
CASE WHEN product_type = &#39;衣服&#39; THEN CONCAT(&#39;A ： &#39;,product_type) WHEN product_type = &#39;办公用品&#39; THEN CONCAT(&#39;B ： &#39;,product_type) WHEN product_type = &#39;厨房用具&#39; THEN CONCAT(&#39;C ： &#39;,product_type) ELSE NULL END 聚合函数： 聚合函数的使用前提是结果集已经确定，而WHERE还处于确定结果集的过程中，所以相互矛盾会引发错误。 如果想指定条件，可以在SELECT，HAVING（下面马上会讲）以及ORDER BY子句中使用聚合函数。
sum()、avg() 、 max() 、min() 、 count()
count()
COUNT(常量) 和 COUNT(*)表示的是直接查询符合条件的数据库表的行数。而COUNT(列名)表示的是查询符合条件的列的值不为NULL的行数。
SELECT SUM(servlet) AS 'servlet的总成绩' FROM student; SELECT COUNT(*) FROM student;
-- 计算去除重复数据后的数据行数 SELECT COUNT(DISTINCT product_type) FROM product; concat将A和B按顺序连接在一起的字符串 split(str, regex)将string类型数据按regex提取，分隔后转换为array。 substr（str,0,len) 截取字符串从0位开始的长度为len个字符。 SUBSTRING_INDEX (原始字符串， 分隔符，n)获取原始字符串按照分隔符分割后，第 n 个分隔符之前（或之后）的子字符串，支持正向和反向索引，索引起始值分别为 1 和 -1。 LOCATE()查找某字符在长字符中的位置 LEFT()、RIGHT()左边或者右边的字符 LOWER()、UPPER()转换为小写或者大写 LTRIM()、RTIM()去除左边或者右边的空格 LENGTH()长度 SOUNDEX()转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。
SELECT * FROM mytable WHERE SOUNDEX(col1) = SOUNDEX(&#39;apple&#39;) 常用的日期提取函数包括 year()/month()/day()/hour()/minute()/second()
CURRENT_DATE &ndash; 获取当前日期 CURRENT_TIME &ndash; 当前时间 CURRENT_TIMESTAMP &ndash; 当前日期和时间 EXTRACT(日期元素 FROM 日期)&ndash; 截取日期元素 EXTRACT(YEAR FROM CURRENT_TIMESTAMP) AS year
AddDate()增加一个日期（天、周等） AddTime()增加一个时间（时、分等） CurDate()返回当前日期 CurTime()返回当前时间 Date()返回日期时间的日期部分 to_date(&quot;1970-01-01 00:00:00&quot;)把时间的字符串形式转化为时间类型，再进行后续的计算； datediff(enddate,stratdate) 计算两个时间的时间差（day)； date_sub(stratdate,days) 返回开始日期startdate减少days天后的日期。
SELECT ADDDATE(&#34;2017-06-15&#34;, INTERVAL 10 DAY); date_add(startdate,days) 返回开始日期startdate增加days天后的日期。 Date_Format()返回一个格式化的日期或时间串 date(paidTime).date_format(paidTime，%Y-%m-d%)
CAST（转换前的值 AS 想要转换的数据类型） &ndash; 将NULL转换为其他值
SELECT CAST(&#39;0001&#39; AS SIGNED INTEGER) AS int_col; COALESCE(数据1，数据2，数据3……) &ndash; 返回可变参数 A 中左侧开始第 1个不是NULL的值
SIN()正弦 COS()余弦 TAN()正切 ABS()绝对值 SQRT()平方根 MOD()余数 EXP()指数 PI()圆周率 RAND()随机数 percentile() 百分位函数
分组查询：group by 使用COUNT等聚合函数时，SELECT子句中如果出现列名，只能是GROUP BY子句中指定的列名（也就是聚合键）。
SELECT子句中可以通过AS来指定别名，但在GROUP BY中不能使用别名。因为在DBMS中 ,SELECT子句在GROUP BY子句后执行。
group by 后可加聚合函数，where 后不能加聚合函数
-- 按照商品种类统计数据行数 SELECT product_type, COUNT(*) FROM product GROUP BY product_type; --- error SELECT product_name, COUNT(*) FROM product GROUP BY purchase_price; 分组后筛选： having WHERE子句只能指定记录（行）的条件，而不能用来指定组的条件（例如，“数据行数为 2 行”或者“平均值为 500”等）。
可以使用数字、聚合函数和GROUP BY中指定的列名（聚合键）。
SELECT product_type, COUNT(*) FROM product GROUP BY product_type HAVING COUNT(*) = 2; -- 错误形式（因为product_name不包含在GROUP BY聚合键中） SELECT product_type, COUNT(*) FROM product GROUP BY product_type HAVING product_name = &#39;圆珠笔&#39;; DISTINCT 唯一 想要计算值的种类时，可以在COUNT函数的参数中使用DISTINCT。 SELECT DISTINCT Director FROM movies ASC; 分页查询：limit offset 起始行,查询行数起始行从0开始 把结果集分页，每页3条记录。要获取第1页的记录
SELECT * FROM student LIMIT 3 OFFSET 0; 排序： order by ORDER BY中列名可使用别名 asc: 正序（默认）desc：反序
SELECT * FROM movies ORDER BY Director ASC,Year DESC LIMIT 10 OFFSET 0; 视图 视图是一个虚拟的表，不同于直接操作数据表，视图是依据SELECT语句来创建的，将频繁使用的SELECT语句保存以提高效率。
简单
对于使用视图的用户不需要关心表的结构、关联条件和筛选条件。因为这张虚拟表中保存的就是已经过滤好条件的结果集
安全
视图可以设置权限 , 致使访问视图的用户只能访问他们被允许查询的结果集
数据独立
一旦视图的结构确定了，可以屏蔽表结构变化对用户的影响，源表增加列对视图没有影响；源表修改列名，则可以通过修改视图来解决，不会造成对访问者的影响
CREATE VIEW &lt;视图名称&gt;(&lt;列名1&gt;,...) AS &lt;SELECT语句&gt; -- 创建一个视图。将查询出来的结果保存到这张虚拟表中，查询城市和所属国家 CREATE VIEW city_country (city_id,city_name,cid,country_name) AS SELECT t1.*,t2.country_name FROM city t1,country t2 WHERE t1.cid=t2.id; -- 查询视图。查询这张虚拟表，就等效于查询城市和所属国家 SELECT * FROM city_country; -- 查询所有数据表，视图也会查询出来 SHOW TABLES; 需要注意的是视图名在数据库中需要是唯一的，不能与其他视图和表重名。在创建视图时也尽量使用限制不允许通过视图来修改表
视图不仅可以基于真实表，我们也可以在视图的基础上继续创建视图。虽然在视图上继续创建视图的语法没有错误，但是我们还是应该尽量避免这种操作。这是因为对多数 DBMS 来说， 多重视图会降低 SQL 的性能。
需要注意的是在一般的DBMS中定义视图时不能使用ORDER BY语句。下面这样定义视图是错误的。
CREATE VIEW productsum (product_type, cnt_product) AS SELECT product_type, COUNT(*) FROM product GROUP BY product_type ORDER BY product_type; 为什么不能使用 ORDER BY 子句呢？这是因为视图和表一样，数据行都是没有顺序的。
在 MySQL中视图的定义是允许使用 ORDER BY 语句的，但是若从特定视图进行选择，而该视图使用了自己的 ORDER BY 语句，则视图定义中的 ORDER BY 将被忽略。
修改视图 因为视图是一个虚拟表，所以对视图的操作就是对底层基础表的操作，所以在修改时只有满足底层基本表的定义才能成功修改。
对于一个视图来说，如果包含以下结构的任意一种都是不可以被更新的：
聚合函数 SUM()、MIN()、MAX()、COUNT() 等。 DISTINCT 关键字。 GROUP BY 子句。 HAVING 子句。 UNION 或 UNION ALL 运算符。 FROM 子句中包含多个表。 视图归根结底还是从表派生出来的，因此，如果原表可以更新，那么 视图中的数据也可以更新。反之亦然，如果视图发生了改变，而原表没有进行相应更新的话，就无法保证数据的一致性了。
-- 修改视图2的列名city_id为id ALTER VIEW city_country2 (id,city_name,cid,country_name) AS SELECT t1.*,t2.country_name FROM city t1,country t2 WHERE t1.cid=t2.id; -- 修改视图表中的城市名称北京为北京市 UPDATE city_country SET city_name=&#39;北京市&#39; WHERE city_name=&#39;北京&#39;; 删除视图 DROP VIEW &lt;视图名1&gt; [ , &lt;视图名2&gt; …] -- 注意：需要有相应的权限才能成功删除。 DROP VIEW city_country; 子查询和视图的关系 子查询就是将用来定义视图的 SELECT 语句直接用于 FROM 子句当中。其中AS studentSum可以看作是子查询的名称，而且由于子查询是一次性的，所以子查询不会像视图那样保存在存储介质中， 而是在 SELECT 语句执行之后就消失了。
连接查询（多表查询） 笛卡尔积查询 把两个表中具有相同 主键ID的数据连接起来
单纯的select * from a,b是笛卡尔乘积。比如a表有5条数据，b表有3条数据，那么最后的结果有5*3=15条数据。
但是如果对两个表进行关联:select * from a,b where a.id = b.id 意思就变了，此时就等价于：select * from a inner join b on a.id = b.id。即就是内连接。 INNER JOIN 产生的结果是AB的交集。 FULL [OUTER] JOIN 产生A和B的并集。 LEFT [OUTER] JOIN 产生表A的完全集，而B表中匹配的则有值，没有匹配的则以null值取代。 RIGHT [OUTER] JOIN 产生表B的完全集，而A表中匹配的则有值，没有匹配的则以null值取代
CROSS JOIN 把表A和表B的数据进行一个N*M的组合，即笛卡尔积。
内连接查询 INNER JOIN：只保留两张表中完全匹配的结果集
-- 标准语法 SELECT 列名 FROM 表名1 [INNER] JOIN 表名2 ON 条件; -- 隐式内连接语法 SELECT 列名 FROM 表名1,表名2 WHERE 条件; SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p INNER JOIN Orders o ON p.Id_P=o.Id_P and 1=1　ORDER BY p.LastName 左连接查询 查询左表的全部数据，和左右两张表有交集部分的数据 -- 标准语法 SELECT 列名 FROM 表名1 LEFT [OUTER] JOIN 表名2 ON 条件; SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p LEFT JOIN Orders o ON p.Id_P=o.Id_P ORDER BY p.LastName 右连接 查询右表的全部数据，和左右两张表有交集部分的数据 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p RIGHT JOIN Orders o ON p.Id_P=o.Id_P ORDER BY p.LastName 全连接查询 FULL JOIN ,返回左表和右表中所有的行。
SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p FULL JOIN Orders o ON p.Id_P=o.Id_P ORDER BY p.LastName 子查询 查询语句中嵌套了查询语句。我们就将嵌套查询称为子查询！ -- 结果是单行单列的可以作为条件，使用运算符进行判断！ -- 根据查询出来的最高年龄，查询姓名和年龄 SELECT NAME,age FROM USER WHERE age = (SELECT MAX(age) FROM USER); -- 结果是多行单列的可以作为条件，使用运算符in或not in进行判断！ -- 根据id查询订单 SELECT number,uid FROM orderlist WHERE uid IN (SELECT id FROM USER WHERE NAME=&#39;张三&#39; OR NAME=&#39;李四&#39;); -- 结果是多行多列的可以作为一张虚拟表参与查询！ -- 查询订单表中id大于4的订单信息和所属用户信息 SELECT * FROM USER u,(SELECT * FROM orderlist WHERE id&gt;4) o WHERE u.id=o.uid; 自关联查询 同一张表中有数据关联。可以多次查询这同一个表！ -- 查询所有员工的姓名及其直接上级的姓名，没有上级的员工也需要查询 /* 分析： 员工姓名 employee表 直接上级姓名 employee表 条件：employee.mgr = employee.id 查询左表的全部数据，和左右两张表交集部分数据，使用左外连接 */ SELECT t1.name,	-- 员工姓名 t1.mgr,	-- 上级编号 t2.id,	-- 员工编号 t2.name -- 员工姓名 FROM employee t1 -- 员工表 LEFT OUTER JOIN employee t2 -- 员工表 ON t1.mgr = t2.id; UNION 将两个或更多查询的结果组合起来，并生成一个结果集，通常返回的列名取自第一个查询。
所有查询的列数和列顺序必须相同。每个查询中涉及表的列的数据类型必须相同或兼容。
默认会去除相同行，如果需要保留相同行，使用 UNION ALL。
只能包含一个 ORDER BY 子句，并且必须位于语句的最后。
在一个查询中从不同的表返回结构数据。 对一个表执行多个查询，按一个查询返回数据。 组合查询
SELECT cust_name, cust_contact, cust_email FROM customers WHERE cust_state IN (&#39;IL&#39;, &#39;IN&#39;, &#39;MI&#39;) UNION SELECT cust_name, cust_contact, cust_email FROM customers WHERE cust_name = &#39;Fun4All&#39;; JOIN vs UNION JOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。
UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置）
远程连接 ## 查看当前的用户 USE mysql; SELECT * FROM user; ## 修改密码 @前用户名@后地址（ % 代表可以任意ip访问） mysql ALTER USER &#34;root&#34;@&#34;localhost&#34; IDENTIFIED BY &#34;root&#34;; ## 创建新用户 CREATE USER &#39;new_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;passwd&#39;; ## 给用户赋权限 这里我赋的是全部的权限，*.* 表示数据库库的所有库和表，对应权限存储在mysql.user表中 GRANT ALL ON *.* TO &#39;new_user&#39;@&#39;%&#39;; GRANT SELECT, UPDATE ON day16.employee TO &#39;eric&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;123456&#39;; ## 刷新权限 flush privileges; ## 取消远程控制 update user set host=&#39;localhost&#39; where user=&#39;用户名&#39;; 删除用户 delete from user where user=&quot;用户名&quot; and host='host权限（localhost/%）';
&ndash;开放3306端口&ndash; 1.控制面板—系统和安全—windows防火墙—攻击设置—入栈规则 2.新建规则—选择端口 3.指定开放的端口 4.允许连接，一直点下一步即可 PyMySQL模块 是默认开启MySQL的事务功能的， 因此，进行 &ldquo;增&rdquo;、 &ldquo;删&rdquo;、&ldquo;改&quot;的时候，一定要使用db.commit()提交事务 一定要使用try…except…语句，因为万一没插入成功，其余代码都无法执行。当语句执行不成功， 我们就db.rollback()回滚到操作之前的状态；当语句执行成功，我们就db.commit()提交事务。
import pymysql # 使用pymysql连接上mysql数据库服务器，创建了一个数据库对象； db=pymysql.connect(host=&#39;localhost&#39;,user=&#39;root&#39;, password=&#39;&#39;, port=3306, db=&#39;test&#39;, charset=&#39;utf8&#39;) # 开启mysql的游标功能，创建一个游标对象； cursor = db.cursor() # 建表语句； sql = &#34;&#34;&#34;create table person( id int auto_increment primary key not null, name varchar(10) not null, age int not null) charset=utf8&#34;&#34;&#34; # 执行sql语句； cursor.execute(sql) # 一次性插入一条数据； name = &#34;猪八戒&#34; age = 8000 sql = &#34;&#34;&#34; insert into person(name,age) values (&#34;猪八戒&#34;,8000) &#34;&#34;&#34; try: cursor.execute(sql) db.commit() print(&#34;插入成功&#34;) except: print(&#34;插入失败&#34;) db.rollback() # 要执行的SQL语句； sql = &#34;select * from person&#34; # execute(query, args)：执行单条sql语句，接收的参数为sql语句本身和使用的参数列表，返回值为受影响的行数； # executemany(query, args)：执行单条sql语句，但是重复执行参数列表里的参数，返回值为受影响的行数； cursor.execute(sql) # fetchone()：返回一条结果行； # fetchmany(size)：接收size条返回结果行。如果size的值大于返回的结果行的数量，则会返回cursor.arraysize条数据； # fetchall()：接收全部的返回结果行； data = cursor.fetchone() print(data) #关闭游标 cursor.close() # 关闭数据库连接 db.close() pandas 中的read_sql()方法，将提取到的数据直接转化为DataFrame，进行操作
df1 = pd.read_sql(&#34;select * from student where ssex=&#39;男&#39;&#34;,db) display(df1) df2 = pd.read_sql(&#34;select * from student where ssex=&#39;女&#39;&#34;,db) display(df2) 数据约束（表约束） 约束 说明 PRIMARY KEY 主键约束 PRIMARY KEY AUTO_INCREMENT 主键、自动增长 UNIQUE 唯一约束 NOT NULL 非空约束 FOREIGN KEY 外键约束 FOREIGN KEY ON UPDATE CASCADE 外键级联更新 FOREIGN KEY ON DELETE CASCADE 外键级联删除 主键约束特点 主键约束包含：非空和唯一两个功能 一张表只能有一个列作为主键 -- 删除主键 ALTER TABLE student DROP PRIMARY KEY; -- 添加主键 ALTER TABLE student MODIFY id INT PRIMARY EKY; -- 删除唯一约束 ALTER TABLE student DROP INDEX id; -- 添加唯一约束 ALTER TABLE student MODIFY id INT UNIQUE; -- 删除自动增长 ALTER TABLE student MODIFY id INT; -- 添加自动增长 ALTER TABLE student MODIFY id INT AUTO_INCREMENT; -- 删除非空约束 ALTER TABLE student MODIFY id INT; -- 添加非空约束 ALTER TABLE student MODIFY id INT NOT NULL; 外键约束
让表和表之间产生关系，从而保证数据的准确性！ CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主表主键列名) -- 删除外键 ALTER TABLE orderlist DROP FOREIGN KEY ou_fk1; -- 添加外键约束 ALTER TABLE orderlist ADD CONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id); -- 创建user用户表 CREATE TABLE USER( id INT PRIMARY KEY AUTO_INCREMENT, -- id NAME VARCHAR(20) NOT NULL -- 姓名 ); -- 添加用户数据 INSERT INTO USER VALUES (NULL,&#39;张三&#39;),(NULL,&#39;李四&#39;),(NULL,&#39;王五&#39;); -- 创建orderlist订单表 CREATE TABLE orderlist( id INT PRIMARY KEY AUTO_INCREMENT, -- id number VARCHAR(20) NOT NULL, -- 订单编号 uid INT, -- 外键字段，订单所属用户 CONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id) -- 添加外键约束 ); -- 添加订单数据 INSERT INTO orderlist VALUES (NULL,&#39;hm001&#39;,1),(NULL,&#39;hm002&#39;,1), (NULL,&#39;hm003&#39;,2),(NULL,&#39;hm004&#39;,2), (NULL,&#39;hm005&#39;,3),(NULL,&#39;hm006&#39;,3); -- 添加一个订单，但是没有所属用户。无法添加 INSERT INTO orderlist VALUES (NULL,&#39;hm007&#39;,8); -- 删除王五这个用户，但是订单表中王五还有很多个订单呢。无法删除 DELETE FROM USER WHERE NAME=&#39;王五&#39;; 级联更新和级联删除 当我想把user用户表中的某个用户删掉，我希望该用户所有的订单也随之被删除 当我想把user用户表中的某个用户id修改，我希望订单表中该用户所属的订单用户编号也随之修改 -- 添加外键约束，同时添加级联更新和级联删除 ALTER TABLE orderlist ADD CONSTRAINT ou_fk1 FOREIGN KEY (uid) REFERENCES USER(id) ON UPDATE CASCADE ON DELETE CASCADE; -- 将王五用户的id修改为5 订单表中的uid也随之被修改 UPDATE USER SET id=5 WHERE id=3; -- 将王五用户删除 订单表中该用户所有订单也随之删除 DELETE FROM USER WHERE id=5; 多表设计 一对一 人和身份证！ 在任意一个表建立外键，去关联另外一个表的主键
一对多
一个用户可以有多个订单！。一个分类下可以有多个商品！ 在多的一方，建立外键约束，来关联另一方主键
多对多 学生和课程。一个学生可以选择多个课程，一个课程也可以被多个学生选择！ 需要借助第三张表中间表，中间表至少包含两个列，这两个列作为中间表的外键，分别关联两张表的主键
-- 创建student表 CREATE TABLE student( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(20) ); -- 添加数据 INSERT INTO student VALUES (NULL,&#39;张三&#39;),(NULL,&#39;李四&#39;); -- 创建course表 CREATE TABLE course( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(10) ); -- 添加数据 INSERT INTO course VALUES (NULL,&#39;语文&#39;),(NULL,&#39;数学&#39;); -- 创建中间表 CREATE TABLE stu_course( id INT PRIMARY KEY AUTO_INCREMENT, sid INT, -- 用于和student表的id进行外键关联 cid INT, -- 用于和course表的id进行外键关联 CONSTRAINT sc_fk1 FOREIGN KEY (sid) REFERENCES student(id), -- 添加外键约束 CONSTRAINT sc_fk2 FOREIGN KEY (cid) REFERENCES course(id) -- 添加外键约束 ); -- 添加数据 INSERT INTO stu_course VALUES (NULL,1,1),(NULL,1,2),(NULL,2,1),(NULL,2,2); 窗口函数 开窗函数的引入是为了既显示聚合前的数据又显示聚合后的数据。即在每一行的最后一列添加聚合函数的结果。
应用：1、topN问题或者组内排序问题2、连续登录问题
!!!!!!!!!!!!!窗口函数原则上只能写在select子句中
原则上，窗口函数只能在SELECT子句中使用。 窗口函数OVER 中的ORDER BY 子句并不会影响最终结果的排序。其只是用来决定窗口函数按何种顺序计算。 -- 聚合类型SUM\MIN\MAX\AVG\COUNT sum() OVER([PARTITION BY xxx][ORDER BY xxx [DESC]]) -- 排序类型：ROW_NUMBER\RANK\DENSE_RANK ROW_NUMBER() OVER([PARTITION BY xxx][ORDER BY xxx [DESC]]) -- 分区类型：NTILE NTILE(number) OVER([PARTITION BY xxx][ORDER BY xxx [DESC]]) -- PARTITON BY 是用来分组，即选择要看哪个窗口，类似于 GROUP BY 子句的分组功能，但是 PARTITION BY 子句并不具备 GROUP BY 子句的汇总功能，并不会改变原始表中记录的行数。 -- ORDER BY 是用来排序，即决定窗口内，是按那种规则(字段)来排序的。 执行顺序 在加入窗口函数的基础上SQL的执行顺序也会发生变化，具体的执行顺序如下（window就是窗口函数） rank()排序相同时会重复，总数不会变 ，意思是会出现1、1、3这样的排序结果； dense_rank() 排序相同时会重复，总数会减少，意思是会出现1、1、2这样的排序结果。 row_number() 则在排序相同时不重复，会根据顺序排序。
SELECT product_id ,product_name ,sale_price ,SUM(sale_price) OVER (ORDER BY product_id) AS current_sum ,AVG(sale_price) OVER (ORDER BY product_id) AS current_avg FROM product; 从上面的例子可以看出，在没有partition by 的情况下，是把整个表作为一个大的窗口，SUM（）相当于向下累加，AVG（）相当于求从第一行到当前行的平均值，其他的聚合函数均是如此。
注意点：
1 、在使用专用的窗口函数时，例如rank、lag等，rank（）括号里是不需要指定任何字段的，直接空着就可以； 2 、在使用聚合函数做窗口函数时，SUM（）括号里必须有字段，得指定对哪些字段执行聚合的操作。在学习的初期很容易弄混，不同函数括号里是否需写相应的字段名；
滑动窗口函数 Preceding Rows 2 preceding 中文的意思是之前的两行，preceding可以把它理解为不含当前行情况下截止到之前几行。根据上图可以看出在每一行，都会求出当前行附近的3行(当前行+附近2行)数据的平均值，这种方法也叫作移动平均。
Following Rows 2 following 中文意思是之后的两行，跟preceding正好相反，Preceding是向前，following是向后。 preceding跟following相结合 从以上的运行结果可以看出是把每一行（当前行）的前一行和后一行作为汇总的依据。
ROLLUP - 计算合计及小计 常规的GROUP BY 只能得到每个分类的小计，有时候还需要计算分类的合计，可以用 ROLLUP关键字。
SELECT product_type ,regist_date ,SUM(sale_price) AS sum_price FROM product GROUP BY product_type, regist_date WITH ROLLUP; 备份与还原 命令行方式 # 备份(不需要登陆) mysqldump -u root -p 数据库名 &gt; 保存路径 # 恢复 mysql -u root -p drop database 已备份数据库名; create database 已备份数据库名; use 已备份数据库名; source 保存路径; 存储过程和函数 存储过程和函数是 事先经过编译并存储在数据库中的一段 SQL 语句的集合
存储过程和函数的好处
存储过程和函数可以重复使用，减轻开发人员的工作量。类似于java中方法可以多次调用 减少网络流量，存储过程和函数位于服务器上，调用的时候只需要传递名称和参数即可 减少数据在数据库和应用服务器之间的传输，可以提高数据处理的效率 将一些业务逻辑在数据库层面来实现，可以减少代码层面的业务处理 函数必须有返回值，存储过程没有返回值
-- 修改分隔符为$ DELIMITER$ -- 创建存储过程，封装分组查询学生总成绩的sql语句 CREATE PROCEDURE stu_group() BEGIN SELECT gender,SUM(score) getSum FROM student GROUP BY gender ORDER BY getSum ASC; END$ -- 修改分隔符为分号 DELIMITER ; -- 调用stu_group存储过程 CALL stu_group(); -- 查询数据库中所有的存储过程 标准语法 SELECT * FROM mysql.proc WHERE db=&#39;数据库名称&#39;; -- 删除stu_group存储过程 DROP PROCEDURE stu_group; 存储过程语法 变量 -- 定义变量 DECLARE 变量名 数据类型 [DEFAULT 默认值]; -- 变量赋值 SET 变量名 = 变量值; SELECT 列名 INTO 变量名 FROM 表名 [WHERE 条件]; -- 定义两个int变量，用于存储男女同学的总分数 DELIMITER $ CREATE PROCEDURE pro_test3() BEGIN -- 注意： DECLARE定义的是局部变量，只能用在BEGIN END范围之内 DECLARE men,women INT DEFAULT 10; -- 定义变量 SET men = 1; -- 为变量赋值 SELECT NAME; -- 查询变量 SELECT SUM(score) INTO men FROM student WHERE gender=&#39;男&#39;; -- 计算男同学总分数赋值给men SELECT SUM(score) INTO women FROM student WHERE gender=&#39;女&#39;; -- 计算女同学总分数赋值给women SELECT men,women; -- 查询变量 END$ DELIMITER ; if -- IF IF 判断条件1 THEN 执行的sql语句1; [ELSEIF 判断条件2 THEN 执行的sql语句2;] ... [ELSE 执行的sql语句n;] END IF; /* 定义一个int变量，用于存储班级总成绩 定义一个varchar变量，用于存储分数描述 根据总成绩判断： 380分及以上 学习优秀 320 ~ 380 学习不错 320以下 学习一般 */ DELIMITER $ CREATE PROCEDURE pro_test4() BEGIN -- 定义总分数变量 DECLARE total INT; -- 定义分数描述变量 DECLARE description VARCHAR(10); -- 为总分数变量赋值 SELECT SUM(score) INTO total FROM student; -- 判断总分数 IF total &gt;= 380 THEN SET description = &#39;学习优秀&#39;; ELSEIF total &gt;= 320 AND total &lt; 380 THEN SET description = &#39;学习不错&#39;; ELSE SET description = &#39;学习一般&#39;; END IF; -- 查询总成绩和描述信息 SELECT total,description; END$ DELIMITER ; 输入参数 DELIMITER $ -- 标准语法 CREATE PROCEDURE 存储过程名称([IN|OUT|INOUT] 参数名 数据类型) BEGIN 执行的sql语句; END$ /* IN:代表输入参数，需要由调用者传递实际数据。默认的 OUT:代表输出参数，该参数可以作为返回值 INOUT:代表既可以作为输入参数，也可以作为输出参数 */ DELIMITER ; @变量名: 这种变量要在变量名称前面加上“@”符号，叫做用户会话变量，代表整个会话过程他都是有作用的，这个类似于全局变量一样。 @@变量名: 这种在变量前加上 &#34;@@&#34; 符号, 叫做系统变量 /* 输入总成绩变量，代表学生总成绩 输出分数描述变量，代表学生总成绩的描述 根据总成绩判断： 380分及以上 学习优秀 320 ~ 380 学习不错 320以下 学习一般 */ DELIMITER $ CREATE PROCEDURE pro_test6(IN total INT,OUT description VARCHAR(10)) BEGIN -- 判断总分数 IF total &gt;= 380 THEN SET description = &#39;学习优秀&#39;; ELSEIF total &gt;= 320 AND total &lt; 380 THEN SET description = &#39;学习不错&#39;; ELSE SET description = &#39;学习一般&#39;; END IF; END$ DELIMITER ; -- 调用pro_test6存储过程 CALL pro_test6(310,@dscription); -- 查询总成绩描述 SELECT @description; case -- 标准语法 CASE 表达式 WHEN 值1 THEN 执行sql语句1; [WHEN 值2 THEN 执行sql语句2;] ... [ELSE 执行sql语句n;] END CASE; /* 输入总成绩变量，代表学生总成绩 定义一个varchar变量，用于存储分数描述 根据总成绩判断： 380分及以上 学习优秀 320 ~ 380 学习不错 320以下 学习一般 */ DELIMITER $ CREATE PROCEDURE pro_test7(IN total INT) BEGIN -- 定义变量 DECLARE description VARCHAR(10); -- 使用case判断 CASE WHEN total &gt;= 380 THEN SET description = &#39;学习优秀&#39;; WHEN total &gt;= 320 AND total &lt; 380 THEN SET description = &#39;学习不错&#39;; ELSE SET description = &#39;学习一般&#39;; END CASE; -- 查询分数描述信息 SELECT description; END$ DELIMITER ; -- 调用pro_test7存储过程 CALL pro_test7(390); CALL pro_test7((SELECT SUM(score) FROM student)); while -- 标准语法 初始化语句; WHILE 条件判断语句 DO 循环体语句; 条件控制语句; END WHILE; /* 计算1~100之间的偶数和 */ DELIMITER $ CREATE PROCEDURE pro_test8() BEGIN -- 定义求和变量 DECLARE result INT DEFAULT 0; -- 定义初始化变量 DECLARE num INT DEFAULT 1; -- while循环 WHILE num &lt;= 100 DO -- 偶数判断 IF num%2=0 THEN SET result = result + num; -- 累加 END IF; -- 让num+1 SET num = num + 1; END WHILE; -- 查询求和结果 SELECT result; END$ DELIMITER ; -- 调用pro_test8存储过程 CALL pro_test8(); repeat 注意：repeat循环是条件满足则停止。while循环是条件满足则执行
-- 标准语法 初始化语句; REPEAT 循环体语句; 条件控制语句; UNTIL 条件判断语句 END REPEAT; /* 计算1~10之间的和 */ DELIMITER $ CREATE PROCEDURE pro_test9() BEGIN -- 定义求和变量 DECLARE result INT DEFAULT 0; -- 定义初始化变量 DECLARE num INT DEFAULT 1; -- repeat循环 REPEAT -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1; -- 停止循环 UNTIL num&gt;10 END REPEAT; -- 查询求和结果 SELECT result; END$ DELIMITER ; -- 调用pro_test9存储过程 CALL pro_test9(); loop 注意：loop可以实现简单的循环，但是退出循环需要使用其他的语句来定义。我们可以使用leave语句完成！ 如果不加退出循环的语句，那么就变成了死循环。
-- 标准语法 初始化语句; [循环名称:] LOOP 条件判断语句 [LEAVE 循环名称;] 循环体语句; 条件控制语句; END LOOP 循环名称; -- 注意：loop可以实现简单的循环，但是退出循环需要使用其他的语句来定义。我们可以使用leave语句完成！ -- 如果不加退出循环的语句，那么就变成了死循环。 /* 计算1~10之间的和 */ DELIMITER $ CREATE PROCEDURE pro_test10() BEGIN -- 定义求和变量 DECLARE result INT DEFAULT 0; -- 定义初始化变量 DECLARE num INT DEFAULT 1; -- loop循环 l:LOOP -- 条件成立，停止循环 IF num &gt; 10 THEN LEAVE l; END IF; -- 累加 SET result = result + num; -- 让num+1 SET num = num + 1; END LOOP l; -- 查询求和结果 SELECT result; END$ DELIMITER ; -- 调用pro_test10存储过程 CALL pro_test10(); 游标 游标可以遍历返回的多行结果，每次拿到一整行数据 在存储过程和函数中可以使用游标对结果集进行循环的处理 简单来说游标就类似于集合的迭代器遍历 MySQL中的游标只能用在存储过程和函数中 -- 创建游标 DECLARE 游标名称 CURSOR FOR 查询sql语句; -- 打开游标 OPEN 游标名称; -- 使用游标获取数据 FETCH 游标名称 INTO 变量名1,变量名2,...; -- - 关闭游标 CLOSE 游标名称; /* 将student表中所有的成绩保存到stu_score表中 */ DELIMITER $ CREATE PROCEDURE pro_test11() BEGIN -- 定义成绩变量 DECLARE s_score INT; -- 定义标记变量 DECLARE flag INT DEFAULT 0; -- 创建游标,查询所有学生成绩数据 DECLARE stu_result CURSOR FOR SELECT score FROM student; -- 游标结束后，将标记变量改为1 DECLARE EXIT HANDLER FOR NOT FOUND SET flag = 1; -- 开启游标 OPEN stu_result; -- 循环使用游标 REPEAT -- 使用游标，遍历结果,拿到数据 FETCH stu_result INTO s_score; -- 将数据保存到stu_score表中 INSERT INTO stu_score VALUES (NULL,s_score); UNTIL flag=1 END REPEAT; -- 关闭游标 CLOSE stu_result; END$ DELIMITER ; -- 调用pro_test11存储过程 CALL pro_test11(); -- 查询stu_score表 SELECT * FROM stu_score; 存储函数 存储函数有返回值，存储过程没有返回值(参数的out其实也相当于是返回数据了)
DELIMITER $ -- 标准语法 CREATE FUNCTION 函数名称([参数 数据类型]) RETURNS 返回值类型 BEGIN 执行的sql语句; RETURN 结果; END$ DELIMITER ; -- 调用存储函数 SELECT 函数名称(实际参数); -- 删除存储函 DROP FUNCTION 函数名称; /* 定义存储函数，获取学生表中成绩大于95分的学生数量 */ DELIMITER $ CREATE FUNCTION fun_test1() RETURNS INT BEGIN -- 定义统计变量 DECLARE result INT; -- 查询成绩大于95分的学生数量，给统计变量赋值 SELECT COUNT(*) INTO result FROM student WHERE score &gt; 95; -- 返回统计结果 RETURN result; END$ DELIMITER ; -- 调用fun_test1存储函数 SELECT fun_test1(); 触发器 触发器是与表有关的数据库对象，可以在 insert/update/delete 之前或之后，触发并执行触发器中定义的SQL语句。触发器的这种特性可以协助应用在数据库端确保数据的完整性 、日志记录 、数据校验等操作 。 使用别名 NEW 和 OLD 来引用触发器中发生变化的记录内容，这与其他的数据库是相似的。现在触发器还只支持行级触发，不支持语句级触发。 触发器类型 OLD的含义 NEW的含义 INSERT 型触发器 无 (因为插入前状态无数据) NEW 表示将要或者已经新增的数据 UPDATE 型触发器 OLD 表示修改之前的数据 NEW 表示将要或已经修改后的数据 DELETE 型触发器 OLD 表示将要或者已经删除的数据 无 (因为删除后状态无数据) DELIMITER $ CREATE TRIGGER 触发器名称 BEFORE|AFTER INSERT|UPDATE|DELETE ON 表名 [FOR EACH ROW] -- 行级触发器 BEGIN 触发器要执行的功能; END$ DELIMITER ; 触发器演示 通过触发器记录账户表的数据变更日志。包含：增加、修改、删除
创建账户表 -- 创建db9数据库 CREATE DATABASE db9; -- 使用db9数据库 USE db9; -- 创建账户表account CREATE TABLE account( id INT PRIMARY KEY AUTO_INCREMENT,	-- 账户id NAME VARCHAR(20),	-- 姓名 money DOUBLE	-- 余额 ); -- 添加数据 INSERT INTO account VALUES (NULL,&#39;张三&#39;,1000),(NULL,&#39;李四&#39;,2000); 创建日志表 -- 创建日志表account_log CREATE TABLE account_log( id INT PRIMARY KEY AUTO_INCREMENT,	-- 日志id operation VARCHAR(20),	-- 操作类型 (insert update delete) operation_time DATETIME,	-- 操作时间 operation_id INT,	-- 操作表的id operation_params VARCHAR(200) -- 操作参数 ); 创建INSERT触发器 -- 创建INSERT触发器 DELIMITER $ CREATE TRIGGER account_insert AFTER INSERT ON account FOR EACH ROW BEGIN INSERT INTO account_log VALUES (NULL,&#39;INSERT&#39;,NOW(),new.id,CONCAT(&#39;插入后{id=&#39;,new.id,&#39;,name=&#39;,new.name,&#39;,money=&#39;,new.money,&#39;}&#39;)); END$ DELIMITER ; -- 向account表添加记录 INSERT INTO account VALUES (NULL,&#39;王五&#39;,3000); -- 查询account表 SELECT * FROM account; -- 查询日志表 SELECT * FROM account_log; 创建UPDATE触发器 -- 创建UPDATE触发器 DELIMITER $ CREATE TRIGGER account_update AFTER UPDATE ON account FOR EACH ROW BEGIN INSERT INTO account_log VALUES (NULL,&#39;UPDATE&#39;,NOW(),new.id,CONCAT(&#39;修改前{id=&#39;,old.id,&#39;,name=&#39;,old.name,&#39;,money=&#39;,old.money,&#39;}&#39;,&#39;修改后{id=&#39;,new.id,&#39;,name=&#39;,new.name,&#39;,money=&#39;,new.money,&#39;}&#39;)); END$ DELIMITER ; -- 修改account表 UPDATE account SET money=3500 WHERE id=3; -- 查询account表 SELECT * FROM account; -- 查询日志表 SELECT * FROM account_log; 创建DELETE触发器 -- 创建DELETE触发器 DELIMITER $ CREATE TRIGGER account_delete AFTER DELETE ON account FOR EACH ROW BEGIN INSERT INTO account_log VALUES (NULL,&#39;DELETE&#39;,NOW(),old.id,CONCAT(&#39;删除前{id=&#39;,old.id,&#39;,name=&#39;,old.name,&#39;,money=&#39;,old.money,&#39;}&#39;)); END$ DELIMITER ; -- 删除account表数据 DELETE FROM account WHERE id=3; -- 查询account表 SELECT * FROM account; -- 查询日志表 SELECT * FROM account_log; 查看、删除触发器 SHOW TRIGGERS; DROP TRIGGER account_delete; 事务 一条或多条 SQL 语句组成一个执行单元，其特点是这个单元要么同时成功要么同时失败，单元中的每条 SQL 语句都相互依赖，形成一个整体，如果某条 SQL 语句执行失败或者出现错误，那么整个单元就会回滚，撤回到事务最初的状态，如果单元中所有的 SQL 语句都执行成功，则事务就顺利执行。
操作事务的三个步骤
开启事务：记录回滚点，并通知服务器，将要执行一组操作，要么同时成功、要么同时失败 执行sql语句：执行具体的一条或多条sql语句 结束事务(提交|回滚) 提交：没出现问题，数据进行更新 回滚：出现问题，数据恢复到开启事务时的状态 -- 开启事务 START TRANSACTION; -- 张三给李四转账500元 -- 1.张三账户-500 UPDATE account SET money=money-500 WHERE NAME=&#39;张三&#39;; -- 2.李四账户+500 -- 出错了... UPDATE account SET money=money+500 WHERE NAME=&#39;李四&#39;; -- 回滚事务(出现问题) ROLLBACK; -- 提交事务(没出现问题) COMMIT; -- 修改为手动提交 SET @@AUTOCOMMIT=0; -- 查看提交方式 SELECT @@AUTOCOMMIT; -- 1代表自动提交 0代表手动提交 事务的四大特征(ACID) 原子性(atomicity) 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能对数据库有任何影响 一致性(consistency) 一致性是指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态 拿转账来说，假设张三和李四两者的钱加起来一共是2000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是2000，这就是事务的一致性 隔离性(isolcation) 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离 持久性(durability) 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作 事务的隔离级别 隔离级别的概念 多个客户端操作时 ,各个客户端的事务之间应该是隔离的，相互独立的 , 不受影响的。
而如果多个事务操作同一批数据时，则需要设置不同的隔离级别 , 否则就会产生问题 。
四种隔离级别
1 读未提交 read uncommitted 2 读已提交 read committed 3 可重复读 repeatable read 4 串行化 serializable 可能引发的问题 问题 现象 脏读 是指在一个事务处理过程中读取了另一个未提交的事务中的数据 , 导致两次查询结果不一致 不可重复读 是指在一个事务处理过程中读取了另一个事务中修改并已提交的数据, 导致两次查询结果不一致 幻读 select 某记录是否存在，不存在，准备插入此记录，但执行 insert 时发现此记录已存在，无法插入。或不存在执行delete删除，却发现删除成功 -- 修改数据库隔离级别为read uncommitted SET GLOBAL TRANSACTION ISOLATION LEVEL read uncommitted; -- 查看隔离级别 SELECT @@TX_ISOLATION; -- 修改后需要断开连接重新开 事务隔离级别演示 脏读的问题
窗口1 -- 查询账户表 select * from account; -- 设置隔离级别为read uncommitted set global transaction isolation level read uncommitted; -- 开启事务 start transaction; -- 转账 update account set money = money - 500 where id = 1; update account set money = money + 500 where id = 2; -- 窗口2查询转账结果 ,出现脏读(查询到其他事务未提交的数据) -- 窗口2查看转账结果后，执行回滚 rollback; 窗口2 -- 查询隔离级别 select @@tx_isolation; -- 开启事务 start transaction; -- 查询账户表 select * from account; 解决脏读的问题和演示不可重复读的问题
窗口1 -- 设置隔离级别为read committed set global transaction isolation level read committed; -- 开启事务 start transaction; -- 转账 update account set money = money - 500 where id = 1; update account set money = money + 500 where id = 2; -- 窗口2查看转账结果，并没有发生变化(脏读问题被解决了) -- 执行提交事务。 commit; -- 窗口2查看转账结果，数据发生了变化(出现了不可重复读的问题，读取到其他事务已提交的数据) 窗口2 -- 查询隔离级别 select @@tx_isolation; -- 开启事务 start transaction; -- 查询账户表 select * from account; 解决不可重复读的问题
窗口1 -- 设置隔离级别为repeatable read set global transaction isolation level repeatable read; -- 开启事务 start transaction; -- 转账 update account set money = money - 500 where id = 1; update account set money = money + 500 where id = 2; -- 窗口2查看转账结果，并没有发生变化 -- 执行提交事务 commit; -- 这个时候窗口2只要还在上次事务中，看到的结果都是相同的。只有窗口2结束事务，才能看到变化(不可重复读的问题被解决) 窗口2 -- 查询隔离级别 select @@tx_isolation; -- 开启事务 start transaction; -- 查询账户表 select * from account; -- 提交事务 commit; -- 查询账户表 select * from account; 幻读的问题和解决
窗口1 -- 设置隔离级别为repeatable read set global transaction isolation level repeatable read; -- 开启事务 start transaction; -- 添加一条记录 INSERT INTO account VALUES (3,&#39;王五&#39;,1500); -- 查询账户表，本窗口可以查看到id为3的结果 SELECT * FROM account; -- 提交事务 COMMIT; 窗口2 -- 查询隔离级别 select @@tx_isolation; -- 开启事务 start transaction; -- 查询账户表，查询不到新添加的id为3的记录 select * from account; -- 添加id为3的一条数据，发现添加失败。出现了幻读 INSERT INTO account VALUES (3,&#39;测试&#39;,200); -- 提交事务 COMMIT; -- 查询账户表，查询到了新添加的id为3的记录 select * from account; 解决幻读的问题 /* 窗口1 */ -- 设置隔离级别为serializable set global transaction isolation level serializable; -- 开启事务 start transaction; -- 添加一条记录 INSERT INTO account VALUES (4,&#39;赵六&#39;,1600); -- 查询账户表，本窗口可以查看到id为4的结果 SELECT * FROM account; -- 提交事务 COMMIT; /* 窗口2 */ -- 查询隔离级别 select @@tx_isolation; -- 开启事务 start transaction; -- 查询账户表，发现查询语句无法执行，数据表被锁住！只有窗口1提交事务后，才可以继续操作 select * from account; -- 添加id为4的一条数据，发现已经存在了，就不会再添加了！幻读的问题被解决 INSERT INTO account VALUES (4,&#39;测试&#39;,200); -- 提交事务 COMMIT; 隔离级别总结 隔离级别 名称 出现脏读 出现不可重复读 出现幻读 数据库默认隔离级别 1 read uncommitted 读未提交 是 是 是 2 read committed 读已提交 否 是 是 Oracle / SQL Server 3 repeatable read 可重复读 否 否 是 MySQL 4 **serializable ** 串行化 否 否 否 注意：隔离级别从小到大安全性越来越高，但是效率越来越低 , 所以不建议使用READ UNCOMMITTED 和 SERIALIZABLE 隔离级别.
索引 MySQL数据库中的索引：是帮助MySQL高效获取数据的一种数据结构！所以，索引的本质就是数据结构。
在表数据之外，数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式指向数据， 这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。
一张数据表，用于保存数据。 一个索引配置文件，用于保存索引，每个索引都去指向了某一个数据(表格演示)
索引的分类 功能分类
普通索引： 最基本的索引，它没有任何限制。 唯一索引：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值组合必须唯一。 主键索引：一种特殊的唯一索引，不允许有空值。一般在建表时同时创建主键索引。 组合索引：顾名思义，就是将单列索引进行组合。 外键索引：只有InnoDB引擎支持外键索引，用来保证数据的一致性、完整性和实现级联操作。 全文索引：快速匹配全部文档的方式。InnoDB引擎5.6版本后才支持全文索引。MEMORY引擎不支持。 结构分类
B+Tree索引 ：MySQL使用最频繁的一个索引数据结构，是InnoDB和MyISAM存储引擎默认的索引类型。 Hash索引 : MySQL中Memory存储引擎默认支持的索引类型。 注意：如果一个表中有一列是主键，那么就会默认为其创建主键索引！(主键列不需要单独创建索引)
-- 创建索引 CREATE [UNIQUE|FULLTEXT] INDEX 索引名称 [USING 索引类型] -- 默认是B+TREE ON 表名(列名...); -- 为student表中姓名列创建一个普通索引 CREATE INDEX idx_name ON student(NAME); -- 为student表中年龄列创建一个唯一索引 CREATE UNIQUE INDEX idx_age ON student(age); -- 查看student表中的索引 SHOW INDEX FROM student; -- -------------------alter语句添加索引 -- 普通索引 ALTER TABLE 表名 ADD INDEX 索引名称(列名); -- 组合索引 ALTER TABLE 表名 ADD INDEX 索引名称(列名1,列名2,...); -- 主键索引 ALTER TABLE 表名 ADD PRIMARY KEY(主键列名); -- 外键索引(添加外键约束，就是外键索引) ALTER TABLE 表名 ADD CONSTRAINT 外键名 FOREIGN KEY (本表外键列名) REFERENCES 主表名(主键列名); -- 唯一索引 ALTER TABLE 表名 ADD UNIQUE 索引名称(列名); -- 全文索引(mysql只支持文本类型) ALTER TABLE 表名 ADD FULLTEXT 索引名称(列名); -- 为student表中name列添加全文索引 ALTER TABLE student ADD FULLTEXT idx_fulltext_name(name); -- 查看student表中的索引 SHOW INDEX FROM student; -- 删除student表中的idx_score索引 DROP INDEX idx_score ON student; -- 查看student表中的索引 SHOW INDEX FROM student; 索引的实现原理 索引是在MySQL的存储引擎中实现的，所以每种存储引擎的索引不一定完全相同，也不是所有的引擎支持所有的索引类型。这里我们主要介绍InnoDB引擎的实现的B+Tree索引。 B+Tree是一种树型数据结构，是B-Tree的变种。通常使用在数据库和操作系统中的文件系统，特点是能够保持数据稳定有序。我们逐步的来了解一下。 磁盘存储 系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位 位于同一个磁盘块中的数据会被一次性读取出来，而不是需要什么取什么。 InnoDB存储引擎中有页（Page）的概念，页是其磁盘管理的最小单位。InnoDB存储引擎中默认每个页的大小为16KB。 InnoDB引擎将若干个地址连接磁盘块，以此来达到页的大小16KB，在查询数据时如果一个页中的每条数据都能有助于定位数据记录的位置，这将会减少磁盘I/O次数，提高查询效率。 BTree BTree结构的数据可以让系统高效的找到数据所在的磁盘块。为了描述BTree，首先定义一条记录为一个二元组[key, data] ，key为记录的键值，对应表中的主键值，data为一行记录中除主键外的数据。对于不同的记录，key值互不相同。BTree中的每个节点根据实际情况可以包含大量的关键字信息和分支，如下图所示为一个3阶的BTree：
查找顺序：
模拟查找15的过程 : 1.根节点找到磁盘块1，读入内存。【磁盘I/O操作第1次】 比较关键字15在区间（&lt;17），找到磁盘块1的指针P1。 2.P1指针找到磁盘块2，读入内存。【磁盘I/O操作第2次】 比较关键字15在区间（&gt;12），找到磁盘块2的指针P3。 3.P3指针找到磁盘块7，读入内存。【磁盘I/O操作第3次】 在磁盘块7中找到关键字15。 -- 分析上面过程，发现需要3次磁盘I/O操作，和3次内存查找操作。 -- 由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。而3次磁盘I/O操作是影响整个BTree查找效率的决定因素。BTree使用较少的节点个数，使每次磁盘I/O取到内存的数据都发挥了作用，从而提高了查询效率。 B+Tree B+Tree是在BTree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 从上一节中的BTree结构图中可以看到每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 B+Tree相对于BTree区别： 非叶子节点只存储键值信息。数据记录都存放在叶子节点中。 所有叶子节点之间都有一个连接指针。 将上一节中的BTree优化，由于B+Tree的非叶子节点只存储键值信息，假设每个磁盘块能存储4个键值及指针信息，则变成B+Tree后其结构如下图所示： 通常在B+Tree上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。因此可以对B+Tree进行两种查找运算：
【有范围】对于主键的范围查找和分页查找 【有顺序】从根节点开始，进行随机查找 实际情况中每个节点可能不能填充满，因此在数据库中，B+Tree的高度一般都在2~4层。MySQL的InnoDB存储引擎在设计时是将根节点常驻内存的，也就是说查找某一键值的行记录时最多只需要1~3次磁盘I/O操作。
索引的设计原则 索引的设计可以遵循一些已有的原则，创建索引的时候请尽量考虑符合这些原则，便于提升索引的使用效率，更高效的使用索引。
创建索引时的原则 对查询频次较高，且数据量比较大的表建立索引。 使用唯一索引，区分度越高，使用索引的效率越高。 索引字段的选择，最佳候选列应当从where子句的条件中提取，如果where子句中的组合比较多，那么应当挑选最常用、过滤效果最好的列的组合。 使用短索引，提升索引访问的I/O效率，也可以提升总体的访问效率。假如构成索引的字段总长度比较短，那么在给定大小的存储块内可以存储更多的索引值，相应的可以有效的提升MySQL访问索引的I/O效率。 索引可以有效的提升查询数据的效率，但索引数量不是多多益善，索引越多，维护索引的代价自然也就水涨船高。对于插入、更新、删除等DML操作比较频繁的表来说，索引过多，会引入相当高的维护代价，降低DML操作的效率，增加相应操作的时间消耗。另外索引过多的话，MySQL也会犯选择困难病，虽然最终仍然会找到一个可用的索引，但无疑提高了选择的代价。 联合索引的特点 在mysql建立联合索引时会遵循最左前缀匹配的原则，即最左优先，在检索数据时从联合索引的最左边开始匹配，
ALTER TABLE user ADD INDEX index_three(name,address,phone); -- 联合索引index_three实际建立了(name)、(name,address)、(name,address,phone)三个索引。所以下面的三个SQL语句都可以命中索引。 SELECT * FROM user WHERE address = &#39;北京&#39; AND phone = &#39;12345&#39; AND name = &#39;张三&#39;; SELECT * FROM user WHERE name = &#39;张三&#39; AND address = &#39;北京&#39;; SELECT * FROM user WHERE name = &#39;张三&#39;; -- 上面三个查询语句执行时会依照最左前缀匹配原则，检索时分别会使用索引 (name,address,phone) (name,address) (name) 进行数据匹配。 索引的字段可以是任意顺序的，如： -- 优化器会帮助我们调整顺序，下面的SQL语句都可以命中索引 SELECT * FROM user WHERE address = &#39;北京&#39; AND phone = &#39;12345&#39; AND name = &#39;张三&#39;; -- 联合索引中最左边的列不包含在条件查询中，下面的SQL语句就不会命中索 SELECT * FROM user WHERE address=&#39;北京&#39; AND phone=&#39;12345&#39;; 锁 锁的概念 在我们学习事务的时候，讲解过事务的隔离性，可能会出现脏读、不可重复读、幻读的问题，当时我们的解决方式是通过修改事务的隔离级别来控制，但是数据库的隔离级别我们并不推荐修改。所以，锁的作用也可以解决掉之前的问题！
锁机制 : 数据库为了保证数据的一致性，而使各种共享的资源在被并发访问时变得有序所设计的一种规则。
举例，在电商网站购买商品时，商品表中只存有1个商品，而此时又有两个人同时购买，那么谁能买到就是一个关键的问题。
这里会用到事务进行一系列的操作：
先从商品表中取出物品的数据 然后插入订单 付款后，再插入付款表信息 更新商品表中商品的数量 以上过程中，使用锁可以对商品数量数据信息进行保护，实现隔离，即只允许第一位用户完成整套购买流程，而其他用户只能等待，这样就解决了并发中的矛盾问题。
在数据库中，数据是一种供许多用户共享访问的资源，如何保证数据并发访问的一致性、有效性，是所有数据库必须解决的一个问题，MySQL由于自身架构的特点，在不同的存储引擎中，都设计了面对特定场景的锁定机制，所以引擎的差别，导致锁机制也是有很大差别的。
锁的分类 按操作分类： 共享锁：也叫读锁。数据可以被多个事务查间，但是不能修改 排他锁：也叫写锁。当前的操作没有完成前，不能被其他事务加锁查询或修改 注意：锁的兼容性 共享锁和共享锁 兼容 共享锁和排他锁 冲突 排他锁和排他锁 冲突 排他锁和共享锁 冲突 按粒度分类： 表级锁：操作时，会锁定整个表。开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率高，并发度最低。偏向于MyISAM存储引擎！ 行级锁：操作时，会锁定当前操作行。开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高。偏向于InnoDB存储引擎！ 页级锁：锁的粒度、发生冲突的概率和加锁的开销介于表锁和行锁之间，会出现死锁，并发性能一般。 按使用方式分类： 悲观锁：每次查询数据时都认为别人会修改，很悲观，所以查询时加锁。 乐观锁：每次查询数据时都认为别人不会修改，很乐观，但是更新时会判断一下在此期间别人有没有去更新这个数据 不同存储引擎支持的锁 存储引擎 表级锁 行级锁 页级锁 MyISAM 支持 不支持 不支持 InnoDB 支持 支持 不支持 MEMORY 支持 不支持 不支持 BDB 支持 不支持 支持 演示InnoDB锁 -- 共享锁 SELECT语句 LOCK IN SHARE MODE; -- 排他锁 SELECT语句 FOR UPDATE; 数据准备 -- 创建db13数据库 CREATE DATABASE db13; -- 使用db13数据库 USE db13; -- 创建student表 CREATE TABLE student( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(10), age INT, score INT ); -- 添加数据 INSERT INTO student VALUES (NULL,&#39;张三&#39;,23,99),(NULL,&#39;李四&#39;,24,95), (NULL,&#39;王五&#39;,25,98),(NULL,&#39;赵六&#39;,26,97); -- 窗口1 /* 共享锁：数据可以被多个事务查询，但是不能修改 */ -- 开启事务 START TRANSACTION; -- 查询id为1的数据记录。加入共享锁 SELECT * FROM student WHERE id=1 LOCK IN SHARE MODE; -- 查询分数为99分的数据记录。加入共享锁(注意：InnoDB引擎如果不采用带索引的列。则会提升为表锁) SELECT * FROM student WHERE score=99 LOCK IN SHARE MODE; -- 提交事务 COMMIT; -- 窗口2 -- 开启事务 START TRANSACTION; -- 查询id为1的数据记录(普通查询，可以查询) SELECT * FROM student WHERE id=1; -- 查询id为1的数据记录，并加入共享锁(可以查询。共享锁和共享锁兼容) SELECT * FROM student WHERE id=1 LOCK IN SHARE MODE; -- 修改id为1的姓名为张三三(不能修改，会出现锁的情况。只有窗口1提交事务后，才能修改成功) UPDATE student SET NAME=&#39;张三三&#39; WHERE id = 1; -- 修改id为2的姓名为李四四(修改成功，InnoDB引擎默认是行锁) UPDATE student SET NAME=&#39;李四四&#39; WHERE id = 2; -- 修改id为3的姓名为王五五(注意：InnoDB引擎如果不采用带索引的列。则会提升为表锁) UPDATE student SET NAME=&#39;王五五&#39; WHERE id = 3; -- 提交事务 COMMIT; -- 窗口1 /* 排他锁：加锁的数据，不能被其他事务加锁查询或修改 */ -- 开启事务 START TRANSACTION; -- 查询id为1的数据记录，并加入排他锁 SELECT * FROM student WHERE id=1 FOR UPDATE; -- 提交事务 COMMIT; -- 窗口2 -- 开启事务 START TRANSACTION; -- 查询id为1的数据记录(普通查询没问题) SELECT * FROM student WHERE id=1; -- 查询id为1的数据记录，并加入共享锁(不能查询。因为排他锁不能和其他锁共存) SELECT * FROM student WHERE id=1 LOCK IN SHARE MODE; -- 查询id为1的数据记录，并加入排他锁(不能查询。因为排他锁不能和其他锁共存) SELECT * FROM student WHERE id=1 FOR UPDATE; -- 修改id为1的姓名为张三(不能修改，会出现锁的情况。只有窗口1提交事务后，才能修改成功) UPDATE student SET NAME=&#39;张三&#39; WHERE id=1; -- 提交事务 COMMIT; 演示MyISAM锁 -- 读锁。加锁 LOCK TABLE 表名 READ; -- 解锁(将当前会话所有的表进行解锁) UNLOCK TABLES; -- 写锁。加锁 LOCK TABLE 表名 WRITE; -- 解锁(将当前会话所有的表进行解锁) UNLOCK TABLES; -- 创建product表 CREATE TABLE product( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(20), price INT )ENGINE = MYISAM; -- 指定存储引擎为MyISAM -- 添加数据 INSERT INTO product VALUES (NULL,&#39;华为手机&#39;,4999),(NULL,&#39;小米手机&#39;,2999), (NULL,&#39;苹果&#39;,8999),(NULL,&#39;中兴&#39;,1999); -- 窗口1 /* 读锁：所有连接只能读取数据，不能修改 */ -- 为product表加入读锁 LOCK TABLE product READ; -- 查询product表(查询成功) SELECT * FROM product; -- 修改华为手机的价格为5999(修改失败) UPDATE product SET price=5999 WHERE id=1; -- 解锁 UNLOCK TABLES; -- 窗口2 -- 查询product表(查询成功) SELECT * FROM product; -- 修改华为手机的价格为5999(不能修改，窗口1解锁后才能修改成功) UPDATE product SET price=5999 WHERE id=1; -- 窗口1 /* 写锁：其他连接不能查询和修改数据 */ -- 为product表添加写锁 LOCK TABLE product WRITE; -- 查询product表(查询成功) SELECT * FROM product; -- 修改小米手机的金额为3999(修改成功) UPDATE product SET price=3999 WHERE id=2; -- 解锁 UNLOCK TABLES; -- 窗口2 -- 查询product表(不能查询。只有窗口1解锁后才能查询成功) SELECT * FROM product; -- 修改小米手机的金额为2999(不能修改。只有窗口1解锁后才能修改成功) UPDATE product SET price=2999 WHERE id=2; 演示悲观锁和乐观锁 我们之前所学的行锁，表锁不论是读写锁都是悲观锁。
悲观锁和乐观锁使用前提
对于读的操作远多于写的操作的时候，这时候一个更新操作加锁会阻塞所有的读取操作，降低了吞吐量。最后还要释放锁，锁是需要一些开销的，这时候可以选择乐观锁。 如果是读写比例差距不是非常大或者系统没有响应不及时，吞吐量瓶颈的问题，那就不要去使用乐观锁，它增加了复杂度，也带来了业务额外的风险。这时候可以选择悲观锁。 乐观锁的实现方式
版本号
给数据表中添加一个version列，每次更新后都将这个列的值加1。 读取数据时，将版本号读取出来，在执行更新的时候，比较版本号。 如果相同则执行更新，如果不相同，说明此条数据已经发生了变化。 用户自行根据这个通知来决定怎么处理，比如重新开始一遍，或者放弃本次更新。 -- 创建city表 CREATE TABLE city( id INT PRIMARY KEY AUTO_INCREMENT, -- 城市id NAME VARCHAR(20), -- 城市名称 VERSION INT -- 版本号 ); -- 添加数据 INSERT INTO city VALUES (NULL,&#39;北京&#39;,1),(NULL,&#39;上海&#39;,1),(NULL,&#39;广州&#39;,1),(NULL,&#39;深圳&#39;,1); -- 修改北京为北京市 -- 1.查询北京的version SELECT VERSION FROM city WHERE NAME=&#39;北京&#39;; -- 2.修改北京为北京市，版本号+1。并对比版本号 UPDATE city SET NAME=&#39;北京市&#39;,VERSION=VERSION+1 WHERE NAME=&#39;北京&#39; AND VERSION=1; 时间戳
和版本号方式基本一样，给数据表中添加一个列，名称无所谓，数据类型需要是timestamp 每次更新后都将最新时间插入到此列。 读取数据时，将时间读取出来，在执行更新的时候，比较时间。 如果相同则执行更新，如果不相同，说明此条数据已经发生了变化。 锁的总结 表锁和行锁
行锁：锁的粒度更细，加行锁的性能损耗较大。并发处理能力较高。InnoDB引擎默认支持！ 表锁：锁的粒度较粗，加表锁的性能损耗较小。并发处理能力较低。InnoDB、MyISAM引擎支持！ InnoDB锁优化建议
尽量通过带索引的列来完成数据查询，从而避免InnoDB无法加行锁而升级为表锁。
合理设计索引，索引要尽可能准确，尽可能的缩小锁定范围，避免造成不必要的锁定。
尽可能减少基于范围的数据检索过滤条件。
尽量控制事务的大小，减少锁定的资源量和锁定时间长度。
在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率。
对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁的产生。
存储引擎 MySQL体系结构 体系结构的概念
任何一套系统当中，每个部件都能起到一定的作用！ MySQL的体系结构
体系结构详解 客户端连接 支持接口：支持的客户端连接，例如C、Java、PHP等语言来连接MySQL数据库 第一层：网络连接层 连接池：管理、缓冲用户的连接，线程处理等需要缓存的需求。 例如：当客户端发送一个请求连接，会从连接池中获取一个连接进行使用。 第二层：核心服务层 管理服务和工具：系统的管理和控制工具，例如备份恢复、复制、集群等。 SQL接口：接受SQL命令，并且返回查询结果。 查询解析器：验证和解析SQL命令，例如过滤条件、语法结构等。 查询优化器：在执行查询之前，使用默认的一套优化机制进行优化sql语句 缓存：如果缓存当中有想查询的数据，则直接将缓存中的数据返回。没有的话再重新查询！ 第三层：存储引擎层 插件式存储引擎：管理和操作数据的一种机制，包括(存储数据、如何更新、查询数据等) 第四层：系统文件层 文件系统：配置文件、数据文件、日志文件、错误文件、二进制文件等等的保存 MySQL存储引擎 引擎的概念
生活中，引擎就是整个机器运行的核心，不同的引擎具备不同的功能。 MySQL存储引擎的概念
MySQL数据库使用不同的机制存取表文件 , 机制的差别在于不同的存储方式、索引技巧、锁定水平以及广泛的不同的功能和能力，在MySQL中 , 将这些不同的技术及配套的功能称为存储引擎 在关系型数据库中数据的存储是以表的形式存进行储的，所以存储引擎也可以称为表类型（即存储和操作此表的类型）。 Oracle , SqlServer等数据库只有一种存储引擎 , 而MySQL针对不同的需求, 配置MySQL的不同的存储引擎 , 就会让数据库采取了不同的处理数据的方式和扩展功能。 通过选择不同的引擎 ,能够获取最佳的方案 , 也能够获得额外的速度或者功能，提高程序的整体效果。所以了解引擎的特性 , 才能贴合我们的需求 , 更好的发挥数据库的性能。 MySQL支持的存储引擎
MySQL5.7支持的引擎包括：InnoDB、MyISAM、MEMORY、Archive、Federate、CSV、BLACKHOLE等 其中较为常用的有三种：InnoDB、MyISAM、MEMORY 常用引擎的特性对比 常用的存储引擎 MyISAM存储引擎 访问快,不支持事务和外键。表结构保存在.frm文件中，表数据保存在.MYD文件中，索引保存在.MYI文件中。 InnoDB存储引擎(MySQL5.5版本后默认的存储引擎) 支持事务 ,占用磁盘空间大 ,支持并发控制。表结构保存在.frm文件中，如果是共享表空间，数据和索引保存在 innodb_data_home_dir 和 innodb_data_file_path定义的表空间中，可以是多个文件。如果是多表空间存储，每个表的数据和索引单独保存在 .ibd 中。 MEMORY存储引擎 内存存储 , 速度快 ,不安全 ,适合小量快速访问的数据。表结构保存在.frm中。 特性对比 特性 MyISAM InnoDB MEMORY 存储限制 有(平台对文件系统大小的限制) 64TB 有(平台的内存限制) 事务安全 不支持 支持 不支持 锁机制 表锁 表锁/行锁 表锁 B+Tree索引 支持 支持 支持 哈希索引 不支持 不支持 支持 全文索引 支持 支持 不支持 集群索引 不支持 支持 不支持 数据索引 不支持 支持 支持 数据缓存 不支持 支持 N/A 索引缓存 支持 支持 N/A 数据可压缩 支持 不支持 不支持 空间使用 低 高 N/A 内存使用 低 高 中等 批量插入速度 高 低 高 外键 不支持 支持 不支持 -- 查询数据库支持的存储引擎 SHOW ENGINES; -- 表含义: - support : 指服务器是否支持该存储引擎 - transactions : 指存储引擎是否支持事务 - XA : 指存储引擎是否支持分布式事务处理 - Savepoints : 指存储引擎是否支持保存点 -- 查看db9数据库所有表的存储引擎 SHOW TABLE STATUS FROM db9; -- 查看db9数据库中stu_score表的存储引擎 SHOW TABLE STATUS FROM db9 WHERE NAME = &#39;stu_score&#39;; -- 创建engine_test表，指定存储引擎为MyISAM CREATE TABLE engine_test( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(10) )ENGINE = MYISAM; -- 查询engine_test表的引擎 SHOW TABLE STATUS FROM db11 WHERE NAME = &#39;engine_test&#39;; -- 修改engine_test表的引擎为InnoDB ALTER TABLE engine_test ENGINE = INNODB; -- 查询engine_test表的引擎 SHOW TABLE STATUS FROM db11 WHERE NAME = &#39;engine_test&#39;; 总结：引擎的选择 MyISAM ：由于MyISAM不支持事务、不支持外键、支持全文检索和表级锁定，读写相互阻塞，读取速度快，节约资源，所以如果应用是以查询操作和插入操作为主，只有很少的更新和删除操作，并且对事务的完整性、并发性要求不是很高，那么选择这个存储引擎是非常合适的。 InnoDB : 是MySQL的默认存储引擎， 由于InnoDB支持事务、支持外键、行级锁定 ，支持所有辅助索引(5.5.5后不支持全文检索)，高缓存，所以用于对事务的完整性有比较高的要求，在并发条件下要求数据的一致性，读写频繁的操作，那么InnoDB存储引擎是比较合适的选择，比如BBS、计费系统、充值转账等 MEMORY：将所有数据保存在RAM中，在需要快速定位记录和其他类似数据环境下，可以提供更快的访问。MEMORY的缺陷就是对表的大小有限制，太大的表无法缓存在内存中，其次是要确保表的数据可以恢复，数据库异常终止后表中的数据是可以恢复的。MEMORY表通常用于更新不太频繁的小表，用以快速得到访问结果。 总结：针对不同的需求场景，来选择最适合的存储引擎即可！如果不确定、则使用数据库默认的存储引擎！ ]]></content></entry><entry><title>面试-MySQL题</title><url>/post/%E9%9D%A2%E8%AF%95-mysql%E9%A2%98/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[数据库事务ACID四大特性 “一致性”为最终目标，其他特性都是为达到这个目标而采取的措施和手段。
原子性：事务中包括的所有操作要么都做，要么都不做 持久性：事务一旦提交.对数据库的改变是永久的 隔离性：一个事务内部的操作及使用的数据对并发的其他事务是隔离的 一致性：事务必须使数据库从一个一致性状态变到另一个一致性状态 事务并发所引起的问题： 脏读：一个事务读到另外一个事务还没有提交的数据，我们称之为脏读。（进行存款事务时候，还没有存完，允许查询事务） 事务B执行过程中修改了数据X，在未提交前，事务A读取了X，而事务B却回滚了，这样事务A就形成了脏读。
不可重复读(Unrepeatable Read)：在数据库访问中，一个事务范围内两个相同的查询却返回了不同数据。这是由于查询时系统中其他事务修改的提交而引起的。 事务A首先读取了一条数据，然后执行逻辑的时候，事务B将这条数据改变了，然后事务A再次读取的时候，发现数据不匹配了，就是所谓的不可重复读了。
幻读(phantom read)：是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样. mysql的4种事务隔离级别 1、未提交读(Read Uncommitted)：允许脏读，也就是可能读取到其他会话中未提交事务修改的数据 2、提交读(Read Committed)：只能读取到已经提交的数据。Oracle等多数数据库默认都是该级别 (不重复读) 3、可重复读(Repeated Read)：可重复读。在同一个事务内的查询都是事务开始时刻一致的，InnoDB默认级别。在SQL标准中，该隔离级别消除了不可重复读，但是还存在幻象读，但是innoDB解决了幻读 4、串行读(Serializable)：完全串行化的读，每次读都需要获得表级共享锁，读写相互都会阻塞
优化技巧 明知只有一条查询结果，那请使用 “LIMIT 1” “LIMIT 1”可以避免全表扫描，找到对应结果就不会再继续扫描了。 尽量避免使用 “SELECT *” 如果不查询表中所有的列，尽量避免使用 SELECT *，因为它会进行全表扫描，不能有效利用索引，增大了数据库服务器的负担，以及它与应用程序客户端之间的网络IO开销。 delete和truncate table 如果一个表中有自增字段，使用truncate table和没有WHERE子句的delete删除所有记录后，这个自增字段将起始值恢复成1.如果你不想这样做的话，可以在delete语句中加上永真的WHERE， delete FROM table1 WHERE 1; 顺序 聚合语句(sum,min,max,avg,count)要比having子句优先执行，所以having后面可以使用聚合函数。而where子句在查询过程中执行优先级别优先于聚合语句(sum,min,max,avg,count)，所有where条件中不能使用聚合函数。
#　题
获取员工其当前的薪水比其manager当前薪水还高的相关信息 第一列给出员工的emp_no， 第二列给出其manager的manager_no， 第三列给出该员工当前的薪水emp_salary, 第四列给该员工对应的manager当前的薪水manager_salary
SELECT es.emp_no, ms.emp_no, es.salary, ms.salary /*员工工资表*/ FROM (SELECT e.emp_no, e.dept_no, s.salary FROM dept_emp AS e INNER JOIN salaries AS s ON e.emp_no=s.emp_no /*当前？*/ WHERE e.to_date=&#39;9999-01-01&#39; AND s.to_date=&#39;9999-01-01&#39;) AS es /*经理工资表*/ INNER JOIN (SELECT m.emp_no, m.dept_no, s.salary FROM dept_manager AS m INNER JOIN salaries AS s ON m.emp_no=s.emp_no WHERE m.to_date=&#39;9999-01-01&#39; AND s.to_date=&#39;9999-01-01&#39;) AS ms ON es.dept_no=ms.dept_no WHERE es.salary&gt;ms.salary; 查找所有员工自入职以来的薪水涨幅情况 给出员工编号emp_no以及其对应的薪水涨幅growth，并按照growth进行升序，以上例子输出为 （注:可能有employees表和salaries表里存在记录的员工，有对应的员工编号和涨薪记录，但是已经离职了，离职的员工salaries表的最新的to_date!=&lsquo;9999-01-01&rsquo;，这样的数据不显示在查找结果里面，以上emp_no为2的就是这样的）
SELECT cs.emp_no, (cs.salary-fs.salary) AS growth FROM (SELECT emp_no, salary FROM salaries WHERE to_date=&#39;9999-01-01&#39;) AS cs /*当前的工资*/ INNER JOIN (SELECT e.emp_no, s.salary FROM salaries AS s INNER JOIN employees AS e ON s.emp_no=e.emp_no WHERE s.from_date=e.hire_date)AS fs /*刚入职的工资*/ ON cs.emp_no=fs.emp_no ORDER BY growth ASC; 对所有员工的薪水按照salary进行按照1-N的排名，相同salary并列且按照emp_no升序排列 SELECT s1.emp_no, s1.salary, /*计算比自己工资高的人数，如果自己的工资是最高的，即没有人比自己工资高，那么计算后结果为 0。 如果自己是第二高，那么结果为 1。最后在结果上 + 1，就是上面所说的会跳数字方式。*/ (SELECT COUNT(DISTINCT s2.salary)+1 FROM salaries AS s2 WHERE s2.salary&gt;s1.salary AND s2.to_date=&#39;9999-01-01&#39;) AS rank FROM salaries AS s1 WHERE s1.to_date=&#39;9999-01-01&#39; ORDER BY rank, s1.emp_no; 窗口函数
SELECT emp_no, salary, DENSE_RANK() OVER (ORDER BY salary DESC) AS rank FROM salaries WHERE to_data=&#39;9999-01-01&#39; ORDER BY rank, emp_no; 汇总各个部门当前员工的title类型的分配数目，即结果给出部门编号dept_no、dept_name、其部门下所有的员工的title以及该类型title对应的数目count，结果按照dept_no升序排 先将dept_emp和titles内联，生成带emp_no, dept_no, title 的员工信息表，再和departments右联。
select d.dept_no, d.dept_name, et.title, count(*) from (select e.emp_no, e.dept_no, t.title from dept_emp as e inner join titles as t on e.emp_no=t.emp_no where e.to_date=&#39;9999-01-01&#39; and t.to_date=&#39;9999-01-01&#39;) as et left join departments as d on et.dept_no=d.dept_no group by d.dept_no, et.title 复购率 date_format(paidtime, '%Y-%m-01') date_sub(t2.m, interval 1 month)
select t1.pt, count(t1.pt), count(t2.pt) from(select userid, date_format(paidtime, &#39;%Y-%m-01&#39;) as pt from orderinfo where ispaid=&#39;已支付&#39; group by userid, pt) as t1 left join(select userid, date_format(paidtime, &#39;%Y-%m-01&#39;) as pt from orderinfo where ispaid=&#39;己支付&#39; group by userid, pt) as t2 on t1.userid=t2.userid and t1.pt=date_sub(t2.pt, interval 1 month) group by t1.pt; SQL取出所有用户对商品的行为特征 请用一句SQL取出所有用户对商品的行为特征，特征分为已购买、购买未收藏、收藏未购买、收藏且购买（输出结果如下表）
case when pay time is not null then 1 else 0 end
(select a.user_id,a.item_id, (CASE when a.item_id is not null then 1 else 0 end) as &#39;已购买&#39;, (CASE when a.item_id is not null and b.item_id is null then 1 else 0 end) as &#39;购买未收藏&#39;, (CASE when a.item_id is not null and b.item_id is not null then 1 else 0 end) as &#39;收藏且购买&#39;, (CASE when a.item_id is null and b.item_id is not null then 1 else 0 end) as &#39;收藏未购买&#39; from orders a left join favorites b on a.user_id = b.user_id and a.item_id = b.item_id ) union (select b.user_id,b.item_id, (CASE when a.item_id is not null then 1 else 0 end) as &#39;已购买&#39;, (CASE when a.item_id is not null and b.item_id is null then 1 else 0 end) as &#39;购买未收藏&#39;, (CASE when a.item_id is not null and b.item_id is not null then 1 else 0 end) as &#39;收藏且购买&#39;, (CASE when a.item_id is null and b.item_id is not null then 1 else 0 end) as &#39;收藏未购买&#39; from orders a right join favorites b on a.user_id = b.user_id and a.item_id = b.item_id ); left join favorites
有一张学生成绩表sc（sno 学号，class 课程，score 成绩），请查询出每个学生的英语、数学的成绩（行转列，一个学生只有一行记录） if(class='english',score,0) in('english','math')
select sno, sum(if(class=&#39;english&#39;,score,0)) as english, sum( if(class=&#39;math&#39;,score,0) ) as math from sc where class in(&#39;english&#39;,&#39;math&#39;) group by sno 各科成绩前两名的记录 select * from (select *, row_number() over (partition by 课程号 order by 成绩 desc) as 排名 from score) as a where 排名&lt;=2; 连续登录问题 假设有一张含两列(用户id、登陆日期)的表，查询每个用户连续登陆的天数、最早登录时间、最晚登录时间和登录次数。 第一步，先用row_number（）函数排序，然后用登录日期减去排名，得到辅助列日期，如果辅助列日期是相同的话，证明用户是连续登录。
第二步，用user_id和辅助列作为分组依据，分到一组的就是连续登录的用户。在每一组中最小的日期就是最早的登陆日期，最大的日期就是最近的登陆日期，对每个组内的用户进行计数,就是用户连续登录的天数。 求解连续登录五天的用户 第一步，用lead函数进行窗口偏移，查找每个用户5天后的登陆日期是多少，如果是空值，说明他没有登录。运行的代码为 在lead函数里，为何偏移行数的参数设置为4而不是5呢，这是因为求解的是连续登录5天的用户，包括当前行在内一共是5行，所以应该向下偏移4行。运行的结果如下： 第二步，用datediff函数计算 （日期-第五次登陆日期）+1是否等于5，等于5证明用户是连续5天登录的，为空值或者大于5都不是5天连续登陆的用户。 第三步，用where设定条件，差值=5筛选连续登录的用户。 用lead函数求解连续登录的问题还有一个好处就是当表中的数据不在同一个月份时也可以完美的解决，不用再考虑月份带来的影响。
2018年9月1日当天，总在线时长超过20分钟以上的用户的付费总金额 A表为游戏登出表，用户每次下线时记录一条：dt（登出日期），servertime（登出时间），userid（用户id），onlinetime（当次在线时长（单位：秒），int）； B表为游戏充值表，用户每次充值记录一条：dt（充值日期），servertime（充值时间），userid（用户id），money（充值金额，int）
SELECT A.userid, SUM(IFNULL(B.money, 0)) FROM A LEFT JOIN B ON A.dt = B.dt AND A.userid = B.userid WHERE A.dt = &#39;2018-09-01&#39; GROUP BY A.userid HAVING SUM(A.onlinetime) &gt; 20*60; ]]></content></entry><entry><title>面试-Normalize</title><url>/post/%E9%9D%A2%E8%AF%95-normalize/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[Batch Normalization（BN，2015年）
Layer Normalization（LN，2016年）
Instance Normalization（IN，2017年）
Group Normalization（GN，2018年）
它们都是从激活函数的输入来考虑、做文章的，以不同的方式对激活函数的输入进行 Norm 的。
另外，还需要注意它们的映射参数γ和β的区别：对于 BN，IN，GN， 其γ和β都是维度等于通道数 C 的向量。而对于 LN，其γ和β都是维度等于 normalized_shape 的矩阵。
最后，BN 和 IN 可以设置参数：momentum和track_running_stats来获得在整体数据上更准确的均值和标准差。LN 和 GN 只能计算当前 batch 内数据的真实均值和标准差
我们将输入的 feature map shape 记为**[N, C, H, W]**，其中N表示batch size，即N个样本；C表示通道数；H、W分别表示特征图的高度、宽度。这几个方法主要的区别就是在：
BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN；
LN在通道方向上，对C、H、W归一化，主要对RNN效果明显；
IN在图像像素上，对H、W做归一化，用在风格化迁移；
GN将channel分组，然后再做归一化。
每个子图表示一个特征图，其中N为批量，C为通道，（H，W）为特征图的高度和宽度。通过蓝色部分的值来计算均值和方差，从而进行归一化。
如果把特征图比喻成一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 有W 个字符。
\1. BN 求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页&hellip;&hellip;），再除以每个页码下的字符总数：N×H×W，因此可以把 BN 看成求“平均书”的操作（注意这个“平均书”每页只有一个字），求标准差时也是同理。
\2. LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的字符总数：C×H×W，即求整本书的“平均字”，求标准差时也是同理。
\3. IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H×W，即求每页书的“平均字”，求标准差时也是同理。
\4. GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，求每个小册子的“平均字”和字的“标准差”。
Batch Normalization, BN 论文链接：https://arxiv.org/pdf/1502.03167.pdf
为什么要进行BN呢？
（1）在深度神经网络训练的过程中，通常以输入网络的每一个mini-batch进行训练，这样每个batch具有不同的分布，使模型训练起来特别困难。
（2）Internal Covariate Shift (ICS) 问题：在训练的过程中，激活函数会改变各层数据的分布，随着网络的加深，这种改变（差异）会越来越大，使模型训练起来特别困难，收敛速度很慢，会出现梯度消失的问题。
**BN的主要思想：**针对每个神经元，**沿着通道计算每个batch的均值、方差，‘强迫’数据保持均值为0，方差为1的正态分布，**避免发生梯度消失。
具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 &hellip;&hellip; 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以 N×H×W 而不是单纯除以 N，最后得到的是一个代表这个 batch 第1个通道平均值的数字，而不是一个 H×W 的矩阵）。求通道 1 的方差也是同理。对所有通道都施加一遍这个操作，就得到了所有通道的均值和方差。
**BN的使用位置：**全连接层或卷积操作之后，激活函数之前。
BN算法过程：
沿着通道计算每个batch的均值μ 沿着通道计算每个batch的方差σ² 做归一化 加入缩放和平移变量 γ 和 β 其中 ε 是一个很小的正值，比如 **。**加入缩放和平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。
BN的作用：
（1）允许较大的学习率；
（2）减弱对初始化的强依赖性，用在激活层之前。其作用可以加快模型训练时的收敛速度，
（3）保持隐藏层中数值的均值、方差不变，让数值更稳定，训练过程更加稳定，避免梯度爆炸或者梯度消失。
（4）有轻微的正则化作用（相当于给隐藏层加入噪声，类似Dropout）
BN存在的问题：
（1）每次是在一个batch上计算均值、方差，如果batch size太小，则计算的均值、方差不足以代表整个数据分布。
（2）**batch size太大：**会超过内存容量；需要跑更多的epoch，导致总训练时间变长；会直接固定梯度下降的方向，导致很难更新。
Layer Normalization, LN 论文链接：https://arxiv.org/pdf/1607.06450v1.pdf
针对BN不适用于深度不固定的网络（sequence长度不一致，如RNN），LN对深度网络的某一层的所有神经元的输入按以下公式进行normalization操作。
LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。
对于特征图 ，LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。其均值和标准差公式为：
Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN不依赖于batch size和输入sequence的长度，因此可以用于batch size为1和RNN中。LN用于RNN效果比较明显，但是在CNN上，效果不如BN。
三、 Instance Normalization, IN
论文链接：https://arxiv.org/pdf/1607.08022.pdf
IN针对图像像素做normalization，最初用于图像的风格化迁移。在图像风格化中，生成结果主要依赖于某个图像实例，feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格。所以对整个batch归一化不适合图像风格化中，因而对H、W做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。
对于，IN 对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C 维度，也就是说，它只在 channel 内部求均值和标准差，其公式如下：
四、 Group Normalization, GN（拿小本本get一下）
论文链接：https://arxiv.org/pdf/1803.08494.pdf
**GN是为了解决BN对较小的mini-batch size效果差的问题。**GN适用于占用显存比较大的任务，例如图像分割。对这类任务，可能 batch size 只能是个位数，再大显存就不够用了。而当 batch size 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 也是独立于 batch 的，它是 LN 和 IN 的折中。
**GN的主要思想：**在 channel 方向 group，然后每个 group 内做 Norm，计算的均值和方差，这样就与batch size无关，不受其约束。
**具体方法：**GN 计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。
伪代码如下：
代码如下：
def GroupNorm(x, gamma, beta, G=16): # x_shape:[N, C, H, W] results = 0. eps = 1e-5 x = np.reshape(x, (x.shape[0], G, x.shape[1]/16, x.shape[2], x.shape[3])) x_mean = np.mean(x, axis=(2, 3, 4), keepdims=True) x_var = np.var(x, axis=(2, 3, 4), keepdims=True0) x_normalized = (x - x_mean) / np.sqrt(x_var + eps) results = gamma * x_normalized + beta return results ]]></content></entry><entry><title>面试-标准化、归一化、正则化</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%A0%87%E5%87%86%E5%8C%96%E5%BD%92%E4%B8%80%E5%8C%96%E6%AD%A3%E5%88%99%E5%8C%96/</url><categories><category>面试</category></categories><tags/><content type="html">瘦长的椭圆，会导致趋向最值时梯度下降的震荡；所以需要缩放特征值，使得其取值范围相近。按经验，特征缩放到3倍或1/3是比较可以接受的。 标准化（Standardization）（Z-score） 标准化即为概率论与数理统计中常见的Z-score标准化。在特征值的均值（mean）和标准差（standard deviation）的基础上计算得出。 处理后特征符合标准正态分布[-1,1]。
$z=\frac{x-\mu}{\sigma}$
from sklearn.preprocessing import StandardScaler scaler = StandardScaler().fit(x_train) #标准化，将特征值映射到负无穷到正无穷 x_trainScaler = scaler.transform(x_train) x_testScaler = scaler.transform(x_test) 规范化 (normalization)\归一化： 归一化是将每个样本缩放为单位范数（每个样本的范数为1）。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”，[0,1] $X_{\text {norm }}=\frac{X-X_{\min }}{X_{\max }-X_{\min }}$
from sklearn.preprocessing import Normalizer scaler = Normalizer().fit(x_train)#归一化 x_trainScaler = scaler.transform(x_train) x_testScaler = scaler.transform(x_test) 在实际应用中，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模型。
但对于決策树模型则并不适用，以C4.5为例，决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比跟特征是否经过归一化是无关的因为归ー化并不会改变样本在特征x上的信息增益。
原因1：因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率； 原因2：因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。 向你的模型加入某些规则，加入先验，缩小解空间，减小求出错误解的可能性。你要把你的知识数学化告诉这个模型，对代价函数来说，就是加入对模型“长相”的惩罚
则化通过避免训练完美拟合数据样本的系数而有助于算法的泛化。为了防止过拟合， 增加训练样本是一个好的解决方案。此外， 还可使用数据增强、 L1正则化、 L2 正则化、 Dropout、 DropConnect 和早停（Early stopping） 法等。
L1正则化、 L2 正则化 在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。
可以用于防止过拟合的原因在于更小的权值表示神经网络的复杂度更低、网络参数越小，这说明模型相对简单，越简单的模型引起过度拟合的可能性越小。对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，即抗扰动能力强。
L1正则化 L2 正则化 L1正则化向目标函数添加正则化项，以减少参数的绝对值总和；L1通常是比L2更容易得到稀疏输出的，会把一些不重要的特征直接置零L1正则化中的很多参数向量是稀疏向量，因为很多模型导致参数趋近于0，因此它常用于特征选择。同时一定程度上防止过拟合；
L2正则化中， 添加正则化项的目的在于减少参数平方的总和。机器学习中最常用的正则化方法是对权重施加L2范数约束。
线性回归中，使用L1正则化的为Lasso回归，使用L2正则化的为Ridge回归(岭回归)，既使用L1正则又使用L2正则的为 ElasticNet。
L1相对于L2更能实现权值稀疏，L1是各元素绝对值之和，L2是各元素平方和的根，在对不同参数进行惩罚时，L1无论参数大小如何，对它们的惩罚值都相同，导致那些参数大小和惩罚值相等的参数，一减就变为 0，而L2对参数的惩罚值是根据参数本身的大小来变化的，越小的参数惩罚值越小，越大的参数惩罚值越大，所以最终使得所有参数都接近 0，但不会等于0。
上图代表的意思就是目标函数-平方误差项的等值线和L1、L2范数等值线（左边是L1），我们正则化后的代价函数需要求解的目标就是在经验风险和模型复杂度之间的平衡取舍，在图中形象地表示就是黑色线与彩色线的交叉点。
对于L1范数，其图形为菱形，二维属性的等值线有4个角（高维的会有更多），“突出来的角”更容易与平方误差项进行交叉，而这些“突出来的角”都是在坐标轴上，即W1或则W2为0； 而对于L2范数，交叉点一般都是在某个象限中，很少有直接在坐标轴上交叉的。
因此L1范数正则化项比L2的更容易得到稀疏解。</content></entry><entry><title>面试-方差、偏差、过拟合与欠拟合</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/</url><categories><category>面试</category></categories><tags/><content type="html">过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是模型在训练和预测时表现都不好的情况。
方差、偏差 模型的偏差，指的是模型预测的期望值与真实值之间的差；用于描述模型的拟合能力
模型的方差，指的是模型预测的期望值与预测值之间的差平方和；用于描述模型的稳定性
误差是观察值和总体平均值的偏差，而残差是观察值和样本平均值的偏差。 高偏差：Jtrain和Jcv都很大，并且Jtrain≈Jcv。对应欠拟合。 高方差：Jtrain较小，Jcv远大于Jtrain。对应过拟合。（受数据扰动影响大） 过拟合 数据：获取更多的数据集、 对数据集做扰动和扩增、 交叉验证、 降维 模型：简化模型、决策树的剪枝操作、Droupout、BN（batch normalization）、L1/l2正则化、Early Stopping (提前终止训练)。 集成学习：把多个模型集成在一起，来降低单一模型的过拟合风险，如 Bagging方法。 欠拟合 数据：数据集未打乱、未进行归一化、特征工程中对数据特征的选取有问题 模型：结构简单、 权重初始化方案有问题、选择合适的激活函数、损失函数、 选择合适的优化器和学习速率、batch size过大、训练时间不足、模型训练遇到瓶颈(梯度爆炸和梯度弥散产生的根本原因是，根据链式法则，深度学习中的梯度在逐层累积。)、减小正则化系数 训练集loss不下降（欠拟合） 数据集有问题 当一个数据集噪声过多，或者数据标注有大量错误时，会使得神经网络难以从中学到有用的信息，从而出现摇摆不定的情况。
数据集未打乱，不打乱数据集的话，会导致网络在学习过程中产生一定的偏见问题。比如张三和李四常常出现在同一批数据中，那么结果就是，神经网络看见了张三就会“想起”李四。
未进行归一化，未进行归一化会导致尺度的不平衡，比如1km和1cm的不平衡，因此会导致误差变大，或者在同样的学习率下，模型会以秒速五厘米的步伐，左右两边摇摆不定地，向前走1km。
数据特征的选取不合理，就像数据标注错误一样，会使得神经网络难以找到数据的本质特征进行学习。而机器学习的本质就是在做特征工程，以及清洗数据(逃)。
获取更多的数据集，对数据集做扰动和扩增，比如对图像做旋转，对声音文件进行加噪处理等。最终的效果虽然比不上同等情况下的数据量的增加带来的效果增益，但是在现有条件下，算是扩增数据量的一个有效的方案。
模型结构存在问题 增加深度，也就是增加神经网络的层数就可以了。也可以增加神经网络的宽度，将每一层的神经单元数量增加，但是同等情况下，效果明显不如增加层数，而且要想达到较好的效果，需要增加的神经元数远超过增加一层增加的神经元数。深度深比宽度宽的模型更优这一点，是大家普遍认同的。
权重初始化方案有问题 https://keras.io/zh/initializers/ 这么多初始化方案，其实按照大类来分，主要就三种：均匀分布、正太分布和相同固定值。 全0的初始化，一般只会被用于逻辑斯蒂回归之类的这种二分类问题上，最多是浅层的神经网络上。全为1或者某个其他相同的值的方案则很少见。因为这种初始化方案，会使得网络处于对称状态，导致的结果就是，每个神经元都具有相同的输出，然后在反向传播计算梯度时，会得到一个同一个梯度值，并且进行着同样的参数更新，这是我们不希望看到的。 keras中，神经网络默认初始化全部被初始化为了glorot_uniform，也就是一种均值为0，以0为中心的对称区间均匀分布的随机数。在我的模型上，这种接近于0的均匀分布会导致什么问题呢？那就是梯度的消失，使得训练时的loss难以收敛
正则化过度 选择合适的激活函数、损失函数 卷积层的输出，一般使用ReLu作为激活函数，因为可以有效避免梯度消失，并且线性函数在计算性能上面更加有优势。而循环神经网络中的循环层一般为tanh，或者ReLu，全连接层也多用ReLu，只有在神经网络的输出层，使用全连接层来分类的情况下，才会使用softmax这种激活函数。而在各种机器学习入门教程里面最常讲到的sigmoid函数，想都不要想它，它已经不适用于深度学习了，哪怕是作为其改进版的softmax函数，也仅仅是在输出层才使用。 而损失函数，对于一些分类任务，通常使用交叉熵损失函数，回归任务使用均方误差，有自动对齐的任务使用CTC loss等。
选择合适的优化器和学习速率 神经网络的优化器选取一般选取Adam，但是在有些情况下Adam难以训练，这时候需要使用如SGD之类的其他优化器。 学习率决定了网络训练的速度，但学习率不是越大越好，当网络趋近于收敛时应该选择较小的学习率来保证找到更好的最优点。所以，我们需要手动调整学习率，首先选择一个合适的初始学习率，当训练不动之后，稍微降低学习率，然后再训练一段时间，这时候基本上就完全收敛了。一般学习率的调整是乘以/除以10的倍数。不过现在也有一些自动调整学习率的方案了。
训练时间不足 模型训练遇到瓶颈 这里的瓶颈一般包括：梯度消失、大量神经元失活、梯度爆炸和弥散等。梯度消失时，模型的loss难以下降，可以通过梯度的检验来验证模型当前所处的状态。有时梯度的更新和反向传播代码存在bug时，也会有这样的问题。 在使用Relu激活函数的时候，当每一个神经元的输入X为负时，会使得该神经元输出恒为0，导致失活，由于此时梯度为0，无法恢复。有一种解决方案是使用LeakyRelu，这时，Y轴的左边图线会有一个很小的正梯度，使得神经网络在一定时间后可以得到恢复。不过LeakyRelu并不常用，因为部分神经元失活并不影响结果，相反，这种输出为0还有很多积极的作用。因为Relu方程输入为负时，输出值为0，利用此特性可以很好地忽略掉卷积核输出负相关信息，同时保留相关信息。梯度爆炸和梯度弥散产生的根本原因是，根据链式法则，深度学习中的梯度在逐层累积。
batch size过大 batch size过小，会导致模型后期摇摆不定，迟迟难以收敛，而过大时，模型前期由于梯度的平均，导致收敛速度过慢。一般batch size 的大小常常选取为32，或者16，有些任务下比如NLP中，可以选取8作为一批数据的个数。不过，有时候，为了减小通信开销和计算开销的比例，也可以调整到非常大的值，尤其是在并行和分布式中。
验证集loss不下降（过拟合） 一种是训练集上的loss也不下降，这时问题主要在训练集的loss上，应当先参考上述方法解决。另一种是训练集上的loss可以下降，但验证集上的loss已经不降了，这里我们主要说明这种情况下的问题。
由于验证集是从同一批训练数据中划分出来的，所以一般不存在数据集的问题，所以主要是过拟合。
适当降低模型的规模 适当的正则化和降维 比如通过增加一个正则项，并且人为给定一个正则系数lambda，进行权重衰减，将一些相关性不大的特征项的参数衰减到几乎为0，相当于去掉了这一项特征，这跟降维类似，相当于减少了特征维度。而去掉基本无关的维度，那么就避免了模型对于这一维度特征的过分拟合。还有在神经网络两个层之间增加Dropout和Normal等，也起到了抑制过拟合的作用。
测试集loss不下降 由于训练集和验证集的loss不下降时，应归为前两节的内容，所以这一节中，我们默认训练集和验证集的loss情况是正常的。所以，如果测试集的loss很高，或者正确率很低，那么一般是因为训练数据的分布和场景与测试数据的分布和应用场景不一致。
获取更多的数据集 对数据集做扰动和扩增，比如对图像做旋转，对声音文件进行加噪处理等。最终的效果虽然比不上同等情况下的数据量的增加带来的效果增益，但是在现有条件下，算是扩增数据量的一个有效的方案。
应用场景不一致 比如，一个语音识别模型，输入的数据集都是女性的录音音频，那么对于男性的声音就不能很好的识别出来。
噪声问题 噪声问题是实际应用场景下，频繁遇到的问题。直接容易理解的案例就是，在语音识别中，标准语音数据集都是在安静环境下采集的数据，但是在实际应用中，我们录音时多多少少会有噪声，那么我们需要专门去处理噪声，比如进行一个降噪处理，或者在训练数据中添加噪声等。在图像的识别中，那么就需要考虑图片中的遮挡、雾霾、旋转、镜像和大小远近等问题。</content></entry><entry><title>面试-激活、损失函数</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[Cross Entropy Loss 交叉熵损失函数。马上就能说出它的二分类公式：
在二分类问题模型，真实样本的标签为 [0，1]，分别表示负类和正类。模型的最后通常会经过一个 Sigmoid 函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大。
横坐标是预测输出，纵坐标是交叉熵损失函数 L。显然，预测输出越接近真实样本标签 1，损失函数 L 越小；预测输出越接近 0，L 越大。因此，函数的变化趋势完全符合实际需要的情况。
无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。
BCE BCE损失函数（Binary Cross-Entropy Loss）是交叉熵损失函数（Cross-Entropy Loss）的一种特例，BCE Loss只应用在二分类任务中。针对分类问题，单样本的交叉熵损失为：
import torch import torch.nn as nn bce = nn.BCELoss() bce_sig = nn.BCEWithLogitsLoss() input = torch.randn(5, 1, requires_grad=True) target = torch.empty(5, 1).random_(2) pre = nn.Sigmoid()(input) loss_bce = bce(pre, target) loss_bce_sig = bce_sig(input, target) # 同时，pytorch还提供了已经结合了Sigmoid函数的BCE损失：torch.nn.BCEWithLogitsLoss()，相当于免去了实现进行Sigmoid激活的操作。 input = tensor([[-0.2296], [-0.6389], [-0.2405], [ 1.3451], [ 0.7580]], requires_grad=True) output = tensor([[1.], [0.], [0.], [1.], [1.]]) pre = tensor([[0.4428], [0.3455], [0.4402], [0.7933], [0.6809]], grad_fn=&lt;SigmoidBackward&gt;) print(loss_bce) tensor(0.4869, grad_fn=&lt;BinaryCrossEntropyBackward&gt;) print(loss_bce_sig) tensor(0.4869, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;) Label Smoothing 对于标注数据来说，这个时候我们认为其标注结果是准确的（不然这个结果就没意义了）。但实际上，有一些标注数据并不一定是准确的。那么这时候，使用交叉熵损失函数作为目标函数并不一定是最优的。这会导致模型对正确分类的情况奖励最大，错误分类惩罚最大。如果训练数据能覆盖所有情况，或者是完全正确，那么这种方式没有问题。但事实上，这不可能。所以这种方式可能会带来泛化能力差的问题，即过拟合。
Focal Loss 在目标检测领域对于one-stage的检测器准确率不高的问题，论文作者给出了解释：由于正负样本不均衡的问题（感觉理解成简单-难分样本不均衡比较好）。 什么意思呢，就是说one-stage中能够匹配到目标的候选框（正样本）个数一般只用十几个或几十个，而没匹配到的候选框（负样本）大概有$10^4 - 10^5$个。而负样本大多数都是简单易分的，对训练起不到什么作用，但是数量太多会淹没掉少数但是对训练有帮助的困难样本。
那么正负样本不均衡，会带来什么问题呢？
训练效率低下。 training is inefficient as most locations are easy negatives that contribute no useful learning signal; 模型精度变低。 过多的负样本会主导训练，使模型退化。en masse,the easy negatives can overwhelm training and lead to degenerate models. 如上图，横坐标代表$p_t$，纵坐标表示各种样本所占的loss权重。对于正样本，我们希望p越接近1越好，也就是$p_t$越接近1越好；对于负样本，我们希望p越接近0越好，也就是$p_t$越接近1越好。所以不管是正样本还是负样本，我们总是希望他预测得到的$p_t$ 越大越好。如上图所示，$p_t\in[0.6, 1]$就是我们预测效果比较好的样本（也就是易分样本）了。显然可以想象这部分的样本数量很多，所以占比是比较高的（如图中蓝色线区域），我们用$(1-p_t)^\gamma$ 来降低易分样本的损失占比 / 损失贡献（如图其他颜色的曲线）。
优点：
解决了one-stage object detection中图片中正负样本（前景和背景）不均衡的问题；
降低简单样本的权重，使损失函数更关注困难样本；
缺点：
模型很容易收到噪声干扰：会将噪声当成复杂样本，使模型过拟合退化； 模型的初期，数量多的一类可能主导整个loss，所以训练初期可能训练不稳定； 两个参数$\alpha_t$ 和$\gamma$具体的值很难定义，需要自己调参，调的不好可能效果会更差（论文中的 $\alpha_t$=0.25，$\gamma$=2最好）。 import torch.nn as nn import torch import torch.nn.functional as F class FocalLoss(nn.Module): def __init__(self, alpha=1, gamma=2, logits=False, reduce=True): super(FocalLoss, self).__init__() self.alpha = alpha self.gamma = gamma self.logits = logits # 如果BEC带logits则损失函数在计算BECloss之前会自动计算softmax/sigmoid将其映射到[0,1] self.reduce = reduce def forward(self, inputs, targets): if self.logits: BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False) else: BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False) pt = torch.exp(-BCE_loss) F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss if self.reduce: return torch.mean(F_loss) else: return F_loss FL1 = FocalLoss(logits=False) FL2 = FocalLoss(logits=True) inputs = torch.randn(5, 1, requires_grad=True) targets = torch.empty(5, 1).random_(2) pre = nn.Sigmoid()(inputs) f_loss_1 = FL1(pre, targets) f_loss_2 = FL2(inputs, targets) print(&#39;inputs:&#39;, inputs) inputs: tensor([[-1.3521], [ 0.4975], [-1.0178], [-0.3859], [-0.2923]], requires_grad=True) print(&#39;targets:&#39;, targets) targets: tensor([[1.], [1.], [0.], [1.], [1.]]) print(&#39;pre:&#39;, pre) pre: tensor([[0.2055], [0.6219], [0.2655], [0.4047], [0.4274]], grad_fn=&lt;SigmoidBackward&gt;) print(&#39;f_loss_1:&#39;, f_loss_1) f_loss_1: tensor(0.3375, grad_fn=&lt;MeanBackward0&gt;) print(&#39;f_loss_2&#39;, f_loss_2) f_loss_2 tensor(0.3375, grad_fn=&lt;MeanBackward0&gt;) Lovász-Softmax IoU是评价分割模型分割结果质量的重要指标，因此很自然想到能否用1−IoU（即Jaccard loss）来做损失函数，但是它是一个离散的loss，不能直接求导，所以无法直接用来作为损失函数。为了克服这个离散的问题，可以采用Lovász extension将离散的Jaccard loss 变得连续，从而可以直接求导，使得其作为分割网络的loss function。Lovász-Softmax相比于交叉熵函数具有更好的效果。
import torch from torch.autograd import Variable import torch.nn.functional as F import numpy as np try: from itertools import ifilterfalse except ImportError: # py3k from itertools import filterfalse as ifilterfalse # --------------------------- MULTICLASS LOSSES --------------------------- def lovasz_softmax(probas, labels, classes=&#39;present&#39;, per_image=False, ignore=None): &#34;&#34;&#34; Multi-class Lovasz-Softmax loss probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1). Interpreted as binary (sigmoid) output with outputs of size [B, H, W]. labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1) classes: &#39;all&#39; for all, &#39;present&#39; for classes present in labels, or a list of classes to average. per_image: compute the loss per image instead of per batch ignore: void class labels &#34;&#34;&#34; if per_image: loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes) for prob, lab in zip(probas, labels)) else: loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes) return loss def lovasz_softmax_flat(probas, labels, classes=&#39;present&#39;): &#34;&#34;&#34; Multi-class Lovasz-Softmax loss probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1) labels: [P] Tensor, ground truth labels (between 0 and C - 1) classes: &#39;all&#39; for all, &#39;present&#39; for classes present in labels, or a list of classes to average. &#34;&#34;&#34; if probas.numel() == 0: # only void pixels, the gradients should be 0 return probas * 0. C = probas.size(1) losses = [] class_to_sum = list(range(C)) if classes in [&#39;all&#39;, &#39;present&#39;] else classes for c in class_to_sum: fg = (labels == c).float() # foreground for class c if (classes is &#39;present&#39; and fg.sum() == 0): continue if C == 1: if len(classes) &gt; 1: raise ValueError(&#39;Sigmoid output possible only with 1 class&#39;) class_pred = probas[:, 0] else: class_pred = probas[:, c] errors = (Variable(fg) - class_pred).abs() errors_sorted, perm = torch.sort(errors, 0, descending=True) perm = perm.data fg_sorted = fg[perm] losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted)))) return mean(losses) def flatten_probas(probas, labels, ignore=None): &#34;&#34;&#34; Flattens predictions in the batch &#34;&#34;&#34; if probas.dim() == 3: # assumes output of a sigmoid layer B, H, W = probas.size() probas = probas.view(B, 1, H, W) B, C, H, W = probas.size() probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C) # B * H * W, C = P, C labels = labels.view(-1) if ignore is None: return probas, labels valid = (labels != ignore) vprobas = probas[valid.nonzero().squeeze()] vlabels = labels[valid] return vprobas, vlabels def xloss(logits, labels, ignore=None): &#34;&#34;&#34; Cross entropy loss &#34;&#34;&#34; return F.cross_entropy(logits, Variable(labels), ignore_index=255) # --------------------------- HELPER FUNCTIONS --------------------------- def isnan(x): return x != x def mean(l, ignore_nan=False, empty=0): &#34;&#34;&#34; nanmean compatible with generators. &#34;&#34;&#34; l = iter(l) if ignore_nan: l = ifilterfalse(isnan, l) try: n = 1 acc = next(l) except StopIteration: if empty == &#39;raise&#39;: raise ValueError(&#39;Empty mean&#39;) return empty for n, v in enumerate(l, 2): acc += v if n == 1: return acc return acc / n 检测速度 前传耗时：从输入一张图像到输出最终结果所消耗的时间，包括前处理耗时（如图 像归一化）、网络前传耗时、后处理耗时（如非极大值抑制） 每秒帧数FPS(Frames Per Second)：每秒钟能处理的图像数量 浮点运算量(FLOPS floating-point operations per second)：处理一张图像所需要的浮点运算数量，跟具体软硬件没有关系，可以公平地比较不同算法之间的检测速度。
]]></content></entry><entry><title>面试-技术问题</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98/</url><categories><category>面试</category></categories><tags/><content type="html">排序算法 不稳定：快选堆希
O(n*log2n) 快归堆希
1.插入排序 ： 从第二个元素开始，每轮，已排序后面的元素向前比较，找到比他小或相等的元素，放到其后面 2.冒泡排序 ：每轮，从头开始和后面的比较交换，把未排序最大的放到后面。
3.选择排序 ：每轮，从已排序后面的元素开始，选出最小的放到已排序的后面
4.计数排序：arr每个元素放进max-min个桶里，依次从桶序列里取出放进arr
5.归并排序 ：递归排序左右子序列， 将两个排好的子序列，合并
6.快速排序：根据基准分成左，中，右，递归排序左右子序列，拼接
7.希尔排序
8.堆排序
解决思路： ①取列表前k个元素建⽴⼀个⼩根堆。堆顶就是⽬前第k⼤的数。 ②依次向后遍历原列表，对于列表中的元素，如果⼩于堆顶，则忽略该元素；如果⼤于堆顶，则将堆顶更换为该元素，并且对堆进⾏⼀次调整； ③遍历列表所有元素后，倒序弹出堆顶 时间复杂度为：O(nlogn)；空间复杂度：O(1)
AUC 模型评估指标 L1正则化和L2正则化的区别 在损失函数中给每个参数 w 加上权重，引入模型复杂度指标，从而抑制模型噪声，减小过拟合。
可以用于防止过拟合的原因在于更小的权值表示神经网络的复杂度更低、网络参数越小，这说明模型相对简单，越简单的模型引起过度拟合的可能性越小。
标准化、归一化、正则 类别数据不平衡的问题 训练策略 数据 模型/网络结构 loss函数 评价指标
类别数据不平衡的问题 模型压缩方法 anchor-free目标检测网络
BN 面试-Normal.md 你们的训练集和验证集，测试集分别是多少？怎么来的？ （1）说一下 Yolo V1、V2、V3的区别
（2）谈一下你最近看过的印象深刻的一篇论文
（3）resnet为什么可以更深？
faster rcnn损失函数构成
yolov3中anchor尺寸设置
smooth l1 loss式子以及为什么是这样的
语义分割中miou计算公式
了解的语义分割算法
直接转置卷积和先上采样再卷积的区别
数据增强的常用方法，以及项目里用的数据增强，目标检测中的数据增强 Cutmix 是从一幅图中随机裁剪出一个 ROI，然后覆盖当前图像中对应的区域
Mosaic简单的说就是把四张训练图片缩放拼成一张图，Mosaic有利于提升小目标的检测，这是因为一般在数据集中小目标在图片中分布不均匀，这导致在常规的训练中小目标的学习总是不太充分。使用mosaic数据增强后，在遍历每个张图片包含了四张图片具有小目标的可能性就很大了，同时，每张图都有不同程度的缩小，即使没有小目标，通过缩小，原来的目标尺寸也更接近小目标的大小，这对模型学习小目标很有利。
Cutout就是随机选择一个固定大小的正方形区域，然后采用全0填充就OK了，当然为了避免填充0值对训练的影响，应该要对数据进行中心归一化操作，norm到0。
除了这两种方法外YOLOv5还使用了图像扰动，改变亮度、对比度、饱和度、色调，加噪声，随机缩放，随机裁剪（random crop），翻转，旋转，随机擦除
矩形训练就是将图像resize到变成可以被步长整除并且最接近需要输入的大小，从而实现最小填充，
主流分割模型介绍 4，分类的损失函数为什么是交叉熵而不是mse？
遇到nan怎么办
两阶段检测网络（Faster RCNN 系列）和一阶段检测网络（YOLO 系列）有什么区别？以及为什么两阶段比一阶段精度高？ 双阶段网络算法更精细，把任务分成了正负样本分类、bbox 初次回归以及类别分类和 bbox 二次回归。 而 YOLO 算法更简单粗暴，使用 backbone 对输入图像提取特征后，将特征图划分成 S×S 的网格，物体的中心坐标落在哪个网络内，该网格（grid）就负责预测目标的置信度、类别和 bbox；YOLOv2-v5 通过 1×1 卷积输出特定通道数的特征图来，特征图有 N 个通道，对应的每个 grid 都会有 N 个值，分别对应置信度、类别和 bbox 坐标。 如何在模型训练的时候判断是否过拟合，及模型过拟合问题如何解决？ 方差、偏差、过拟合与欠拟合 Python 深拷贝和浅拷贝 魔法函数 内存管理机制 迭代器应该实现哪两个函数
C++ 静态多态和动态多态 C++11的新特性 智能指针 静态库与动态库 引用和指针的区别 （1）内存 （2）重载与重写 （3）多态的实现（虚函数） （4）sizeof 指针的大小 （5）map和unordered_map的底层实现 1，虚函数原理及作用？
2，C++ 构造函数和析构函数的初始化顺序。
3，智能指针描述下？
4，static 关键字作用？
5，STL 库的容器有哪些，讲下你最熟悉的一种及常用函数。
6，vector 和 数组的区别？vector 扩容在内存中是怎么操作的？
7，引用和指针的区别？
8，C++ 中定义 int a = 2,; int b = 2 和 Python 中定义 a = 2 b=3 有什么区别？
9，OpenCV 读取图像返回后的矩阵在内存中是怎么保存的？
10，内存对齐原理描述，为什么需要内存对齐？
11，散列表的实现原理？
12，虚拟地址和物理内存的关系？
LeetCode 1，二分查找算法 + 可运行代码。
2，白板写链表反转。
3，包含 min 函数的栈 + 可运行代码（剑指 Offer 30. 包含min函数的栈）
4，最长回文子串 + 时间复杂度
5，TOP k 问题-最小的 K 个数 + 说下你知道哪几种解法，及各自时间复杂度
6，返回转置后的矩阵（逆时针） 1，lc 46，全排列（lc表示leetcode，下同） 2，lc 73，矩阵置0 7，冒泡排序及优化
8，求数组中比左边元素都大同时比右边元素都小的元素，返回这些元素的索引
9，手写快速排序
10，手写 softmax 算子 + 解释代码及衍生问题
12，无重复字符的最长子串
13，N 皇后问题
14，求最大的第 k 个数</content></entry><entry><title>面试-距离、相似度计算——欧氏距离,曼哈顿距离,闵可夫斯基距离,汉明距离,夹角余弦</title><url>/post/%E9%9D%A2%E8%AF%95-%E8%B7%9D%E7%A6%BB%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E6%AC%A7%E6%B0%8F%E8%B7%9D%E7%A6%BB%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB%E5%A4%B9%E8%A7%92%E4%BD%99%E5%BC%A6/</url><categories><category>面试</category></categories><tags/><content type="html">在机器学习领域，被俗称为距离，却不满足三条距离公理的不仅仅有余弦距离（满足正定性和对称性，但是不满足三角不等式)，还有KL距离（ Kulback- Leibler Divergence),也叫作相对熵（不满足对称性和三角不等式），它常用于计算两个分布之间的差异
闵可夫斯基距离(Minkowski Distance) $d_{1,2}=\sqrt[p]{\sum_{k=1}^{n}\left|x_{k}-x_{k+1}\right|^{p}}$ 其中p是一个变参数。 当p=1时，就是曼哈顿距离 当p=2时，就是欧氏距离 当p→∞时，就是切比雪夫距离
曼哈顿(Manhattan)距离 等于两个点在坐标系上绝对轴距总和。
$d_{1，2}=\left|x_{1}-x_{2}\right|+\left|y_{1}-y_{2}\right|$
欧氏距离 在二维空间中，两点的欧式距离就是： 欧氏距离：$|A-B|2=\sqrt{\left(x{2}-x_{1}\right)^{2}+\left(y_{2}-y_{1}\right)^{2}}$ 同理，我们也可以求得两点在n维空间中的距离： 切比雪夫距离 ( Chebyshev Distance ) 两个点坐标数值差的绝对值的最大值 $d_{1,2}=max( | x_2-x_1 | , | y_2-y_1 | ) $
马氏距离 马氏距离又称为数据的协方差距离，它是一种有效的计算两个未知样本集的相似度的方法。马氏距离的结果也是将数据投影到N(0,1)区间并求其欧式距离，与标准化欧氏距离不同的是它认为各个维度之间不是独立分布的，所以马氏距离考虑到各种特性之间的联系。尺度无关，考虑数据之间的联系 最典型的就是根据距离作判别问题，即假设有n个总体，计算某个样品X归属于哪一类的问题。此时虽然样品X离某个总体的欧氏距离最近，但是未必归属它，比如该总体的方差很小，说明需要非常近才能归为该类。对于这种情况，马氏距离比欧氏距离更适合作判别。
夹角余弦距离 欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。
在机器学习问题中，通常将特征表示为向量的形式，所以在分析两个特征向量之间的相似性时，常使用余弦相似度来表示。余弦相似度的取值范围是「-1,1]，相同的两个向量之间的相似度为1。如果希望得到类似于距离的表示，将1減去余弦相似度即为余弦距离。因此，余弦距离的取值范围为[0,2]，相同的两个向量余弦距离为0
对于两个向量A和B，其余弦相似度定义为: $cos (A, B)=\frac{A \cdot B}{|A|{2}|B|{2}}$
在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。
如果使用词频或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦相似度的话，它们之间的夹角可能很小，因而相似度高。此外，在文本、图像、视频等领域，
余弦相似度在高维情况下依然保持“相同时为1,正交时为0,相反时为-1&amp;quot;”的性质，而欧氏距离的数值则受维度的影响，范围不固定，并且含义也比较模糊。
汉明距离 两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。
应用：信息编码（为了增强容错性，应使得编码间的最小汉明距离尽可能大）。</content></entry><entry><title>面试-面试3（计算机基础）</title><url>/post/%E9%9D%A2%E8%AF%95-%E9%9D%A2%E8%AF%953%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[顺序表（线性表）可随机存储，存储密度大（无指针）。 单链表（线性表）顺序存储，非连续存储空间，更容易表示逻辑结构。
n个元素进栈，共有$\frac{C^n_{2n}}{n+1}$种出栈方式（卡特兰数）
栈的应用：递归、进制转换、迷宫求解、局部变量、括号匹配 队列应用：广度优先、层次遍历、资源竞争、缓冲区
中缀-&gt;后缀 1） 数字直接加入 2）非括号运算符，入栈前将优先级比它高的弹出。
遇到（ 入栈 遇到 ）把 ( 和后面的运算符都弹出。 将最后栈里的运算符弹出 有n个节点的树有n-1个边 树中节点数等于所有节点的度和加1，$N=1+N_1+2N_2=N_0+N_1+N_2$ 树的路径长度=根到每个节点路径长度总和 二叉树：每个节点最多有两个子树，有左右之分不可颠倒$1+N_2=N_0$ 平衡二叉树：左右子树高度差不超过1 完全二叉树：如果编号为i（1≤i≤n）的结点与满二叉树中编号为i的结点在二叉树中的位置相同 前缀编码：字符集中没有一个编码是，另一个编码的前缀
B树 每个节点至多m个子树（m-1个关键字） 根节点不是终端的话，至少有两个子树 除根节点外，非叶节点至少m/2向上取整个子树 叶结点都在一层，不带信息
按照二叉树的定义，4个节点的二叉树有多少种？ 0个节点的二叉树有1种,即f(0)=1; 1个节点的二叉树有1种,即f(1)=1; 2个节点的二叉树有2种，即f(2)=2； 3个节点的二叉树肯定先得固定一个根节点，然后还剩2个节点，这两个节点有三种排列方式，根节点左边两个、根节点左边一个右边一个、根节点右边两个，这样的话就可以用f(0),f(1)和f(2)来求了：$f(3)=f(2)f(0)+f(1)f(1)+f(0)f(2)=21+11+12=5$;同理$f(4)=f(3)f(0)+f(2)f(1)+f(1)f(2)+f(0)f(3)=51+21+12+15=14$;于是就有了递推公式：$f(n)=f(n-1)*f(0)+f(n-2)*f(1)+···+f(1)*f(n-2)+f(0)f(n-1)$。 #前中后序遍历 光有前序遍历和后序遍历是无法还原二叉树的。 已知某二叉树的前序遍历为A-B-D-F-G-H-I-E-C,中序遍历为F-D-H-G-I-B-E-A-C,请还原这颗二叉树。
已知某二叉树的中序遍历为F-D-H-G-I-B-E-A-C,后序遍历为F-H-I-G-D-E-B-C-A,请还原这颗二叉树。
完全图：任意两个顶点存在边（有向图$\frac{n(n&ndash;1)}{2}$，无向图$n(n-1)$) 连通图：任意两个顶点连通 连通分量：无向图中的极大连通子图 7个顶点任意情况下连通边最少：6个是完全图6*5/2+1（另一个顶点）
进程状态 运行状态：进程正在处理机上运行 就绪状态：进程已获得了除处理机之外的一切所需资源 阻塞状态：进程正在等待某一事件而暂停运行 创建状态：进程正在被创建，尚未转到就绪状态 结東状态：进程正从系统中消失，分为正常结束和异常退出 就绪状态→运行状态：经过处理机调度，就绪进程得到处理机资源 运行状态→就绪状态：时间片用完或在可剥夺系统中有更高优先级进程进入 运行状态→阻塞状态：进程需要的某一资源还没准备好 阻塞状态→就绪状态：进程需要的资源已准备好
死锁 产生条件： 系统资源的竞争 进程推进顺序非法 死锁产生的必要条件 互斥条件 不剥夺条件 请求和保持条件 循环等待条件（循环等待链）
处理策略： 死锁预防 破坏四个必要条件 避免死锁 资源动态分配过程中，防止系统进入不安全状态；银行家算法 死锁的检测和解除 资源剥夺；撤销进程；进程回退 网络层 OSI中的层
层 功能 协议 设备 应用层 为操作系统或网络应用程序提供访问网络服务的接口。 FTP（文件传送协议）、Telnet（远程登录协议）、DNS（域名解析协议）、SMTP（邮件传送协议），POP3协议（邮局协议），HTTP协议（Hyper Text Transfer Protocol） (TCP/IP无)表示层 数据格式化、转换 (TCP/IP无)会话层 管理主机之间的会话进程 传输层 提供端对端传输 T CP协议（Transmission Control Protocol，传输控制协议）、UDP协议（User Datagram Protocol，用户数据报协议） 网关 网络层 路由选择 IP协议（Internet Protocol，因特网互联协议）;ICMP协议（Internet Control Message Protocol，因特网控制报文协议）;ARP协议（Address Resolution Protocol，地址解析协议）;RARP协议（Reverse Address Resolution Protocol，逆地址解析协议）。 路由器 数据链路层 提供可靠的数据传输 以太网协议 网桥和交换机 物理层 提供一个传输数据的可靠的物理媒体 中继器（Repeater，也叫放大器）和集线器 地址解析协议，即ARP（Address Resolution Protocol），是根据IP地址获取物理地址的一个TCP/IP协议。 DNS是域名系统(DomainNameSystem)，将URL转换为IP地址 NAT网络地址转换(Network Address Translation)属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技.
用浏览器访问一个Internet网站，可能使用到的协议有 应用层用到&ndash;HTTP协议 传输层&ndash;tcp协议 网络层&ndash;ARP协议 控制信息的传递&ndash;ICMP 服务器后台处理&ndash;IP协议(DNS) 打开网页&mdash;HTML
IP地址 255.255.255.255该IP地址指的是受限的广播地址。
受限广播地址只能用于本地网络，路由器不会转发以受限广播地址为目的地址的分组；一般广播地址既可在本地广播，也可跨网段广播。
localhost
不联网 ，不使用网卡，不受防火墙和网卡限制 ，本机访问
127.0.0.0/8
不联网 ，网卡传输，受防火墙和网卡限制 ，本机访问
用作回环地址，在主机上发送给127开头的IP地址的数据包会被发送的主机自己接收。
本机IP
联网 ，网卡传输 ，受防火墙和网卡限制 ，本机或外部访问
0.0.0.0
它表示本机中所有的IPV4地址。
私有地址(private address)
也叫专用地址，它们不会在全球使用，只具有本地意义。 A类私有地址：10.0.0.0/8，范围是：10.0.0.0~10.255.255.255 B类私有地址：172.16.0.0/12，范围是：172.16.0.0~172.31.255.255 C类私有地址：192.168.0.0/16，范围是：192.168.0.0~192.168.255.255
子网掩码
标志两个IP地址是否同属于一个子网的，也是32位二进制地址，其每一个为1代表该位是网络位，为0代表主机位。如果两个IP地址在子网掩码的按位与的计算下所得结果相同，即表明它们共属于同一子网中。
子网划分 主机位全0代表网络地址 主机位全1代表广播地址
解析：先判断为C类地址，要五个子网，需要三个1：11100000；剩下五个主机数，2的五次方-2=30&gt;28，所以主机数也够，因此子网掩码应为：255.255.255.224； 三次握手 TCP报文 序列号seq：占4个字节，用来标记数据段的顺序 确认号ack：占4个字节，期待收到对方下一个报文段的第一个数据字节的序号 确认位ACK：占1位，仅当ACK=1时，确认号字段ack才有效 同步位SYN：连接建立时用于同步序号。SYN这个标志位只有在TCP连接时才会被置1，握手完成后SYN标志位被置0。 终止FIN：用来释放一个连接。FIN=1表示：此报文段的发送方的数据已经发送完毕，并要求释放运输连接 四次挥手 为什么要三次握手呢？主要是为了信息对等和防止出现请求超时导致脏连接。
只有三次握手之后才能够保证两台服务器都具备发报和收报能力。
为什么连接的时候是三次握手，关闭的时候却是四次握手
答：Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，&ldquo;你发的FIN报文我收到了&rdquo;。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。
如果已经建立了连接，但是客户端突然出现故障了怎么办?
答：TCP还设有一个保活计时器，服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。
为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？ 这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL*8，所以需要等这么长时间来确认服务端确实已经收到了。
http、https HTTP：运行在 TCP 之上，明文传输，客户端与服务器端都无法验证对方的身份 HTTPS：身披 SSL( Secure Socket Layer )外壳的 HTTP，运行于 SSL 上，SSL 运行于 TCP 之上， 是添加了加密和认证机制的 HTTP。 状态码的类别：
类别 原因短语 1XX Informational（信息性状态码） 接受的请求正在处理 2XX Success（成功状态码） 请求正常处理完毕 3XX Redirection（重定向状态码） 需要进行附加操作以完成请求 4XX Client Error（客户端错误状态码） 服务器无法处理请求 5XX Server Error（服务器错误状态码） 服务器处理请求出错 TCP和UDP有哪些区别。 GET和POST区别 GET：从服务器上获取数据，也就是所谓的查，仅仅是获取服务器资源，不进行修改。 POST：向服务器提交数据，这就涉及到了数据的更新，也就是更改服务器的数据。 PUT：英文含义是放置，也就是向服务器新添加数据，就是所谓的增。 DELETE：从字面意思也能看出，这种方式就是删除服务器数据的过程。
Get是不安全的，因为在传输过程，数据被放在请求的URL中；Post的所有操作对用户来说都是不可见的。 但是这种做法也不是绝对的，也可以在get请求加上 request body，给 post请求带上 URL 参数。
Get请求提交的url中的数据最多只能是2048字节，这个限制是浏览器或者服务器给添加的，http协议并没有对url长度进行限制，目的是为了保证服务器和浏览器能够正常运行，防止有人恶意发送请求。Post请求则没有大小限制。
Get限制Form表单的数据集的值必须为ASCII字符；而Post支持整个ISO10646字符集。
Get执行效率却比Post方法好。Get是form提交的默认方法。
对称加密与非对称加密 对称密钥加密是指加密和解密使用同一个密钥的方式这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方； 而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。 由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，非常的慢
简述 cookie？和 session的区别 cookie是由Web**服务器保存在用户浏览器上的小文件（key-value格式），包含用户相关的信息。**客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户身份。
session在服务器端， cookie在客户端（浏览器） session的运行依赖 session id，而 session id是存在 cookie中的，也就是说，如果浏览器禁用了 cookie，同时 session也会失效，存储 Session时，键与 Cookie中的sessionid相同，值是开发人员设置的键值对信息，进行了base64编码，过期时间由开发人员设置 cookie安全性比 session差 SQL注入 解決方式：通过传参数方式解决SQL注入 关系型数据库和非关系型数据库 关系模型指的就是二维表格模型，好比Excel文件中的表格，强调用表格的方式存储数据（有字段，表与表之间还有关系）
Oracle Microsoft SQL Server MySQL：数据保存在磁盘中，检索的话，会有一定的操作，访问速度相 对慢 SQLite（手机端的） 非关系型数据库,即NoSQL（not only SQL），非关联型的，强调Key-Value的方式存储数据
MongoDB Redis：内存型，数据保存在内存中，速度快 进程与线程的区别 1.线程是程序执行的最小单位，而进程是操作系统分配资源的最小单位 2. 一个进程由一个或多个线程组成，线程是一个进程中代码的不同执行路线 3. 进程之间相互独立，但同一进程下的各个线程之间共享程序的内存空间(包括代码段，数据集，堆等)及一些进程级的资源(如打开文件和信号等)，某进程内的线程在其他进程不可见； 4. 调度和切换：线程上下文切换比进程上下文切换要快得多
]]></content></entry><entry><title>面试-面试问题</title><url>/post/%E9%9D%A2%E8%AF%95-%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[别说自己有什么，说你能给公司带来什么 每次面试一定要录音！！！ 之后再把自己的回答整理出QA,不断的修改自己的回答视角，纠正自己在回答时候提供的情绪价值。
自我介绍 需要准备一个三分钟的个人介绍，太长太短都不合适，个人介绍主要包含个人信息、个人工作介绍(1.5-2min)、个人总结，三个部分，个人工作介绍只需讲核心贡献，配1-2个突出业绩即可。这部分千万不要给自己挖坑，说的每一个可能延伸问题都要提前有准备。
参考模板：
【基本信息】面试官您好，我叫xXx,在互联网行业有7年的市场营销经验，并经历了从初级市场专员向高级市场经理的过程 【工作经历】（以讲故事的形式串联起几段工作经历）我先后经历了A\B趴C几家公司，主要负责 XX、X、Xx工作内容，主导完成了x项目，达成了xx业绩，取得了xx绩效/奖励（如有），出于xx原因，我决定寻求新的工作机会 【能力匹配度】我认为自己在该行业内有X、XX、)x优势，并完全熟悉了解该岗位的xx、Xx、Xx工作内容，可以很快的上手 【求职意愿】基于我对自己的职业规划与对贵公司Xx岗位的了解，我决定应聘该岗位，非常荣幸您给我此次面试机会，也非常期待成为贵公司的一员
【基本信息】你好，我叫xXx，有一年的工作经验，研究生期间主要做时间序列分类的算法研究的，具体来说就是研究WIFI无线信号的分类，毕业后在中国银行的一家子公司做信科管培生 【工作经历】我先是在前端组轮岗，主要工作是员工管理系统和风控系统前端页面开发。然后是到算法组轮岗，做卡号识别的算法开发。因为前端系统重构需要人比较多所以之后还回去前端组做开发，但是我个人还是对算法比较感兴趣。所以离职，然后在找新的工作。 【求职意愿】基于我对自己的职业规划与对咱们这个岗位的了解，我决定应聘该岗位，非常感谢给我这次面试机会。 介绍项目 这类问题的回答框架可以是：
Situation：介绍一下项目背景，上下游整体的把握，以及项目主要价值。 Task：在这个项目中你所做的主要工作，一些核心关键点（重任） Action：针对核心关键点你采取的具体策略与方法。 Result：这个项目中你的一些成果，最好可以用数据量化。 举个栗子：在XXX项目背景下，我主要对XXX部分进行研究，使用了XXX方法，存在XXX问题/反馈XXX现象，针对这些问题/现象，尝试了XXX方法/XXX思考/XXX研究，提升了XXX效果/有XXX成果价值。（这是一个极简的整体脉络，可以适当补充内容，突出你的优势）
项目的一些反思，总结，以及现在看来还能优化的点，这样可以使很早之前的项目能随着时间不断发展迭代。（这部分不用写在简历上，避免每个项目过于冗长，但面试的时候可以主动和面试官交流）
这项比赛由xx主办，有x双支队伍参赛，具有0x特点。…我们发现基线模型在小目标上精度较差所以决定从这里入手，优化模型在小目标上的性能为了提升小目标检测能力，我们做了3方面的优化尝试：1)在网络结构上，我们尝试xx2)在损失函数上，我们采用x3)我们使用了新的数据预处理方法双其中，我主要负责xXX最终，我们取得了xxx的精度，较基线模型提高了Xx,获得第X名
我们首先训练了一个基线模型尝试了xxx方法，但没有明显效果又尝试了xxx方法，效果还是不好我们发现效果不好的原因是小目标精度较差我们又尝试了xx方案做了xx实验最终取得了x精度
总结下社招面试问项目最主要的问题套路：
1.你项目为什么这么设计，你这样设计有什么好处，解决了什么问题
2.这么设计有什么瓶颈吗，有什么改善的方案
3.项目遇到的难点，你是怎么解决的，为什么用这种方式解决，还有更好的方式么
4.讲下你工作的项目，你做的什么。问各种细节的原理，实现，优缺点
我们之前是在前端组因为我们做的业务系统和管理系统一直在做迭代，今年要迭代到3.0，比较缺人，所以先去了那边。
这个算法是用在我们业务办理系统办理业务时识别上传的卡号的，之前是用的百度的接口，因为总行安全政策，这块要自己来做，然后管培生是需要在不同的组轮岗的，所以让我们这批人都去算法组了，这个项目周期，
从二月份开始，先是搜集数据做标注，查找资料，构思方案，做传统方法baseline。
三月份，深度学习方法，模型搭建，训练
四月份，调优，消融实验，枝剪
五月份，最后版本上线
DBNET
一开始考虑尝试用传统方法做，想着推理速度应该会很快，而且可解释性也好点，但是做出baseline1 后，效果非常差，主要还是因为受拍摄角度、光照及文字背景的影响。然后查资料进行图像矫正2 、和形态学处理，效果还是不太好。然后决定用深度学习方法来做。也是查论文和资料，一个比较经典的网络是 CTPN +CRNN3+ ctc [^ctc ]，效果虽然还不错，但是速度比较慢[^CTPN慢]
在算法迭代的时候就想到也许能用YOLO去解决这个问题，查到有一篇论文是用yolo3实现的。但是尝试过v3速度提上去了，但是精度又下降了，主要难点在于各种联名卡，复杂的背景使得卡号定位不是特别的准确，尝试了v54，结果非常好！
然后分别进行检测和分类（各层的个数不同，但每个点都有）将生成的所有default box都集合起来，全部丢到NMS中，输出筛选后的default box。
项目中遇到最困难的技术问题是什么，最后怎么解决 还是代码的问题吧，一开始找到了一个不太完整的CTPN网络的代码，但是整体的流程还有每个函数的功能写的都不是很清晰，改了很久还是不行。
最后把整个流程分解，对照论文，数据处理、网络搭建、训练测试各个部分的补充完善。然后再整合起来。
我觉得遇到问题，保持良好心态，查找资料，分解任务然后再开始行动。
你有哪些优点，那些缺点 往知识、技能的方向去回答（要能举出例子防止被反问）
把缺点往更高的职位说谈一个与你应聘岗位有些距离的缺点
讲述正在改进或者将来如何改正缺点
参考模板： 我的执行能力不错，领导布置的任务我都能保质保量的按时完成（通过优点引出缺点）。但正是因为如此，我主动的思考相对的就会少一些，更多的是体现在完成任务方面。如何更长远的考虑，站在大局观上看待工作，这些更深度的方面还需要加强（用更高职位的能力来回答）
适应性很强，能沉下心，每一次都能很快适应新的岗位要求，比如说我能够在很短的时间熟悉新的知识，比如曾经在Xx的时候我就在3天之内学习x并最终获得了Xx很好的结果。
工作中缺乏规划。接受一个任务想尽快赶完，可能对项目的整体情况缺乏比较更深的思考。我觉得这也和我的经验不足有关，如何在更高的视角看事情是我需要不断学习探索的。
有点冒险，想到啥做啥。
你上一份工作为什么离职 避免将“离职原因”说的太详细，不要常入自己的主观负面情绪，尽量将问题引导到个人原因、外部因素、职业规划方向
参考模板：
因为之前的公司发展方向与我的职业规划不符，我希望换一份工作能让我有更大的突破。而贡公司所招岗位更加符合我对未来的规划，我希望能将有限的精力放在更适合的方向。
首先，因为要对管理系统的前端代码进行重构，所以定岗安排我去做前端，但是我觉得还是做算法比较有前景。 然后，就是感觉国企的晋升空间比较小，所以说准备回郑州重新找工作，还可以领购房补贴，生活补贴 为什么有一年空窗期 问题分析：面试官问到这个问题，说明他对你的能力还是比较认可的，想和你有合作的机会才会这么问。
他想看看你的职场适应能力如何。长时间没有工作，还能得上工作的节奏吗？ 看看你的工作能力怎么样。这么长时间没工作，是不是因为工作能力不行找不到？ 了解一下你的心态如何。在家自由散漫惯了，还能适应工作中被约原吗？ 回答思路：
不要回答离职后什么事都没干，可以说明为了接下来的工作做了哪些努力 如果因为个人身体原因或者家庭原因，在末尾一定要表明现在已经处理完毕，可以放心投入工作中 参考模板：
创业、考研、考公失败 在之前的公司离职后，想让自己有个提升，于是利用离职期间去筹备创业/考研/考公，不过很遗憾尝试后失败了，但我也通过这次行动锻炼了自身能力，能够在失败中汲取经验，相信这对我接下来的工作会很有帮助。了解了费公司的信息后，发现我的X能力与贵公司岗位要求非常匹配，希望能够得到这次工作机会，相信我能够满足公司需求，胜任这个岗位。 家人生病 之前一直忙于工作，对家人有所疏忽。前阵子家人生病需要照顾，正好在离职期间，素性放缓工作脚步，回家照顾家人。在这期间我也自学了这个行业的相关知识，对自己的能力进一步提升。虽然有一段时间的空窗期，但这并不会影响我接下来的工作。目前家人身体已经无恙，我可以放心的投入到接下来的工作中。 8 准备回郑州 找工作后顺便家里装修一下
9 郑州找工作 算法相关的工作确实比较少，找到的薪资方面和我的预期有差距
10 郑州疫情 然后就是因为疫情在家里呆了一个多月，
11 这个月来上海找工作 月初来昆山这边，开始找在上海的工作
你能接受加班吗 不能够表现得完全拒绝加班(会被认为太耿直，不以工作为重)
也不能完全支持加班(容易挖坑，有活就给你)
回答时不要询问加班费/调休等福利（会认为你为了加班费混加班时长），应当放到谈薪阶段在进行询问。
参考模板：
(理解加班+表明态度+借机反问)
首先我能够理解，任何职位在项目紧急的情况下都是会有一定程度的加班，必要的加班我可以接受(能够理解接受加班，但不接受无效加班)
其次，我不太能接受那种无意义且没有目的性的加班，我更希望将时间花在学习知识、提升自身等更有意义的事情上(表明态度，拒绝无效加班)。我会努力提高自身工作效率，尽量保证在工作时间内将任务完成，不拖团队后腿(再次表明态度，提倡高效工作，不团队拖后腿)。如果需要长期加班，我会反思是否是因为自身工作效率的问题，积极提升个人能力，优化工作方法及流程，减少无效加班(分析问题、解决问题)。 顺便我想问一下，这个岗位的加班频率是怎么样的？一般是什么原因造成的？团队中其他同事的加班情况如何？我好为此提前做些准备。（了解真实情况）
首先，项目紧急的情况下，当然可以加班。
但是，我觉得既然加班了就要有结果，所以我不太能接受那种没有目的性的加班。
另外，如果需要长期加班，我会反思是否是因为自身工作效率的问题，积极提升个人能力，优化工作方法及流程，减少无效加班
你未来的职业规划是怎样的 面试时回答主要强调你能给部门做出什么成绩，要让领导觉得让你入职对他有利。
我会在短时间内尽快融入工作环境，尽快做出成果。 多积累经验提升自身的专业技能，尽快做到能够在工作中独当一面，独立负责工作项目，解决所遇到的问题。 后续会根据自身的能力的变化、公司的需要来进行调整。关于职业规划，我目前的考虑是这样的，谢谢。 你目前拿到几个offer，如果收到更好的，还会选择我们吗 表明数量，阐述公司成绩，强调个人与岗位的匹配度，最后表明入职意愿
参考模板：
我目前手里有两个offer,不过我会更加倾向于咱们公司（表明态度）。因为我通过官网、公众号等平台了解到，公司在X行业处于领先地位(阐述公司成绩），再加上公司所招岗位主要负责X问题，而我之前有X经验，擅长X,与贵公司所需要的能力恰好匹配（强调个人与岗位的匹配度）。综合考虑，我认为贵公司是我最佳的选择（表明入职意愿）
在投递简历之前，我在招聘信息与公司网站上进行了一些了解，再加上刚刚与您沟通时了解到的信息，综合下来后，我觉得贵公司对我来说是最好的也是最合适的。
有两个外包的，还有一个临港的机器人，但是感觉太远。其他的还在沟通
你期望的薪资是多少 回答思路：
多个相同岗位，相同人数规模的公司，看看他们的薪资区间，做到对薪资心中有数 谈薪资要根据具体面试过程，来灵活的决定，而不是想好固定的一个数 根据你的市场和所面试岗位具体薪资范围的调查结果，给到面试企业薪资预期的具体数字。例如：岗位薪资6-8k,你的面试过程很顺利，那么可以说目标是8k;如果面试过程中回答不好，面试官看起来不太满意，那么回答适当降低薪资。可以说希望薪资6k或5k 参考模板：
贵公司在岗位招聘上是7一10K的工资（应聘岗位薪资区间），根据我调查同行业的平均薪资水平（市场平均薪资调研），加上我在之前公司的工作能力以及薪资的综合考虑（结合自身能力与过去的薪资），希望能拿到10K的薪资(根据面试表现增减。比如前公司薪资为7k,面试过程顺利，可以说希望薪资9k或10k;面试表现一般，可以说希望薪资是8k或7k)。
薪酬包(Package)涉及多种形式，这里只谈一切可到手报酬，比如： 基本工资、绩效工资、季度年度奖金（未发待发也算）、13薪、股票期权，岗位津贴、职称津贴、节假日补贴、交通补贴、饭补、电话补贴等、公积金也必须加进去(毕竟有的辣鸡公司以**最低比例缴纳**，或者干脆不交，盲目跳过去就亏死了。)以上全盘计入年度薪酬包，再除以12个月，才是你真实的月平均工资，这部分构成了你跳槽加薪的基础。 为什么选择我们公司，为什么应聘这个岗位 问题分析：
看看你是做过调研后投递简历，还是广撒网海投简历
看看你是否有明确的职业规划（个人能力与岗位是否适配
你的应变能力如何，是否有足够的情商回答思路： 公司信息 能力匹配 入职意愿
参考模板： 首先，贵公司所在行业是我想发展深耕的，我通过官网、公众号等平台了解到（信息收集渠道），公司在X行业处于领先地位，主要负责业务，X产品在市场上也有很高的知名度（公司业务情况）。
其次，贵公司所招岗位主要负责X问题，而我之前有XX经验，擅长X,与贵公司所需要的能力恰好匹配（强调个人与公司岗位的匹配程度）。
最后，我认为工作“合适”才是最好的，根据我在网上的了解，加上刚刚与您沟通所了解到的信息，综合下来，我觉得贵公司就是我最好也是最合适的选择，同样的，我也希望我能是贵公司最优的选择（强调入职意愿）·
问题分析：面试官想了解你的求职动机，是否清楚岗位职责，对未来是否有合理的规划回答思路：
岗位职责+个人能力+职业规划参考模板：
我在互联网渠道了解到，X岗位主要负责XX方面，需要用到X能力（岗位职责），而我在之前的工作中主要负责XX,拥有XX经验，与XX岗位也比较匹配（个人能力与岗位匹配）。选择了这个行业就打算努力深耕，从工作中逐步积累经验，提升自身的专业技能，摄取更多行业相关知识，尽快做到能够在工作中独当一面（未来职业规划），希望能够得到在费公司工作的机会（强调入职意愿）。
你对我们公司了解有多少 回答思路： 可以针对公司、岗位提前做一个面试作品 在招聘网站、公司官网、公众号等平台进行了解
我常常刷到公司发布的文章，公司主要负责X商品的开发，产品在XX行业有很强的竞争力，是大部分年轻人的首选品牌。（根据了解到的信息进行阐述即可）
有什么问题想了解 回答思路： 如果确实没有问题，不要过于直接，婉转的表明问题已经通过之前的交流解决了，随后再次强调入职意愿。
现在还处于被动阶段，不要直接询问给你的薪资福利，建议在公司确定录用你时再进行询问。网上能搜寻到的信息就不要再问一遍，会显得准备不足。
岗位相关
请问我所应聘的岗位目前团队有多少人 部门后面的发展目标或者方向是什么？ 这个岗位具体负责的工作内容？ 利益相关
公司的晋升制度是怎么样的？ 公司是否有相关的培训机制？ 您认为这个岗位在公司的内部的发展如何？ “万金油”
如果有幸加入贵公司，有什么需要我提前准备的吗？ 您当初选择这家公司的原因是什么？ 您觉得要做好这个职位，我有哪些方面需要提升的？ 图像灰度化、图像二值化、形态学操作、边缘检测5、提取轮廓及区域定位&#160;&#x21a9;&#xfe0e;
图像的倾斜矫正的算法很多，比如基于 Radon 6变换的倾斜矫正算法以及基于霍夫变换7的倾斜矫正算法，还有许多改进的倾斜矫正算。霍夫变换是用来找出物件中的特征，根据霍夫变换寻找银行的边缘直线，从而获取水平、垂直方向银行卡的边缘直线&#160;&#x21a9;&#xfe0e;
加入激活函数Swish。加入BatchNorm。加入SE注意力机制。适当加深模型。&#160;&#x21a9;&#xfe0e;
输入端：Mosaic数据增强、自适应锚框计算、自适应图片缩放。Backbone：Focus结构、CSP结构。Neck：FPN+PAN。Prediction：CIOU_Loss—Bounding box损失函数、DIOU_nms&#160;&#x21a9;&#xfe0e;
Sobel边缘检测算法比较简单，实际应用中效率比canny边缘检测效率要高，但是边缘不如Canny检测的准确，但是很多实际应用的场合，sobel边缘却是首选，Sobel算子是高斯平滑与微分操作的结合体，所以其抗噪声能力很强，用途较多。尤其是效率要求较高，而对细纹理不太关心的时候。&#160;&#x21a9;&#xfe0e;
&#160;&#x21a9;&#xfe0e; &#160;&#x21a9;&#xfe0e; ]]></content></entry><entry><title>面试-模型调参</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[
贪心调参 （坐标下降） 所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。
选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。）
贪心算法的基本思路 1.建立数学模型来描述问题 2.把求解的问题分成若干个子问题 3.对每个子问题求解，得到子问题的局部最优解 4.把子问题的解局部最优解合成原来问题的一个解
例子 有一个背包，最多能承载150斤的重量，现在有7个物品，重量分别为[35, 30, 60, 50, 40, 10, 25]，它们的价值分别为[10, 40, 30, 50, 35, 40, 30]
分别求出子问题的最优解再堆叠出全局最优解
按照制订的规则（价值）进行计算，顺序是：4 2 6 5 。 最终的总重量是：130。 最终的总价值是：165。 按照制订的规则（重量）进行计算，顺序是：6 7 2 1 5 。 最终的总重量是：140。 最终的总价值是：155。 按照制订的规则（单位密度）进行计算，顺序是：6 2 7 4 1。 最终的总重量是：150。 最终的总价值是：170。
三、该算法存在的问题 不能保证求得的最后解是最佳的 不能用来求最大值或最小值的问题 只能求满足某些约束条件的可行解的范围 四、贪心算法适用的问题
贪心策略适用的前提是： 1、原问题复杂度过高； 2、求全局最优解的数学模型难以建立； 3、求全局最优解的计算量过大； 4、没有太大必要一定要求出全局最优解，“比较优”就可以。
坐标下降法是一类优化算法，其最大的优势在于不用计算待优化的目标函数的梯度。不循环使用各个参数进行调整，而是贪心地选取了对整体模型性能影响最大的参数。参数对整体模型性能的影响力是动态变化的，故每一轮坐标选取的过程中，这种方法在对每个坐标的下降方向进行一次直线搜索（line search）
# -------------------------------贪心调参 （坐标下降）----------------------------------- objective = [&#39;regression&#39;, &#39;regression_l1&#39;, &#39;mape&#39;, &#39;huber&#39;, &#39;fair&#39;] num_leaves = [3, 5, 10, 15, 20, 40, 55] max_depth = [3, 5, 10, 15, 20, 40, 55] greedy_parameters_lgb = dict() greedy_score = 100 for o in objective: score = np.mean(cross_val_score(LGBMRegressor(objective=o), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))) if greedy_score &gt; score: greedy_parameters_lgb[&#39;objective&#39;] = o greedy_score = score greedy_score = 100 for l in num_leaves: score = np.mean(cross_val_score(LGBMRegressor(objective=greedy_parameters_lgb[&#39;objective&#39;], num_leaves=l), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))) if greedy_score &gt; score: greedy_parameter_lgb[&#39;num_leaves&#39;] = l greedy_score = score greedy_score = 100 for d in max_depth: score = np.mean(cross_val_score(LGBMRegressor(objective=greedy_parameters_lgb[&#39;objective&#39;], num_leaves=greedy_parameters_lgb[&#39;num_leaves&#39;], max_depth=d), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))) if greedy_score &gt; score: greedy_parameters_lgb[&#39;max_depth&#39;] = d greedy_score = score print(greedy_parameter_lgb) print(np.mean(cross_val_score(LGBMRegressor(**greedy_parameter_lgb), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error)))) 网格调参GridSearchCV 网格搜索可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格搜索有很大概率找到全局最优值。然而，这种搜索方案十分消耗计算资源和时间，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最优值。
# ------------------------------------Grid Search 调参--------------------------------- clf_lgb = GridSearchCV(LGBMRegressor(), cv=5, {&#39;objective&#39;: [&#39;regression&#39;, &#39;regression_l1&#39;, &#39;mape&#39;, &#39;huber&#39;, &#39;fair&#39;], &#39;num_leaves&#39;: [3, 5, 10, 15, 20, 40, 55], &#39;max_depth&#39;: [3, 5, 10, 15, 20, 40, 55]}) clf_lgb = clf.fit(x_train_tree, y_train_tree) print(clf_lgb.best_params_) print(np.mean(cross_val_score(LGBMRegressor(**clf_lgb.best_params_), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error)))) 贝叶斯优化 贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息；而贝叶斯优化算法则充分利用了之前的信息。
贝叶斯优化算法通过对目标函数形状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它学习目标函数形状的方法是，
首先根据先验分布，假设一个搜集函数； 然后，毎一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布； 最后，算法测试由后验分布给出的全局最值最可能出现的位置的点。
对于贝叶斯优化算法，有一个需要注意的地方，一旦找到了ー个局部最优值，它会在该区域不断采样，所以很容易陷入局部最优值。 为了弥补这个缺陷，贝叶斯优化算法会在探索和利用之间找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现全局最值的区域进行采样。
# -----------------------------------------贝叶斯调参-------------------------- def bayes_cv_lgb(num_leaves, max_depth, subsample, min_child_samples): val = cross_val_score(LGBMRegressor(objective=&#39;regression_l1&#39;, num_leaves=int(num_leaves), # 优化只能优化连续超参数加上int()转为离散超参数。 max_depth=int(max_depth), subsample = subsample, min_child_samples = int(min_child_samples)), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error)).mean() return 1 - val # 只支持最大值，需要在前面加上负号， bo_lgb = BayesianOptimization(bayes_cv, {&#39;num_leaves&#39;: (2, 100), &#39;max_depth&#39;: (2, 100), &#39;subsample&#39;: (0.1, 1), &#39;min_child_samples&#39; : (2, 100)}) bo_lgb.maximize() print(bo_lgb.max) bo_parameters_lgb = bo_lgb.max[&#39;params&#39;] for i in [&#39;num_leaves&#39;, &#39;max_depth&#39;, &#39;min_child_samples&#39;]: bo_parameters_lgb[i] = int(bo_parameters_lgb[i]) print(np.mean(cross_val_score(LGBMRegressor(**bo_parameters), X=x_train_tree, y=y_train_tree, verbose=0, cv=5, scoring=make_scorer(mean_absolute_error))) ) ]]></content></entry><entry><title>面试-模型评估指标</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[
回归 RMSE（Root Mean Square Error）均方根误差 衡量观测值与真实值之间的偏差。常用来作为机器学习模型预测结果衡量的标准。如果存在个别偏离程度非常大的离群点（ Outlier）时，即使离群点数量非常少，也会让RMSE指标变得很差。 $RMSE = \sqrt{\frac{1}{m} \sum_{i=1}^{m}(\hat{y_i}-y_i)^2} $
MSE（Mean Square Error）均方误差 通过平方的形式便于求导，所以常被用作线性回归的损失函数。 $MSE = \frac{1}{m} \sum_{i=1}^{m} (\hat{y_i}-y_i)^2 $
L2 loss对异常敏感，用了MSE为代价函数的模型因为要最小化这个异常值带来的误差，就会尽量贴近异常值，也就是对outliers（异常值）赋予更大的权重。这样就会影响总体的模型效果。
MAE（Mean Absolute Error）平均绝对误差 是绝对误差的平均值。可以更好地反映预测值误差的实际情况。 $MAE = \frac{1}{m} \sum_{i=1}^{m} |\hat{y_i}-y_i| $
相比MSE来说，MAE在数据里有不利于预测结果异常值的情况下鲁棒性更好。
SD（Standard Deviation）标准差 方差的算术平均根。用于衡量一组数值的离散程度。 $SD = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (avg(x)-x_i)^2} $
R2(R- Square）拟合优度 R2=SSR/SST=1-SSE/SST 其中：SST=SSR+SSE，
SST(total sum of squares)为总离差平方和，$S S_{\text {tot}}=\sum\left(y_{i}-\overline{y}{i}\right)^{2}$ SSR(regression sum of squares)为回归平方和，$S S{\text {reg}}=\sum\left(\hat{y_{i}}-\overline{y}{i}\right)^{2}$ SSE(error sum of squares) 为残差平方和，$S S{\text {res}}=\sum\left(y_{i}-\hat{y}_{i}\right)^{2}$
其中$\overline{y}$表示$y$的平均值得到$R^2$表达式为： $R^{2}=1-\frac{S S_{\text {res}}}{S S_{\text {tot}}}=1-\frac{\sum\left(y_{i}-\hat{y}{i}\right)^{2}}{\sum\left(y{i}-\overline{y}\right)^{2}}$
$R^2$因变量的变异能通过回归关系被由自変量解释的比例取值范国是0~1，R越近1表明回归平方和占总平方和的比例越大回归线与各观则点越接近，回归的拟合程度就越好。所以R也称为拟合优度（ Goodness of Fit）的统计量
Error = Bias + Variance Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。
分类 第一个字母T或F，代表这个分类结果是否正确，第二个字母P或N，代表分类器认为是正例还是负例。
准确率（accuracy）
所有预测正确的样本/总的样本 = （TP+TN）/总
from sklearn.metrics import accuracy accuracy = accuracy_score(y_test, y_predict) 查准率（precision) 预测为正的样本中有多少是真的正样本。两种可能，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP) $P = \frac{TP}{TP+FP}$
from sklearn.metrics import precision_score precision = precision_score(y_test, y_predict) 查全率/召回率(recall) 样本中的正样本有多少被预测正确了。两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN)： $R = \frac{TP}{TP+FN}$
from sklearn.metrics import recall_score recall = recall_score(y_test, y_predict)#recall得到的是一个list，是每一类的召回率 F1 recall 和 precision是负相关的
置信度提高→TP↓，FN↑，FP↓↓→precision↑，recall↓
置信度降低→TP↑，FN↓，FP↑↑→precision↓，recall↑
对于搜索应用，在保证召回率的条件下，尽量提升精确率。即减少假阳性率、搜索出无关的信息。
对于癌症检测、地震检测、金融欺诈等，则在保证精确率的条件下，尽量提升召回率。**减少假阴性率、**漏检
是准确率和召回率的调和平均 $$ F_{1}=2 \cdot \frac{\text { precision } \cdot \text {recall}}{\text {precision}+\text {recall}} $$
$F1=\frac{ 2TP }{ 2TP+FP+FN }$ from sklearn.metrics import f1_score f1_score(y_test, y_predict) 在一个总样本中，正样本占90%，负样本占10%，样本是严重不平衡的，只需要将全部样本预测为正样本 准确率为90% 查准率为90% 召回率100% F1 为18/19
#　PR曲线(AP,mAP)
PR曲线是准确率和召回率的点连成的线。 PR曲线下的面积就定义为AP(Average precision)，
均精度均值(mAP)：把每个类别的AP都算一遍，再取平均值
mAP@0.5 &amp; mAP@0.5 :0.95：就是mAP是用Precision和Recall作为两轴作图后围成的面积，m表示平均，@后面的数表示判定iou为正负样本的阈值，@0.5:0.95表示阈值取0.5:0.05:0.95后取均值。（0.5是iou阈值=0.5时mAP的值），mAP只是一个形容PR曲线面积的代替词叫做平均准确率，越高越好。
TPR(True Positive Rate)真正例率/查准率 真实的正例中，被预测为正例的比例：TPR = TP/(TP+FN)。 FPR(False Positive Rate)假正例率 真实的反例中，被预测为正例的比例：FPR = FP/(TN+FP)。 ROC-AUC兼顾正负例，能够更加稳定地反映模型本身的好坏。 PR曲线与ROC曲线的相同点是都采用了TPR (Recall)，都可以用AUC来衡量分类器的效果。不同点是ROC曲线使用了FPR，而PR曲线使用了Precision 因此PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。 ROC(Receiver Operating Characteristic）曲线，又称接受者操作特征曲线 通过动态地调整截断点，从最高的得分开始（实际上是从正无穷开始，对应着ROC曲线的零点），逐渐调整到最低得分，每一个截断点都会对应一个FPR和TPR，在ROC图上绘制出每个截断点对应的位置再连接所有点就得到最终的ROC曲线。
ROC的含义为概率曲线，AUC的含义为正负类可正确分类的程度。 TPR(True Positive Rate)真正例率/查准率 真实的正例中，被预测为正例的比例：TPR = TP/(TP+FN)。
FPR(False Positive Rate)假正例率 真实的反例中，被预测为正例的比例：FPR = FP/(TN+FP)。
理想分类器TPR=1，FPR=0。ROC曲线越接近左上角，代表模型越好，即ACU接近1
截断点thresholds 指的就是区分正负预测结果的阈值
AUC AUC值为ROC曲线所覆盖的区域面积，显然,AUC越大,分类器分类效果越好。
AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。0.5 &lt; AUC &lt; 1，优于随机猜测。AUC = 0.5，跟随机猜测一样。AUC &lt; 0.5，比随机猜测还差。
计算：分别随机从正负样本集中抽取一个正样本，一个负样本，正样本的预测值大于负样本的概率。
例题：对于样本 (A, B, C, D, E) , 已知其对应的label为 (0, 1, 1 ,0 ,1)， 模型A的预估值为 (0.2, 0.4, 0.7, 0.3, 0.5), 模型 B 的预估值为(0.1, 0.3, 0.9, 0.2, 0.5)， 模型 A 和 模型 B 的 AUC 一样 本题样本对（一个正样本，一个负样本组成一个样本对）共有3*2=6个， 分别是（B，A）（B，D）（C，A）（C，D）（E，A）（E，D）。 模型A对应概率为(0.4,0.2)，(0.4,0.3)，(0.7,0.2)，(0.7,0.3)，(0.5,0.2)，(0.5,0.3)， 可得其对应AUC为：(1+1+1+1+1+1)/6 = 1。同理，模型B也等于1。
Binary-class classification import numpy as np np.random.seed(10) import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.preprocessing import label_binarize from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve X, y = make_classification(n_samples=80000) # print(X[0], y[0]) # (80000, 20) (80000,) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) X_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train, y_train, test_size=0.5) from keras.models import Sequential from keras.layers import Dense from sklearn.metrics import auc model = Sequential() model.add(Dense(20, input_dim=20, activation=&#39;relu&#39;)) model.add(Dense(40, activation=&#39;relu&#39;)) model.add(Dense(1, activation=&#39;sigmoid&#39;)) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X_train, y_train, epochs=5, batch_size=100, verbose=1) y_pred = model.predict(X_test).ravel() print(y_pred.shape) fpr, tpr, thresholds = roc_curve(y_test, y_pred) roc_auc = auc(fpr, tpr) plt.figure(1) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr, label=&#39;Keras (area = {:.3f})&#39;.format(roc_auc)) plt.xlabel(&#39;False positive rate&#39;) plt.ylabel(&#39;True positive rate&#39;) plt.title(&#39;ROC curve&#39;) plt.legend(loc=&#39;best&#39;) plt.show() # Zoom in view of the upper left corner. plt.figure(2) plt.xlim(0, 0.2) plt.ylim(0.8, 1) plt.plot([0, 1], [0, 1], &#39;k--&#39;) plt.plot(fpr, tpr, label=&#39;Keras (area = {:.3f})&#39;.format(roc_auc)) plt.xlabel(&#39;False positive rate&#39;) plt.ylabel(&#39;True positive rate&#39;) plt.title(&#39;ROC curve (zoomed in at top left)&#39;) plt.legend(loc=&#39;best&#39;) plt.show() # (Optional) Prediction probability density function(PDF) import numpy as np from scipy.interpolate import UnivariateSpline from matplotlib import pyplot as plt def plot_pdf(y_pred, y_test, name=None, smooth=500): positives = y_pred[y_test == 1] negatives = y_pred[y_test == 0] N = positives.shape[0] n = N//smooth s = positives p, x = np.histogram(s, bins=n) # bin it into n = N//10 bins x = x[:-1] + (x[1] - x[0])/2 # convert bin edges to centers f = UnivariateSpline(x, p, s=n) plt.plot(x, f(x)) N = negatives.shape[0] n = N//smooth s = negatives p, x = np.histogram(s, bins=n) # bin it into n = N//10 bins x = x[:-1] + (x[1] - x[0])/2 # convert bin edges to centers f = UnivariateSpline(x, p, s=n) plt.plot(x, f(x)) plt.xlim([0.0, 1.0]) plt.xlabel(&#39;density&#39;) plt.ylabel(&#39;density&#39;) plt.title(&#39;PDF-{}&#39;.format(name)) plt.show() plot_pdf(y_pred, y_test, &#39;Keras&#39;) 宏平均（Macro-averaging）和微平均（Micro-averaging）： 用途：用于多个类别的分类 宏平均：先计算每一类的F1，然后3类求平均。
微平均：先计算出所有类别总共的TP，FP和FN，然后按公式求。
Multi-class classification from sklearn.datasets import make_classification from sklearn.preprocessing import label_binarize from keras.models import Sequential from keras.layers import Dense import numpy as np from scipy import interp import matplotlib.pyplot as plt from itertools import cycle from sklearn.model_selection import train_test_split from sklearn.metrics import roc_curve, auc # 标签共三类 n_classes = 3 X, y = make_classification(n_samples=80000, n_features=20, n_informative=3, n_redundant=0, n_classes=n_classes, n_clusters_per_class=2) # print(X.shape, y.shape) # print(X[0], y[0]) # (80000, 20) (80000,) # [-1.90920853 -1.30052757 -0.76903467 -3.2546519 -0.02947816 0.14105006 # 0.43556031 -0.81300607 -0.94553296 -0.92774495 1.49041451 -0.4443121 # -1.16342165 -0.32997815 -1.02907045 -0.39950447 -0.711287 0.51382424 # 2.88822258 -2.0935274 ] # 1 # Binarize the output相当于one_hot y = label_binarize(y, classes=[0, 1, 2]) # print(y.shape, y[0]) # (80000, 3) [0 1 0] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) model = Sequential() model.add(Dense(20, input_dim=20, activation=&#39;relu&#39;)) model.add(Dense(40, activation=&#39;relu&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X_train, y_train, epochs=1, batch_size=100, verbose=1) y_pred = model.predict(X_test) # print(y_pred.shape) # (40000, 3) # Compute ROC curve and ROC area for each class fpr = dict() tpr = dict() roc_auc = dict() for i in range(n_classes): # scores = np.array([0.1, 0.4, 0.35, 0.8]) # fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) # y 就是标准值，scores 是每个预测值对应的阳性概率，比如0.1就是指第一个数预测为阳性的概率为0.1，很显然， # y 和 socres应该有相同多的元素，都等于样本数。pos_label=2 是指在y中标签为2的是标准阳性标签，其余值是阴性。 # 接下来选取一个阈值计算TPR/FPR,阈值的选取规则是在scores值中从大到小的以此选取，于是第一个选取的阈值是0.8 # label=[1,1,2,2] scores=[0.1,0.4,0.35,0.8] thresholds=[0.8,0.4,0.35,0.1] 以threshold为0.8为例，将0.8与 # scores 中所有值比较大小得到预测值，[0,0,0,1].对于label中两个1，其概率分别为0.1，0.4，小于阈值0.8，判定为 # 负样本，而他们的label是1，说明他们确实是负样本，判断正确，是两个TN；两个2，对应概率为0.35，0.8，0.35小于 # 0.8，判定为负样本，但是label是2，应该是个正样本，所以这是个FN；最后0.8&gt;=0.8,这是个TP，所以最后的结果是 # ：1个TP，2个TN，1个FN，0个FP fpr[i], tpr[i], thresholds = roc_curve(y_test[:, i], y_pred[:, i]) # (40000,) # print(fpr[i].shape)# (5491,)# (6562,)# (4271,) roc_auc[i] = auc(fpr[i], tpr[i]) # 计算microROC曲线和ROC面积 # .ravel()将多维数组转换为一维数组 fpr[&#34;micro&#34;], tpr[&#34;micro&#34;] , thresholds = roc_curve(y_test.ravel(), y_pred.ravel()) # (120000,) roc_auc[&#34;micro&#34;] = auc(fpr[&#34;micro&#34;], tpr[&#34;micro&#34;]) # 计算macroROC曲线和ROC面积 # 首先，汇总所有的假阳性率 # np.unique() 该函数是去除数组中的重复数字，并进行排序之后输出。 # print(np.concatenate([fpr[i] for i in range(n_classes)]).shape) (16324,) all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) # (7901,) # 然后插值所有的ROC曲线在这一点 # np.zeros_like() 这个函数的意思就是生成一个和你所给数组a相同shape的全0数组。 mean_tpr = np.zeros_like(all_fpr) for i in range(n_classes): mean_tpr += interp(all_fpr, fpr[i], tpr[i]) # 最后求平均值并计算AUC mean_tpr /= n_classes fpr[&#34;macro&#34;] = all_fpr tpr[&#34;macro&#34;] = mean_tpr roc_auc[&#34;macro&#34;] = auc(fpr[&#34;macro&#34;], tpr[&#34;macro&#34;]) # Plot all ROC curves plt.figure(1) plt.plot(fpr[&#34;micro&#34;], tpr[&#34;micro&#34;], color=&#39;deeppink&#39;, linestyle=&#39;:&#39;, linewidth=4, label=&#39;micro-average ROC curve (area = {0:0.2f})&#39;.format(roc_auc[&#34;micro&#34;])) plt.plot(fpr[&#34;macro&#34;], tpr[&#34;macro&#34;],color=&#39;navy&#39;, linestyle=&#39;:&#39;, linewidth=4, label=&#39;macro-average ROC curve (area = {0:0.2f})&#39;.format(roc_auc[&#34;macro&#34;])) colors = cycle([&#39;aqua&#39;, &#39;darkorange&#39;, &#39;cornflowerblue&#39;]) for i, color in zip(range(n_classes), colors): plt.plot(fpr[i], tpr[i], color=color, linewidth=2, label=&#39;ROC curve of class {0} (area = {1:0.2f})&#39;.format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], &#39;k--&#39;, linewidth=2) plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;Some extension of Receiver Operating Characteristic to multi-class&#39;) plt.legend(loc=&#39;best&#39;) plt.show() # Zoom in view of the upper left corner. plt.figure(2) plt.xlim(0, 0.2) plt.ylim(0.8, 1) plt.plot(fpr[&#34;micro&#34;], tpr[&#34;micro&#34;],color=&#39;deeppink&#39;, linestyle=&#39;:&#39;, linewidth=4, label=&#39;micro-average ROC curve (area = {0:0.2f})&#39;.format(roc_auc[&#34;micro&#34;])) plt.plot(fpr[&#34;macro&#34;], tpr[&#34;macro&#34;],color=&#39;navy&#39;, linestyle=&#39;:&#39;, linewidth=4, label=&#39;macro-average ROC curve (area = {0:0.2f})&#39;.format(roc_auc[&#34;macro&#34;])) colors = cycle([&#39;aqua&#39;, &#39;darkorange&#39;, &#39;cornflowerblue&#39;]) for i, color in zip(range(n_classes), colors): plt.plot(fpr[i], tpr[i], color=color, linewidth=2, label=&#39;ROC curve of class {0} (area = {1:0.2f})&#39;.format(i, roc_auc[i])) plt.plot([0, 1], [0, 1], &#39;k--&#39;, linewidth=2) plt.xlabel(&#39;False Positive Rate&#39;) plt.ylabel(&#39;True Positive Rate&#39;) plt.title(&#39;ROC curve (zoomed in at top left)&#39;) plt.legend(loc=&#39;best&#39;) plt.show() 混淆矩阵confusion matrix： def plot_confusion_matrix(title, y_true, y_pred, labels): import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix cm = confusion_matrix(y_true, y_pred) # np.newaxis的作用就是在这一位置增加一个一维，这一位置指的是np.newaxis所在的位置，比较抽象，需要配合例子理解。 # x1 = np.array([1, 2, 3, 4, 5]) # the shape of x1 is (5,) # x1_new = x1[:, np.newaxis] # now, the shape of x1_new is (5, 1) cm_normalized = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] # print (cm, &#39;\n\n&#39;, cm_normalized) # [[1 0 0 0 0] # [0 1 0 0 0] # [0 0 1 0 0] # [0 0 0 1 0] # [0 0 0 0 1]] # [[1. 0. 0. 0. 0.] # [0. 1. 0. 0. 0.] # [0. 0. 1. 0. 0.] # [0. 0. 0. 1. 0.] # [0. 0. 0. 0. 1.]] tick_marks = np.array(range(len(labels))) + 0.5 # [0.5 1.5 2.5 3.5 4.5 5.5] np.set_printoptions(precision=2) plt.figure(figsize=(10, 8), dpi=120) ind_array = np.arange(len(labels)) x, y = np.meshgrid(ind_array, ind_array) # print(ind_ａrray, &#39;\n\n&#39;, x, &#39;\n\n&#39;, y) # [0 1 2 3 4 5] # [[0 1 2 3 4 5] # [0 1 2 3 4 5] # [0 1 2 3 4 5] # [0 1 2 3 4 5] # [0 1 2 3 4 5] # [0 1 2 3 4 5]] # [[0 0 0 0 0 0] # [1 1 1 1 1 1] # [2 2 2 2 2 2] # [3 3 3 3 3 3] # [4 4 4 4 4 4] # [5 5 5 5 5 5]] intFlag = 0 # 标记在图片中对文字是整数型还是浮点型 for x_val, y_val in zip(x.flatten(), y.flatten()): # plt.text()函数用于设置文字说明。 if (intFlag): c = cm[y_val][x_val] plt.text(x_val, y_val, &#34;%d&#34; % (c,), color=&#39;red&#39;, fontsize=8, va=&#39;center&#39;, ha=&#39;center&#39;) else: c = cm_normalized[y_val][x_val] if (c &gt; 0.01): plt.text(x_val, y_val, &#34;%0.2f&#34; % (c,), color=&#39;red&#39;, fontsize=7, va=&#39;center&#39;, ha=&#39;center&#39;) else: plt.text(x_val, y_val, &#34;%d&#34; % (0,), color=&#39;red&#39;, fontsize=7, va=&#39;center&#39;, ha=&#39;center&#39;) cmap = plt.cm.binary if(intFlag): plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) else: plt.imshow(cm_normalized, interpolation=&#39;nearest&#39;, cmap=cmap) plt.gca().set_xticks(tick_marks, minor=True) plt.gca().set_yticks(tick_marks, minor=True) plt.gca().xaxis.set_ticks_position(&#39;none&#39;) plt.gca().yaxis.set_ticks_position(&#39;none&#39;) plt.grid(True, which=&#39;minor&#39;, linestyle=&#39;-&#39;) plt.gcf().subplots_adjust(bottom=0.15) plt.title(title) plt.colorbar() xlocations = np.array(range(len(labels))) plt.xticks(xlocations, labels, rotation=90) plt.yticks(xlocations, labels) plt.ylabel(&#39;Index of True Classes&#39;) plt.xlabel(&#39;Index of Predict Classes&#39;) plt.savefig(&#39;confusion_matrix.jpg&#39;, dpi=300) plt.show() title=&#39;Confusion Matrix&#39; labels = [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;F&#39;, &#39;G&#39;] y_true = [1, 2, 3, 4, 5]# np.loadtxt(r&#39;/home/dingtom/a.txt&#39;) y_pred = [1, 2, 3, 4, 5]# np.loadtxt(r&#39;/home/dingtom/b.txt&#39;) plot＿confusion_matrix(title, y_true,y_pred, labels) 参考： https://github.com/Tony607/ROC-Keras/blob/master/ROC-Keras.ipynb 图像 IOU（交并比） 它是模型所预测的检测框(bbox)和真实的检测框(ground truth)的交集和并集之间的比例。用于比较有限样本集之间的相似性与差异性。Jaccard值越大，样本相似度越高。
def iou_score(output, target): &#39;&#39;&#39;计算IoU指标&#39;&#39;&#39; intersection = np.logical_and(target, output) union = np.logical_or(target, output) return np.sum(intersection) / np.sum(union) # 生成随机两个矩阵测试 target = np.random.randint(0, 2, (3, 3)) output = np.random.randint(0, 2, (3, 3)) d = iou_score(output, target) # ---------------------------- target = array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]) output = array([[1, 0, 1], [0, 1, 0], [0, 0, 0]]) d = 0.4 Dice系数 一种集合相似度度量指标,通常用于计算两个样本的相似度,值的范围0~1 ,分割结果最好时值为1 ,最差时值为0 。Dice相似系数对mask的内部填充比较敏感。
Dice系数与分类评价指标中的F1 score很相似
所以，Dice系数不仅在直观上体现了target与prediction的相似程度，同时其本质上还隐含了精确率和召回率两个重要指标。
import numpy as np def dice(output, target): &#39;&#39;&#39;计算Dice系数&#39;&#39;&#39; smooth = 1e-6 # 避免0为除数 intersection = (output * target).sum() return (2. * intersection + smooth) / (output.sum() + target.sum() + smooth) # 生成随机两个矩阵测试 target = np.random.randint(0, 2, (3, 3)) output = np.random.randint(0, 2, (3, 3)) d = dice(output, target) # ---------------------------- target = array([[1, 0, 0], [0, 1, 1], [0, 0, 1]]) output = array([[1, 0, 1], [0, 1, 0], [0, 0, 0]]) d = 0.5714286326530524 ]]></content></entry><entry><title>面试-模型融合</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[简单加权融合 回归： 算术平均融合（Arithmetic mean） 加权算术平均法 几何平均融合（Geometric mean） 分类： 投票（Voting） 绝对多数投票法：最终结果必须在投票中占一半以上。 相对多数投票法：最终结果在投票中票数最多。 硬投票：对多个模型直接进行投票，不区分模型结果的相对重要度，最终投票数最多的类为最终被预测的类。 软投票：增加了设置权重的功能，可以为不同模型设置不同权重，进而区别模型不同的重要度。 综合： 排序融合（Rank averaging） log融合 from sklearn import datasets from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier from sklearn.model_selection import train_test_split from xgboost.sklearn import XGBClassifier iris = datasets.load_iris() x=iris.data y=iris.target x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3) clf1 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=3, min_child_weight=2, subsample=0.7, colsample_bytree=0.6, objective=&#39;binary:logistic&#39;) clf2 = RandomForestClassifier(n_estimators=50, max_depth=1, min_samples_split=4, min_samples_leaf=63, oob_score=True) clf3 = SVC(C=0.1) # 硬投票 eclf = VotingClassifier(estimators=[(&#39;xgb&#39;, clf1), (&#39;rf&#39;, clf2), (&#39;svc&#39;, clf3)], voting=&#39;hard&#39;, n_jobs=-1) # 软 # eclf = VotingClassifier(estimators=[(&#39;xgb&#39;, clf1), (&#39;rf&#39;, clf2), (&#39;svc&#39;, clf3)], voting=&#39;soft&#39;, weights=[2, 1, 1]) for clf, label in zip([clf1, clf2, clf3, eclf], [&#39;XGBBoosting&#39;, &#39;Random Forest&#39;, &#39;SVM&#39;, &#39;Ensemble&#39;]): scores = cross_val_score(clf, x, y, cv=5, scoring=&#39;accuracy&#39;) print(&#34;Accuracy: %0.2f (+/- %0.2f) [%s]&#34; % (scores.mean(), scores.std(), label)) boosting/bagging （在xgboost，Adaboost，GBDT中已经用到多树的提升方法）
stacking/blending Stacking Stacking本质上就是这么直接的思路，但是直接这样有时对于如果训练集和测试集分布不那么一致的情况下是有一点问题的，其问题在于用初始模型训练的标签再利用真实标签进行再训练，毫无疑问会导致一定的模型过拟合训练集，这样或许模型在测试集上的泛化能力或者说效果会有一定的下降，因此现在的问题变成了如何降低再训练的过拟合性，这里我们一般有两种方法。
次级模型尽量选择简单的线性模型 利用K折交叉验证 # ---------------------------------------Stacking # ___________________________train,val(作为test) base_models_stacking = [LinearRegression(**parm_lr), SVR(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), BaggingRegressor()] train_pre_stacking = np.zeros((len(x_train_tree), len(base_models_stacking))) val_pre_stacking = np.zeros((len(x_val_tree), len(base_models_stacking))) # 5折stacking n_splits = 5 skf = KFold(n_splits=n_splits).split(x_train_tree, y_train_tree) for i, model in enumerate(base_models_stacking): #依次训练各个单模型 val_pre_skf = np.zeros((len(x_val_tree), n_splits)) for j, (train_train_index, train_val_index) in enumerate(skf): #5-Fold交叉训练，使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。 x_train, y_train = x_train_tree[train_train_index], y_train_tree[train_train_index] x_val, y_val = x_train_tree[train_val_index], y_train_tree[train_val_index] model.fit(x_train, y_train) print(x_train.shape) train_pre_stacking[train_val_index, i] = model.predict(x_val) # 训练集预测结果 val_pre_skf[:, j] = model.predict(x_val_tree) # 验证集预测结果i val_pre_stacking[:, i] = val_pre_skf.mean() # 验证集预测结果 print(&#34;{} MAE Score: {}&#34;.format(str(model).split(&#39;(&#39;)[0], mean_absolute_error(y_val_tree, val_pre_stacking[:, i]))) stack_model = LinearRegression(**parm_lr) stack_model.fit(train_pre_stacking, y_train_tree) # (len_train, num_models), len_train print(&#39;Stacking Model MAE Score: {} &#39;.format(mean_absolute_error(y_val_tree, stack_model.predict(val_pre_stacking)))) import warnings warnings.filterwarnings(&#39;ignore&#39;) import itertools import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import StackingClassifier from sklearn.model_selection import cross_val_score from mlxtend.plotting import plot_learning_curves from mlxtend.plotting import plot_decision_regions iris = datasets.load_iris() X, y = iris.data[:, 1:3], iris.target clf1 = KNeighborsClassifier(n_neighbors=1) clf2 = RandomForestClassifier(random_state=1) clf3 = GaussianNB() lr = LogisticRegression() sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], meta_classifier=lr) label = [&#39;KNN&#39;, &#39;Random Forest&#39;, &#39;Naive Bayes&#39;, &#39;Stacking Classifier&#39;] clf_list = [clf1, clf2, clf3, sclf] fig, ax = plt.subplots(2, 2, figsize=(20, 20), dpi=100) clf_cv_mean = [] clf_cv_std = [] for i in range(len(clf_list)): clf = clf_list[i] label = labels[i] scores = cross_val_score(clf, X, y, cv=3, scoring=&#39;accuracy&#39;) print(&#34;Accuracy: %.2f (+/- %.2f) [%s]&#34; %(scores.mean(), scores.std(), label)) clf_cv_mean.append(scores.mean()) clf_cv_std.append(scores.std()) clf.fit(X, y) fig = plot_decision_regions(X=X, y=y, clf=clf, ax=ax[i//2, i%2]) plt.title(label) plt.show() Blending 采用了和stacking同样的方法，不过只从训练集中选择一个fold的结果，再和原始特征进行concat作为元学习器meta learner的特征，测试集上进行同样的操作。
在第一层，70%的数据上训练多个模型，预测30%数据、测试集。 在第二层，用第一层30%数据预测的结果作为训练集继续训练，用第一层预测的测试集结果作为测试集
其优点在于：
比stacking简单（因为不用进行k次的交叉验证来获得stacker feature） 避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集 缺点在于： 使用了很少的数据（第二阶段的blender只使用training set10%的量） blender可能会过拟合，stacking使用多次的交叉验证会比较稳健 #------------------------------------blending # ___________________________train, val(作为test) base_models_blending = [LinearRegression(**parm_lr), SVR(), DecisionTreeRegressor(), RandomForestRegressor(), GradientBoostingRegressor(), BaggingRegressor()] x_train_70, x_train_30, y_train_70, y_train_30 = train_test_split(x_train_tree, y_train_tree, test_size=0.3, random_state=2020) train_30_pre_blending = np.zeros((len(x_train_30), len(base_models_blending))) val_pre_blending = np.zeros((len(x_val_tree), len(base_models_blending))) for i, model in enumerate(base_models_blending): #依次训练各个单模型 model.fit(x_train_70, y_train_70) train_30_pre_blending[:, i] = model.predict(x_train_30) # 对于测试集，直接用这k个模型的预测值作为新的特征。 val_pre_blending[:, i] = model.predict(x_val_tree) print(&#39;{} MAE Score: {}&#39;.format(str(model).split(&#39;(&#39;)[0], mean_absolute_error(y_val_tree, val_pre_blending[:, i]))) blend_model = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30) blend_model.fit(train_30_pre_blending, y_train_30) print(&#39;Blending Model MAE Score: {} &#39;.format(mean_absolute_error(y_val_tree, blend_model.predict(val_pre_blending)))) ]]></content></entry><entry><title>面试-排序算法</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[ 排序算法 平均时间复杂度 最好/差时间复杂度 空间复杂度 数据对象稳定性 冒泡排序 O(n2) O(n)/O(n2) O(1) 稳定 选择排序 O(n2) O(n2)/O(n2) O(1) 不稳定 插入排序 O(n2) O(n)/O(n2) O(1) 稳定 希尔排序 O(n^(1.3-2)) O(1) 不稳定 快速排序 O(nlog2n) O(nlog2n)/O(n2) O(log2n) 不稳定 堆排序 O(nlog2n) O(nlog2n)/O(nlog2n) O(1) 不稳定 归并排序 O(nlog2n) O(nlog2n)/O(nlog2n) O(n) 稳定 计数排序 O(n+m) O(n+m) O(n+m) 稳定 桶排序 O(n) O(n) O(m) 稳定 基数排序 O(k*n) O(n2) 稳定 不稳定：快选堆希 稳定：排序后，关键字相同的元素保持原顺序中的相对位置不变
O(n*log2n) 快归堆希 快、归，使用了递归需要栈保存信息 插入排序：基本有序的数组排序。快速排序此时最差(取第n个为基准） 快速排序：内部排序算法平均性能最优。 堆排序：取一组数中K个最大\小的元素。 基数排序：所有中国人生日排序。 冒泡、插入有时可达线性时间 冒泡排序 每一轮比较可能多个元素移动位置，而元素位置的互换是需要消耗资源的，所以这是一种偏慢的排序算法，仅适用于对于含有较少元素的数列进行排序。
def bubbleSort(arr): for i in range(len(arr)-1): for j in range(len(arr)-1-i): # 每轮，从头开始和后面的比较交换，把未排序最大的放到后面 if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr bubbleSort(a) **稳定性：**它是指对同样的数据进行排序，会不会改变它的相对位置。比如 [ 1, 3, 2, 4, 2 ] 经过排序后，两个相同的元素 2 位置会不会被交换。冒泡排序是比较相邻两个元素的大小，显然不会破坏稳定性。
**空间复杂度：**由于整个排序过程是在原数据上进行操作，故为 O(1);
**时间复杂度：**由于嵌套了 2 层循环，故为 O(n*n);
选择排序 交换次数比冒泡排序少，就 减少cpu的消耗，所以在数据量小的时候可以用选择排序，实际适用的场合非常少。
def selectionSort(arr): for i in range(len(arr)-1): min_index = i for j in range(i+1, len(arr)): # 每轮，从已排序后面的元素开始，选出最小的放到已排序的后面 if arr[j] &lt; arr[min_index]: min_index = j arr[i], arr[min_index] = arr[min_index], arr[i] return arr a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] selectionSort(a) 稳定性：排序过程中元素是按顺序进行遍历，相同元素相对位置不会发生变化，故稳定。
空间复杂度：在原序列进行操作，故为 O( 1 );
时间复杂度：需要 2 次循环遍历，故为 O( n * n );
插入排序 def insertionSort(arr): for i in range(1, len(arr)): # 从第二个元素开始，每轮，已排序后面的元素向前比较，找到比他小或相等的元素，放到其后面 sorted_index = i-1 current_value = arr[i] while sorted_index&gt;=0 and current_value&lt;arr[sorted_index]: arr[sorted_index+1] = arr[sorted_index] # 往后腾空位 sorted_index -= 1 arr[sorted_index+1] = current_value return arr a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] insertionSort(a) 稳定性：它是从后往前遍历已排序好的序列，相同元素不会改变位置，故为稳定排序； 空间复杂度：它是在原序列进行排序，故为 O ( 1 ); 时间复杂度：排序的过程中，首先要遍历所有的元素，然后在已排序序列中找到合适的位置并插入。共需要 2 层循环，故为 O ( n * n );
希尔排序 是插入排序的一种更高效的改进版本。但希尔排序是非稳定排序算法。先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序.
def shellSort(arr): gap = len(arr)//2 while gap &gt; 0: for i in range(gap, len(arr)): sorted_index = i-gap # 要与前面元素比较的索引 current_value = arr[i] # 不能超过gap而且前面的数大，交换，跟更前面的比较 while sorted_index&gt;=0 and current_value&lt;arr[sorted_index]: arr[sorted_index+gap] = arr[sorted_index] # 往后腾空位 sorted_index -= gap arr[sorted_index+gap] = current_value gap = int(gap/2) return arr a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] print(shellSort(a)) 稳定性：它可能会把相同元素分到不同的组中，那么两个相同的元素就有可能调换相对位置，故不稳定。
空间复杂度：由于整个排序过程是在原数据上进行操作，故为 O(1);
时间复杂度：希尔排序的时间复杂度与增量序列的选取有关，例如希尔增量时间复杂度为O(n²)，而Hibbard增量的希尔排序的时间复杂度为O(log n的3/2)，希尔排序时间复杂度的下界是n*log2n
快速排序 步骤为：
挑选基准值：从数列中挑出一个元素，称为&quot;基准&quot;（pivot）; 分割：重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（与基准值相等的数可以到任何一边）。在这个分割结束之后，对基准值的排序就已经完成; 递归排序子序列：递归地将小于基准值元素的子序列和大于基准值元素的子序列排序。 def quickSort(arr): if len(arr) &lt;= 1: return arr pivot = arr[len(arr)//2] # 根据基准分成左中右，递归排序左右子序列，拼接 left = [i for i in arr if i &lt; pivot] middle = [i for i in arr if i == pivot] right = [i for i in arr if i &gt; pivot] return quickSort(left) + middle + quickSort(right) arr = [10, 7, 8, 9, 1, 5] print(quickSort(arr)) 归并排序 def merge_sort(arr): if len(arr) &lt;= 1: return arr left = merge_sort(arr[:len(arr)//2]) # 递归排序左右子序列， 将两个排好的子序列合并 right = merge_sort(arr[len(arr)//2:]) sorted = [] while left and right: sorted.append(left.pop(0) if left[0] &lt;= right[0] else right.pop(0)) sorted.extend(right if right else left) return sorted a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] print(merge_sort(a)) 堆排序 堆的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。
递归调整大根堆。(在堆中做结构调整使得父节点的值大于子节点)
从下到上，递归调整大根堆，生成大根堆 堆顶最大，和未排序末尾交换，得到新的无序区(R0,R1,……Ri-1)和新的有序区(Ri,……Rn)，从上到下，不管已排序好的，调整为大根堆 # 递归调整为大根堆。在堆中做结构调整使得父节点的值大于子节点 def heapify(arr, heap_size, root): largest = root # 指示最大值索引 left = 2*root+1 # 左子节点索引 right = left+1 # 右子节点索引 # 递归，找出最大值索引 if left&lt;heap_size and arr[largest]&lt;arr[left]:largest = left if right&lt;heap_size and arr[largest]&lt;arr[right]:largest = right # 交换 if largest != root: arr[root], arr[largest] = arr[largest], arr[root] # 交换 heapify(arr, heap_size, largest) # 递归 def heapSort(arr): # 从下到上 生成大根堆 for i in range(len(arr)//2-1, 0, -1): # heapify如果初始根比子节点大，则结束 print(i) heapify(arr, len(arr), i) # 堆顶最大，和未排序末尾交换，得到新的无序区(R0,R1,……Ri-1)和新的有序区(Ri,……Rn) for i in range(len(arr)-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] heapify(arr, i, 0) # 从上到下不管已排序好的，调整为大根堆 return arr a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] print(heapSort(a)) 计数排序 def countingSort(arr): min_num, max_num = min(arr), max(arr) # arr每个元素放进max-min个桶里，依次从桶里取出放进arr buckets = [0]*(max_num-min_num+1) # 装进桶里 for i in arr: buckets[i-min_num]+=1 # 依次从桶里取出 arr.clear() for i, buc in enumerate(buckets): while buc&gt;0: arr.append(i+min_num) buc -= 1 return arr a = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] countingSort(a) 桶排序 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点：
在额外空间充足的情况下，尽量增大桶的数量 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中 同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。
def bucket_sort(arr): min_num = min(arr) max_num = max(arr) # 桶的大小 bucket_range = (max_num-min_num) / len(arr) # 桶数组 buckets = [ [] for i in range(max_num-min_num+1)] # 向桶数组填数 for i in range(len(arr)): buckets[int((arr[i]-min_num)//bucket_range)].append(arr[i]) arr.clear() # 回填，这里桶内部排序直接调用了sorted for b in buckets: for i in sorted(b): arr.append(i) return arr arr = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] print(bucket_sort(arr)) 基数排序 取得数组中的最大数，并取得位数； 对数位较短的数前面补零； 先从个位开始，根据位值(0-9)分别放到0~9号桶中; 再将放置在0~9号桶中的数据按顺序放到数组中; ![
image]( https://upload-images.jianshu.io/upload_images/18339009-d06d049a0791bb94?imageMogr2/auto-orient/strip )
def radix_sort(arr): digit_index = 0 # 指示当前位数 digit_len = 1 # 最长位数 # 找出列表中最大的位数 while 10**digit_len &lt; max(arr): digit_len += 1 while digit_index &lt; digit_len: buckets =[[] for _ in range(10)] #初始化桶数组 for i in arr: buckets[i//(10**digit_index) % 10].append(i) # 当前位上的数字 arr.clear() for b in buckets: # 放回原序列 for i in b: arr.append(i) digit_index = digit_index + 1 return arr arr = [12, 11, 13, 5, 0, 6, 7, 0, 12, 34, 54, 2, 3] print(radix_sort(arr)) ]]></content></entry><entry><title>面试-缺失值处理</title><url>/post/%E9%9D%A2%E8%AF%95-%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[删除 样本数据量十分大且缺失值不多的情况下非常有效，但如果样本量本身不大且缺失也不少，那么不建议使用。
#删除数据表中含有空值的行 df.dropna(how=&#39;any&#39;) 不处理 补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，一些模型无法应对具有缺失值的数据，因此要对缺失值进行处理。然而还有一些模型本身就可以应对具有缺失值的数据，此时无需对数据进行处理，比如Xgboos等树模型。
分箱：（缺失值一个箱） 虚拟变量其实就是缺失值的一种衍生变量。具体做法是通过判断特征值是否有缺失值来定义一个新的二分类变量。比如，特征为A含有缺失值，我们衍生出一个新的特征B，如果A中特征值有缺失，那么相应的B中的值为1，如果A中特征值没有缺失，那么相应的B中的值为0。
data_train[&#39;CabinCat&#39;] = data_train[&#39;Cabin&#39;].copy() data_train.loc[ (data_train.CabinCat.notnull()), &#39;CabinCat&#39; ] = &#34;No&#34; data_train.loc[ (data_train.CabinCat.isnull()), &#39;CabinCat&#39; ] = &#34;Yes&#34; fig, ax = plt.subplots(figsize=(10,5)) sns.countplot(x=&#39;CabinCat&#39;, hue=&#39;Survived&#39;,data=data_train) plt.show() data_train[['Cabin','CabinCat']].head(10)
补全 均值、众数、中位数、 对于定类数据：使用 众数（mode）填补，比如一个学校的男生和女生的数量，男生500人，女生50人，那么对于其余的缺失值我们会用人数较多的男生来填补。 对于定量（定比）数据：使用平均数（mean）或中位数（median）填补，比如一个班级学生的身高特征，对于一些同学缺失的身高值就可以使用全班同学身高的平均值或中位数来填补。一般如果特征分布为正太分布时，使用平均值效果比较好，而当分布由于异常值存在而不是正太分布的情况下，使用中位数效果比较好。
注：此方法虽然简单，但是不够精准，可能会引入噪声，或者会改变特征原有的分布。如果缺失值是随机性的，那么用平均值比较适合保证无偏，否则会改变原分布。
#使用price均值对NA进行填充 df[&#39;price&#39;].fillna(df[&#39;price&#39;].mean()) df[&#39;price&#39;].fillna(df[&#39;price&#39;].median()) 建模预测、多重插补、压缩感知补全、矩阵补全等 利用其它变量做模型的输入进行缺失变量的预测，与我们正常建模的方法一样，只是目标变量变为了缺失值。如果其它特征变量与缺失变量无关，则预测的结果毫无意义。如果预测结果相当准确，则又说明这个变量完全没有必要进行预测，因为这必然是与特征变量间存在重复信息。
回归预测： 缺失值是连续的，即定量的类型，才可以使用回归来预测。
极大似然估计（Maximum likelyhood）： 在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，那么通过观测数据的边际分布可以对未知参数进行极大似然估计（Little and Rubin）。这种方法也被称为忽略缺失值的极大似然估计，对于极大似然的参数估计实际中常采用的计算方法是期望值最大化(Expectation Maximization，EM）。该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂，且仅限于线性模型。
多重插补（Mutiple imputation）： 多值插补的思想来源于贝叶斯估计，认为待插补的值是随机的，它的值来自于已观测到的值。具体实践上通常是估计出待插补的值，然后再加上不同的噪声，形成多组可选插补值。根据某种选择依据，选取最合适的插补值。
热卡填补（Hot deck imputation）： 热卡填充法是在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。通常会找到超出一个的相似对象，在所有匹配对象中没有最好的，而是从中随机的挑选一个作为填充值。这个问题关键是不同的问题可能会选用不同的标准来对相似进行判定，以及如何制定这个判定标准。该方法概念上很简单，且利用了数据间的关系来进行空值估计，但缺点在于难以定义相似标准，主观因素较多。
K最近距离邻法（K-means clustering） 通过K均值的聚类方法将所有样本进行聚类划分，然后再通过划分的种类的均值对各自类中的缺失值进行填补。归其本质还是通过找相似来填补缺失值。
多重插补： 我们看到，以上提出的拟合和替换方法都是单一的插补方法，而多重插补弥补了单一插补的缺陷，它并没有试图去通过模拟值去估计每个缺失值，而是提出缺失数据值的一个随即样本（这些样本可以是不同的模型拟合结果的组合）。这种程序的实施恰当地反映了由于缺失值引起的不确定性，使得统计推断有效。多重插补推断可以分为以下3个步骤：
为每个缺失值产生一套可能的插补值，这些值反映了无响应模型的不确定性； 每个插补数据集合都用针对完整数据集的统计方法进行统计分析； 对来自各个插补数据集的结果，根据评分函数进行选择，产生最终的插补值；
随机森林： 这也是Kaggle竞赛中大佬们经常使用的一个办法，具体实现方式与正常一样，只是将缺失值作为目标变量即可. def set_missing_ages(df): # 把已有的数值型特征取出来丢进Random Forest Regressor中 age_df = df[[&#39;Age&#39;,&#39;Fare&#39;, &#39;Parch&#39;, &#39;SibSp&#39;, &#39;Pclass&#39;]] # 乘客分成已知年龄和未知年龄两部分 known_age = age_df[age_df.Age.notnull()].as_matrix() unknown_age = age_df[age_df.Age.isnull()].as_matrix() # y即目标年龄 y = known_age[:, 0] # X即特征属性值 X = known_age[:, 1:] # fit到RandomForestRegressor之中 rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1) rfr.fit(X, y) # 用得到的模型进行未知年龄结果预测 predictedAges = rfr.predict(unknown_age[:, 1:]) # print predictedAges # 用得到的预测结果填补原缺失数据 df.loc[ (df.Age.isnull()), &#39;Age&#39; ] = predictedAges return df, rfr ]]></content></entry><entry><title>面试-神经网络的计算量和参数量</title><url>/post/%E9%9D%A2%E8%AF%95-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%A1%E7%AE%97%E9%87%8F%E5%92%8C%E5%8F%82%E6%95%B0%E9%87%8F/</url><categories><category>面试</category></categories><tags/><content type="html">FLOPS，即每秒浮点操作次数FLoating point OPerations per Second这个指标来衡量GPU的运算能力。 MACC，即乘加数Multiply-ACCumulate operation，或者叫MADD，来衡量模型的计算量
全连接层的参数量和计算量 $parameter=(d_{in} +1)\times d_{out}$ $d_{in}$表示输入的特征向量的维度， $+1$表示偏置，$d_{out}$表示输出维度 $FLOPS=[d_{in} +(d_{in} -1)+1]\times d_{out}$ $d_{in} $表示乘法运算量 $d_{in} -1$表示加法运算量(权重矩阵与输入/上一层值的矩阵向量相乘所需的加法运算) $+1$表示偏置
具有300个输入神经元和100个输出神经元的全连接层的参数量是：300×100+100=30100
卷积层的参数量和计算量 stride=1，N个输入与M个输出，第m个通道的最终输出由下式给出 $\text { parameter }=\left(k_{w} \times k_{h} \times C_{i n}+1\right) \times C_{\text {out }}$ $k_{w} , k_{h}$表示一个卷积核的宽和高，$+1$表示偏置，$C_{out }$个卷积核 $F L O P s=\left[\left(C_{i n} \times k_{w} \times k_{h}\right)+\left(C_{i n} \times k_{w} \times k_{h}-1\right)+1\right] \times C_{\text {out }} \times w \times h$ $k_{w} ,k_{h}$ 分别表示卷积核的宽和高，其中 $C_{i n} \times k_{w} \times k_{h}$ 表示乘法计算量， $C_{i n} \times k_{w} \times k_{h}-1$表示加法计算量，$+1$表示偏置， $w,h$ 表示上一层 feature map的宽和高。
池化层 其他类型的层，例如池化层。这些其他层类型肯定需要时间，但它们不使用点积，因此不能用MACC测量。
示例：在112×112具有128通道的特征图上具有过滤器大小2和步幅2的最大池化层需要112×112×128=1,605,632 FLOPS或1.6兆 FLOPS。当然，如果步幅与滤波器尺寸不同例如3×3窗口，2×2步幅），则这些数字会稍微改变。
但是，在确定网络的复杂性时，通常会忽略这些附加层。毕竟，与具有100个 MFLOPSE的卷积/全连接层相比，1.6 MFLOPS非常小。因此，它成为网络总计算复杂度的舍入误差。
Batch normalization 在现代网络中，通常在每个卷积层之后都包含一个 batch norm层。针对每个输出值，计算公式如下 $z=gamma*(y-mean)/sqrt(variance epsilon) beta$ 此处，y是上一层的输出图中的元素。我们首先通过减去该输出通道的平均值并除以标准偏差来对该值进行归一化( epsilon用于确保不除以0,通常为0.001),然后，我们将系数gamm缩放，然后添加一个偏差或偏移beta。每个通道都有自己的 gamma,beta,均值和方差值
因此，如果卷积层的输出中有C个通道，则 Batch normalization层将学习C×4参数，如下图所示
通常将Batch normalization应用于卷积层的输出后，在ReLU之前，我们可以做一些数学运算以使Batch normalization层消失！由于在全连接层中进行的卷积或矩阵乘法只是一堆点积，它们是线性变换，而上面给出的Batch normalization公式也是线性变换，因此我们可以将这两个公式组合为一个变换。我们只需要将Batch normalization参数合并到前面各层的权重中，也就是说我们可以完全忽略Batch Normalization层的影响，因为我们在进行推理时实际上将其从模型中删除了。
LSTM计算量和参数量估计 LSTM里面有 4 个非线性变换（3 个 门 + 1 个 tanh），每一个非线性变换说白了就是一个全连接网络，形如：$W\left[h_{t-1}, x_{t}\right]+b$ 。其中，$h_{t-1}, x_{t}$维度是$d_x + d_h$，易得MACC为： $$[(d_x + d_h) +1 ]d_h4$$ $+1$表示偏置 四个非线性变换中，还会对全连接层的输出进行激活函数计算（三个sigmoid和一个tanh），对于sigmoid的计算量为：$(d_x + d_h) * d_h 4 3 $个FLOPS。tanh的计算公式为：$\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$，其中共有八个加，减，乘，除，求幂，平方根等计算，所以计算量为：$(d_x + d_h) * d_h * 8$个FLOPS。
除此之外，LSTM除了在四个非线性变换中的计算，还有三个矩阵乘法（不是点积）、一个加法、一个tanh计算，其中三个矩阵乘法都是shape为$(batch, d_h)$，则这四个运算的计算量为：$batch * d_h*12 + batch * d_h * 8$ #???? 综上所述，LSTM的计算量为： $d_x * d_h * 8 + d_h * (d_h + 20) $个FLOPS
对于特征维128的输入，LSTM单元数为64的网络来说，LSTM的参数量为：((128 + 64) * 64 + 64) * 4 = 49408 激活函数计算量 非线性激活函数，例如ReLU或 sigmoid不用MACC进行度量，而是使用 FLOPST进行度量，原因是它们不做点积，ReLU： $max(a ，0)$ 这是在GPU上的一项操作，激活函数仅应用于层的输出，例如在具有$J$个输出神经元的完全连接层上，ReLU计算$J$次，因此我们将其判定为$ J \space FLOPS$。而对于 Sigmoid激活函数来说，有不一样了，它涉及到了一个指数，所以成本更高 $y=\frac{1}{1 +\exp(-z)}$ 在计算 FLOPS时，我们通常将加，减，乘，除，求幕，平方根等作为单个FLOP进行计数，由于在 Sigmoid激活函数中有四个不同的运算，因此计算为$4 FLOPS$。
实际上，通常不计这些操作，因为它们只占总时间的一小部分，更多时候我们主要对矩阵乘法和点积感兴趣，对于参数量？注意了它压根没有参数</content></entry><entry><title>面试-数据分析指标</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8C%87%E6%A0%87/</url><categories><category>面试</category></categories><tags/><content type="html">DAU(Daily Active Users) DNU(Daily New Users) ARPU(Average Revenue per User) ARPPU(Average Revenue per Paying User) APA (Active Payment Account)
ROI(投资回报率） = $（收入－成本）／投入＊100% $ 它所表达的是 收回了多少，而非利润，更准确点说就是销售收入。
ARPU(Average Revenue Per User)：某段时间平均每用户收入 LTV（Life Time Value）：用户生命周期价值 一月份用户流失率30%，3.333个月流失光，ARPU=100，这批用户LTV=300
UV（独立访客）：即Unique Visitor，访问您网站的一台电脑客户端为一个访客。 PV(page view)，即页面浏览量，或点击量；用户每1次对网站中的每个网页访问均被记录1次。用户对同一页面的多次访问，访问量累计。 以上来自百度百科。 简单来说： UV = 一个人（记录身份证号） PV = 人次（只认次数，不认人）</content></entry><entry><title>面试-数据集的划分</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86/</url><categories><category>面试</category></categories><tags/><content type="html">Holdout检验 按一定比例划分为训练集和测试集 这种方法也称为保留法。我们通常取8-2、7-3、6-4、5-5比例切分，直接将数据随机划分为训练集和测试集，然后使用训练集来生成模型，再用测试集来测试模型的正确率和误差，以验证模型的有效性。 在验证集上计算出来的最后评估指标与原始分组有很大关系。
k-fold cross validation交叉验证法 交叉验证一般采用k折交叉验证，即，往往k取为10。在这种数据集划分法中，我们将数据集划分为k个子集，每个子集均做一次测试集，每次将其余的作为训练集。在交叉验证时，我们重复训练k次，每次选择一个子集作为测试集，并将k次的平均交叉验证的正确率作为最终的结果。
K越大，Bias越小。Variance越大 最后，我们要说说K的选取。事实上，和开头给出的文章里的部分内容一样，K的选取是一个Bias和Variance的trade-off。 K越大，每次投入的训练集的数据越多，模型的Bias越小。但是K越大，又意味着每一次选取的训练集之前的相关性越大（考虑最极端的例子，当k=N，也就是在LOOCV里，每次都训练数据几乎是一样的）。而这种大相关性会导致最终的test error具有更大的Variance。 一般来说，根据经验我们一般选择k=5或10。
Bootstrap自助法 不管是 Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步减小，这可能会影响模型训练效果。有没有能维持训练集样本规模的验证方法呢？自助法可以比较好地解决这个问题。自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证，这就是自助法的验证过程。</content></entry><entry><title>面试-数据结构算法</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E7%AE%97%E6%B3%95/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[ 数组 优点：找到某一下下标（index)的元素所需时间O(1) 缺点：需要分配连续空间；查询需要遍历整个数组时间为 O(n)；删除添加时间O(n)
重要的基本特征 利用数组来表示多又树的结构，和优先队列有些类似 优先队列是用数组来表示完全二叉树，而树状数组是多又树 树状数组的第一个元素是空节点 如果节点 treely是teel的父节点，那么需要满足y=x-(x&amp;(x)
链表 优点：灵活分配空间；删除添加时间O(1) 缺点：查询时间O(n)
栈 只关心上一次操作；能O(1)时间内查找到更前一次操作
队列 **常用的场景：**广度优先搜索
双端队列 可以利用一个双链表 队列的头尾两端能在O(1)的时间内进行查看、添加和删除
**常用的场景：**实现一个长度动态变化的窗口或者连续区间
树 常用的场景：递归算法 优先队列 与普通队列的区别：每次取出的元素是队列中优先级最高的 **常用的场景：**从杂乱无章的数据中按照一定的顺序（或者优先级）筛选数据 **本质：**二叉堆的结构，堆在英文里叫 Binary Heap 利用一个数组结构来实现完全二叉树
特性： 数组里的第一个元素$ array[0]$拥有最高的优先级 给定一个下标$i$，那么对于元素 $array[i]$而言 父节点对应的元素下标是$（i-1)/2$ 左侧子节点对应的元素下标是$2i+1$ 右側子节点对应的元素不标是$2i+2$ 数组中每个元素的优先级都必须要高于它两侧子节点 其基本操作两个: 向上筛选（ sift up/ bubble up) 向下筛选（ sift down/ bubble down） 初始化一个大小为n的堆的时间为O(n)
前缀树 **重要性质：**每个节点至少包含两个基本属性 children：数组或者集合，罗列出每个分支当中包含的所有字符 isend：布尔值，表示该节点是否为某字符串的结尾 根节点是空的 创建： 对每个字符串的字符进行遍历 从前缀树的根节点开始，将每个字符加入到节点的 children字符集当中 如果字符集已经包含了这个字符，跳过 如果当前字符是字符串的最后一个，把当前节点的 isend标记为真
搜索 从前缀树的根节点出发，逐个匹配输入的前缀字符 如果遇到了，继续往下一层搜索 如果没遇到，立即返回
线段树 按照二叉树的形式存储数据的结构，每个节点保存的都是数组里某一段的总和 树状数组 重要的基本特征： 利用数组来表示多叉树的结构，和优先队列有些类似 优先队列是用数组来表示完全二叉树，而树状数组是多叉树 树状数组的第一个元素是空节点 如果节点 tree[y]是tree[x]的父节点，那么需要满足$y=x-(x&amp;(x))$
图 **图的存储和表达方式：**邻接矩阵、邻接链表 图的遍历：深度优先、广度优先 二部图的检测（Bipartite）、树的检测、环的检测：有向图、无向图 拓扑排序 联合-查找算法（Union-Find) 最短路径： Dijkstra、Bellman-Ford
DFS解决什么问题 DFS解决的是连通性的问题，即给定两一个起始点（或某种起始状态）和一个终点（或某种最终状态判断是否有一条路径能从起点连接到终点。 很多情况下，连通的路径有很多条，只需要找出一条即可，DFS只关心路径存在与否，不在乎其长短。 算法的思想 从起点出发，选择一个可选方向不断向前，直到无法继续为止。然后尝试另外一种方向，直到最后走到终点 DFS复杂度分析 由于DFS是图论里的算法，分析利用DFS解题的复杂度时，应当借用图论的思想，图有两种表示方式 邻接表（图里有V个顶点，E条边） 访问所有顶点的时间为O(V)，查找所有顶点的部居的时间为O(E)，所以总的时间复杂度是$O(V+E)$ 邻接矩阵（图里有V个顶点，E条边） 查找每个顶点的邻居需要O(V)的时间，所以查找整个矩阵的时候需要$O(V^2)$的时间 广度优先搜索简称BFS 广度优先搜索一般用来解决最短路径的问题 广度优先的搜索是从起始点出发，一层一层地进行 每层当中的点距离起始点的步数都是相同的 **双端BFS(( 同时从起始点和终点开始进行的广度优先的搜索称为双端BFS 双端BFS可以大大地提高搜索的效率 例如，想判断社交应用程序中两个人之间需要经过多少朋友介绍才能互相认识 如何对这个图进行广度优先的遍历呢？ 1.广度优先遍历需要借用的数据结构是队列（ Queue 2.队列特点是先进先出（FIFO） 动态规划 递归解决斐波那契数列 复杂度高
递归+记忆化，我们如果能够记住我们之前每次运行的结果，那么就可以从第一项一直递推到最后一项，求解出我们需要的结果，如果我们把这其中每一项称为一个状态，比如这里的fib(0),fib(1),fib(2),fib(3)&hellip;，那么我们我们想求解的任一目标值实际也就是一个状态，比如这里的fib(6)，我们要做的就是把前面已知的状态递推到待求的目标状态上，所以动态就是它是有很多状态的，我们需要在这些状态间转移，规划出最优解（状态）的路线。这就需要一个状态转移方程，比如Fib(n) = Fib(n-1) + Fib(n-2)就是从前面的状态转移到现状态的转移方程（方式）。 总结一下： 状态定义（建数组） 状态转移方程（数组存值）
递归 此类求 多少种可能性 的题目一般都有 递推性质 ，即 f(n)f(n) 和 f(n-1)f(n−1)…f(1)f(1) 之间是有联系的。 剑指 Offer 10- II. 青蛙跳台阶问题 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。
** 1、第一递归函数功能** 假设 f(n) 的功能是求青蛙跳上一个n级的台阶总共有多少种跳法. 2、找出递归结束的条件 我说了，求递归结束的条件，你直接把 n 压缩到很小很小就行了，因为 n 越小，我们就越容易直观着算出 f(n) 的多少，所以当 n = 1时，你知道 f(1) 为多少吧？够直观吧？即 f(1) = 1。代码如下： if(n == 1):return 1 3：找出函数的等价关系式 每次跳的时候，小青蛙可以跳一个台阶，也可以跳两个台阶，也就是说，每次跳的时候，小青蛙有两种跳法。 第一种跳法：第一次我跳了一个台阶，那么还剩下n-1个台阶还没跳，剩下的n-1个台阶的跳法有f(n-1)种。 第二种跳法：第一次跳了两个台阶，那么还剩下n-2个台阶还没，剩下的n-2个台阶的跳法有f(n-2)种。 所以，小青蛙的全部跳法就是这两种跳法之和了，即 f(n) = f(n-1) + f(n-2)。至此，等价关系式就求出来了。 当 n = 2 时，显然会有 f(2) =2 但是f(2) = f(1) + f(0)=2。递归结束.递归结束条件要严谨 考虑是否重复计算 f(n) = f(n-1) + f(n-2),重复计算了两次 f(5)，五次 f(4) class Solution: def __init__(self): self.arr = {} # 减少重复计算 def numWays(self, n: int) -&gt; int: if n in self.arr: return self.arr[n] if n &lt; 2: # 递归结束条件要严谨 return 1 self.arr[n] = (self.numWays(n-1)+self.numWays(n-2))%1000000007 return self.arr[n] ]]></content></entry><entry><title>面试-数据挖掘步骤</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%AD%A5%E9%AA%A4/</url><categories><category>面试</category></categories><tags/><content type="html"></content></entry><entry><title>面试-数据预处理</title><url>/post/%E9%9D%A2%E8%AF%95-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[ import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import missingno as msno # 用于可视化缺失值分布 import scipy.stats as st 预览数据： 读入外部数据 data = pd.read_excel(io=r'C:\Users\Administrator\Desktop\datas\data.xlsx') dataset = pd.read_csv('../datasets/Data.csv')
查看数据的规模 data.shape
预览数据 data.head().append(data.tail()) edu各种值个数,每列都看一下防止数据倾斜，可以直接删掉 data['edu'].value_counts() edu列值排序 data['edu'].sort_values(ascending=False) 数据类型,是否存在除了nan以外的特殊符号异常。 data.info() 相关统计量 data.describe(include='all')
查看表中各变量的数据类型 当数据集变大时，需要转换数据类型来节省内存。 data.dtypes
数值型转字符型 data['id'] = data['id'].astype(str)
字符型转数值型 data['custom_amt'] = data['custom_amt'].str[1:].astype(float)
字符型转日期型 data['order_date'] = pd.to_datetime(data['order_date'], format = '%Y年%m月%d日')
字符整理 删除列- col 1中&amp;#后面的所有字符(包括&amp;#) df[ col_1 ].replace(&amp;#.* , , regex=True, inplace=True)
预测值分布 机器学习中很多model都假设数据或参数服从正态分布。当样本不服从正态分布时，可以做如下转换： 线性变化z-scores 使用Boxcox变换 使用yeo-johnson变换
盲目假设变量服从正态分布可能导致不准确的结果，要结合分析。例如：不能假设股票价格服从正态分布，因为价格不能为负，故我们可以将股票价格假设为服从对数正态分布，以确保其值≥0；而股票收益可能是负数，因此收益可以假设服从正态分布。当样本数据表明质量特征的分布为非正态时，应用基于正态分布的方法会作出不正确的判决。约翰逊分布族即为经约翰(yeo-johnson)变换后服从正态分布的随机变量的概率分布，约翰逊分布体系建立了三族分布，分别为有界SB 、对数正态SL和无界SU。
本案例的预测值为价格，显然不符合正态分布，故分别采用无界约翰逊分布Johnson SU、正态分布normal、对数正态分布lognormal，综合来看无界约翰逊分布对price的拟合效果更好。
import seaborn as sns import scipy.stats as st y = Train_data[&#39;price&#39;] plt.figure(1); plt.title(&#39;Johnson SU&#39;) sns.distplot(y, kde=False, fit=st.johnsonsu) plt.figure(2); plt.title(&#39;Normal&#39;) sns.distplot(y, kde=False, fit=st.norm) plt.figure(3); plt.title(&#39;Log Normal&#39;) sns.distplot(y, kde=False, fit=st.lognorm) 偏度和峰度 print(&#34;Skewness: %f&#34; % data[&#39;age&#39;].skew()) print(&#34;Kurtosis: %f&#34; % data[&#39;age&#39;].kurt()) sns.distplot(data[&#39;age&#39;],color=&#39;orange&#39;) 大的值很少，其实该处可将其当作异常值处理填充或删除，本文中经过log变换之后，分布较均匀，可据此进行预测，这也是预测问题常用的技巧
第1步：处理重复、丢失数据 判断数据中是否存在重复观测 data.duplicated().any() 使用any“方法”（即序列中只要存在一个True，则返回True）。
将冗余信息删除。 data.drop_duplicates() 如需使drop_duplicates“方法”的删除功能作用在原始数据中，必须将inplace参数设置为True。 data.drop_duplicates(subset=['name','age']) 用户的姓名和年龄相同就认为是重复数据
统计各类空值的个数 data.isnull().sum()
可视化缺省值
import missingno as msno msno.matrix(data.sample(250)) msno.bar(data.sample(250)) 补全空值 sklearn.preprocesing.imputer from sklearn.preprocessing import Imputer # 这里我们需要使用pandas的iloc(区分于loc根据index来索引，iloc利用行号来索引)方法来对数据进行处理 X = data.iloc[ : , :-1].values imputer = Imputer(missing_values=&#34;NaN&#34;, strategy=&#34;mean&#34;, axis=0) imputer = imputer.fit(X[ : , 1:3]) X[ : , 1:3] = imputer.transform(X[ : , 1:3]) pandas
data[&#39;gender&#39;].fillna(data[&#39;gender&#39;].mode()[0], inplace = True) data[&#39;age&#39;].fillna(data[&#39;age&#39;].median(), inplace = True) data[&#39;edu&#39;].fillna(data[&#39;edu&#39;].mode()[0], inplace = True) 删除空的列。 data1.isnull().sum() drop_column = [&#39;gender&#39;,&#39;age&#39;, &#39;edu&#39;] data1.drop(drop_column, axis=1, inplace = True) 增加一列 data[&#39;FamilySize&#39;] = data[&#39;SibSp&#39;] + data[&#39;Parch&#39;] + 1 # 满足条件(condition)，输出x，不满足输出y。 data[&#39;IsAlone&#39;] = np.where(data[&#39;FamilySize&#39;] &gt; 1,0,1) # expand，这个参数取True时，会把切割出来的内容当做一列 data1[&#39;Title&#39;] = data1[&#39;Name&#39;].str.split(&#34;, &#34;, expand=True)[1].str.split(&#34;.&#34;, expand=True)[0] 第2步：类别转为数字 sklearn.preprocesing.LabelEncoder
from sklearn.preprocessing import LabelEncoder, OneHotEncoder labelencoder_X = LabelEncoder() X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0]) #Creating a dummy variable onehotencoder = OneHotEncoder(categorical_features = [0]) X = onehotencoder.fit_transform(X).toarray() labelencoder_Y = LabelEncoder() Y = labelencoder_Y.fit_transform(Y) pandas
num_encode = {&#39;gender&#39; : { &#39;male&#39; :1, &#39;female&#39; :0}} data.replace(num_encode, inplace=True) 第3步：拆分数据集为测试集合和训练集合 sklearn.crossvalidation.rain_test_split
from sklearn.model_selection import train_test_split X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0) 第4步：特征缩放 sklearn.preprocessing.StandardScalar
from sklearn.preprocessing import StandardScaler sc_X = StandardScaler() X_train = sc_X.fit_transform(X_train) X_test = sc_X.transform(X_test) 如图所示，通过6步完成数据预处理。
此例用到的 数据 ， 代码 。
]]></content></entry><entry><title>面试-特征构造、选择</title><url>/post/%E9%9D%A2%E8%AF%95-%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0%E9%80%89%E6%8B%A9/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[特征构造 统计量特征：计数、求和、比例、标准差等 时间特征：相对时间、绝对时间，节假日，双休日等 地理信息：分桶 非线性变换：取log、平方、根号 数据分桶：等频、等距分桶、Dest-KS分桶、卡方分桶 特征组合、交叉：人、商品
数据分桶 等频分桶：区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。 等距分桶：从最小值到最大值之间,均分为 N 等份； Best-KS分桶：类似利用基尼指数进行二分类； 卡方分桶：自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验：具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。 为什么要做数据分桶呢？ 离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展； 离散后的特征对异常值更具鲁棒性，如 age&gt;30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰； LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合； 离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量变为 M*N 个变量，进一步引入非线形，提升了表达能力； 特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化 当然还有很多原因，LightGBM 在改进 XGBoost 时就增加了数据分桶，增强了模型的泛化性
特征选择 特征选择主要有两个功能： 减少特征数量、降维，使模型泛化能力更强，减少过拟合 增强对特征和特征值之间的理解
从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。
过滤式（filter）： 概述：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。然后再训练学习器。特征选择过程与后续学习器无关
方法： Relief、方差选择、相关系数、卡方检验、互信息
数值型特征，方差很小的特征可不要 from sklearn.feature_selection import VarianceThreshold # 默认方差为0的特征会自动删除 X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]] selector = VarianceThreshold() # 默认threshold=0.0 print(selector.fit_transform(X)) # variances_查看样本各个特征的方差 # get_params(deep=True)：获取估计器参数，以字典形式返回 # get_support(indices=False)：参数为False，返回满足的特征的索引为True，则特征列受否满足为True或False； # inverse_transform(X)：反转转换操作，返回X，剔除的特征列值用0替换的数组 scores按升序排序，选择排前k名所对应的特征 sklearn.feature_selection.SelectKBest(score_func=&lt;function f_classif&gt;, k=10) scores按升序排序，选择排前百分percentile所对应的特征 sklearn.feature_selection.SelectPercentile(score_func=&lt;function f_classif&gt;, percentile=10)
分类特征,取值个数高度偏斜的那种可以先去掉
相关系数排序 选择相关系数大于阈值的部分特征；（当然有时候根据字段含义也可以选） 连续数据，正态分布，线性关系，用pearson相关系数是最恰当，当然用spearman相关系数也可以，效率没有pearon相关系数高。上述任一条件不满足，就用spearman相关系数，不能用pearson相关系数。 两个定序测量数据（顺序变量）之间也用spearman相关系数，不能用pearson相关系数。Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。
利用假设检验得到特征与输出值之间的相关性 分类问题：chi2, f_classif, mutual_info_classif 回归问题：f_regression, mutual_info_regression 方法有比如卡方检验、t检验、F检验等。 卡方检验一般是检查离散变量与离散变量的相关性，当然离散变量的相关性信息增益和信息增益比也是不错的选择（可以通过决策树模型来评估来看）
互信息 利用互信息从信息熵的角度分析相关性
包裹式（wrapper） 确定模型和评价准则之后，对特征空间的不同子集做交叉验证，进而搜索最佳特征子集
方法：LVM（Las Vegas Wrapper）、递归特征消除算法、基于机器学习模型的特征排序 主要思想： 包裹式从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集。包裹式特征选择直接针对给定学习器进行优化。
优缺点： 从最终学习器的性能来看，包裹式比过滤式更好； 计算开销通常比过滤式特征选择要大得多。
特征子集的搜索问题，最容易想到的办法是穷举法，还可以在拉斯维加斯方法框架下使用随机策略进行子集搜索（Las Vegas Wrapper，LVW）。但是由于LVW算法中特征子集搜索采用了随机策略，每次特征子集评价都需要训练学习器，计算开销很大，如果初始特征数很多，算法可能运行很长时间都达不到停止条件，若有运行时间限制，可能给不出解。 因此，我们通常使用的是贪心算法：如前向搜索（在最优的子集上逐步增加特征，直到增加特征并不能使模型性能提升为止）、后向搜索、双向搜索（将前向搜索和后向搜索相结合）。
递归特征消除(Recursive feature elimination) 对特征含有权重的预测模型（如线性模型的权重系数），递归特征消除（RFE）通过递归考虑越来越少的特征集来选择特征。 具体流程如下：首先，对估计器进行初始特征集训练，并通过coef_或feature_importances_属性获取特征的重要性；然后，从当前的特征中删除最不重要的特征；最后，对以上过程进行递归重复，直至达到所需的特征数量。 对手写数字图片每个像素点权重进行排名
# 例1：递归特征消除示例 from sklearn.svm import SVC from sklearn.datasets import load_digits from sklearn.feature_selection import RFE # 导入手写数字数据集 digits = load_digits() print(digits.images.shape) # (1797, 8, 8) X = digits.images.reshape((len(digits.images), -1))# (1797, 64) y = digits.target # 创建RFE并 svc = SVC(kernel=&#34;linear&#34;, C=1) rfe = RFE(estimator=svc, n_features_to_select=1, step=1) rfe.fit(X, y) ranking = rfe.ranking_.reshape(digits.images[0].shape) print(ranking.shape) # 将排名可视化输出 plt.matshow(ranking, cmap=plt.cm.Blues) plt.colorbar() plt.title(&#34;Ranking of pixels with RFE&#34;) plt.show() # estimator,一种具有 fit 方法的监督学习估计器，它通过一个coef_属性或feature_importances_ 提供关于特征重要性的信息。 # n_features_to_select,要选择的特征的数量。如果没有，则选择一半的特征。 # step,如果大于或等于1，则对应于每次迭代中要删除的(整数)特性数。如果在(0.0,1.0)范围内，则对应于在每次迭代中要删除的特性的百分比(向下舍入)。 # n_features_,选择特征的数量。 # support_,所选特征的掩码。 # ranking_,特征的的排名 # estimator_ 嵌入式（embedding）： 概述：结合过滤式和包裹式，学习器训练过程中自动进行了特征选择。先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。
方法：LR+L1、决策树、lasso回归 主要思想： 在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。而嵌入式特征选择在学习器训练过程中自动地进行特征选择。
嵌入式选择最常用的是L1 正则化和L2正则化
正则化项越大，模型越简单，系数越小，当正则化项增大到一定程度时，所有的特征系数都会趋于0，在这个过程中，会有一部分特征的系数先变成0。也就实现了特征选择过程。逻辑回归、线性回归、决策树都可以当作正则化选择特征的基学习器，只有可以得到特征系数或者可以得到特征重要度的算法才可以作为嵌入式选择的基学习器。
将LinearSVC 和SelectFromModel结合来评估特征的重要性进行特征选择，之后用RandomForestClassifier模型使用转换后的输出（即被选出的相关特征）进行训练。
from sklearn.pipeline import Pipeline from sklearn.datasets import load_iris from sklearn.feature_selection import SelectFromModel from sklearn.svm import LinearSVC from sklearn.ensemble import RandomForestClassifier iris = load_iris() X, y = iris.data, iris.target clf = Pipeline([ (&#39;feature_selection&#39;, SelectFromModel(LinearSVC(penalty=&#34;l1&#34;, dual=False, max_iter=3000))), (&#39;classfication&#39;, RandomForestClassifier(n_estimators=100))]) clf.fit(X, y) # sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False, norm_order=1, max_features=None) # estimator,**SelectFromModel与任何训练后有coef_或feature_importances_属性的预测模型一起使用** # threshold.用于特征选择的阈值。重要性大于或等于的特征被保留,也可以设置成一些抽象的值，比如mean,median,1.25*mean等等。 # prefit 是否对传入的基本分类器事先进行训练。 # norm_order # max_features 所选特征数目 # threshold_ # estimator_ ]]></content></entry><entry><title>面试-异常处理</title><url>/post/%E9%9D%A2%E8%AF%95-%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/</url><categories><category>面试</category></categories><tags/><content type="html">异常值检测特征分为类别特征和数字特征 数字特征 相关性分析、查看特征的偏度和峰度、数字特征相互之间的关系可视化、多变量互相回归关系可视化、数字特征的频数可视化
类别特征 ounique分布、箱形图可视化、小提琴图可视化、类别柱形图可视化
异常值处理 箱线图(没有对数据作任何限制性要求) 3-$\sigma$(Sigma)(符合正态分布） BOX-COX转换（处理有偏分布） 长尾截断 聚类、k近邻、One Class SVM、Isolation Forest
关于高势集特征model，也就是类别中取值个数非常多的， 一般可以使用聚类的方式，然后独热
很多模型假设数据服从正态分布 数据整体服从正态分布，样本均值和方差则相互独立。当样本不服从正态分布时，可以做如下转换：
线性变化z-scores：基于原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。将A的原始值x使用z-score标准化到x’
yeo-johnson变换：是幂变换（power transformation）的方法之一，通过构建一组单调函数对随机变量进行数据变换。
Boxcox变换：一种广义幂变换方法，是统计建模中常用的一种数据变换，用于连续的响应变量不满足正态分布的情况。在做线性回归的过程中，一般需要做线性模型假定。
关于box-cox转换，一般是用于连续的变量不满足正态的时候，在做线性回归的过程中，一般线性模型假定: $Y=X\beta + \epsilon$
其中$\epsilon$满足正态分布，但是利用实际数据建立回归模型时，个别变量的系数通不过。例如往往不可观测的误差$\epsilon$可能是和预测变量相关的，不服从正态分布，于是给线性回归的最小二乘估计系数的结果带来误差，为了使模型满足线性性、独立性、方差齐性以及正态性，需改变数据形式，故应用BOX-COX转换。具体详情这里不做过多介绍，当然还有很多转换非正态数据分布的方式： 在一些情况下（P值&amp;lt;0.003）上述方法很难实现正态化处理，所以优先使用BOX-COX转换，但是当P值&amp;gt;0.003时两种方法均可，优先考虑普通的平方变换。 BOX-COX的变换公式： 类别不平衡 极端的例子如有998个反例，但是正例只有2个，那么学习方法只需要返回一个永远将新样本预测为反例的学习器，就能达到99.8%的精度
训练策略 有以下几种方式来缓解数据不均衡的问题，如迁移学习、解耦学习、集成学习。
迁移学习如在imageNet上训练好的模型，将其backbone模型迁移到下游任务上。在不平衡学习中还有另外一种迁移方式，即先在完整的数据集上训练得到的模型，迁移到子数据集（重新构建的类别均衡数据集）上，冻结backbone特征提取部分，训练分类器。亲测效果提升明显。
解耦学习是将学习过程分成用于特征提取的表征学习和用于分类的分类器学习两个部分，
数据 将大类分解成多个小类 数据多的欠采样（删除，原型生成（k-means中心点代替整个簇）），数据少的过采样（复制） 合成样本： SMOTE，图像中的对比度、明亮度、平移、裁剪、旋转等能力，还有如copy-paste,mixup,mosica 等 模型/网络结构 决策树等。加权少类别的样本错分代价（Cost Sensitive算法代价敏感）
loss函数 Focal loss
在标准交叉熵损失基础上修改得到的，通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。
评价指标 AUC等
训练数据不足 迁移学习（ Transfer Learning)微调 生成对抗网络 图像处理（变换角度（旋转、平移、缩放、裁剪），添加噪声扰动，颜色变换。改变图像的亮度、清晰度） 上采样技术，数据合成SMOTE (Synthetic Minority Over-sampling Technique）
数据转换的方式有： 数据归一化(MinMaxScaler)； 标准化(StandardScaler)； 对数变换(log1p)； 转换数据类型(astype)； 独热编码(OneHotEncoder)； 标签编码(LabelEncoder)； 修复偏斜特征(boxcox1p)等。
使用独热编码需要注意以下问题 (1)使用稀疏向量来节省空间。在独热编码下，特征向量只有某一维取值为1,其他位置取值均为0.因此可以利用向量的稀疏表示有效地节省空间，并且目前大部分的算法均接受稀疏向量形式的输入。 （2)配合特征选择来降低维度。高维度特征会带来几方面的问题。一是在K近邻算法中，高维空间下两点之间的距离很难得到有效的衡量；二是在逻辑回归模型中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是通常只有部分维度是对分类、预测有帮助，因此可以考虑配合特征选择来降低维度。</content></entry><entry><title>面试-异常值检测</title><url>/post/%E9%9D%A2%E8%AF%95-%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[简单统计 df.describe() 散点图 3∂原则 **这个原则有个条件：数据需要服从正态分布。**在3∂原则下，**异常值如超过3倍标准差，那么可以将其视为异常值。**正负3∂的概率是99.7%，那么距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。 箱型图 利用箱型图的四分位距（IQR）对异常值进行检测 **四分位距(IQR)就是上四分位与下四分位的差值。**而我们通过IQR的1.5倍为标准，**规定：超过（上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离）的点为异常值。**下面是Python中的代码实现，主要使用了numpy的percentile方法。
Percentile = np.percentile(df[&#39;length&#39;],[0,25,50,75,100]) IQR = Percentile[3] - Percentile[1] UpLimit = Percentile[3]+ageIQR*1.5 DownLimit = Percentile[1]-ageIQR*1.5 也可以使用seaborn的可视化方法boxplot来实现：
f,ax=plt.subplots(figsize=(10,8)) sns.boxplot(y=&#39;length&#39;,data=df,ax=ax) plt.show() 基于模型检测 **构建一个概率分布模型，并计算对象符合该模型的概率，把具有低概率的对象视为异常点。**如果模型是簇的集合，则异常是不显著属于任何簇的对象；如果模型是回归时，异常是相对远离预测值的对象。 离群点的概率定义：离群点是一个对象，关于数据的概率分布模型，它具有低概率。这种情况的前提是必须知道数据集服从什么分布，如果估计错误就造成了重尾分布。
优缺点：（1）有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效；（2）对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。
基于近邻度的离群点检测 统计方法是**利用数据的分布来观察异常值，一些方法甚至需要一些分布条件，**而在实际中数据的分布很难达到一些假设条件，在使用上有一定的局限性。
确定数据集的有意义的邻近性度量比确定它的统计分布更容易。这种方法比统计学方法更一般、更容易使用，因为一个对象的离群点得分由到它的k-最近邻（KNN）的距离给定。 需要注意的是：离群点得分对k的取值高度敏感。**如果k太小，则少量的邻近离群点可能导致较低的离群点得分；**如果K太大，则点数少于k的簇中所有的对象可能都成了离群点。为了使该方案对于k的选取更具有鲁棒性，可以使用k个最近邻的平均距离。
优缺点：（1）简单；（2）缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；（3）该方法对参数的选择也是敏感的；（4）不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。
基于密度的离群点检测 从基于密度的观点来说，离群点是在低密度区域中的对象。基于密度的离群点检测与基于邻近度的离群点检测密切相关，因为密度通常用邻近度定义。
一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。
优缺点：（1）给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；（2）与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；（3）参数选择是困难的。虽然LOF算法通过观察不同的k值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。
基于聚类的方法来做异常点检测 基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。
离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。**这也是k-means算法的缺点，对离群点敏感。**为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。
优缺点：（1）基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；（2）簇的定义通常是离群点的补，因此可能同时发现簇和离群点；（3）产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；（4）聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。
专门的离群点检测 其实以上说到聚类方法的本意是是无监督分类，并不是为了寻找离群点的，只是恰好它的功能可以实现离群点的检测，算是一个衍生的功能。
除了以上提及的方法，还有两个专门用于检测异常点的方法比较常用：One Class SVM和Isolation Forest
]]></content></entry><entry><title>面试-优化算法</title><url>/post/%E9%9D%A2%E8%AF%95-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url><categories><category>面试</category></categories><tags/><content type="html"><![CDATA[主要有三大类：
基本梯度下降法，包括 GD，BGD，SGD； 动量优化法，包括 Momentum，NAG 等； 自适应学习率优化法，包括 Adam，AdaGrad，RMSProp 等 1)梯度下降法(Gradient Descent) - BGD 每一步迭代都需要遍历所有的样本数据,消耗时间长,但是一定能得到最优解. - SGD(Stochastic Gradient Descent）&mdash;&gt;迭代速度快,得到局部最优解(凸函数时得到全局最优解)
MBGD(Mini-batch Gradient Descent)&mdash;&gt;小批量梯度下降法 2)牛顿法和拟牛顿法
牛顿法&mdash;&gt;牛顿法是二阶收敛,收敛速度快; 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
拟牛顿法&mdash;&gt;改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度.
3)共轭梯度法（Conjugate Gradient）
共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。
启发式的方法 启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等。多目标优化算法(NSGAII算法、MOEA/D算法以及人工免疫算法)
5)解决约束的方法&mdash;拉格朗日乘子法
]]></content></entry><entry><title>没有H1-6标题头和评论的文章</title><url>/post/no-header-title.html</url><categories><category>示例</category></categories><tags><tag>toc</tag><tag>标题</tag></tags><content type="html">刘慈欣2018克拉克奖获奖感言（部分内容节选）。
用于测试在没有H1-6标题头时，文章的目录导航是否会直接关闭，并关闭评论功能。
先生们、女士们，晚上好，
很荣幸获得Clarke Award for Imagination in Service to Society Award。
这个奖项是对想象力的奖励，而想象力是人类所拥有的一种似乎只应属于神的能力，它存在的意义也远超出我们的想象。有历史学家说过，人类之所以能够超越地球上的其它物种建立文明，主要是因为他们能够在自己的大脑中创造出现实中不存在的东西。在未来，当人工智能拥有超过人类的智力时，想象力也许是我们对于它们所拥有的惟一优势。
科幻小说是基于想象力的文学，而最早给我留下深刻印象的是Arthur . Clarke的作品。除了Jules Verne和George Wells外，Clarke的作品是最早进入中国的西方现代科幻小说。在上世纪八十年代初，中国出版了他的《2001:A Space Odyssey》和《Rendezvous With Rama》。当时文革刚刚结束，旧的生活和信仰已经崩塌，新的还没有建立起来，我和其他年轻人一样，心中一片迷茫。这两本书第一次激活了我想象力，思想豁然开阔许多，有小溪流进大海的感觉。读完《2001:A Space Odyssey》的那天深夜，我走出家门仰望星空，那时的中国的天空还没有太多的污染，能够看到银河，在我的眼中，星空与过去完全不一样了，我第一次对宇宙的宏大与神秘产生了敬畏感，这是一种宗教般的感觉。而后来读到的《Rendezvous With Rama》，也让我惊叹如何可以用想象力构造一个栩栩如生的想象世界。正是Clarke带给我的这些感受，让我后来成为一名科幻作家。
现在，三十多年过去了，我渐渐发现，我们这一代在上世纪六十年代出生于中国的人，很可能是人类历史上最幸运的人，因为之前没有任何一代人，像我们这样目睹周围的世界发生了如此巨大的变化，我们现在生活的世界，与我们童年的世界已经完全是两个不同的世界，而这种变化还在加速发生着。中国是一个充满着未来感的国度，中国的未来可能充满着挑战和危机，但从来没有像现在这样具有吸引力，这就给科幻小说提供了肥沃的土壤，使其在中国受到了空前的关注，作为一个在六十年代出生在中国的科幻小说家，则是幸运中的幸运。
我期待有那么一天，像那些曾经描写过信息时代的科幻小说一样，描写太空航行的科幻小说也变的平淡无奇了，那时的火星和小行星带都是乏味的地方，有无数的人在那里谋生；木星和它众多的卫星已成为旅游胜地，阻止人们去那里的唯一障碍就是昂贵的价格。
但即使在这个时候，宇宙仍是一个大的无法想象的存在，距我们最近的恒星仍然遥不可及。浩瀚的星空永远能够承载我们无穷的想象力。
谢谢大家。
点击阅读全文</content></entry><entry><title>Mermaid支持流程图</title><url>/post/mermaid-charts.html</url><categories><category>示例</category></categories><tags><tag>流程图</tag><tag>时序图</tag></tags><content type="html"><![CDATA[本主题已支持 Mermaid 实现以纯文本的方式绘制流程图、序列图、甘特图、状态图、关系图行等等，随着 Mermaid 也在逐步发展，后续还会有各种各样的图被引入进来，更多的类型及使用方式可关注其官方网站： https://mermaid-js.github.io/ 。
使用说明 通过 hugo new 命令创建一篇新的文章 在文章头部配置 mermaid: true 使用短代码书写各种类型的图，自带2个参数： align（对齐） 和 bc（背景色），可参考如下使用示例 流程图 {{&lt; mermaid align=&#34;left&#34; &gt;}} graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; {{&lt; /mermaid &gt;}} graph TD; A-->B; A-->C; B-->D; C-->D; 时序图 {{&lt; mermaid bc=&#34;#eee&#34; &gt;}} sequenceDiagram participant Alice participant Bob Alice-&gt;&gt;John: Hello John, how are you? loop Healthcheck John-&gt;&gt;John: Fight against hypochondria end Note right of John: Rational thoughts &lt;br/&gt;prevail! John--&gt;&gt;Alice: Great! John-&gt;&gt;Bob: How about you? Bob--&gt;&gt;John: Jolly good! {{&lt; /mermaid &gt;}} sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! 类图 {{&lt; mermaid &gt;}} classDiagram Class01 &lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --&gt; C2 : Where am i? Class09 --* C3 Class09 --|&gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 &lt;--&gt; C2: Cool label {{&lt; /mermaid &gt;}} classDiagram Class01 <|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --> C2 : Where am i? Class09 --* C3 Class09 --|> Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 <--> C2: Cool label 甘特图 {{&lt; mermaid &gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d {{&lt; /mermaid &gt;}} gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid excludes weekdays 2014-01-10 section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d 实体关系图 {{&lt; mermaid &gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses {{&lt; /mermaid &gt;}} erDiagram CUSTOMER ||--o{ ORDER : places ORDER ||--|{ LINE-ITEM : contains CUSTOMER }|..|{ DELIVERY-ADDRESS : uses 用户旅程 {{&lt; mermaid &gt;}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me {{&lt; /mermaid &gt;}} journey title My working day section Go to work Make tea: 5: Me Go upstairs: 3: Me Do work: 1: Me, Cat section Go home Go downstairs: 5: Me Sit down: 5: Me ]]></content></entry><entry><title>数学公式渲染</title><url>/post/math-formula.html</url><categories><category>示例</category></categories><tags><tag>数学公式</tag><tag>mathjax</tag><tag>katex</tag></tags><content type="html"><![CDATA[本主题支持 mathjax 和 katex 两种不的方案支持数学公式的渲染，可根据自已的需求进行选择。
接下的示例中，将使用 MathJax 方案来展示渲染效果。
使用 hugo new 命令创建一篇新的文章 可以全局启用数据公式渲染，请在项目配置参数 math: katex 或 math: mathjax 或是将该参数配置到需要显示数学公式的页面头部（减少不必要的加载消耗） 注意： 使用 支持的TeX功能 的联机参考资料。
例子 重复的分数 $$ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} \equiv 1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}} {1+\frac{e^{-8\pi}} {1+\cdots} } } } $$
总和记号 $$ \left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right) $$
几何级数之和 我把接下来的两个例子分成了几行，这样它在手机上表现得更好。这就是为什么它们包含 \displaystyle。
$$ \displaystyle\sum_{i=1}^{k+1}i $$
$$ \displaystyle= \left(\sum_{i=1}^{k}i\right) +(k+1) $$
$$ \displaystyle= \frac{k(k+1)}{2}+k+1 $$
$$ \displaystyle= \frac{k(k+1)+2(k+1)}{2} $$
$$ \displaystyle= \frac{(k+1)(k+2)}{2} $$
$$ \displaystyle= \frac{(k+1)((k+1)+1)}{2} $$
乘记号 $$ \displaystyle 1 + \frac{q^2}{(1-q)}+\frac{q^6}{(1-q)(1-q^2)}+\cdots = \displaystyle \prod_{j=0}^{\infty}\frac{1}{(1-q^{5j+2})(1-q^{5j+3})}, \displaystyle\text{ for }\lvert q\rvert &lt; 1. $$
随文数式 这是一些线性数学: $$ k_{n+1} = n^2 + k_n^2 - k_{n-1} $$ ， 然后是更多的文本。
希腊字母 $$ \Gamma\ \Delta\ \Theta\ \Lambda\ \Xi\ \Pi\ \Sigma\ \Upsilon\ \Phi\ \Psi\ \Omega \alpha\ \beta\ \gamma\ \delta\ \epsilon\ \zeta\ \eta\ \theta\ \iota\ \kappa\ \lambda\ \mu\ \nu\ \xi \ \omicron\ \pi\ \rho\ \sigma\ \tau\ \upsilon\ \phi\ \chi\ \psi\ \omega\ \varepsilon\ \vartheta\ \varpi\ \varrho\ \varsigma\ \varphi $$
箭头 $$ \gets\ \to\ \leftarrow\ \rightarrow\ \uparrow\ \Uparrow\ \downarrow\ \Downarrow\ \updownarrow\ \Updownarrow $$
$$ \Leftarrow\ \Rightarrow\ \leftrightarrow\ \Leftrightarrow\ \mapsto\ \hookleftarrow \leftharpoonup\ \leftharpoondown\ \rightleftharpoons\ \longleftarrow\ \Longleftarrow\ \longrightarrow $$
$$ \Longrightarrow\ \longleftrightarrow\ \Longleftrightarrow\ \longmapsto\ \hookrightarrow\ \rightharpoonup $$
$$ \rightharpoondown\ \leadsto\ \nearrow\ \searrow\ \swarrow\ \nwarrow $$
符号 $$ \surd\ \barwedge\ \veebar\ \odot\ \oplus\ \otimes\ \oslash\ \circledcirc\ \boxdot\ \bigtriangleup $$
$$ \bigtriangledown\ \dagger\ \diamond\ \star\ \triangleleft\ \triangleright\ \angle\ \infty\ \prime\ \triangle $$
微积分学 $$ \int u \frac{dv}{dx},dx=uv-\int \frac{du}{dx}v,dx $$
$$ f(x) = \int_{-\infty}^\infty \hat f(\xi),e^{2 \pi i \xi x} $$
$$ \oint \vec{F} \cdot d\vec{s}=0 $$
洛伦茨方程 $$ \begin{aligned} \dot{x} &amp; = \sigma(y-x) \\ \dot{y} &amp; = \rho x - y - xz \\ \dot{z} &amp; = -\beta z + xy \end{aligned} $$
交叉乘积 这在KaTeX中是可行的，但在这种环境中馏分的分离不是很好。
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \\ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
这里有一个解决方案:使用“mfrac”类(在MathJax情况下没有区别)的额外类使分数更小:
$$ \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\ \frac{\partial X}{\partial u} &amp; \frac{\partial Y}{\partial u} &amp; 0 \\ \frac{\partial X}{\partial v} &amp; \frac{\partial Y}{\partial v} &amp; 0 \end{vmatrix} $$
强调 $$ \hat{x}\ \vec{x}\ \ddot{x} $$
有弹性的括号 $$ \left(\frac{x^2}{y^3}\right) $$
评估范围 $$ \left.\frac{x^3}{3}\right|_0^1 $$
诊断标准 $$ f(n) = \begin{cases} \frac{n}{2}, &amp; \text{if } n\text{ is even} \\ 3n+1, &amp; \text{if } n\text{ is odd} \end{cases} $$
麦克斯韦方程组 $$ \begin{aligned} \nabla \times \vec{\mathbf{B}} -, \frac1c, \frac{\partial\vec{\mathbf{E}}}{\partial t} &amp; = \frac{4\pi}{c}\vec{\mathbf{j}} \\ \nabla \cdot \vec{\mathbf{E}} &amp; = 4 \pi \rho \\ \nabla \times \vec{\mathbf{E}}, +, \frac1c, \frac{\partial\vec{\mathbf{B}}}{\partial t} &amp; = \vec{\mathbf{0}} \\ \nabla \cdot \vec{\mathbf{B}} &amp; = 0 \end{aligned} $$
统计学 固定词组：
$$ \frac{n!}{k!(n-k)!} = {^n}C_k {n \choose k} $$
分数在分数 $$ \frac{\frac{1}{x}+\frac{1}{y}}{y-z} $$
ｎ次方根 $$ \sqrt[n]{1+x+x^2+x^3+\ldots} $$
矩阵 $$ \begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\ a_{21} &amp; a_{22} &amp; a_{23}\\ a_{31} &amp; a_{32} &amp; a_{33} \end{pmatrix} \begin{bmatrix} 0 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 \end{bmatrix} $$
标点符号 $$ f(x) = \sqrt{1+x} \quad (x \ge -1) f(x) \sim x^2 \quad (x\to\infty) $$
现在用标点符号:
$$ f(x) = \sqrt{1+x}, \quad x \ge -1 f(x) \sim x^2, \quad x\to\infty $$
]]></content></entry><entry><title>支持用户自定义设计</title><url>/post/custom-files.html</url><categories><category>示例</category></categories><tags><tag>自定义</tag><tag>个性化</tag><tag>布局</tag></tags><content type="html"><![CDATA[对于熟悉前端开发的用户来说，可以通过自定义文件配置，实现对站点的样式和布局进行个性化的调整。其中布局方面主要是支持左侧边栏的站点概览部分，以及站点底部2个位置，但样式的重置可以是整个站点的任意位置。
打开配置参数 首先要明确在配置文件的 params 区域中有配置如下参数：
customFilePath: sidebar: custom_sidebar.html footer: custom_footer.html style: /css/custom_style.css 注意： sidebar 和 footer 的文件命名不可以与它们的参数名称相同，不然会影响系统默认的布局设计，切记！！！ 😄 然后在站点的根目录下创建 layouts/partials 2个目录，用于存放自定布局设计文件，另外在站点根目录下创建 statics/css 2个目录，用于存放自定义 CSS 样式文件。一切就绪后，就可以参考如下的步骤，完成自己的设计想法。
侧边栏设计 在前面创建 partials 目录中新一个后缀名为 html 的文件，可以在里面书写你所想表达的设计或内容，比如引入一些第三方组件内容。示例如下：
&lt;div class=&#34;mydefined animated&#34; itemprop=&#34;custom&#34;&gt; &lt;span&gt;支持自定义CSS和Sidebar布局啦💄💄💄&lt;/span&gt; &lt;/div&gt; 再把该文件的路径配置到相应的参数中，效果请查看左侧边栏底部的效果。
底部设计 在前面创建 partials 目录中新一个后缀名为 html 的文件，可以在里面书写你所想表达的设计或内容，比如引入一些第三方组件内容。示例如下：
&lt;div class=&#34;custom-footer&#34;&gt; Website source code &lt;a href=&#34;https://github.com/hugo-next/hugo-theme-next/tree/develop/exampleSite/layouts/partials/custom-footer.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; &lt;/div&gt; 再把该文件的路径配置到相应的参数中，效果请查看站点底部的效果。
自定义样式 在前面创建 css 目录中新一个后缀名为 css 的文件，然后可以在里面把站点的样式进行重定义，或是增加一些自己定义的样式设计，在写文章时进行引用，示例如下：
.custom-head5 { font-size: 1.2em; color: #ed6c24; font-weight: bold; } 再把该文件的路径配置到相应的参数中，效果参考如下：
我是自定义的标题样式效果!!!
]]></content></entry><entry><title>自定义短语示例</title><url>/post/shortcodes.html</url><categories><category>示例</category></categories><tags><tag>短代码</tag><tag>语法</tag></tags><content type="html"><![CDATA[虽然 Markdown 语法已经非常丰富能够满足我们写文章的绝大部分需求，但是为更好的对文章内容进行更友好的排版，为引设计一套自定义的短语，便于在使用时能够快速引用。
块引用 在引用一些经典名言名句时，可以采用此短语，语法参考如下：
{{&lt; quote &gt;}} ### block quote 写下你想表达的话语！ {{&lt; /quote &gt;}} 实际效果：
希望是无所谓有，无所谓无的，这正如地上的路。
其实地上本没有路，走的人多了，也便成了路。
鲁迅
信息块 支持 default，info，success，warning，danger 等五种不同效果的展示，语法参考如下：
{{&lt; note [class] [no-icon] &gt;}} 书写表达的信息 支持 Markdown 语法 {{&lt; /note &gt;}} 实际效果：
Default Header without icon Welcome to Hugo NexT! Default Header Welcome to Hugo NexT! Info Header Welcome to Hugo NexT! Success Header Welcome to Hugo NexT! Warning Header Welcome to Hugo NexT! Danger Header Welcome to Hugo NexT! ]]></content></entry><entry><title>关于 Hugo NexT 组织</title><url>/about.html</url><categories/><tags/><content type="html">Hugo NexT 组织是由众多喜爱 NexT 主题及风格的世界各地友人共同组建而成，为的就是让这个主题继续在 Hugo 引擎中也能得到发扬光大，在此也欢迎你的加入！
我们的愿景 延续 NexT 经典的黑白调搭配，保持简单的易用性及强大的功能。
使用反馈 加入 GitHub Discussions 或 Gitter 在线讨论 🍻 GitHub Issues 提交错误报告 🐛 GitHub Feature 表新功能的想法 ✨ 同时国内用户也可加入 QQ 群交流： 604710815</content></entry><entry><title>文章目录导航</title><url>/post/table-of-content.html</url><categories><category>示例</category></categories><tags><tag>目录</tag><tag>导航</tag><tag>博客</tag></tags><content type="html">巴顿将军说过：“衡量一个人是否成功，不是看他站到顶峰，而是从顶峰跌落之后的反弹力”，褚时健的人生便是如此，中年发家致富，名利双收，之后又跌落到谷底，等到74岁再创业，10年后带着褚橙归来，东山再起收获亿万财富，他的发展轨迹就是反弹的过程。
早年的故事 起始 2014年的春天，在云南省华宁县和宜良县的交界处，一座名叫矣则的小山村里，一处已经有上百年历史的古旧四合院宅子被拆掉。村委会正带领村民们进行“美丽乡村”的建设，一年以后，旧有村居将再也看不到，代之而起的是钢筋混凝土的新式民居。就像10年、20年前中国大小城市的改造一样，这个群山围绕的小村子也开始陷入“工地模式”。
童年浪花 在江河边长大的孩子几乎都有一个当仁不让的特长：善水。褚时健也不例外，他不仅从小就在南盘江和花鱼塘里扑腾出了上佳的游泳技术，五六岁已经可以一个猛子扎出老远，而且从七八岁就可以在南盘江和河滩上的鱼塘里捉鱼了。
少年故事 褚时健在乡村自由自在生活的十多年，其实正是中国社会风雨飘摇的十多年。特别是1937年卢沟桥事变后，日本人发动全面侵华战争，短短两三年间，中国的大部分国土相继沦陷
激情的青春十年 当上了游击队员 1948年夏天，褚时健回乡，在禄丰车站小学做了一名老师，同时也和褚时仁、褚时杰一起继续保持与共产党组织的联系，做一些传递情报的工作
战火纷飞 因为战斗力相较悬殊，所以游击队只能是靠打一枪换一个地方的办法，专找敌人薄弱的地方攻击，但更多时候，都是在防御和转移阵地。
迎来解放 1949年12月，国民党云南省主席卢汉在昆明宣布起义，云南正式拉开解放的序幕。1950年2月20日，陈赓、宋任穷、周保中率解放军第二野战军第四兵团进入昆明，24日，陈赓宣布云南全境解放。
生活的断层 跌入生活底层 “反右”运动中被打倒的人在“右派”身份确定后，只有一条路可走：下放到农场。农场名副其实，就是干农活儿的地方，必须过和农民一样的生活。
尾声 岁月像一条河 2015年，是褚时健和马静芬结婚60周年，被称为“钻石婚”的纪念年份。这简直是一份人生的奖赏,在中国离婚率愈益升高的当下，60年的婚姻，几乎就像一个前世之梦。一个甲子的相伴相随，褚时健和马静芬共同经历了国家和个人的各种风浪，共同面对过生死。他们两人已经不仅是夫妻，更是一对战友。尽管马静芬偶尔会对褚时健年轻时候的粗心抱怨上两句，但说到最后，她会说一句：“没有我就没有他，没有他也就没有我。”
作者致谢 这本书从2014年初夏开始采访，到今天完稿，历时18个月。封面上“作者”只能是我一个人的名字，但也只有我自己知道，这本书，包含了太多人的心力和体力。我当然首先要致谢王石先生，没有他就没有这本书。我自己细想下来，没有王石先生一直的鞭策和鼓励，也没有我写作工作的今天。从2006年我开始从事专业写作工作以来，他给我创造了很多写作的机会，并且不吝自己诸多人生和学习的体会和感悟，一一传递予我。知遇之恩，感谢非常。
最后，我当然要把最大的感谢致予褚时健先生。不仅是因为他慷慨、坦率面对我的各种提问，更重要的是，在倾听他的故事的过程里，他繁盛的人生经历，他的强大生命力，他对生活、对事业的一片赤子之心，也丰富了我对自己人生的思考。</content></entry><entry><title>Hugo 内置的 Chroma 语法高亮</title><url>/post/syntax-highlighting.html</url><categories><category>示例</category></categories><tags><tag>语法</tag><tag>高亮</tag><tag>Chroma</tag></tags><content type="html"><![CDATA[Hugo 通过 Chroma 提供非常快速的语法高亮显示，现 Hugo 中使用 Chroma 作为代码块高亮支持，它内置在 Go 语言当中，速度是真的非常、非常快，而且最为重要的是它也兼容之前我们使用的 Pygments 方式。
以下通过 Hugo 内置短代码 highlight 和 Markdown 代码块方式分别验证不同语言的代码块渲染效果并能正确高亮显示，有关优化语法突出显示的更多信息，请参阅 Hugo 文档 。
编程语言 GO 199 200 201 202 203 204 205 206 207 208 func GetTitleFunc(style string) func(s string) string { switch strings.ToLower(style) { case &#34;go&#34;: return strings.Title case &#34;chicago&#34;: return transform.NewTitleConverter(transform.ChicagoStyle) default: return transform.NewTitleConverter(transform.APStyle) } } Java import javax.swing.JFrame; //Importing class JFrame import javax.swing.JLabel; //Importing class JLabel public class HelloWorld { public static void main(String[] args) { JFrame frame = new JFrame(); //Creating frame frame.setTitle(&#34;Hi!&#34;); //Setting title frame frame.add(new JLabel(&#34;Hello, world!&#34;));//Adding text to frame frame.pack(); //Setting size to smallest frame.setLocationRelativeTo(null); //Centering frame frame.setVisible(true); //Showing frame } } Python print &#34;Hello, world!&#34; Git 对比 *** /path/to/original &#39;&#39;timestamp&#39;&#39; --- /path/to/new &#39;&#39;timestamp&#39;&#39; *************** *** 1 **** ! This is a line. --- 1 --- ! This is a replacement line. It is important to spell -removed line +new line *** /path/to/original &#39;&#39;timestamp&#39;&#39; --- /path/to/new &#39;&#39;timestamp&#39;&#39; *************** *** 1 **** ! This is a line. --- 1 --- ! This is a replacement line. It is important to spell -removed line +new line 文件 Make 文件 CC=gcc CFLAGS=-I. hellomake: hellomake.o hellofunc.o $(CC) -o hellomake hellomake.o hellofunc.o -I. Markdown 文档 **bold** *italics* [link](www.example.com) 数据内容 JSON 数据 {&#34;employees&#34;:[ {&#34;firstName&#34;:&#34;John&#34;, &#34;lastName&#34;:&#34;Doe&#34;}, ]} XML 内容 &lt;employees&gt; &lt;employee&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt; SQL 查询 SELECT column_name,column_name FROM Table WHERE column_name = &#34;condition&#34; 除以上列举的代码高亮显示外，还支持诸如：C 语言、C++、HTML、CSS、Shell脚本等各主流的代码语言高亮显示，可自行测试效果。
]]></content></entry><entry><title>支持 Emoji 表情</title><url>/post/emoji-support.html</url><categories><category>示例</category></categories><tags><tag>表情</tag><tag>emoji</tag></tags><content type="html">Emoji 可以通过多种方式在 Hugo 项目中启用。
emojify 方法可以直接在模板中调用, 或者使用 行内 Shortcodes .
要全局使用 emoji, 需要在你的 网站配置 中设置 enableEmoji 为 true， 然后你就可以直接在文章中输入 emoji 的代码。
它们以冒号开头和结尾，并且包含 emoji 的 代码：
去露营啦! {:}tent: 很快就回来. 真开心! {:}joy: 呈现的输出效果如下:
去露营啦! ⛺ 很快就回来。
真开心! 😂
以下符号清单是 emoji 代码的非常有用的参考。
表情与情感 笑脸表情 图标 代码 图标 代码 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent 爱意表情 图标 代码 图标 代码 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes 吐舌头表情 图标 代码 图标 代码 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face 国家和地区旗帜 图标 代码 图标 代码 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe</content></entry><entry><title>Markdown 语法支持</title><url>/post/markdown-syntax.html</url><categories><category>示例</category></categories><tags><tag>Markdown</tag><tag>语法</tag></tags><content type="html"><![CDATA[仅以此篇文章来测试下在 NexT 主题中在通过 Hugo 引擎来建站时，是否支持 Markdown 文件内容中所写的各种语法，并展示下实际的效果。
标题样式 让我们从所有可能的标题开始，在 HTML 中 &lt;h1&gt;-&lt;h6&gt;元素分别表示六个不同级别的标题样式，其中 &lt;h1&gt; 为最大标题，&lt;h6&gt;为最小标题，效果如下：
标题 1 标题 2 标题 3 标题 4 标题 5 标题 6 段落格式 根据 W3C 定义的 HTML5 规范 ，HTML 文档由元素和文本组成。每个元素的组成都由一个 开始标记 表示，例如： &lt;body&gt; ，和 结束标记 表示，例如： &lt;/body&gt; 。（某些开始标记和结束标记在某些情况下可以省略，并由其他标记暗示。） 元素可以具有属性，这些属性控制元素的工作方式。例如：超链接是使用 a 元素及其 href 属性形成的。
Markdown 语法 ![图像说明](图像地址) HTML IMG 标签 &lt;img src=&#34;图像地址&#34; width=&#34;宽度&#34; height=&#34;高度&#34; /&gt; SVG 格式 &lt;svg&gt;xxxxxx&lt;/svg&gt; 列表类型 有序列表 第一个元素 第二个元素 第三个元素 无序列表 列表元素 另一个元素 和其它元素 嵌套列表 借助 HTML 的 ul 元素来实现。
第一项 第二项 第二项第一个子项目 第二项第二个子项目 第二项第二分项第一分项 第二项第二分项第二分项 第二项第二分项第三分项 第二项第三个子项目 第二项第三分项第一分项 第二项第三分项第二分项 第二项第三分项第三分项 第三项 自定义列表 通过 HTML 的 dl 元素还支持自定义列表（表格列表）。
Hugo 目录结构 assets config.toml content data theme static Hugo 模板 基础模板 列表模板 单页模板 块引用 blockquote 元素表示从另一个源引用的内容，可以选择引用必须在 footer 或 cite 元素中，也可以选择使用注释和缩写等行内更改。
引用文本 这一行也是同样的引用 同样你也在 blockquote 中使用 Markdown 语法书写
带有引文的 Blockquote 元素效果。
我的目标不是赚大钱,是为了制造好的电脑。当我意识到我可以永远当工程师时，我才创办了这家公司。
— 史蒂夫·沃兹尼亚克 根据 Mozilla 的网站记录，Firefox 1.0 于 2004 年发布，并取得了巨大成功。
表格 表格并不算是 Markdown 的核心要素，但 Hugo 同样支持它。
ID 创建者 模型 年份 1 Honda Accord 2009 2 Toyota Camry 2012 3 Hyundai Elantra 2010 可以使用 : （英文格式冒号）来对表格内容进行对齐。
表格 可以是 很酷 左对齐 居中 右对齐 左对齐 居中 右对齐 左对齐 居中 右对齐 同样也可以在表格中使用 Markdown 语法。
表格 中 使用 Markdown 语法 斜体 粗体 中划线 代码块 Code &lt;!DOCTYPE html&gt; &lt;html lang=&#34;en&#34;&gt; &lt;head&gt; &lt;meta charset=&#34;UTF-8&#34;&gt; &lt;title&gt;Example HTML5 Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Test&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; &lt;!DOCTYPE html&gt; &lt;html lang=&#34;en&#34;&gt; &lt;head&gt; &lt;meta charset=&#34;UTF-8&#34;&gt; &lt;title&gt;Example HTML5 Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Test&lt;/p&gt; &lt;/body&gt; &lt;/html&gt; 其它元素： abbr、sub、sup、kbd等等 GIF 是位图图像格式。
H2O
C6H12O6
Xn + Yn = Zn
按X获胜。或按CTRL+ALT+F显示 FPS 计数器。
比特作为信息论中的信息单位，也被称为 shannon ，以信息论领域的创始人 Claude shannon 的名字命名。
参考：
来自 Mainroad 主题的 Basic Elements 内容 ]]></content></entry><entry><title>agree</title><url>/post/idffafds/</url><categories><category>博客</category></categories><tags/><content type="html"><![CDATA[1 图像处理中的坐标系，水平向右为x轴正方向，竖直向下为y轴正方向。
安装OpenCV-Python,
pip install opencv-python==3.4.2.17 要利用SIFT和SURF等进行特征提取时，还需要安装：
pip install opencv-contrib-python==3.4.2.17 core、highgui、imgproc是最基础的模块，该课程主要是围绕这几个模块展开的，分别介绍如下：
core模块实现了最核心的数据结构及其基本运算，如绘图函数、数组操作相关函数等。 highgui模块实现了视频与图像的读取、显示、存储等接口。 imgproc模块实现了图像处理的基础方法，包括图像滤波、图像的几何变换、平滑、阈值分割、形态学处理、边缘检测、目标检测、运动分析和对象跟踪等。 对于图像处理其他更高层次的方向及应用，OpenCV也有相关的模块实现
features2d模块用于提取图像特征以及特征匹配，nonfree模块实现了一些专利算法，如sift特征。 objdetect模块实现了一些目标检测的功能，经典的基于Haar、LBP特征的人脸检测，基于HOG的行人、汽车等目标检测，分类器使用Cascade Classification（级联分类）和Latent SVM等。 stitching模块实现了图像拼接功能。 FLANN模块（Fast Library for Approximate Nearest Neighbors），包含快速近似最近邻搜索FLANN 和聚类Clustering算法。 ml模块机器学习模块（SVM，决策树，Boosting等等）。 photo模块包含图像修复和图像去噪两部分。 video模块针对视频处理，如背景分离，前景检测、对象跟踪等。 calib3d模块即Calibration（校准）3D，这个模块主要是相机校准和三维重建相关的内容。包含了基本的多视角几何算法，单个立体摄像头标定，物体姿态估计，立体相似性算法，3D信息的重建等等。 G-API模块包含超高效的图像处理pipeline引擎 opencv 的接口使用BGR模式，而 matplotlib.pyplot 接口使用的是RGB模式
b, g, r = cv2.split(srcImage) srcImage_new = cv2.merge([r, g, b]) plt.imshow(srcImage_new) # 通道变换之后对灰度图进行输出的图片颜色仍然为绿色,这是因为我们还是直接使用plt显示图像，它默认使用三通道显示图像 grayImage = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY) # 灰度变换 plt.imshow(grayImage, cmap=&#34;gray&#34;) 图像处理 二值化 # 全局阈值 thresh, dst = cv2.threshold(src, thresh, maxVal, type) type: cv2.THRESH_BINARY 大于阈值的为maxVal,小于的为0 cv2.THRESH_BINARY_INV # 自适应阈值 dst = cv2.adaptiveThreshold(src, maxVal, adaptiveMethod, type, blockSize, C) #type:cv2.THRESH_BINARY #adapttiveMethod:cv2.ADAPTIVE_THRESH_MEAN_C #thresh=blockSize*blockSize矩阵平均值灰度-C，大于thresh的为maxValue 寻找轮廓 contours, hierarchy = cv2.findContours(image, mode, method) # 轮廓检索模式 # Cv2.RETR_EXTERNAL检测外轮廓 # Cv2.RETR_TREE等级树结构的轮廓 # 轮廓近似方法 # Cv2.CHAIN_APPROX_NONE所有点 # Cv2.CHAIN_APPROX_SIMPLE直线两端点 # contours：list结构，列表中每个元素代表一个边沿信息。每个元素是(x,1,2)的三维向量，x表示该条边沿里共有多少个像素点，第三维的那个“2”表示每个点的横、纵坐标； # 注意：如果输入选择cv2.CHAIN_APPROX_SIMPLE，则contours中一个list元素所包含的x点之间应该用直线连接起来，这个可以用cv2.drawContours()函数观察一下效果。 # hierarchy：返回类型是(x,4)的二维ndarray。x和contours里的x是一样的意思。如果输入选择cv2.RETR_TREE，则以树形结构组织输出，hierarchy的四列分别对应下一个轮廓编号、上一个轮廓编号、父轮廓编号、子轮廓编号，该值为负数表示没有对应项。 iamge = cv2.drawContours(image, contours, i, color, thickness) # i：列表中第几个轮廓，-1所有；color：绘制颜色；thickness：线条粗细，-1填充 x, y, w, h = cv2.boundingRect(contours) 用一个最小的矩形，把找到的形状包起来。 x，y是矩阵左上点的坐标，w，h是矩阵的宽和高 几何变换 # 读取图像 img = cv.imread(&#39;messi5.jpg&#39;,0) 参数：要读取的图像；读取方式的标志 cv.IMREAD*COLOR：以彩色模式加载图像，任何图像的透明度都将被忽略。这是默认参数。 cv.IMREAD*GRAYSCALE：以灰度模式加载图像 cv.IMREAD_UNCHANGED：包括alpha通道的加载图像模式。 可以使用1、0或者-1来替代上面三个标志 # 显示图像 # opencv中显示 cv.imshow(&#39;image&#39;,img) cv.waitKey(0) # matplotlib中展示 plt.imshow(img[:,:,::-1]) 参数：显示图像的窗口名称，以字符串类型表示，要加载的图像 注意：在调用显示图像的API后，要调用cv.waitKey()给图像绘留下时间，否则窗口会出现无响应情况，并且图像无法显示出来 # 保存图像 cv.imwrite(&#39;messigray.png&#39;,img) 参数：文件名，要保存在哪里；要保存的图像 # 向图像中添加文字 cv.putText(img,text,station, font, fontsize,color,thickness,cv.LINE_AA) 参数： img: 图像 text：要写入的文本数据 station：文本的放置位置 font：字体 Fontsize :字体大小 通过行和列的坐标值获取该像素点的像素值。对于BGR图像，它返回一个蓝，绿，红值的数组。对于灰度图像，仅返回相应的强度值。使用相同的方法对像素值进行修改。 img = cv.imread(&#39;messi5.jpg&#39;) # 获取某个像素点的值 px = img[100,100] # 仅获取蓝色通道的强度值 blue = img[100,100,0] # 修改某个位置的像素值 img[100,100] = [255,255,255] # 通道拆分 b,g,r = cv.split(img) # 通道合并 img = cv.merge((b,g,r)) # 色彩空间的改变 cv.cvtColor(image，flag) cv.COLOR_BGR2GRAY : BGR↔Gray cv.COLOR_BGR2HSV: BGR→HSV # 图像的加法 OpenCV加法和Numpy加法之间存在差异。OpenCV的加法是饱和操作，而Numpy添加是模运算。 尽量使用 OpenCV 中的函数。 &gt;&gt;&gt; x = np.uint8([250]) &gt;&gt;&gt; y = np.uint8([10]) &gt;&gt;&gt; print( cv.add(x,y) ) # 250+10 = 260 =&gt; 255 &gt;&gt;&gt; print( x+y ) # 250+10 = 260 % 256 = 4 # 图像的混合 这其实也是加法，但是不同的是两幅图像的权重不同，这就会给人一种混合或者透明的感觉。图像混合的计算公式如下：dst = α⋅img1 + β⋅img2 + γ img3 = cv.addWeighted(img1,α,img2,β,γ) 图像旋转 仿射变换 变换前后满足平直性（变换前是直线变换后还是直线）和平行性（变换前平行的线变换后依旧平行）
透射变换 图像金字塔 # 图像缩放 cv2.resize(src,dsize,fx=0,fy=0,interpolation=cv2.INTER_LINEAR) src : 输入图像 dsize: 绝对尺寸，直接指定调整后图像的大小 (2*cols,2*rows) fx,fy: 相对尺寸，将dsize设置为None，(img1,None,fx=0.5,fy=0.5) interpolation：插值方法， cv2.INTER_LINEAR 双线性插值法 cv2.INTER_NEAREST 最临近插值 cv2.INTER_AREA 像素区域重采样{默认} cv2.INTER_CUBIC 双三次插值 # 图像平移 M = np.float32([[1,0,100],[0,1,50]])# 将图像的像素点移动(50,100)的距离： dst = cv.warpAffine(img1,M,dsize=(cols,rows)，borderValue=(0,0,0)) img: 输入图像 M： 2*∗3移动矩阵 dsize: 输出图像的大小，它应该是(宽度，高度)的形式。请记住,width=列数，height=行数。 borderValue为边界填充颜色（注意是BGR顺序，( 0 , 0 , 0 ) (0,0,0)(0,0,0)代表黑色）: # 图像旋转 旋转中图像仍保持这原始尺寸。图像旋转后图像的水平对称轴、垂直对称轴及中心坐标原点都可能会发生变换，因此需要对图像旋转中的坐标进行相应转换。 # 生成旋转矩阵 M = cv.getRotationMatrix2D((cols/2,rows/2),90,1) # center：旋转中心；angle：旋转角度；scale：缩放比例 # 进行旋转变换 dst = cv.warpAffine(img,M,(cols,rows)) # 仿射变换 涉及到图像的形状位置角度的变化，是深度学习预处理中常到的功能,仿射变换主要是对图像的缩放，旋转，翻转和平移等操作的组合。 pts1 = np.float32([[50,50],[200,50],[50,200]])# 2.1 创建变换矩阵 pts2 = np.float32([[100,100],[200,50],[100,250]]) M = cv.getAffineTransform(pts1,pts2) dst = cv.warpAffine(img,M,(cols,rows))# 2.2 完成仿射变换 # 透射变换 pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]]) # 2.1 创建变换矩阵 pts2 = np.float32([[100,145],[300,100],[80,290],[310,300]]) T = cv.getPerspectiveTransform(pts1,pts2) # 2.2 进行变换 dst = cv.warpPerspective(img,T,(cols,rows)) # 图像金字塔 cv.pyrUp(img) #对图像进行上采样 cv.pyrDown(img) #对图像进行下采样 形态学操作 连通性 腐蚀、膨胀 开闭运算
腐蚀、开 消灭噪音
膨胀、闭 填补空洞
礼帽和黑帽 礼帽：噪音提取
黑帽：空洞提取
# 腐蚀、膨胀 cv.erode(img,kernel,iterations) cv.dilate(img,kernel,iterations) # 开闭运算# 礼帽和黑帽 kernel = np.ones((10, 10), np.uint8)# 2 创建核结构 cvOpen = cv.morphologyEx(img1,cv.MORPH_OPEN,kernel) # 开运算 cvClose = cv.morphologyEx(img2,cv.MORPH_CLOSE,kernel)# 闭运算 cvOpen = cv.morphologyEx(img1,cv.MORPH_TOPHAT,kernel) # 礼帽运算 cvClose = cv.morphologyEx(img2,cv.MORPH_BLACKHAT,kernel)# 黑帽运算 图像平滑 图像噪声 椒盐噪声也称为脉冲噪声，是图像中经常见到的一种噪声，它是一种随机出现的白点或者黑点，可能是亮的区域有黑色像素或是在暗的区域有白色像素（或是两者皆有）。椒盐噪声的成因可能是影像讯号受到突如其来的强烈干扰而产生、类比数位转换器或位元传输错误等。例如失效的感应器导致像素值为最小值，饱和的感应器导致像素值为最大值。
高斯噪声是指噪声密度函数服从高斯分布的一类噪声。由于高斯噪声在空间和频域中数学上的易处理性，这种噪声(也称为正态噪声)模型经常被用于实践中。高斯随机变量z的概率密度函数由下式给出：
图像平滑从信号处理的角度看就是去除其中的高频信息，保留低频信息。因此我们可以对图像实施低通滤波。低通滤波可以去除图像中的噪声，对图像进行平滑。
均值滤波的优点是算法简单，计算速度较快，缺点是在去噪的同时去除了很多细节部分，将图像变得模糊。
高斯平滑在从图像中去除高斯噪声方面非常有效。
正态分布是一种钟形曲线，越接近中心，取值越大，越远离中心，取值越小。计算平滑结果时，只需要将&quot;中心点&quot;作为原点，其他点按照其在正态曲线上的位置，分配权重，就可以得到一个加权平均值。
中值滤波对椒盐噪声（salt-and-pepper noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值。是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值。
# 均值滤波 cv.blur(src, ksize, anchor, borderType) src：输入图像 ksize：卷积核的大小 anchor：默认值 (-1,-1) ，表示核中心 borderType：边界类型 # 高斯滤波 cv2.GaussianBlur(src,ksize,sigmaX,sigmay,borderType) src: 输入图像 ksize:高斯卷积核的大小，注意 ： 卷积核的宽度和高度都应为奇数，且可以不同 sigmaX: 水平方向的标准差 sigmaY: 垂直方向的标准差，默认值为0，表示与sigmaX相同 borderType:填充边界类型 # 中值滤波 cv.medianBlur(src, ksize ) src：输入图像 ksize：卷积核的大小 直方图 图像直方图（Image Histogram）是用以表示数字图像中亮度分布的直方图，标绘了图像中每个亮度值的像素个数。这种直方图中，横坐标的左侧为较暗的区域，而右侧为较亮的区域。因此一张较暗图片的直方图中的数据多集中于左侧和中间部分，而整体明亮、只有少量阴影的图像则相反。
“直方图均衡化”是把原始图像的灰度直方图从比较集中的某个灰度区间变成在更广泛灰度范围内的分布。直方图均衡化就是对图像进行非线性拉伸，重新分配图像像素值，使一定灰度范围内的像素数量大致相同。
这种方法提高图像整体的对比度，特别是有用数据的像素值分布比较接近时，在X光图像中使用广泛，可以提高骨架结构的显示，另外在曝光过度或不足的图像中可以更好的突出细节。
上述的直方图均衡，我们考虑的是图像的全局对比度。 的确在进行完直方图均衡化之后，图片背景的对比度被改变了，在猫腿这里太暗，我们丢失了很多信息，所以在许多情况下，这样做的效果并不好。
需要使用自适应的直方图均衡化
整幅图像会被分成很多小块，这些小块被称为“tiles”（在 OpenCV 中 tiles 的 大小默认是 8x8），然后再对每一个小块分别进行直方图均衡化。 所以在每一个的区域中， 直方图会集中在某一个小的区域中）。如果有噪声的话，噪声会被放大。为了避免这种情况的出现要使用对比度限制。对于每个小块来说，**如果直方图中的 bin 超过对比度的上限的话，就把其中的像素点均匀分散到其他 bins 中，然后在进行直方图均衡化。**最后，为了 去除每一个小块之间的边界，再使用双线性差值，对每一小块进行拼接。
# 直方图 cv2.calcHist(images,channels,mask,histSize,ranges[,hist[,accumulate]]) images: 原图像。当传入函数时应该用中括号 [] 括起来，例如：[img]。 channels: 如果输入图像是灰度图，它的值就是 [0]；如果是彩色图像的话，传入的参数可以是 [0]，[1]，[2] 它们分别对应着通道 B，G，R。 mask: 掩模图像。要统计整幅图像的直方图就把它设为 None。但是如果你想统计图像某一部分的直方图的话，你就需要制作一个掩模图像，并使用它。（后边有例子） histSize:BIN 的数目。也应该用中括号括起来，例如：[256]。 ranges: 像素值范围，通常为 [0，256] mask = np.zeros(img.shape[:2], np.uint8)# 2. 创建蒙版 mask[400:650, 200:500] = 255 # 查找直方图的区域上创建一个白色的掩膜图像，否则创建黑色 masked_img = cv.bitwise_and(img,img,mask = mask)# 3.掩模 mask_histr = cv.calcHist([img],[0],mask,[256],[1,256]) # 4. 统计掩膜后图像的灰度图 # 直方图均衡化 dst = cv.equalizeHist(img) # 自适应的直方图均衡化 cv.createCLAHE(clipLimit, tileGridSize) clipLimit: 对比度限制，默认是40 tileGridSize: 分块的大小，默认为8*88∗8 边缘检测 图像边缘检测大幅度地减少了数据量，并且剔除了可以认为不相关的信息，保留了图像重要的结构属性。有许多方法用于边缘检测，它们的绝大部分可以划分为两类：基于搜索和基于零穿越。
Sobel边缘检测算法比较简单，实际应用中效率比canny边缘检测效率要高，但是边缘不如Canny检测的准确，但是很多实际应用的场合，sobel边缘却是首选，Sobel算子是高斯平滑与微分操作的结合体，所以其抗噪声能力很强，用途较多。尤其是效率要求较高，而对细纹理不太关心的时候。
Laplacian是利用二阶导数来检测边缘 。
Canny 边缘检测算法被认为是最优的边缘检测算法。
# sobel边缘检测 Sobel_x_or_y = cv2.Sobel(src, ddepth, dx, dy, dst, ksize, scale, delta, borderType) src：传入的图像 ddepth: 图像的深度 dx和dy: 指求导的阶数，0表示这个方向上没有求导，取值为0、1。 ksize: 是Sobel算子的大小，即卷积核的大小，必须为奇数1、3、5、7，默认为3。注意：如果ksize=-1，就演变成为3x3的Scharr算子。 scale：缩放导数的比例常数，默认情况为没有伸缩系数。 borderType：图像边界的模式，默认值为cv2.BORDER_DEFAULT。 Sobel函数求完导数后会有负值，还有会大于255的值。而原图像是uint8，即8位无符号数，所以Sobel建立的图像位数不够，会有截断。因此要使用16位有符号的数据类型，即cv2.CV_16S。处理完图像后，再使用cv2.convertScaleAbs()函数将其转回原来的uint8格式，否则图像无法显示。Sobel算子是在两个方向计算的，最后还需要用cv2.addWeighted( )函数将其组合起来 x = cv.Sobel(img, cv.CV_16S, 1, 0)# 2 计算Sobel卷积结果 y = cv.Sobel(img, cv.CV_16S, 0, 1) Scale_absX = cv.convertScaleAbs(x) # convert 转换 scale 缩放 Scale_absY = cv.convertScaleAbs(y) result = cv.addWeighted(Scale_absX, 0.5, Scale_absY, 0.5, 0)# 4 结果合成 # laplacian算子 cv2.Laplacian(src, ddepth[, dst[, ksize[, scale[, delta[, borderType]]]]]) Src: 需要处理的图像， Ddepth: 图像的深度，-1表示采用的是原图像相同的深度，目标图像的深度必须大于等于原图像的深度； ksize：算子的大小，即卷积核的大小，必须为1,3,5,7。 result = cv.Laplacian(img,cv.CV_16S) Scale_abs = cv.convertScaleAbs(result) # canny检测 canny = cv2.Canny(image, threshold1, threshold2) image:灰度图， threshold1: minval，较小的阈值将间断的边缘连接起来 threshold2: maxval，较大的阈值检测图像中明显的边缘 lowThreshold = 0 max_lowThreshold = 100 canny = cv.Canny(img, lowThreshold, max_lowThreshold) 模板匹配和霍夫变换 模板匹配，就是在给定的图片中查找和模板最相似的区域，该算法的输入包括模板和图片，整个任务的思路就是按照滑窗的思路不断的移动模板图片，计算其与图像中对应区域的匹配度，最终将匹配度最高的区域选择为最终的结果。
霍夫变换常用来提取图像中的直线和圆等几何形状
res = cv.matchTemplate(img,template,method) img: 要进行模板匹配的图像 Template ：模板 method：实现模板匹配的算法，主要有： 平方差匹配(CV_TM_SQDIFF)：利用模板与图像之间的平方差进行匹配，最好的匹配是0，匹配越差，匹配的值越大。 相关匹配(CV_TM_CCORR)：利用模板与图像间的乘法进行匹配，数值越大表示匹配程度较高，越小表示匹配效果差。 利用相关系数匹配(CV_TM_CCOEFF)：利用模板与图像间的相关系数匹配，1表示完美的匹配，-1表示最差的匹配。 完成匹配后，使用cv.minMaxLoc()方法查找最大值所在的位置即可。如果使用平方差作为比较方法，则最小值位置是最佳匹配位置。 res = cv.matchTemplate(img, template, cv.TM_CCORR)# 2.1 模板匹配 min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)# 2.2 返回图像中最匹配的位置，确定左上角的坐标，并将匹配位置绘制在图像上 # top_left = min_loc# 使用平方差时最小值为最佳匹配位置 top_left = max_loc bottom_right = (top_left[0] + w, top_left[1] + h) cv.rectangle(img, top_left, bottom_right, (0,255,0), 2) # 霍夫线检测 cv.HoughLines(img, rho, theta, threshold) img: 检测的图像，要求是二值化的图像，所以在调用霍夫变换之前首先要进行二值化，或者进行Canny边缘检测 rho、theta: \rhoρ 和\thetaθ的精确度 threshold: 阈值，只有累加器中的值高于该阈值时才被认为是直线。 img = cv.imread(&#39;./image/rili.jpg&#39;)# 1.加载图片，转为二值图 gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) edges = cv.Canny(gray, 50, 150) lines = cv.HoughLines(edges, 0.8, np.pi / 180, 150)# 2.霍夫直线变换 for line in lines:# 3.将检测的线绘制在图像上（注意是极坐标噢） rho, theta = line[0] a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho x1 = int(x0 + 1000 * (-b)) y1 = int(y0 + 1000 * (a)) x2 = int(x0 - 1000 * (-b)) y2 = int(y0 - 1000 * (a)) cv.line(img, (x1, y1), (x2, y2), (0, 255, 0)) plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;霍夫变换线检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() 图像特征提取和描述 模板匹配不适用于尺度变换，视角变换后的图像，这时我们就要使用关键点匹配算法，比较经典的关键点检测算法包括SIFT和SURF等，主要的思路是首先通过关键点检测算法获取模板和测试图片中的关键点；然后使用关键点匹配算法处理即可，这些关键点可以很好的处理尺度变化、视角变换、旋转变化、光照变化等，具有很好的不变性。
角点特征 在角点的地方，无论你向哪个方向移动小图，结果都会有很大的不同。所以可以把它们当 成一个好的特征。
Harris和Shi-Tomas算法 Harris
优点：
旋转不变性，椭圆转过一定角度但是其形状保持不变（特征值保持不变） 对于图像灰度的仿射变化具有部分的不变性，由于仅仅使用了图像的一介导数，对于图像灰度平移变化不变；对于图像灰度尺度变化不变 缺点：
对尺度很敏感，不具备几何尺度不变性。 提取的角点是像素级的 Shi-Tomasi
对Harris算法的改进，能够更好地检测角点
#Hariis检测使用的API是： dst=cv.cornerHarris(src, blockSize, ksize, k) img：数据类型为 ﬂoat32 的输入图像。 blockSize：角点检测中要考虑的邻域大小。 ksize：sobel求导使用的核大小 k ：角点检测方程中的自由参数，取值参数为 [0.04，0.06]. # 1 读取图像，并转换成灰度图像 img = cv.imread(&#39;./image/chessboard.jpg&#39;) gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY) # 2 角点检测 # 2.1 输入图像必须是 float32 gray = np.float32(gray) # 2.2 最后一个参数在 0.04 到 0.05 之间 dst = cv.cornerHarris(gray,2,3,0.04) # 3 设置阈值，将角点绘制出来，阈值根据图像进行选择 img[dst&gt;0.001*dst.max()] = [0,0,255] # 4 图像显示 plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;Harris角点检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() # Shi-Tomasi corners = cv2.goodFeaturesToTrack ( image, maxcorners, qualityLevel, minDistance ) Image: 输入灰度图像 maxCorners : 获取角点数的数目。 qualityLevel：该参数指出最低可接受的角点质量水平，在0-1之间。 minDistance：角点之间最小的欧式距离，避免得到相邻特征点。 返回： Corners: 搜索到的角点，在这里所有低于质量水平的角点被排除掉，然后把合格的角点按质量排序，然后将质量较好的角点附近（小于最小欧式距离）的角点删掉，最后找到maxCorners个角点返回。 import numpy as np import cv2 as cv import matplotlib.pyplot as plt # 1 读取图像 img = cv.imread(&#39;./image/tv.jpg&#39;) gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2 角点检测 corners = cv.goodFeaturesToTrack(gray,1000,0.01,10) # 3 绘制角点 for i in corners: x,y = i.ravel() cv.circle(img,(x,y),2,(0,0,255),-1) # 4 图像展示 plt.figure(figsize=(10,8),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;shi-tomasi角点检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() SIFT/SURF算法 Harris和Shi-Tomasi角点检测算法，这两种算法具有旋转不变性，但不具有尺度不变性
SIFT算法的实质是在不同的尺度空间上查找关键点(特征点)，并计算出关键点的方向。SIFT所查找到的关键点是一些十分突出，不会因光照，仿射变换和噪音等因素而变化的点，如角点、边缘点、暗区的亮点及亮区的暗点，但并不完美，仍然存在实时性不高，有时特征点较少，对边缘光滑的目标无法准确提取特征点等缺陷
SIFT原理：
尺度空间极值检测：构建高斯金字塔，高斯差分金字塔，检测极值点。 关键点定位：去除对比度较小和边缘对极值点的影响。 关键点方向确定：利用梯度直方图确定关键点的方向。 关键点描述：对关键点周围图像区域分块，计算块内的梯度直方图，生成具有特征向量，对关键点信息进行描述。 使用 SIFT 算法进行关键点检测和描述的执行速度比较慢， 需要速度更快的算法。 2006 年 Bay提出了 SURF 算法，是SIFT算法的增强版，它的计算量小，运算速度快，提取的特征与SIFT几乎相同，将其与SIFT算法对比如下：
# 实例化sift sift = cv.xfeatures2d.SIFT_create() # 利用sift.detectAndCompute()检测关键点并计算 kp,des = sift.detectAndCompute(gray,None) gray: 进行关键点检测的图像，注意是灰度图像 返回： kp: 关键点信息，包括位置，尺度，方向信息 des: 关键点描述符，每个关键点对应128个梯度信息的特征向量 # 将关键点检测结果绘制在图像上 cv.drawKeypoints(image, keypoints, outputimage, color, flags) image: 原始图像 keypoints：关键点信息，将其绘制在图像上 outputimage：输出图片，可以是原始图像 color：颜色设置，通过修改（b,g,r）的值,更改画笔的颜色，b=蓝色，g=绿色，r=红色。 flags：绘图功能的标识设置 cv2.DRAW_MATCHES_FLAGS_DEFAULT：创建输出图像矩阵，使用现存的输出图像绘制匹配对和特征点，对每一个关键点只绘制中间点 cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG：不创建输出图像矩阵，而是在输出图像上绘制匹配对 cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS：对每一个特征点绘制带大小和方向的关键点图形 cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS：单点的特征点不被绘制 # 1 读取图像 img = cv.imread(&#39;./image/tv.jpg&#39;) gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2 sift关键点检测 # 2.1 实例化sift对象 sift = cv.xfeatures2d.SIFT_create() # 2.2 关键点检测：kp关键点信息包括方向，尺度，位置信息，des是关键点的描述符 kp,des=sift.detectAndCompute(gray,None) # 2.3 在图像上绘制关键点的检测结果 cv.drawKeypoints(img,kp,img,flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS) # 3 图像显示 plt.figure(figsize=(8,6),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;sift检测&#39;) plt.xticks([]), plt.yticks([]) plt.show() 视频操作 import numpy as np import cv2 as cv # 1.获取视频对象 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 获取视频属性 # retval = cap.get(propId) #0.cv2.CAP_PROP POS MSEC视频文件的当前位置(ms) #1.cv2.CAP_PROP POS FRAMES从0开始索引帧，帧位置 #2.cv2.CAP_PROP_POS AVI RATIO视频文件的相对位置(0表示开始，1表示结束) #3.cv2.CAP_PROP FRAME WIDTH视频流的帧宽度 #4.cv2.CAP PROP FRAME HEIGHT视频流的帧高度 #5.cv2.CAP PROP FPS帧率 #6.cv2.CAP PROP FOURCC编解码器四字符代码 #7.cv2.CAP PROP FRAME COUNT视频文件的帧 # 修改视频的属性信息 # cap.set(propId，value) # 2.判断是否读取成功 while(cap.isOpened()): # 3.获取每一帧图像 ret, frame = cap.read() #ret: 若获取成功返回True，获取失败，返回False #Frame: 获取到的某一帧的图像 # 4. 获取成功显示图像 if ret == True: cv.imshow(&#39;frame&#39;,frame) # 5.每一帧间隔为25ms if cv.waitKey(25) &amp; 0xFF == ord(&#39;q&#39;): break # 6.释放视频对象 cap.release() cv.destoryAllwindows() # 1. 读取视频 cap = cv.VideoCapture(&#34;DOG.wmv&#34;) # 3. 创建保存视频的对象，设置编码格式，帧率，图像的宽高等 out = cv.VideoWriter(&#39;outpy.avi&#39;,cv.VideoWriter_fourcc(&#39;M&#39;,&#39;J&#39;,&#39;P&#39;,&#39;G&#39;), 10, (frame_width,frame_height)) #ilename：视频保存的位置 #fourcc：指定视频编解码器的4字节代码cv2.VideoWriter_fourcc( c1, c2, c3, c4 ) c1,c2,c3,c4: 是视频编解码器的4字节代码，在fourcc.org中找到可用代码列表，与平台紧密相关 #fps：帧率 #frameSize：帧大小 while(True): # 4.获取视频中的每一帧图像 ret, frame = cap.read() if ret == True: # 5.将每一帧图像写入到输出文件中 out.write(frame) else: break # 6.释放资源 cap.release() out.release() cv.destroyAllWindows() 视频追踪 图像是一个矩阵信息，如何在一个视频当中使用meanshift算法来追踪一个运动的物体呢？ 大致流程如下：
首先在图像上选定一个目标区域
计算选定区域的直方图分布，一般是HSV色彩空间的直方图。
对下一帧图像b同样计算直方图分布。
计算图像b当中与选定区域直方图分布最为相似的区域，使用meanshift算法将选定区域沿着最为相似的部分进行移动，直到找到最相似的区域，便完成了在图像b中的目标追踪。
重复3到4的过程，就完成整个视频目标追踪。
通常情况下我们使用直方图反向投影得到的图像和第一帧目标对象的起始位置，当目标对象的移动会反映到直方图反向投影图中，meanshift 算法就把我们的窗口移动到反向投影图像中灰度密度最大的区域了。
直方图反向投影的流程是：
假设我们有一张100x100的输入图像，有一张10x10的模板图像，查找的过程是这样的：
从输入图像的左上角(0,0)开始，切割一块(0,0)至(10,10)的临时图像； 生成临时图像的直方图； 用临时图像的直方图和模板图像的直方图对比，对比结果记为c； 直方图对比结果c，就是结果图像(0,0)处的像素值； 切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像； 重复1～5步直到输入图像的右下角，就形成了直方图的反向投影。 import numpy as np import cv2 as cv # 1.获取图像 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 2.获取第一帧图像，并指定目标位置 ret,frame = cap.read() # 2.1 目标位置（行，高，列，宽） r,h,c,w = 197,141,0,208 track_window = (c,r,w,h) # 2.2 指定目标的感兴趣区域 roi = frame[r:r+h, c:c+w] # 3. 计算直方图 # 3.1 转换色彩空间（HSV） hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV) # 3.2 去除低亮度的值 # mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.))) # 3.3 计算直方图 roi_hist = cv.calcHist([hsv_roi],[0],None,[180],[0,180]) # 3.4 归一化 cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX) # 4. 目标追踪 # 4.1 设置窗口搜索终止条件：最大迭代次数，窗口中心漂移最小值 term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 ) while(True): # 4.2 获取每一帧图像 ret ,frame = cap.read() if ret == True: # 4.3 计算直方图的反向投影 hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # 4.4 进行meanshift追踪 ret, track_window = cv.meanShift(dst, track_window, term_crit) # 4.5 将追踪的位置绘制在视频上，并进行显示 x,y,w,h = track_window img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow(&#39;frame&#39;,img2) if cv.waitKey(60) &amp; 0xFF == ord(&#39;q&#39;): break else: break # 5. 资源释放 cap.release() cv.destroyAllWindows() 视频追踪 meanshift算法除了应用在视频追踪当中，在聚类，平滑等等各种涉及到数据以及非监督学习的场合当中均有重要应用，是一个应用广泛的算法。
图像是一个矩阵信息，如何在一个视频当中使用meanshift算法来追踪一个运动的物体呢？ 大致流程如下：
首先在图像上选定一个目标区域
计算选定区域的直方图分布，一般是HSV色彩空间的直方图。
对下一帧图像b同样计算直方图分布。
计算图像b当中与选定区域直方图分布最为相似的区域，使用meanshift算法将选定区域沿着最为相似的部分进行移动，直到找到最相似的区域，便完成了在图像b中的目标追踪。
重复3到4的过程，就完成整个视频目标追踪。
通常情况下我们使用直方图反向投影得到的图像和第一帧目标对象的起始位置，当目标对象的移动会反映到直方图反向投影图中，meanshift 算法就把我们的窗口移动到反向投影图像中灰度密度最大的区域了。
直方图反向投影的流程是：
假设我们有一张100x100的输入图像，有一张10x10的模板图像，查找的过程是这样的：
从输入图像的左上角(0,0)开始，切割一块(0,0)至(10,10)的临时图像； 生成临时图像的直方图； 用临时图像的直方图和模板图像的直方图对比，对比结果记为c； 直方图对比结果c，就是结果图像(0,0)处的像素值； 切割输入图像从(0,1)至(10,11)的临时图像，对比直方图，并记录到结果图像； 重复1～5步直到输入图像的右下角，就形成了直方图的反向投影。 # Meanshift的API是： cv.meanShift(probImage, window, criteria) probImage: ROI区域，即目标的直方图的反向投影 window： 初始搜索窗口，就是定义ROI的rect criteria: 确定窗口搜索停止的准则，主要有迭代次数达到设置的最大值，窗口中心的漂移值大于某个设定的限值等。 import numpy as np import cv2 as cv # 1.获取图像 cap = cv.VideoCapture(&#39;DOG.wmv&#39;) # 2.获取第一帧图像，并指定目标位置 ret,frame = cap.read() # 2.1 目标位置（行，高，列，宽） r,h,c,w = 197,141,0,208 track_window = (c,r,w,h) # 2.2 指定目标的感兴趣区域 roi = frame[r:r+h, c:c+w] # 3. 计算直方图 # 3.1 转换色彩空间（HSV） hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV) # 3.2 去除低亮度的值 # mask = cv.inRange(hsv_roi, np.array((0., 60.,32.)), np.array((180.,255.,255.))) # 3.3 计算直方图 roi_hist = cv.calcHist([hsv_roi],[0],None,[180],[0,180]) # 3.4 归一化 cv.normalize(roi_hist,roi_hist,0,255,cv.NORM_MINMAX) # 4. 目标追踪 # 4.1 设置窗口搜索终止条件：最大迭代次数，窗口中心漂移最小值 term_crit = ( cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1 ) while(True): # 4.2 获取每一帧图像 ret ,frame = cap.read() if ret == True: # 4.3 计算直方图的反向投影 hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) dst = cv.calcBackProject([hsv],[0],roi_hist,[0,180],1) # 4.4 进行meanshift追踪 ret, track_window = cv.meanShift(dst, track_window, term_crit) # 4.5 将追踪的位置绘制在视频上，并进行显示 x,y,w,h = track_window img2 = cv.rectangle(frame, (x,y), (x+w,y+h), 255,2) cv.imshow(&#39;frame&#39;,img2) if cv.waitKey(60) &amp; 0xFF == ord(&#39;q&#39;): # cv2.waitKey(1)在有按键按下的时候返回按键的ASCII值，否则返回-1 # &amp; 0xFF的按位与操作只取cv2.waitKey(1)返回值最后八位，因为有些系统cv2.waitKey(1)的返回值不止八位 # ord(‘q’)表示q的ASCII值 # 总体效果：按下q键后break else: break # 5. 资源释放 cap.release() cv.destroyAllWindows() meanshift检测的窗口的大小是固定的，而狗狗由近及远是一个逐渐变小的过程，固定的窗口是不合适的。所以我们需要根据目标的大小和角度来对窗口的大小和角度进行修正。CamShift可以帮我们解决这个问题。
CamShift算法全称是“Continuously Adaptive Mean-Shift”（连续自适应MeanShift算法），是对MeanShift算法的改进算法，可随着跟踪目标的大小变化实时调整搜索窗口的大小，具有较好的跟踪效果。
Camshift算法首先应用meanshift，一旦meanshift收敛，它就会更新窗口的大小，还计算最佳拟合椭圆的方向，从而根据目标的位置和大小更新搜索窗口。
#进行camshift追踪 ret, track_window = cv.CamShift(dst, track_window, term_crit) # 绘制追踪结果 pts = cv.boxPoints(ret) pts = np.int0(pts) img2 = cv.polylines(frame,[pts],True, 255,2) 人脸识别 Haar 特征会被使用，就像我们的卷积核，每一个特征是一 个值，这个值等于黑色矩形中的像素值之后减去白色矩形中的像素值之和。
Haar特征值反映了图像的灰度变化情况。例如：脸部的一些特征能由矩形特征简单的描述，眼睛要比脸颊颜色要深，鼻梁两侧比鼻梁颜色要深，嘴巴比周围颜色要深等。
Haar特征可用于于图像任意位置，大小也可以任意改变，所以矩形特征值是矩形模版类别、矩形位置和矩形大小这三个因素的函数。故类别、大小和位置的变化，使得很小的检测窗口含有非常多的矩形特征。
# 训练好的检测器，包括面部，眼睛，猫脸等，都保存在XML文件中，我们可以通过以下程序找到他们： import cv2 as cv print(cv.__file__) # 实例化人脸和眼睛检测的分类器对象 # 实例化级联分类器 classifier =cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) # 加载分类器 classifier.load(&#39;haarcascade_frontalface_default.xml&#39;) # 进行人脸和眼睛的检测 rect = classifier.detectMultiScale(gray, scaleFactor, minNeighbors, minSize,maxsize) Gray: 要进行检测的人脸图像 scaleFactor: 前后两次扫描中，搜索窗口的比例系数 minneighbors：目标至少被检测到minNeighbors次才会被认为是目标 minsize和maxsize: 目标的最小尺寸和最大尺寸 import cv2 as cv import matplotlib.pyplot as plt # 1.以灰度图的形式读取图片 img = cv.imread(&#34;16.jpg&#34;) gray = cv.cvtColor(img,cv.COLOR_BGR2GRAY) # 2.实例化OpenCV人脸和眼睛识别的分类器 face_cas = cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) face_cas.load(&#39;haarcascade_frontalface_default.xml&#39;) eyes_cas = cv.CascadeClassifier(&#34;haarcascade_eye.xml&#34;) eyes_cas.load(&#34;haarcascade_eye.xml&#34;) # 3.调用识别人脸 faceRects = face_cas.detectMultiScale( gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32)) for faceRect in faceRects: x, y, w, h = faceRect # 框出人脸 cv.rectangle(img, (x, y), (x + h, y + w),(0,255,0), 3) # 4.在识别出的人脸中进行眼睛的检测 roi_color = img[y:y+h, x:x+w] roi_gray = gray[y:y+h, x:x+w] eyes = eyes_cas.detectMultiScale(roi_gray) for (ex,ey,ew,eh) in eyes: cv.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2) # 5. 检测结果的绘制 plt.figure(figsize=(8,6),dpi=100) plt.imshow(img[:,:,::-1]),plt.title(&#39;检测结果&#39;) plt.xticks([]), plt.yticks([]) plt.show() # 我们也可在视频中对人脸进行检测： import cv2 as cv import matplotlib.pyplot as plt # 1.读取视频 cap = cv.VideoCapture(&#34;movie.mp4&#34;) # 2.在每一帧数据中进行人脸识别 while(cap.isOpened()): ret, frame = cap.read() if ret==True: gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) # 3.实例化OpenCV人脸识别的分类器 face_cas = cv.CascadeClassifier( &#34;haarcascade_frontalface_default.xml&#34; ) face_cas.load(&#39;haarcascade_frontalface_default.xml&#39;) # 4.调用识别人脸 faceRects = face_cas.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=3, minSize=(32, 32)) for faceRect in faceRects: x, y, w, h = faceRect # 框出人脸 cv.rectangle(frame, (x, y), (x + h, y + w),(0,255,0), 3) cv.imshow(&#34;frame&#34;,frame) if cv.waitKey(1) &amp; 0xFF == ord(&#39;q&#39;): break # 5. 释放资源 cap.release() cv.destroyAllWindows() ]]></content></entry><entry><title>站点示例</title><url>/flinks.html</url><categories/><tags/><content type="html">如想交换本站友情链接，请在评论区留下你的站点信息，格式参考如下：
- name: Hugo-NexT desc: Hugo NexT 官方预览网站。 avatar: https://hugo-next.eu.org/imgs/hugo_next_avatar.png link: https://hugo-next.eu.org</content></entry></search>