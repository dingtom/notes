<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="cv-CNN"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="cv-CNN"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/cv-cnn/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"cv-cnn","permalink":"/post/cv-cnn/","title":"cv-CNN","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>cv-CNN - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>73</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#卷积层>卷积层</a></li><li><a href=#池化层>池化层</a></li><li><a href=#卷积层输出矩阵大小>卷积层输出矩阵大小</a></li><li><a href=#改进点>改进点</a></li><li><a href=#3d-卷积>3D 卷积</a></li><li><a href=#11-卷积>1×1 卷积</a></li><li><a href=#两个33代替一个55>两个3*3代替一个5*5</a></li><li><a href=#转置卷积去卷积棋盘效应>　转置卷积（去卷积、棋盘效应）</a></li><li><a href=#多尺度卷积>多尺度卷积</a></li><li><a href=#深度可分卷积>深度可分卷积</a></li><li><a href=#分组卷积>分组卷积</a></li><li><a href=#空洞卷积>空洞卷积</a></li><li><a href=#转置卷积>转置卷积</a></li><li><a href=#addition--concatenate>Addition / Concatenate</a></li><li><a href=#vgg>VGG</a></li><li><a href=#googlenet>GoogLeNet</a></li><li><a href=#resnet>ResNet</a></li><li><a href=#densenet>DenseNet</a></li><li><a href=#squeezenet>SqueezeNet</a></li><li><a href=#mobilenet>MobileNet</a></li><li><a href=#shufflenet>shuffleNet</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>73</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=275201></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=587></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-18T15:39:56+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/cv-cnn/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="cv-CNN"><meta itemprop=description content="卷积层 卷积提取底层特征减少神经网络中参数个数 局部连接。⽐起全连接，局部连接会⼤⼤减少⽹络的参数。在⼆维图像中，局部像素的关联性很强， 设计局部"></span><header class=post-header><h1 class=post-title itemprop="name headline">cv-CNN
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/cv-CNN.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/cv itemprop=url rel=index><span itemprop=name>cv</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>4156</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>9分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/cv-cnn/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4c00b0451280e2f8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-3adce96cd54eb3f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=卷积层>卷积层</h2><p><code>卷积提取底层特征减少神经网络中参数个数</code></p><ul><li><code>局部连接</code>。⽐起全连接，局部连接会⼤⼤<code>减少⽹络的参数</code>。在⼆维图像中，局部像素的关联性很强，
设计局部连接保证了卷积⽹络对图像局部特征的强响应能⼒。+</li><li><code>下采样</code>。下采样能逐渐降低图像分辨率，实现了数据的降维，并使浅层的局部特征组合成为深层的特
征。下采样还能使计算资源耗费变少，加速模型训练，也能有效控制过拟合。</li><li><code>权值共享</code>。参数共享也能<code>减少整体参数量</code>，增强了⽹络训练的效率。⼀个卷积核的参数权重被整张图
⽚共享，不会因为图像内位置的不同⽽改变卷积核内的参数权重。</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-9ff2bf3400efde22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-83bf8425d394ccdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt="左侧小矩阵的尺寸为过滤器的尺寸，而右侧单位矩阵的深度为过滤器的深度
"></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-bdf823c5a2c28959.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><ul><li>为了避免尺寸的变化可以在当前层的矩阵的边界加入全０填充（zero-padding）．否则中间的像素会多次进入卷积野而边上的进入次数少</li><li>还可以通过设置过滤器移动步长调整结果矩阵的大小</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_14-52-32-636.png alt></p><h2 id=池化层>池化层</h2><p><code>下采样。降维、扩大感知野、减小计算量</code>
<code>实现非线性</code>
<code>实现不变性，（平移不变性、旋转不变性和尺度不变性）</code></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-8b04d457c6018029.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。></p><h2 id=卷积层输出矩阵大小>卷积层输出矩阵大小</h2><p>$n_{\text {output}}=\left[\frac{n_{\text {input}}- \text{kernel_size} +2*padding}{stride} + 1\right]$</p><p>池化层的输出大小公式也与卷积层一样</p><h2 id=改进点>改进点</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-acae173c2caeea35.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=3d-卷积>3D 卷积</h2><p>实际上是对一个 3D 体积执行卷积。但通常而言，我们仍在深度学习中称之为 2D 卷积。这是在 3D 体积数据上的 2D 卷积。<code>过滤器深度与输入层深度一样。这个 3D 过滤器仅沿两个方向移动（图像的高和宽）。</code>这种操作的输出是一张 2D 图像（仅有一个通道）。</p><p>3D 卷积确实存在。这是 2D 卷积的泛化。<code>其过滤器深度小于输入层深度（核大小&lt;通道大小）。因此，3D 过滤器可以在所有三个方向（图像的高度、宽度、通道）上移动。</code>在每个位置，逐元素的乘法和加法都会提供一个数值。因为过滤器是滑过一个 3D 空间，所以输出数值也按 3D 空间排布。也就是说输出是一个 3D 数据。</p><p>3D 卷积可以描述 3D 空间中目标的空间关系。对某些应用（比如生物医学影像中的 3D 分割/重构）而言，这样的 3D 关系很重要，比如在 CT 和 MRI 中，血管之类的目标会在 3D 空间中蜿蜒曲折。</p><h2 id=11-卷积>1×1 卷积</h2><p>⾸发于NIN（Network in Network），后续也在GoogLeNet和ResNet等⽹络中使⽤。</p><ul><li>跨通道交流信息</li><li>降维、升维</li><li>减少参数量</li><li>1×1 卷积+激活函数 增加⾮线性，提升⽹络表达能⼒。</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-cc270f72d13957ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-95f1db364ebe56bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=两个33代替一个55>两个3*3代替一个5*5</h2><p>两个3*3卷积核和⼀个5*5卷积核的感受野相同，但是<code>减少了参数量和计算量</code>，加快了模型训练。与此同时由于卷积核的增加，<code>模型的⾮线性表达能⼒⼤⼤增强</code>。</p><p>参数量：（3*3+1）*2；5*5+1</p><p>过⼤卷积核也有使⽤的空间，在GAN，图像超分辨率，图像融合等领域依然有较多的应⽤</p><h2 id=转置卷积去卷积棋盘效应>　转置卷积（去卷积、棋盘效应）</h2><p><code>上采样生成高分辨率图像、将低维特征图映射到高维空间</code></p><p>转置卷积通过训练过程学习到最优的上采样方式，来代替传统的插值上采样方法，以提升图像分割，图像融合，GAN等特定任务的性能。</p><p>转置卷积并不是卷积的反向操作，从信息论的角度看，卷积运算是不可逆的。转置卷积可以将输出的特征图尺寸恢复卷积前的特征图尺寸，但不恢复原始数值。</p><p>我们一直都可以使用直接的卷积实现转置卷积。对于下图的例子，我们在一个 2×2 的输入（周围加了 2×2 的单位步长的零填充）上应用一个 3×3 核的转置卷积。上采样输出的大小是 4×4。</p><h2 id=多尺度卷积>多尺度卷积</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-9386a104067200fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=深度可分卷积>深度可分卷积</h2><p>深度可分离卷积将传统的卷积分两步进行，分别是<code>depthwise和pointwise</code>。</p><p>首先按照<code>通道进行计算按位相乘的计算</code>，深度可分离卷积中的卷积核都是单通道的，输出不能改变feature map的通道数，此时通道数不变；</p><p>然后将得到将第一步的结果，使用<code>1*1的卷积核</code>进行传统的卷积运算，此时<code>通道数可以进行改变</code>。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-21-33-925.png alt></p><p>减少参数量、计算量</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_15-15-14-698.png alt></p><h2 id=分组卷积>分组卷积</h2><p>特征图局部链接
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-f180d01ed3f2ddf9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=传统>
通道局部链接
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-d02525a2f6f26516.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=参数量减少>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ee939e0c39d7e299.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=Alexnet></p><p>混分组卷积</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-14-56-805.png alt></p><p>SE注意力机制</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-15-32-142.png alt></p><h2 id=空洞卷积>空洞卷积</h2><ol><li><p><code>扩大感受野</code>。神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，<code>语义层次更高的特征</code></p></li><li><p>获取多尺度上下文信息。当多个带有不同dilation rate的空洞卷积核叠加时，不同的感受野会带来多尺度信息，这对于分割任务是非常重要的。</p></li><li><p><code>可以降低计算量</code>，不需要引入额外的参数，如上图空洞卷积示意图所示，实际卷积时只有带有红点的元素真正进行计算</p></li></ol><p>图像分割领域，图像输入到CN(典型的网络比如FCN)中有两个关键</p><p><code>一个是 pooling减小图像尺寸增大感受野，另一个是 upsampling扩大图像尺寸。</code></p><p><code>在先减小再增大尺寸的过程中，肯定有一些信息损失掉了</code>，那么能不能设计一种<code>新的操作不通过 Pooling也能有较大的感受野</code>看到更多的信息呢？
<img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-15-58-227.png alt></p><p>与正常的卷积不同的是,空洞卷积引入了一个称为 “<code>扩张率(dilation rate)</code>”的超参数(hyper-parameter)，该参数定义了卷积核处理数据时各值的间距。扩张率中文也叫空洞数(Hole Size)。</p><p>在此以3*3卷积为例,展示普通卷积和空洞卷积之间的区别</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/06/atHGAJnj7FqwxOl.png alt=quicker_1adcb4bc-878f-40dc-a302-c41d066d463c.png></p><p>从左到右分别为a、b、c子图,三幅图是相互独立进行卷积的(区别于下面图4),大框表示输入图像(感受野默认为1),黑色的圆点表示3*3的卷积核,灰色地带表示卷积后的感受野</p><ul><li>a是普通的卷积过程(dilation rate = 1),卷积后的感受野为3</li><li>b是dilation rate = 2的空洞卷积,卷积后的感受野为5</li><li>c是dilation rate = 3的空洞卷积,卷积后的感受野为8</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/06/AyL7B823DcbVWEU.png alt=quicker_28831eb3-2ba2-46dd-841d-2971bd6e417c.png></p><p>1个 7 x 7 的卷积层的正则等效于 3 个 3 x 3 的卷积层的叠加。而这样的设计可以大幅度的减少参数，有正则化的效果，参数少了就没那么容易发生过拟合。</p><h2 id=转置卷积>转置卷积</h2><p><code>转置卷积（Transposed Convolution）</code>它和空洞卷积的思路正好相反，是为上采样而生，也应用于语义分割当中，而且他的计算也和空洞卷积正好相反，<code>先对输入的feature map间隔补0，卷积核不变，然后使用标准的卷积进行计算，得到更大尺寸的feature map</code>。<code>转置卷积不是卷积的逆运算</code></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-27-07-564.png alt></p><p>首先回顾下普通卷积，以stride=1，padding=0，kernel_size=3为例，假设输入特征图大小是4x4的（假设输入输出都是单通道），通过卷积后得到的特征图大小为2x2。一般使用卷积的情况中，要么特征图变小（stride > 1），要么保持不变（stride = 1），当然也可以通过四周padding让特征图变大但没有意义。</p><p>转置卷积它只能恢复到原来的大小（shape）数值与原来不同。转置卷积的运算步骤可以归为以下几步：</p><ul><li><p>在输入特征图元素间填充s-1行、列0（其中s表示转置卷积的步距）</p></li><li><p>在输入特征图四周填充k-p-1行、列0（其中k表示转置卷积的kernel_size大小，p为转置卷积的padding，注意这里的padding和卷积操作中有些不同）</p></li><li><p>将卷积核参数上下、左右翻转</p></li><li><p>做正常卷积运算（填充0，步距1）</p></li></ul><p>下面假设输入的特征图大小为2x2（假设输入输出都为单通道），通过转置卷积后得到4x4大小的特征图。这里使用的转置卷积核大小为k=3，stride=1，padding=0的情况（忽略偏执bias）。</p><p>首先在元素间填充s-1=0行、列0（等于0不用填充）
然后在特征图四周填充k-p-1=2行、列0
接着对卷积核参数进行上下、左右翻转
最后做正常卷积（填充0，步距1）
<img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/07/6HUADX2yzZ3vkmt.png alt=quicker_8295a47a-6d1d-4c18-8ba8-b4a724e696d4.png></p><p>$$H_{out} = \frac{H_{in}+2p-k}{s}+1$$</p><p>$$H_{in} = (H_{out}-1)\times s +k-2p$$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>nn<span style=color:#f92672>.</span>ConvTranspose2d
</span></span><span style=display:flex><span>n_channels, out_channels, kernel_size, stride, padding<span style=color:#960050;background-color:#1e0010>、</span>
</span></span><span style=display:flex><span>output_padding<span style=color:#960050;background-color:#1e0010>：</span>在计算得到的输出特征图的高<span style=color:#960050;background-color:#1e0010>、</span>宽方向各填充几行或列0<span style=color:#960050;background-color:#1e0010>（</span>注意<span style=color:#960050;background-color:#1e0010>，</span>这里只是在上下以及左右的一侧one side填充<span style=color:#960050;background-color:#1e0010>，</span>并不是两侧都填充<span style=color:#960050;background-color:#1e0010>，</span>有兴趣自己做个实验看下<span style=color:#960050;background-color:#1e0010>），</span>默认为0不使用<span style=color:#960050;background-color:#1e0010>。</span>
</span></span><span style=display:flex><span>groups<span style=color:#960050;background-color:#1e0010>：</span>当使用到组卷积时才会用到的参数<span style=color:#960050;background-color:#1e0010>，</span>默认为1即普通卷积<span style=color:#960050;background-color:#1e0010>。</span>
</span></span><span style=display:flex><span>bias<span style=color:#960050;background-color:#1e0010>：</span>是否使用偏执bias<span style=color:#960050;background-color:#1e0010>，</span>默认为True使用<span style=color:#960050;background-color:#1e0010>。</span>
</span></span><span style=display:flex><span>dilation<span style=color:#960050;background-color:#1e0010>：</span>当使用到空洞卷积<span style=color:#960050;background-color:#1e0010>（</span>膨胀卷积<span style=color:#960050;background-color:#1e0010>）</span>时才会使用到的参数<span style=color:#960050;background-color:#1e0010>，</span>默认为1即普通卷积<span style=color:#960050;background-color:#1e0010>。</span>
</span></span></code></pre></div><p>$$H_{in} = (H_{out}-1)\times s +dilation \times (k-1)-2p +out_padding +1$$</p><p>Resnet</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-23/2022-11-23_17-16-46-897.png alt></p><h2 id=addition--concatenate>Addition / Concatenate</h2><p>Addition和Concatenate分支操作统称为shortcut，</p><p>Addition是在ResNet中提出，两个相同维度的feature map相同位置点的值直接相加，得到新的相同维度feature map，<code>这个操作可以融合之前的特征，增加信息的表达，</code></p><p>Concatenate操作是在Inception中首次使用，被DenseNet发扬光大，和addition不同的是，它只要求两个feature map的HW相同，通道数可以不同，然后两个feature map在通道上直接拼接，得到一个更大的feature map，<code>它保留了一些原始的特征，增加了特征的数量，使得有效的信息流继续向后传递。</code></p><h2 id=vgg>VGG</h2><ul><li><p>更深的网络有助于性能的提升，11层、13层、16层、19层</p></li><li><p>更深的网络不好训练，容易过拟合</p></li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-17-30-944.png alt></p><h2 id=googlenet>GoogLeNet</h2><ul><li>引入Inception结构（不同尺度卷积，再加一块）</li><li>中间层辅助Loss单元</li><li>最后的全连接替换为averagepooling</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-24-25-278.png alt></p><p>v2用3*3代替5*5</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-11/2022-12-11_23-58-21-681.png alt></p><h2 id=resnet>ResNet</h2><p>明更深的网络在训练过程中的难度更大</p><p>不再学习从 x 到 H(x) 的基本映射关系，而是学习这两者之间的差异，也就是「残差（residual）」。</p><p>然后，为了计算 H(x)，我们只需要将这个残差加到输入上即可。假设残差为 F(x)=H(x)-x，那么现在我们的网络不会直接学习 H(x) 了，而是学习 F(x)+x。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-29-44-640.png alt>
ResNet 的每一个「模块（block）」都由一系列层和一个「捷径（shortcut）」连接组成，<code>这个「捷径」将该模块的输入和输出连接到</code>了一起<code>。然后在元素层面上执行「加法（add）」运算，</code></p><p>ResNet本质上就干了一件事：<code>降低数据中信息的冗余度</code>。具体说来，就是对非冗余信息采用了线性激活（通过skip connection获得无冗余的identity部分），然后对冗余信息采用了非线性激活（通过ReLU对identity之外的其余部分进行信息提取/过滤，提取出的有用信息即是残差）。其中，提取identity这一步，就是ResNet思想的核心。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_18-02-16-982.png alt></p><h2 id=densenet>DenseNet</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-34-16-632.png alt></p><h2 id=squeezenet>SqueezeNet</h2><ul><li><p>1*1卷积替换3*3卷积</p></li><li><p>3*3卷积采用更少的channel数</p></li><li><p>降采样后置</p></li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-35-03-541.png alt></p><h2 id=mobilenet>MobileNet</h2><ul><li>先深度可分离，再1乘1卷积。减少参数量</li><li>ReLU6，移动端部署精度没有那么高</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-08/2022-12-08_17-37-42-315.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-12/2022-12-12_00-05-57-871.png alt></p><h2 id=shufflenet>shuffleNet</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-12/2022-12-12_00-10-37-397.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-12/2022-12-12_00-08-06-836.png alt></p><p>v2</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-12/2022-12-12_00-14-20-731.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/tomding1995/picture/raw/master/2022-12-12/2022-12-12_00-13-40-991.png alt></p><blockquote><ul><li>通道宽度相同、减少分组卷积、减少分支、减少Element-wise</li><li>提出Channel Split,将module的输入通道划分为两部分，一部分直接用于Concat。算是变相的group:操作</li><li>去掉element-wise add,用Concat直接连接通道分支</li><li>将Shuffle单元提到外面，整体的卷积之后再做通道随机打乱，使得各channels之间的信息相互交通</li><li>不再用分组卷积，都用普通卷积</li></ul></blockquote></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
cv-CNN</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/cv-cnn/ title=cv-CNN>/post/cv-cnn/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/linux-%E8%BF%9C%E7%A8%8B%E4%B8%BB%E6%9C%BAip%E5%8F%98%E6%9B%B4%E9%82%AE%E7%AE%B1%E6%8F%90%E9%86%92/ rel=next title=linux-远程主机ip变更邮箱提醒><i class="fa fa-chevron-left"></i> linux-远程主机ip变更邮箱提醒</a></div><div class="post-nav-prev post-nav-item"><a href=/post/cv-ctpn/ rel=prev title=cv-CTPN>cv-CTPN
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>