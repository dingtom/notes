<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="面试-Normalize"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="面试-Normalize"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E9%9D%A2%E8%AF%95-normalize/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E9%9D%A2%E8%AF%95-normalize","permalink":"/post/%E9%9D%A2%E8%AF%95-normalize/","title":"面试-Normalize","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>面试-Normalize - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>73</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>73</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=274465></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=585></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-08T09:49:29+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E9%9D%A2%E8%AF%95-normalize/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="面试-Normalize"><meta itemprop=description content="Batch Normalization（BN，2015年） Layer Normalization（LN，2016年） Instance Normalization（IN，2017"></span><header class=post-header><h1 class=post-title itemprop="name headline">面试-Normalize
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e9%9d%a2%e8%af%95-Normalize.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E9%9D%A2%E8%AF%95 itemprop=url rel=index><span itemprop=name>面试</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>2944</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>6分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E9%9D%A2%E8%AF%95-normalize/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p>Batch Normalization（BN，2015年）</p><p>Layer Normalization（LN，2016年）</p><p>Instance Normalization（IN，2017年）</p><p>Group Normalization（GN，2018年）</p><p>它们都是从激活函数的输入来考虑、做文章的，以不同的方式<code>对激活函数的输入进行 Norm</code> 的。</p><p>另外，还需要注意它们的映射参数γ和β的区别：对于 <code>BN，IN，GN， 其γ和β都是维度等于通道数 C 的向量。而对于 LN，其γ和β都是维度等于 normalized_shape 的矩阵。</code></p><p>最后，<code>BN 和 IN 可以设置参数：momentum和track_running_stats来获得在整体数据上更准确的均值和标准差。LN 和 GN 只能计算当前 batch 内数据的真实均值和标准差</code></p><p>我们将输入的 <code>feature map shape</code> 记为<code>[N, C, H, W]</code>，其中N表示batch size，即N个样本；C表示通道数；H、W分别表示特征图的高度、宽度。这几个方法主要的区别就是在：</p><ol><li>BN是在batch上，<code>对N、H、W做归一化</code>，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于<code>固定深度的前向神经网络，如CNN</code>，不适用于RNN；</li><li>LN在通道方向上，<code>对C、H、W归一化</code>，主要<code>对RNN效果明显</code>；</li><li>IN在图像像素上，channel内做归一化,<code>对H、W做归一化，用在风格化迁移</code>；</li><li>GN将<code>channel分组，然后再做归一化</code>。然后每个group内做归一化 算（C//G）HW的均值 这样与batchsize无关 不受其约束。</li><li>SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。</li></ol><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3f22GUFz9Cqd7dUvGrjaHKNZDoRdqcqRic3KEqQUT9TWASBsdYvcJEeA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>每个子图表示一个特征图，其中N为批量，C为通道，（H，W）为特征图的高度和宽度。通过蓝色部分的值来计算均值和方差，从而进行归一化。</p><p><code>如果把特征图</code><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl326oLlrVTsibY7xcA4jhkWEvibPCLg583ibqJGzgMsfvYiag0XqGCKnbgdQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片><code>比喻成一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 有W 个字符。</code></p><p>\1. BN 求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页&mldr;&mldr;），再除以每个页码下的字符总数：N×H×W，因此可以把 BN 看成求“平均书”的操作（注意这个“平均书”每页只有一个字），求标准差时也是同理。</p><p>\2. LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的字符总数：C×H×W，即求整本书的“平均字”，求标准差时也是同理。</p><p>\3. IN 求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H×W，即求每页书的“平均字”，求标准差时也是同理。</p><p>\4. GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G 页的小册子，求每个小册子的“平均字”和字的“标准差”。</p><h1 id=batch-normalization-bn>Batch Normalization, BN</h1><p>论文链接：https://arxiv.org/pdf/1502.03167.pdf</p><p><code>为什么要进行BN呢？</code></p><p>（1）在深度神经网络训练的过程中，通常以输入网络的每一个mini-batch进行训练，这样每个batch具有不同的分布，使模型训练起来特别困难。</p><p>（2）Internal Covariate Shift (ICS) 问题：在训练的过程中，激活函数会改变各层数据的分布，随着网络的加深，这种改变（差异）会越来越大，使模型训练起来特别困难，收敛速度很慢，会出现梯度消失的问题。</p><p><code>BN的主要思想：</code>针对每个神经元，<code>沿着通道计算每个batch的均值、方差，‘强迫’数据保持均值为0，方差为1的正态分布，</code>避免发生梯度消失。</p><p>具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道 &mldr;&mldr; 加上第 N 个样本第1个通道，求平均，得到通道 1 的均值（注意是除以 N×H×W 而不是单纯除以 N，最后得到的是一个代表这个 batch 第1个通道平均值的数字，而不是一个 H×W 的矩阵）。求通道 1 的方差也是同理。对所有通道都施加一遍这个操作，就得到了所有通道的均值和方差。</p><p><code>BN的使用位置：</code>全连接层或卷积操作之后，激活函数之前。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/23/XVCN5UjncI98BPO.png alt=quicker_5f02f8ae-dcfb-4ef7-b8cf-69f43426d72b.png></p><p><code>BN算法过程：</code></p><ul><li>沿着通道计算每个batch的均值μ</li><li>沿着通道计算每个batch的方差σ²</li><li>做归一化</li><li>加入缩放和平移变量 γ 和 β</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3DUcNnJYv7jIguMSABsBicLdKef0ibqEXtRg74Hvqa2jzRgRiaArJL7emw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>其中 ε 是一个很小的正值，<code>比如</code><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl33yLVUbXuOScfJicqvI5SPBeibicMyraucKiaLm0QWoc674X4WGg5XNfQjg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片> <code>。``加入缩放和平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。</code> 这两个参数是用来学习的参数。</p><p><code>BN的作用：</code></p><p>（1）允许较大的学习率；</p><p>（2）减弱对初始化的强依赖性，用在激活层之前。其作用可以<code>加快模型训练时的收敛速度</code>，</p><p>（3）保持隐藏层中数值的均值、方差不变，让数值更稳定，训练过程更加稳<code>定，避免梯度爆炸或者梯度消失</code>。</p><p>（4）有轻微的正则化作用（相当于给隐藏层加入噪声，类似Dropout）</p><p><code>BN存在的问题：</code></p><p>（1）每次是在一个batch上计算均值、方差，如果batch size太小，则计算的<code>均值、方差不足以代表整个数据分布。</code></p><p>（2）<code>batch size太大：</code>会超过内存容量；需要跑更多的epoch，导致总训练时间变长；会直接固定梯度下降的方向，导致很难更新。</p><h1 id=layer-normalization-ln>Layer Normalization, LN</h1><p>论文链接：https://arxiv.org/pdf/1607.06450v1.pdf</p><p>针对BN不适用于深度不固定的网络（sequence长度不一致，如RNN），<code>LN对深度网络的某一层的所有神经元的输入</code>按以下公式进行normalization操作。</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3g3vldd49EIdJ8LAbxkmqTFqOV99iaR3MofvQlPljgWPrqZJXjdndicSQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>LN中同层神经元的输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。</p><p>对于特征图 <img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl326oLlrVTsibY7xcA4jhkWEvibPCLg583ibqJGzgMsfvYiag0XqGCKnbgdQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片> ，LN 对每个样本的 C、H、W 维度上的数据求均值和标准差，保留 N 维度。其均值和标准差公式为：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3CNqxDhojaCcIlWrHZ7UOdsuBdC23OlZKpicibkM3FGjB8C5OZY4a6R4w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。LN不依赖于batch size和输入sequence的长度，因此可以用于batch size为1和RNN中。LN用于RNN效果比较明显，但是在CNN上，效果不如BN。</p><p><code>三、 Instance Normalization, IN</code></p><p>论文链接：https://arxiv.org/pdf/1607.08022.pdf</p><p>IN针对图像像素做normalization，最初用于图像的风格化迁移。在图像风格化中，生成结果主要依赖于某个图像实例，feature map 的各个 channel 的均值和方差会影响到最终生成图像的风格。所以对整个batch归一化不适合图像风格化中，因而对H、W做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。</p><p>对于，IN 对每个样本的 H、W 维度的数据求均值和标准差，保留 N 、C 维度，也就是说，它只在 channel 内部求均值和标准差，其公式如下：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3pgbhRbLASCd1G8wgWAVmryR8P0QictI3OVcKYIibgY6MLW4KTCCxwynA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p><code>四、 Group Normalization, GN（拿小本本get一下）</code></p><p>论文链接：https://arxiv.org/pdf/1803.08494.pdf</p><p><code>GN是为了解决BN对较小的mini-batch size效果差的问题。</code>GN适用于占用显存比较大的任务，例如图像分割。对这类任务，可能 batch size 只能是个位数，再大显存就不够用了。而当 batch size 是个位数时，BN 的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN 也是独立于 batch 的，它是 LN 和 IN 的折中。</p><p><code>GN的主要思想：</code>在 channel 方向 group，然后每个 group 内做 Norm，计算<img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_png/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3ajSibaHfgqAvBjJCx9c5laRnQrlHMfRib08LksiaNpvibcuvAzHV4LqC4Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片>的均值和方差，这样就与batch size无关，不受其约束。</p><p><code>具体方法：</code>GN 计算均值和标准差时，把每一个样本 feature map 的 channel 分成 G 组，每组将有 C/G 个 channel，然后将这些 channel 中的元素求均值和标准差。各组 channel 用其对应的归一化参数独立地归一化。</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3GClwr0IlnKHcAtyHRye6SI7vlWelc5MjHIkJegGw5eeq5ZCxIuAuCA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>伪代码如下：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://mmbiz.qpic.cn/mmbiz_jpg/vJe7ErxcLmiaBxLx7pBRq3BYgrqJ33Fl3ib9KuqbWduucUUTNOwlibN50XvBsfuibFWjpABvLd5iaA6A9Ka8MznPr9w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1" alt=图片></p><p>代码如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>def GroupNorm(x, gamma, beta, G=16):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    # x_shape:[N, C, H, W]
</span></span><span style=display:flex><span>    results = 0.
</span></span><span style=display:flex><span>    eps = 1e-5
</span></span><span style=display:flex><span>    x = np.reshape(x, (x.shape[0], G, x.shape[1]/16, x.shape[2], x.shape[3]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x_mean = np.mean(x, axis=(2, 3, 4), keepdims=True)
</span></span><span style=display:flex><span>    x_var = np.var(x, axis=(2, 3, 4), keepdims=True0)
</span></span><span style=display:flex><span>    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
</span></span><span style=display:flex><span>    results = gamma * x_normalized + beta
</span></span><span style=display:flex><span>    return results
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
面试-Normalize</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E9%9D%A2%E8%AF%95-normalize/ title=面试-Normalize>/post/%E9%9D%A2%E8%AF%95-normalize/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E9%9D%A2%E8%AF%95-mysql%E9%A2%98/ rel=next title=面试-MySQL题><i class="fa fa-chevron-left"></i> 面试-MySQL题</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E9%9D%A2%E8%AF%95-%E6%A0%87%E5%87%86%E5%8C%96%E5%BD%92%E4%B8%80%E5%8C%96%E6%AD%A3%E5%88%99%E5%8C%96/ rel=prev title=面试-标准化、归一化、正则化>面试-标准化、归一化、正则化
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>