<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="ml-循环神经网络(RNN)"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="ml-循环神经网络(RNN)"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn","permalink":"/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/","title":"ml-循环神经网络(RNN)","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>ml-循环神经网络(RNN) - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>74</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>74</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=272619></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=582></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-08T09:49:29+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="ml-循环神经网络(RNN)"><meta itemprop=description content="RNN 神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后"></span><header class=post-header><h1 class=post-title itemprop="name headline">ml-循环神经网络(RNN)
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/ml-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%28RNN%29.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ml itemprop=url rel=index><span itemprop=name>ml</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>2961</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>6分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h1 id=rnn>RNN</h1><p>神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。</p><p>基础的神经网络只在层与层之间建立了权连接，<code>RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。</code></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e0bfd84cbf5b873f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=RNN时间维度权值共享，CNN空间维度权值共享>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-0866a2838f94c9c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=将输入截断成小片段输入RNN>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-54085bad8a0926c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=语言模型：用于由上文预测下文></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-59102f9ebe861276.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt="image Captioning CNN得到的向量V输入RNN"></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-79739fa6d2c113eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-0d31094fca6c985b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-bcb943cd30cd17c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fb1429457073005b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h1 id=rnn存在的问题>RNN存在的问题</h1><blockquote><p><code>梯度消失的问题</code>
RNN网络的<code>激活函数一般选用双曲正切</code>，而不是sigmod函数，（RNN的激活函数除了双曲正切，RELU函数也用的非常多）原因在于RNN网络在求解时涉及时间序列上的大量求导运算，使用sigmod函数容易出现梯度消失，且sigmod的导数形式较为复杂。事实上，即使使用双曲正切函数，传统的RNN网络依然存在梯度消失问题。
无论是梯度消失还是梯度爆炸，都是源于<code>网络结构太深，造成网络权重不稳定</code>，从本质上来讲是因为梯度反向传播中的连乘效应，类似于：$0.99^{100}=0.36$，于是梯度越来越小，开始消失，另一种极端情况就是$1.1^{100}=13780$。
<code>长期依赖的问题</code>还有一个问题是无法“记忆”长时间序列上的信息，这个bug直到LSTM上引入了单元状态后才算较好地解决
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-bd0f9824aa206013.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p></blockquote><h1 id=lstm>LSTM</h1><p>下面来了解一下LSTM（long short-term memory）。长短期记忆网络是RNN的一种变体，<code>RNN由于梯度消失的原因只能有短期记忆，LSTM网络通过精妙的门控制将短期记忆与长期记忆结合起来，并且一定程度上解决了梯度消失的问题。</code>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4477ce50e52769e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><blockquote><p>$f_{t}=\sigma\left(W_{f} \cdot\left[h_{t-1}, x_{t}\right]+b_{f}\right)$
$i_{t}=\sigma\left(W_{i} \cdot \left[h_{t-1}, x_{t}\right]+b_{i}\right)$
$o_{t}=\sigma\left(W_{o}\left[h_{t-1}, x_{t}\right]+b_{o}\right)$
$\tilde{C}<em>{t}=\tanh \left(W</em>{C} \cdot\left[h_{t-1}, x_{t}\right]+b_{C}\right)$
$C_{t}=f_{t} ⊙C_{t-1}+i_{t} ⊙ \tilde{C}$
$h_{t}=o_{t} ⊙ \tanh \left(C_{t}\right)$</p></blockquote><p><code>遗忘门：</code>主要控制<code>是否遗忘上一层的记忆细胞状态</code>， <code>输入分别是当前时间步序列数据，上一时间步的隐藏状态</code>，进行矩阵相乘，经<code>sigmoid激活后，获得一个值域在[0, 1]</code>的输出F，<code>再跟上一层记忆细胞进行对应元素相乘</code>，输出F中越接近0，代表需要遗忘上层记忆细胞的元素。
<code>候选记忆细胞：</code>这里的区别在于将sigmoid函数<code>换成tanh激活函数，因此输出的值域在[-1, 1]。</code>
<code>输入门：</code>与遗忘门类似，也是<code>经过sigmoid激活后，获得一个值域在[0, 1]的输出</code>。它用于<code>控制当前输入X经过候选记忆细胞如何流入当前时间步的记忆细胞</code>。 如果输入门输出接近为0，而遗忘门接近为1，则当前记忆细胞一直保存过去状态
<code>输出门：</code>也是通过<code>sigmoid激活，获得一个值域在[0,1]的输出</code>。主要<code>控制记忆细胞到下一时间步隐藏状态的信息流动</code></p><blockquote><p>其中，四个蓝色的小矩形就是普通神经网络的隐藏层结构，其中第一、二和四的激活函数是sigmoid，第三个的激活函数是tanh。<code>t时刻的输入X和t-1时刻的输出h(t-1)进行拼接</code>，然后输入cell中，其实可以这样理解，我们的输入X(t)分别feed进了四个小蓝矩形中，每个小黄矩形中进行的运算和正常的神经网络的计算一样（矩阵乘法），有关记忆的部分完全由各种门结构来控制（就是0和1），同时在输入时不仅仅有原始的数据集，同时还加入了上一个数据的输出结果，也就是h(t-1)，那么讲来LSTM和正常的神经网络类似，只是在输入和输出上加入了一点东西。cell中可以大体分为两条横线，上面的横线用来控制长时记忆，下面的横线用来控制短时记忆。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6c1db768e97d7731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-abb67948303b6369.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p></blockquote><p>不过LSTM<code>依旧不能解决梯度“爆炸”的问题</code>。但似乎梯度爆炸相对于梯度消失，问题没有那么严重。一般靠裁剪后的优化算法即可解决，比如gradient clipping（如果梯度的范数大于某个给定值，将梯度同比收缩）。
梯度剪裁的方法一般有两种：
1.一种是当梯度的某个维度绝对值大于某个上限的时候，就剪裁为上限。
2.另一种是梯度的L2范数大于上限后，让梯度除以范数，避免过大。</p><p>LSTM是一种拥有三个“门”的特殊网络结构，包括遗忘门、输入门、输出门。<code>所谓“门”结构就是一个使用sigmoid神经网络和一个按位做乘法的操作</code>，这两个操作合在一起就是一个“门”结构。</p><h1 id=lstm与gru的区别>LSTM与GRU的区别</h1><p>LSTM与GRU二者结构十分相似，<code>不同在于</code>：</p><ol><li><code>新的记忆都是根据之前状态及输入进行计算</code>，但是<code>GRU中有一个重置门控制之前状态的进入量</code>，而在LSTM里没有类似门；</li><li><code>产生新的状态方式不同</code>，LSTM有两个不同的门，分别是遗忘门(forget gate)和输入门(input gate)，而GRU只有一种更新门(update gate)；</li><li><code>LSTM对新产生的状态可以通过输出门(output gate)进行调节</code>，而GRU对输出无任何调节。</li><li><code>GRU的优点更加简单</code>，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，然后它可以扩大模型的规模。</li><li>LSTM更加强大和灵活，因为它有三个门而不是两个。</li></ol><h1 id=gru结构>GRU结构</h1><p>GRU是LSTM网络的一种效果很好的变体，它较LSTM网络的结构更加简单，而且效果也很好，因此也是当前非常流形的一种网络。GRU既然是LSTM的变体，因此也是可以解决RNN网络中的长依赖问题。
在LSTM中引入了三个门函数：输入门、遗忘门和输出门来控制输入值、记忆值和输出值。而在<code>GRU模型中只有两个门：分别是更新门和重置门</code>。具体结构如下图所示：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-1148da5d5fe3836f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/kkweishe/images/raw/master/ML/2019-8-16_13-58-58.png alt></p><blockquote><p>$$R_t=\sigma(X_tW_{xr}+H_{t-1}W_{hr}+b_r))$$</p><p>$Z_t=\sigma(X_tW_{xz}+H_{t-1}W_{hz}+b_z))$</p><p>$\tilde{H}<em>t=tanh(X_tW</em>{xh}+(R_t⊙H_{t-1})W_{hh}+b_h))$</p></blockquote><p><code>更新⻔可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新，</code></p><p><code>重置⻔可以⽤来丢弃与预测⽆关的历史信息。</code></p><p>我们对⻔控循环单元的设计稍作总结：</p><ul><li>重置⻔有助于捕捉时间序列⾥短期的依赖关系；</li><li>更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。</li></ul><h1 id=bilstm>BILSTM</h1><p>如果能像访问过去的上下文信息一样，访问未来的上下文，这样对于许多序列标注任务是非常有益的。例如，<code>在最特殊字符分类的时候，如果能像知道这个字母之前的字母一样，知道将要来的字母，这将非常有帮助。</code></p><p>双向循环神经网络（BRNN）的基本思想是提出<code>每一个训练序列向前和向后分别是两个循环神经网络（RNN），而且这两个都连接着一个输出层。</code>这个结构<code>提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息</code>。下图展示的是一个沿着时间展开的双向循环神经网络。六个独特的权值在每一个时步被重复的利用，六个权值分别对应：输入到向前和向后隐含层（w1, w3），隐含层到隐含层自己（w2, w5），向前和向后隐含层到输出层（w4, w6）。<code>值得注意的是：向前和向后隐含层之间没有信息流，这保证了展开图是非循环的。</code></p><blockquote><p><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/rnn_cell/BasicLSTMCell?hl=en" title=tf.nn.rnn_cell.BasicLSTMCell() rel="noopener external nofollow noreferrer" target=_blank class=exturl>tf.nn.rnn_cell.BasicLSTMCell()
<i class="fa fa-external-link-alt"></i>
</a>来实现BILSTM
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-00f5836861f76cb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=image.png></p></blockquote><h1 id=深层循环神经网络drnn>深层循环神经网络（DRNN）</h1><blockquote><p><a href="https://tensorflow.google.cn/api_docs/python/tf/nn/rnn_cell/MultiRNNCell?hl=en" title="tf.rnn_cell.MultiRNNCell(lstm * number_of_layer)" rel="noopener external nofollow noreferrer" target=_blank class=exturl>tf.rnn_cell.MultiRNNCell(lstm * number_of_layer)
<i class="fa fa-external-link-alt"></i>
</a>来构建DRNN，其中number_of_layer表示了有多少层
<a href="https://tensorflow.google.cn/api_docs/python/tf/nn/rnn_cell/DropoutWrapper?hl=en" title=tf.nn.rnn_cell.DropoutWrapper rel="noopener external nofollow noreferrer" target=_blank class=exturl>tf.nn.rnn_cell.DropoutWrapper
<i class="fa fa-external-link-alt"></i>
</a>来实现dropout功能
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ed0824fa3d0cc8ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=image.png></p></blockquote><h1 id=tensorflow-lstm预测正弦函数>tensorflow lstm预测正弦函数</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import tensorflow as tf
</span></span><span style=display:flex><span>from tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat
</span></span><span style=display:flex><span>from tensorflow.python.ops import array_ops as array_ops_
</span></span><span style=display:flex><span>import matplotlib.pyplot as plt
</span></span><span style=display:flex><span>learn = tf.contrib.learn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>#### 1. 设置神经网络的参数。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>HIDDEN_SIZE = 30
</span></span><span style=display:flex><span>NUM_LAYERS = 2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TIMESTEPS = 10
</span></span><span style=display:flex><span>TRAINING_STEPS = 3000
</span></span><span style=display:flex><span>BATCH_SIZE = 32
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>TRAINING_EXAMPLES = 10000
</span></span><span style=display:flex><span>TESTING_EXAMPLES = 1000
</span></span><span style=display:flex><span>SAMPLE_GAP = 0.01
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>#### 2. 定义生成正弦数据的函数。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def generate_data(seq):
</span></span><span style=display:flex><span>    X = []
</span></span><span style=display:flex><span>    y = []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    for i in range(len(seq) - TIMESTEPS - 1):
</span></span><span style=display:flex><span>        X.append([seq[i: i + TIMESTEPS]])
</span></span><span style=display:flex><span>        y.append([seq[i + TIMESTEPS]])
</span></span><span style=display:flex><span>    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>#### 3. 定义lstm模型。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def lstm_model(X, y):/;;;p
</span></span><span style=display:flex><span>    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE)
</span></span><span style=display:flex><span>    cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * NUM_LAYERS)
</span></span><span style=display:flex><span>    x_ = tf.unpack(X, axis=1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    output, _ = tf.nn.rnn(cell, x_, dtype=tf.float32)
</span></span><span style=display:flex><span>    output = output[-1]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    # 通过无激活函数的全联接层计算线性回归，并将数据压缩成一维数组的结构。
</span></span><span style=display:flex><span>    predictions = tf.contrib.layers.fully_connected(output, 1, None)
</span></span><span style=display:flex><span>    predictions = array_ops_.squeeze(predictions, squeeze_dims=[1])
</span></span><span style=display:flex><span>    loss = tf.contrib.losses.mean_squared_error(predictions, y)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    train_op = tf.contrib.layers.optimize_loss(
</span></span><span style=display:flex><span>        loss, tf.contrib.framework.get_global_step(),
</span></span><span style=display:flex><span>        optimizer=&#34;Adagrad&#34;, learning_rate=0.1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    return predictions, loss, train_op
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>#### 4. 进行训练。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 封装之前定义的lstm。
</span></span><span style=display:flex><span>regressor = SKCompat(learn.Estimator(model_fn=lstm_model,model_dir=&#34;Models/model_2&#34;))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 生成数据。
</span></span><span style=display:flex><span>test_start = TRAINING_EXAMPLES * SAMPLE_GAP
</span></span><span style=display:flex><span>test_end = (TRAINING_EXAMPLES + TESTING_EXAMPLES) * SAMPLE_GAP
</span></span><span style=display:flex><span>train_X, train_y = generate_data(np.sin(np.linspace(
</span></span><span style=display:flex><span>    0, test_start, TRAINING_EXAMPLES, dtype=np.float32)))
</span></span><span style=display:flex><span>test_X, test_y = generate_data(np.sin(np.linspace(
</span></span><span style=display:flex><span>    test_start, test_end, TESTING_EXAMPLES, dtype=np.float32)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 拟合数据。
</span></span><span style=display:flex><span>regressor.fit(train_X, train_y, batch_size=BATCH_SIZE, steps=TRAINING_STEPS)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 计算预测值。
</span></span><span style=display:flex><span>predicted = [[pred] for pred in regressor.predict(test_X)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 计算MSE。
</span></span><span style=display:flex><span>rmse = np.sqrt(((predicted - test_y) ` 2).mean(axis=0))
</span></span><span style=display:flex><span>print (&#34;Mean Square Error is: %f&#34; % rmse[0])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>#### 5. 画出预测值和真实值的曲线。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plot_predicted, = plt.plot(predicted, label=&#39;predicted&#39;)
</span></span><span style=display:flex><span>plot_test, = plt.plot(test_y, label=&#39;real_sin&#39;)
</span></span><span style=display:flex><span>plt.legend([plot_predicted, plot_test],[&#39;predicted&#39;, &#39;real_sin&#39;])
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
ml-循环神经网络(RNN)</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/ title=ml-循环神经网络(RNN)>/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/ml-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9Cgan/ rel=next title=ml-生成对抗网络(GAN)><i class="fa fa-chevron-left"></i> ml-生成对抗网络(GAN)</a></div><div class="post-nav-prev post-nav-item"><a href=/post/ml-%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/ rel=prev title=ml-自动机器学习>ml-自动机器学习
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>