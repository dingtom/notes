<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="cv-语义分割"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="cv-语义分割"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2","permalink":"/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/","title":"cv-语义分割","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>cv-语义分割 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>71</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#labelme>Labelme</a><ul><li><a href=#格式转换>格式转换</a><ul><li><a href=#转换语义分割标签>转换语义分割标签</a></li><li><a href=#转换实例分割标签>转换实例分割标签</a></li></ul></li></ul></li></ul><ul><li><ul><li><a href=#pascal-voc>PASCAL VOC</a></li><li><a href=#ms-coco>MS COCO</a><ul><li><a href=#验证目标检测任务map>验证目标检测任务mAP</a></li></ul></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>71</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=273436></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=584></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-03T01:30:21+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="cv-语义分割"><meta itemprop=description content="标注工具 Labelme 支持目标检测、语义分割、实例分割等任务。今天针对分割任务的数据标注进行简单的介绍。开源项目地址： https://github.com/wkentaro/labelme 安装非常简单，直接使用pip安装"></span><header class=post-header><h1 class=post-title itemprop="name headline">cv-语义分割
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/cv-%e8%af%ad%e4%b9%89%e5%88%86%e5%89%b2.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/cv itemprop=url rel=index><span itemprop=name>cv</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>12066</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>25分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h1 id=标注工具>标注工具</h1><h2 id=labelme>Labelme</h2><p>支持目标检测、语义分割、实例分割等任务。今天针对分割任务的数据标注进行简单的介绍。开源项目地址：
<a href=https://github.com/wkentaro/labelme title=https://github.com/wkentaro/labelme rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/wkentaro/labelme
<i class="fa fa-external-link-alt"></i></a></p><p>安装非常简单，直接使用<code>pip</code>安装即可：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-mysql data-lang=mysql><span style=display:flex><span>pip install labelme
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>安装完成后在终端输入</span><span style=color:#f92672>`</span>labelme<span style=color:#f92672>`</span><span style=color:#960050;background-color:#1e0010>即可启动：</span>
</span></span><span style=display:flex><span>labelme
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 建议大家按照我提供的目录格式事先准备好数据，然后在该根目录下启动labelme
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#960050;background-color:#1e0010>├──</span> img_data: <span style=color:#960050;background-color:#1e0010>存放你要标注的所有图片</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>├──</span> data_annotated: <span style=color:#960050;background-color:#1e0010>存放后续标注好的所有</span>json<span style=color:#960050;background-color:#1e0010>文件</span>
</span></span><span style=display:flex><span><span style=color:#960050;background-color:#1e0010>└──</span> label.txt: <span style=color:#960050;background-color:#1e0010>所有类别信息</span>
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-02/2022-11-02_10-43-52-902.png alt></p><p>虽然在labelme中能够在标注时添加标签，但我个人强烈建议事先创建一个<code>label.txt</code>标签（放在上述位置中），然后启动labelme时直接读取。标签格式如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>__ignore__
</span></span><span style=display:flex><span>_background_
</span></span><span style=display:flex><span>dog
</span></span><span style=display:flex><span>cat
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 每一行代表一个类型的名称，前两行是固定格式__ignore__和_background_都加上，否则后续使用作者提供的转换脚本（转换成PASCAL VOC格式和MS COCO格式）时会报错。也就是从第三行开始就是我们需要分割的目标类别。这里以分割猫狗为例。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 在创建好标签后，启动labelme并读取标签文件（注意启动根目录），其中--labels指定了标签文件的路径。
</span></span><span style=display:flex><span>labelme --labels label.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 读取标签后，我们在界面右侧能够看到Label List中已经载入了刚刚我们自己创建的标签文件，并且不同类别用不同的颜色表示。
</span></span></code></pre></div><ul><li>养成良好习惯，先将保存路径设置好。</li></ul><p>先点击左上角File，Change Output Dir设置标注结果的保存目录，这里就设置成前面说好的data_annotated。
建议将Save With Image Data取消掉，默认是选中的。如果选中，会在保存的标注结果中将图像数据也保存在.json文件中（个人觉得没必要，还占空间）。</p><ul><li>标注目标</li></ul><p>首先点击左侧的CreatePolygons按钮开始绘制多边形，然后用鼠标标记一个一个点把目标边界给标注出来（鼠标放置在第一个点上，点击一下会自动闭合边界）。标注后会弹出一个选择类别的选择框，选择对应类别即可。</p><p>如果标注完一个目标后想修改目标边界，可以点击工具左侧的EditPolygons按钮，然后选中要修改的目标，拖拉边界点即可进行微调。如果要在边界上新增点，把鼠标放在边界上点击鼠标右键选择Add Point to Edge即可新增边界点。如果要删除点，把鼠标放在边界点上点击鼠标右键选择Remove Selected Point即可删除边界点</p><p>标注完一张图片后，点击界面左侧的<code>Save</code>按钮即可保存标注结果，默认每张图片的标注信息都用一个json文件存储。</p><ul><li>保存json文件格式</li></ul><p>标注得到的json文件格式如下，将一张图片中的所有目标的坐标都保存在shapes列表中，列表中每个元素对应一个目标，其中label记录了该目标的类别名称。points记录了一个目标的左右坐标信息。其他信息不在赘述。根据以下信息，其实自己就可以写个脚本取读取目标信息了。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;version&#34;</span>: <span style=color:#e6db74>&#34;4.5.9&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;flags&#34;</span>: <span style=color:#f92672>{}</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;shapes&#34;</span>: <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>{</span>
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;label&#34;</span>: <span style=color:#e6db74>&#34;dog&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;points&#34;</span>: <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>          108.09090909090907,
</span></span><span style=display:flex><span>          687.1818181818181
</span></span><span style=display:flex><span>        <span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        ....
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>          538.090909090909,
</span></span><span style=display:flex><span>          668.090909090909
</span></span><span style=display:flex><span>        <span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>        <span style=color:#f92672>[</span>
</span></span><span style=display:flex><span>          534.4545454545454,
</span></span><span style=display:flex><span>          689.0
</span></span><span style=display:flex><span>        <span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;group_id&#34;</span>: null,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;shape_type&#34;</span>: <span style=color:#e6db74>&#34;polygon&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#e6db74>&#34;flags&#34;</span>: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>}</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>]</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;imagePath&#34;</span>: <span style=color:#e6db74>&#34;../img_data/1.jpg&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;imageData&#34;</span>: null,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;imageHeight&#34;</span>: 690,
</span></span><span style=display:flex><span>  <span style=color:#e6db74>&#34;imageWidth&#34;</span>: <span style=color:#ae81ff>690</span>
</span></span><span style=display:flex><span><span style=color:#f92672>}</span>
</span></span></code></pre></div><h3 id=格式转换>格式转换</h3><h4 id=转换语义分割标签>转换语义分割标签</h4><p>原作者为了方便，也提供了一个脚本，帮我们方便的将json文件转换成PASCAL VOC的语义分割标签格式。示例项目链接：https://github.com/wkentaro/labelme/tree/master/examples/semantic_segmentation.
在该链接中有个labelme2voc.py脚本，将该脚本下载下来后，放在上述项目根目录下，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_voc是生成PASCAL VOC数据的目录。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>python labelme2voc<span style=color:#f92672>.</span>py data_annotated data_dataset_voc <span style=color:#f92672>--</span>labels label<span style=color:#f92672>.</span>txt
</span></span></code></pre></div><p>执行后会生成如下目录：</p><ul><li><p>data_dataset_voc/JPEGImages</p></li><li><p>data_dataset_voc/SegmentationClass</p></li><li><p>data_dataset_voc/SegmentationClassPNG</p></li><li><p>data_dataset_voc/SegmentationClassVisualization</p></li><li><p>data_dataset_voc/class_names.txt</p></li></ul><p>其中JPEGImages就和之前PASCAL VOC数据讲解中说的一样，就是存储原图像文件。而SegmentationClassPNG就是语义分割需要使用的PNG标签图片。<code>class_names.txt</code>存储的是所有的类别信息，包括背景。</p><h4 id=转换实例分割标签>转换实例分割标签</h4><p>原作者为了方便，这里提供了两个脚本，帮我们方便的将json文件转换成PASCAL VOC的实例分割标签格式以及MS COCO的实例分割标签格式。示例项目链接：https://github.com/wkentaro/labelme/tree/master/examples/instance_segmentation.
在该链接中有个labelme2voc.py脚本，将该脚本下载下来后，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_voc是生成PASCAL VOC数据的目录。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>labelme2voc<span style=color:#f92672>.</span>py data_annotated data_dataset_voc <span style=color:#f92672>--</span>labels label<span style=color:#f92672>.</span>txt
</span></span></code></pre></div><p>执行后会生成如下目录：</p><ul><li><p>data_dataset_voc/JPEGImages</p></li><li><p>data_dataset_voc/SegmentationClass</p></li><li><p>data_dataset_voc/SegmentationClassPNG</p></li><li><p>data_dataset_voc/SegmentationClassVisualization</p></li><li><p>data_dataset_voc/SegmentationObject</p></li><li><p>data_dataset_voc/SegmentationObjectPNG</p></li><li><p>data_dataset_voc/SegmentationObjectVisualization</p></li><li><p>data_dataset_voc/class_names.txt</p></li></ul><p>除了刚刚讲的语义分割文件夹外，还生成了针对实例分割的标签文件，主要就是SegmentationObjectPNG目录：</p><p>在该链接中有个labelme2coco.py脚本，将该脚本下载下来后，执行以下指令即可（注意，执行脚本的根目录必须和刚刚启动labelme的根目录相同，否则会出现找不到图片的错误）。其中data_annotated是刚刚标注保存的json标签文件夹，data_dataset_coco是生成MS COCO数据类型的目录。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>python labelme2coco<span style=color:#f92672>.</span>py data_annotated data_dataset_coco <span style=color:#f92672>--</span>labels label<span style=color:#f92672>.</span>txt
</span></span></code></pre></div><p>其中annotations.json就是MS COCO的标签数据文件，如果不了解可以看下我之前写的MS COCO介绍。</p><h1 id=数据集>数据集</h1><h3 id=pascal-voc>PASCAL VOC</h3><p>PASCAL VOC挑战赛主要包括以下几类：<code>图像分类(Object Classification)</code>，<code>目标检测(Object Detection)</code>，<code>目标分割(Object Segmentation)</code>，<code>行为识别(Action Classification)</code> 等。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-14/2022-10-14_11-23-38-217.png alt></p><p>该数据集有20个分类：
Person:person
Animal:bird,cat,cow,dog,horse,sheep
Vehicle:aeroplane,bicycle,boat,bus,car,motorbike,train
Indoor:bottle,chair,dining table,potted plant,sofa,tv/monitor</p><p><a href=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ title=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>http://host.robots.ox.ac.uk/pascal/VOC/voc2012/
<i class="fa fa-external-link-alt"></i></a></p><p>下载后将文件进行解压，解压后的文件目录结构如下所示：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>VOCdevkit
</span></span><span style=display:flex><span>    └── VOC2012
</span></span><span style=display:flex><span>         ├── Annotations               所有的图像标注信息(XML文件)
</span></span><span style=display:flex><span>         ├── ImageSets    
</span></span><span style=display:flex><span>         │   ├── Action                人的行为动作图像信息
</span></span><span style=display:flex><span>         │   ├── Layout                人的各个部位图像信息
</span></span><span style=display:flex><span>         │   │
</span></span><span style=display:flex><span>         │   ├── Main                  目标检测分类图像信息
</span></span><span style=display:flex><span>         │   │     ├── train.txt       训练集(5717)
</span></span><span style=display:flex><span>         │   │     ├── val.txt         验证集(5823)
</span></span><span style=display:flex><span>         │   │     └── trainval.txt    训练集+验证集(11540)
</span></span><span style=display:flex><span>         │   │
</span></span><span style=display:flex><span>         │   └── Segmentation          目标分割图像信息
</span></span><span style=display:flex><span>         │         ├── train.txt       训练集(1464)
</span></span><span style=display:flex><span>         │         ├── val.txt         验证集(1449)
</span></span><span style=display:flex><span>         │         └── trainval.txt    训练集+验证集(2913)
</span></span><span style=display:flex><span>         │ 
</span></span><span style=display:flex><span>         ├── JPEGImages                所有图像文件
</span></span><span style=display:flex><span>         ├── SegmentationClass         语义分割png图（基于类别）
</span></span><span style=display:flex><span>         └── SegmentationObject        实例分割png图（基于目标）
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-14/2022-10-14_11-39-50-410.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-14/2022-10-14_17-39-54-719.png alt></p><p>如果IOU>0.5则预测为正样本，检测到同一目标多个检测，第一个检测为正样本，其余视为负样本</p><h3 id=ms-coco>MS COCO</h3><p>MS COCO是一个非常大型且常用的数据集，其中包括了目标检测，分割，图像描述等。其主要特性如下：</p><ul><li>Object segmentation: 目标级分割</li><li>Recognition in context: 图像情景识别</li><li>Superpixel stuff segmentation: 超像素分割</li><li>330K images (>200K labeled): 超过33万张图像，标注过的图像超过20万张</li><li>1.5 million object instances: 150万个对象实例</li><li>80 object categories: 80个目标类别</li><li>91 stuff categories: 91个材料类别</li><li>5 captions per image: 每张图像有5段情景描述</li><li>250,000 people with keypoints: 对25万个人进行了关键点标注</li></ul><p><a href=http://cocodataset.org/ title=http://cocodataset.org/ rel="noopener external nofollow noreferrer" target=_blank class=exturl>http://cocodataset.org/
<i class="fa fa-external-link-alt"></i></a></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-14/2022-10-14_11-54-59-495.png alt></p><p>MS COCO标注文件格式</p><p>官网有给出一个关于标注文件的格式说明，可以通过以下链接查看：
<a href=https://cocodataset.org/#format-data title=https://cocodataset.org/#format-data rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://cocodataset.org/#format-data
<i class="fa fa-external-link-alt"></i></a></p><p><img src=/imgs/img-lazy-loading.gif data-src=C:%5cUsers%5cWENCHAO%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5c1665884955753.png alt></p><p>下面是使用<code>pycocotools</code>读取图像以及对应bbox信息的简单示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>Linux系统安装pycocotools：
</span></span><span style=display:flex><span>pip install pycocotools  
</span></span><span style=display:flex><span>Windows系统安装pycocotools：
</span></span><span style=display:flex><span>pip install pycocotools-windows
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pycocotools.coco <span style=color:#f92672>import</span> COCO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image, ImageDraw
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>json_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/data/coco2017/annotations/instances_val2017.json&#34;</span>
</span></span><span style=display:flex><span>img_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/data/coco2017/val2017&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># load coco data</span>
</span></span><span style=display:flex><span>coco <span style=color:#f92672>=</span> COCO(annotation_file<span style=color:#f92672>=</span>json_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># get all image index info</span>
</span></span><span style=display:flex><span>ids <span style=color:#f92672>=</span> list(sorted(coco<span style=color:#f92672>.</span>imgs<span style=color:#f92672>.</span>keys()))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;number of images: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(len(ids)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># get all coco class labels</span>
</span></span><span style=display:flex><span>coco_classes <span style=color:#f92672>=</span> dict([(v[<span style=color:#e6db74>&#34;id&#34;</span>], v[<span style=color:#e6db74>&#34;name&#34;</span>]) <span style=color:#66d9ef>for</span> k, v <span style=color:#f92672>in</span> coco<span style=color:#f92672>.</span>cats<span style=color:#f92672>.</span>items()])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 遍历前三张图像</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> img_id <span style=color:#f92672>in</span> ids[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 获取对应图像id的所有annotations idx信息</span>
</span></span><span style=display:flex><span>    ann_ids <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>getAnnIds(imgIds<span style=color:#f92672>=</span>img_id)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 根据annotations idx信息获取所有标注信息</span>
</span></span><span style=display:flex><span>    targets <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>loadAnns(ann_ids)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># get image file name</span>
</span></span><span style=display:flex><span>    path <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>loadImgs(img_id)[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;file_name&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># read image</span>
</span></span><span style=display:flex><span>    img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(img_path, path))<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#39;RGB&#39;</span>)
</span></span><span style=display:flex><span>    draw <span style=color:#f92672>=</span> ImageDraw<span style=color:#f92672>.</span>Draw(img)
</span></span><span style=display:flex><span>    <span style=color:#75715e># draw box to image</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> target <span style=color:#f92672>in</span> targets:
</span></span><span style=display:flex><span>        x, y, w, h <span style=color:#f92672>=</span> target[<span style=color:#e6db74>&#34;bbox&#34;</span>]
</span></span><span style=display:flex><span>        x1, y1, x2, y2 <span style=color:#f92672>=</span> x, y, int(x <span style=color:#f92672>+</span> w), int(y <span style=color:#f92672>+</span> h)
</span></span><span style=display:flex><span>        draw<span style=color:#f92672>.</span>rectangle((x1, y1, x2, y2))
</span></span><span style=display:flex><span>        draw<span style=color:#f92672>.</span>text((x1, y1), coco_classes[target[<span style=color:#e6db74>&#34;category_id&#34;</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># show image</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>imshow(img)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p>下面是使用<code>pycocotools</code>读取图像segmentation信息的简单示例：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> random
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pycocotools.coco <span style=color:#f92672>import</span> COCO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pycocotools <span style=color:#f92672>import</span> mask <span style=color:#66d9ef>as</span> coco_mask
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> PIL <span style=color:#f92672>import</span> Image, ImageDraw
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>json_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/data/coco2017/annotations/instances_val2017.json&#34;</span>
</span></span><span style=display:flex><span>img_path <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/data/coco2017/val2017&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># random pallette</span>
</span></span><span style=display:flex><span>pallette <span style=color:#f92672>=</span> [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> [random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>255</span>) <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>255</span><span style=color:#f92672>*</span><span style=color:#ae81ff>3</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># load coco data</span>
</span></span><span style=display:flex><span>coco <span style=color:#f92672>=</span> COCO(annotation_file<span style=color:#f92672>=</span>json_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># get all image index info</span>
</span></span><span style=display:flex><span>ids <span style=color:#f92672>=</span> list(sorted(coco<span style=color:#f92672>.</span>imgs<span style=color:#f92672>.</span>keys()))
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;number of images: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(len(ids)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># get all coco class labels</span>
</span></span><span style=display:flex><span>coco_classes <span style=color:#f92672>=</span> dict([(v[<span style=color:#e6db74>&#34;id&#34;</span>], v[<span style=color:#e6db74>&#34;name&#34;</span>]) <span style=color:#66d9ef>for</span> k, v <span style=color:#f92672>in</span> coco<span style=color:#f92672>.</span>cats<span style=color:#f92672>.</span>items()])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 遍历前三张图像</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> img_id <span style=color:#f92672>in</span> ids[:<span style=color:#ae81ff>3</span>]:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 获取对应图像id的所有annotations idx信息</span>
</span></span><span style=display:flex><span>    ann_ids <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>getAnnIds(imgIds<span style=color:#f92672>=</span>img_id)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 根据annotations idx信息获取所有标注信息</span>
</span></span><span style=display:flex><span>    targets <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>loadAnns(ann_ids)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># get image file name</span>
</span></span><span style=display:flex><span>    path <span style=color:#f92672>=</span> coco<span style=color:#f92672>.</span>loadImgs(img_id)[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#39;file_name&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#75715e># read image</span>
</span></span><span style=display:flex><span>    img <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>open(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(img_path, path))<span style=color:#f92672>.</span>convert(<span style=color:#e6db74>&#39;RGB&#39;</span>)
</span></span><span style=display:flex><span>    img_w, img_h <span style=color:#f92672>=</span> img<span style=color:#f92672>.</span>size
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    masks <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    cats <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> target <span style=color:#f92672>in</span> targets:
</span></span><span style=display:flex><span>        cats<span style=color:#f92672>.</span>append(target[<span style=color:#e6db74>&#34;category_id&#34;</span>])  <span style=color:#75715e># get object class id</span>
</span></span><span style=display:flex><span>        polygons <span style=color:#f92672>=</span> target[<span style=color:#e6db74>&#34;segmentation&#34;</span>]   <span style=color:#75715e># get object polygons</span>
</span></span><span style=display:flex><span>        rles <span style=color:#f92672>=</span> coco_mask<span style=color:#f92672>.</span>frPyObjects(polygons, img_h, img_w)
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> coco_mask<span style=color:#f92672>.</span>decode(rles)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(mask<span style=color:#f92672>.</span>shape) <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>            mask <span style=color:#f92672>=</span> mask[<span style=color:#f92672>...</span>, <span style=color:#66d9ef>None</span>]
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>any(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        masks<span style=color:#f92672>.</span>append(mask)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    cats <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(cats, dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>int32)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> masks:
</span></span><span style=display:flex><span>        masks <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>stack(masks, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        masks <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((<span style=color:#ae81ff>0</span>, height, width), dtype<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>uint8)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># merge all instance masks into a single segmentation map</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># with its corresponding categories</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> (masks <span style=color:#f92672>*</span> cats[:, <span style=color:#66d9ef>None</span>, <span style=color:#66d9ef>None</span>])<span style=color:#f92672>.</span>max(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># discard overlapping instances</span>
</span></span><span style=display:flex><span>    target[masks<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>0</span>) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>    target <span style=color:#f92672>=</span> Image<span style=color:#f92672>.</span>fromarray(target<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>uint8))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    target<span style=color:#f92672>.</span>putpalette(pallette)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>imshow(target)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h4 id=验证目标检测任务map>验证目标检测任务mAP</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-16/2022-10-16_11-19-56-917.png alt></p><p>根据官方文档给的预测结果格式可以看到，我们需要以列表的形式保存结果，列表中的每个元素对应一个检测目标（每个元素都是字典类型），每个目标记录了四个信息：</p><ul><li>image_id记录该目标所属图像的id（int类型）</li><li>category_id记录预测该目标的类别索引，注意这里索引是对应stuff中91个类别的索引信息（int类型）</li><li>bbox记录预测该目标的边界框信息，注意对应目标的[xmin, ymin, width, height] (list[float]类型)</li><li>score记录预测该目标的概率（float类型）</li></ul><p>AP@.5(IOU=0.5)；AP@.75(IOU=0.75)</p><p>mAP=(mAP.5:.05:.95) /10 （ start from 0.5 to 0.95 with a step size of 0.05十个mAP平均）</p><h1 id=分割检测>分割、检测</h1><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/31/WTwFk2NeqJ6SRxb.png alt=quicker_49402ab9-9843-4939-b810-0dec623ac765.png></p><p>图像分类：图像中的气球是一个类别。
语义分割：分割出气球和背景。
目标检测：图像中有7个目标气球，并且检测出每个气球的坐标位置。
实例分割：图像中有7个不同的气球，在像素层面给出属于每个气球的像素。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/30/SpOhMr68QWbF2yg.png alt=quicker_f5c4e0b4-cbb5-44a4-bb19-7515cb96470d.png></p><p>高分辨率特征(较浅的卷积层)<code>感知域较</code>小，有利于feature map和原图进行对齐的，也就是我说的可以提供更多的位置信息。</p><p>低分辨率信息(深层的卷积层)由于<code>感知域较大</code>，能够学习到更加<code>抽象</code>一些的特征，可以提供更多的上下文信息，即强语义信息，这有利于像素的精确分类。</p><p><code>上采样</code>（意义在于将小尺寸的高维度feature map恢复回去）一般包括2种方式：</p><p><code>Resize</code>，如双线性插值直接对图像进行缩放（这种方法在原文中提到）</p><p><code>Deconvolution</code>（反卷积），也叫Transposed Convolution(转置卷积)，可以理解为卷积的逆向操作。</p><h1 id=fcn>FCN</h1><p>FCN首先将一幅RGB图像输入到卷积神经网络后，经过多次卷积以及池化过程得到一系列的特征图，然后<code>利用反卷积层对最后一个卷积层得到的特征图进行上采样</code>，使得上采样后特征图与原图像的大小一样，从而实现对特征图上的每个像素值进行预测的同时保留其在原图像中的空间位置信息，最后对上采样特征图进行逐像素分类，逐个像素计算softmax分类损失。</p><p>主要特点：</p><ul><li>不含全连接层（FC）的全卷积（Fully Conv）网络。从而可适应任意尺寸输入。</li><li>引入增大数据尺寸的反卷积（Deconv）层。能够输出精细的结果。</li><li>结合不同深度层结果的跳级（skip）结构。同时确保鲁棒性和精确性。</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/aAzVlHI98TebNSZ.png alt=quicker_fdb13fc8-47b1-42ca-8b6d-ad8d4ce801c2.png></p><p>反卷积和卷积类似，都是相乘相加的运算。只不过后者是多对一，前者是一对多。而反卷积的前向和反向传播，只用颠倒卷积的前后向传播即可。如下图所示：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/FCKN4js6LuwVXEl.png alt=quicker_e53dd05f-9c15-4350-95b3-a538a553abab.png>要注意的是，unpooling和反卷积是有几种不同的操作的。反池化很好理解，其中一种比较常用的操作是只需记住池化过程中的位置（最大值出现的地方），在unpooling的过程中，将对应位置置相应的值，其余位置置0即可。另外还有一种操作就是将所有unpooling的值全部填充下采样（pooling）的值。</p><p>经过全卷积后的结果进行反卷积，基本上就能实现语义分割了，但是得到的结果通常是比较粗糙的。</p><p>图中，image是原图像，conv1,conv2..,conv5为卷积操作，pool1,pool2,..pool5为pool操作（pool就是使得图片变为原图的1/2），注意con6-7是最后的卷积层，最右边一列是upsample后的end to end结果。<code>必须说明的是图中nx是指对应的特征图上采样n倍（即变大n倍），并不是指有n个特征图，如32x upsampled 中的32x是图像只变大32倍，不是有32个上采样图像，又如2x conv7是指conv7的特征图变大2倍。</code></p><p><code>（1）FCN-32s过程</code></p><p>只需要留意第一行，网络里面有5个pool，所以conv7的特征图是原始图像1/32，可以发现最左边image的是32x32（假设以倍数计），同时我们知道在FCN中的卷积是不会改变图像大小（或者只有少量像素的减少，特征图大小基本不会小很多）。看到pool1是16x16，pool2是8x8，pool3是4x4，pool4是2x2，pool5是1x1，所以conv7对应特征图大小为1x1，然后再经过32x upsampled prediction 图片变回32x32。FCN作者在这里增加一个卷积层，卷积后的大小为输入图像的<code>32</code>(2^5)倍，我们简单假设这个卷积核大小也为32，这样就是需要通过反馈训练32x32个权重变量即可让图像实现end to end，完成了一个32s的upsample。FCN作者称做后卷积，他也提及可以称为反卷积。事实上在源码中卷积核的大小为64，同时没有偏置bias。还有一点就是FCN论文中最后结果都是21×*，这里的21是指FCN使用的数据集分类，总共有21类。</p><p><code>（2）FCN-16s过程</code></p><p>现在我们把1,2两行一起看，忽略32x upsampled prediction，说明FCN-16s的upsample过程。FCN作者在conv7先进行一个2x conv7操作，其实这里也只是增加1个卷积层，这次卷积后特征图的大小为conv7的<code>2</code>倍，可以从pool5与2x conv7中看出来。此时2x conv7与pool4的大小是一样的，FCN作者提出对pool4与2x conv7进行一个fuse操作（<code>事实上就是将pool4与2x conv7相加，另一篇博客说是拼接，个人认为是拼接</code>）。fuse结果进行16x upsampled prediction，与FCN-32s一样，也是增加一个卷积层，卷积后的大小为输入图像的<code>16</code>(2^4)倍。我们知道pool4的大小是2x2，放大16倍，就是32x32，这样最后图像大小也变为原来的大小，至此完成了一个16s的upsample。现在我们可以知道，FCN中的upsample实际是通过增加卷积层，通过bp反馈的训练方法训练卷积层达到end to end，这时<code>卷积层的作用可以看作是pool的逆过程</code>。</p><p><code>（3）FCN-8s过程</code></p><p>这是我们看第1行与第3行，忽略32x upsampled prediction。conv7经过一次4x upsample，即使用一个卷积层，特征图输出大小为conv7的4倍，所得4x conv7的大小为4x4。然后pool4需要一次2x upsample，变成2x pool4，大小也为4x4。再把4x conv7，2x pool4与pool3进行fuse，得到求和后的特征图。最后增加一个卷积层，使得输出图片大小为pool3的8倍，也就是8x upsampled prediction的过程，得到一个end to end的图像。实验表明<code>FCN-8s优于FCN-16s，FCN-32s</code>。 我们可以发现，如果继续仿照FCN作者的步骤，我们可以对pool2，pool1实现同样的方法，可以有FCN-4s，FCN-2s，最后得到end to end的输出。这里作者给出了明确的结论，超过FCN-8s之后，结果并不能继续优化。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/GMbESzDXdZeiwaq.png alt=quicker_56103cc9-bdb3-4cab-bb6a-c12d18d7965a.png></p><p>图中，image是原图像，conv1,conv2..,conv5为卷积操作，pool1,pool2,..pool5为pool操作（pool就是使得图片变为原图的1/2），注意con6-7是最后的卷积层，最右边一列是upsample后的end to end结果。<code>必须说明的是图中nx是指对应的特征图上采样n倍（即变大n倍），并不是指有n个特征图，如32x upsampled 中的32x是图像只变大32倍，不是有32个上采样图像，又如2x conv7是指conv7的特征图变大2倍。</code></p><p><code>（1）FCN-32s过程</code></p><p>只需要留意第一行，网络里面有5个pool，所以conv7的特征图是原始图像1/32，可以发现最左边image的是32x32（假设以倍数计），同时我们知道在FCN中的卷积是不会改变图像大小（或者只有少量像素的减少，特征图大小基本不会小很多）。看到pool1是16x16，pool2是8x8，pool3是4x4，pool4是2x2，pool5是1x1，所以conv7对应特征图大小为1x1，然后再经过32x upsampled prediction 图片变回32x32。FCN作者在这里增加一个卷积层，卷积后的大小为输入图像的<code>32</code>(2^5)倍，我们简单假设这个卷积核大小也为32，这样就是需要通过反馈训练32x32个权重变量即可让图像实现end to end，完成了一个32s的upsample。FCN作者称做后卷积，他也提及可以称为反卷积。事实上在源码中卷积核的大小为64，同时没有偏置bias。还有一点就是FCN论文中最后结果都是21×*，这里的21是指FCN使用的数据集分类，总共有21类。</p><p><code>（2）FCN-16s过程</code></p><p>现在我们把1,2两行一起看，忽略32x upsampled prediction，说明FCN-16s的upsample过程。FCN作者在conv7先进行一个2x conv7操作，其实这里也只是增加1个卷积层，这次卷积后特征图的大小为conv7的<code>2</code>倍，可以从pool5与2x conv7中看出来。此时2x conv7与pool4的大小是一样的，FCN作者提出对pool4与2x conv7进行一个fuse操作（<code>事实上就是将pool4与2x conv7相加，另一篇博客说是拼接，个人认为是拼接</code>）。fuse结果进行16x upsampled prediction，与FCN-32s一样，也是增加一个卷积层，卷积后的大小为输入图像的<code>16</code>(2^4)倍。我们知道pool4的大小是2x2，放大16倍，就是32x32，这样最后图像大小也变为原来的大小，至此完成了一个16s的upsample。现在我们可以知道，FCN中的upsample实际是通过增加卷积层，通过bp反馈的训练方法训练卷积层达到end to end，这时<code>卷积层的作用可以看作是pool的逆过程</code>。</p><p><code>（3）FCN-8s过程</code></p><p>这是我们看第1行与第3行，忽略32x upsampled prediction。conv7经过一次4x upsample，即使用一个卷积层，特征图输出大小为conv7的4倍，所得4x conv7的大小为4x4。然后pool4需要一次2x upsample，变成2x pool4，大小也为4x4。再把4x conv7，2x pool4与pool3进行fuse，得到求和后的特征图。最后增加一个卷积层，使得输出图片大小为pool3的8倍，也就是8x upsampled prediction的过程，得到一个end to end的图像。实验表明<code>FCN-8s优于FCN-16s，FCN-32s</code>。 我们可以发现，如果继续仿照FCN作者的步骤，我们可以对pool2，pool1实现同样的方法，可以有FCN-4s，FCN-2s，最后得到end to end的输出。这里作者给出了明确的结论，超过FCN-8s之后，结果并不能继续优化。</p><p>结合上述的FCN的全卷积与upsample，在upsample最后加上softmax，就可以对不同类别的大小概率进行估计，实现end to end。最后输出的图是一个概率估计，对应像素点的值越大，其像素为该类的结果也越大。<code>FCN的核心贡献在于提出使用卷积层通过学习让图片实现end to end分类。</code></p><blockquote><p><code>事实上，FCN有一些短处</code>，例如使用了较浅层的特征，因为fuse操作会加上较上层的pool特征值，导致高维特征不能很好得以使用，同时也因为使用较上层的pool特征值，导致FCN对图像大小变化有所要求，如果测试集的图像远大于或小于训练集的图像，FCN的效果就会变差。</p></blockquote><h1 id=segnet>SegNet</h1><p>Segnet是用于进行像素级别图像分割的全卷积网络，分割的核心组件是一个encoder 网络，及其相对应的decoder网络，后接一个象素级别的分类网络。</p><p>encoder网络：其结构与VGG16网络的前13层卷积层的结构相似。</p><p>decoder网络：作用是将由encoder的到的低分辨率的feature maps 进行映射得到与输入图像feature map相同的分辨率进而进行像素级别的分类。</p><p>最终解码器的输出被送入soft-max分类器以独立的为每个像素产生类别概率。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/VN5MAh1e46WJUBK.png alt=quicker_15980490-03ee-472c-8b84-902828eeac1f.png></p><p>Segnet的亮点：<code>decoder利用与之对应的encoder阶段中进行max-pooling时的pooling index 进行非线性上采样</code>，这样做的好处是上采样阶段就不需要进行学习。 上采样后得到的feature maps 是非常稀疏的，因此，需要进一步选择合适的卷积核进行卷积得到dense featuremaps 。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/5u3fUAXbPiFWe1B.png alt=quicker_70e69d53-897c-4edc-b1a3-da05921c3a62.png></p><h1 id=unet>UNet</h1><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/31/JFk612wB48r9mvl.png alt=quicker_e2b7c8fb-0081-4b01-b46c-69b87f6858f4.png></p><ol><li>首先进行Conv+Pooling下采样；</li><li>然后反卷积进行上采样，crop之前的低层feature map，进行融合；</li><li>再次上采样。</li><li>重复这个过程，直到获得输出388x388x2的feature map，</li><li>最后经过softmax获得output segment map。总体来说与FCN思路非常类似。</li></ol><p><code>UNet的encoder下采样4次，一共下采样16倍，对称地，其decoder也相应上采样4次，将encoder得到的高级语义特征图恢复到原图片的分辨率。</code></p><p>它采用了与FCN不同的<code>特征融合</code>方式：</p><ol><li>FCN采用的是<code>逐点相加</code>，对应tensorflow的tf.add()函数</li><li>U-Net采用的是<code>channel维度拼接融合</code>，对应tensorflow的tf.concat()函数</li></ol><p>Unet结构特点</p><blockquote><p>UNet相比于FCN和Deeplab等，共进行了4次上采样，并在同一个stage使用了skip connection，而不是直接在高级语义特征上进行监督和loss反传，这样就保证了最后恢复出来的特征图融合了更多的low-level的feature，也使得不同scale的feature得到了的融合，从而可以进行多尺度预测和DeepSupervision。4次上采样也使得分割图恢复边缘等信息更加精细。</p></blockquote><p>为什么适用于医学图像？</p><blockquote><p>\1. 因为医学图像边界模糊、梯度复杂，需要较多的高分辨率信息。高分辨率用于精准分割。
\2. 人体内部结构相对固定，分割目标在人体图像中的分布很具有规律，语义简单明确，低分辨率信息能够提供这一信息，用于目标物体的识别。</p><p>UNet结合了<code>低分辨率信息（提供物体类别识别依据）和高分辨率信息</code>（提供精准分割定位依据），完美适用于医学图像分割。</p></blockquote><h1 id=deeplab>DeepLab</h1><p>基于全卷积对称语义分割模型得到的分割结果比较粗糙，忽略了像素与像素之间的空间一致性关系。于是Google提出了一种新的扩张卷积语义分割模型，考虑了像素与像素之间的空间一致性关系，可以在不增加数量的情况下增加感受野。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/V1MXwxTdqUYjEly.png alt=quicker_4964e7e0-fcc8-4e8d-9159-998a8891536c.png></p><ul><li>Deeplabv1是由深度卷积网路和概率图模型级联而成的语义分割模型，由于深度卷积网路在重复最大池化和下采样的过程中会丢失很多的细节信息，所以采用<code>扩张卷积算法增加感受野以获得更多上下文信</code>息。考虑到深度卷积网路在图像标记任务中的空间不敏感性限制了它的定位精度，采用了<code>完全连接条件随机场（Conditional Random Field， CRF）来提高模型捕获细节的能力</code>。</li><li>Deeplabv2予以分割模型增加了ASPP（Atrous spatial pyramid pooling）结构，利用多个不同采样率的扩张卷积提取特征，再将特征融合以捕获不同大小的上下文信息。</li><li>Deeplabv3语义分割模型，在ASPP中加入了全局平均池化，同时在平行扩张卷积后添加批量归一化，有效地捕获了全局语义信息。</li><li>DeepLabV3+语义分割模型在Deeplabv3的基础上增加了编-解码模块和Xception主干网路，增加编解码模块主要是为了恢复原始的像素信息，使得分割的细节信息能够更好的保留，同时编码丰富的上下文信息。增加Xception主干网络是为了采用深度卷积进一步提高算法的精度和速度。在inception结构中，先对输入进行1*1卷积，之后将通道分组，分别使用不同的3*3卷积提取特征，最后将各组结果串联在一起作为输出。</li></ul><blockquote><p>主要特点：</p><ul><li>在多尺度上为分割对象进行带洞空间金字塔池化（ASPP）</li><li>通过使用DCNNs（空洞卷积）提升了目标边界的定位</li><li>降低了由DCNN的不变性导致的定位准确率</li></ul></blockquote><h1 id=refinenet>RefineNet</h1><p>RefineNet采用了通过细化中间激活映射并分层地将其链接到结合多尺度激活，同时防止锐度损失。网络由独立的RefineNet模块组成，每个模块对应于ResNet。</p><p>每个RefineNet模块由三个主要模块组成，即剩余卷积单元（RCU），多分辨率融合（MRF）和链剩余池（CRP）。RCU块由一个自适应块组成卷积集，微调预训练的ResNet权重对于分割问题。MRF层融合不同的激活物使用卷积上采样层来创建更高的分辨率地图。最后，在CRP层池中使用多种大小的内核用于从较大的图像区域捕获背景上下文。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/n4CqyKYlxwZm6D9.png alt=quicker_2cd234a9-55a2-4f01-9df6-f668a22809c5.png></p><blockquote><p>主要特点：</p><ul><li>提出一种多路径refinement网络，称为RefineNet。这种网络可以使用各个层级的features，使得语义分割更为精准。</li><li>RefineNet中所有部分都利用resdiual connections (identity mappings)，使得梯度更容易短向或者长向前传，使端对端的训练变得更加容易和高效。</li><li>提出了一种叫做chained residual pooling的模块，它可以从一个大的图像区域捕捉背景上下文信息。</li></ul></blockquote><h1 id=pspnet>PSPNet</h1><p>深度卷积神经网络的每一层特征对语义分割都有影响，如何将高层特征的语义信息与底层识别的边界与轮廓信息结合起来是一个具有挑战性的问题。</p><p>金字塔场景稀疏网络语义分割模型（Pyramid Scene Parsing Network，PSP）首先结合预训练网络 ResNet和扩张网络来提取图像的特征，得到原图像 1/8 大小的特征图，然后，采用金字塔池化模块将特征图同时通过四个并行的池化层得到四个不同大小的输出，将四个不同大小的输出分别进行上采样，还原到原特征图大小，最后与之前的特征图进行连接后经过卷积层得到最后的预测分割图像。</p><ul><li>PSPNet为像素级场景解析提供了有效的全局上下文先验</li><li>金字塔池化模块可以收集具有层级的信息，比全局池化更有代表性</li><li>在计算量方面，我们的PSPNet并没有比原来的空洞卷积FCN网络有很大的增加</li><li>在端到端学习中，全局金字塔池化模块和局部FCN特征可以被同时训练</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/VIU1rKW4LQJhZFt.png alt=quicker_94da9d25-ad62-4f89-81d5-fff7e145a90e.png></p><blockquote><p>主要特点：</p><ul><li>金字塔场景解析网络是建立在FCN之上的基于像素级分类网络。将大小不同的内核集中在一起激活地图的不同区域创建空间池金字塔。</li><li>特性映射来自网络被转换成不同分辨率的激活，并经过多尺度处理池层，稍后向上采样并与原始层连接进行分割的feature map。</li><li>学习的过程利用辅助分类器进一步优化了像ResNet这样的深度网络。不同类型的池模块侧重于激活的不同区域地图。</li></ul></blockquote><h1 id=基于全卷积的gan语义分割模型>基于全卷积的GAN语义分割模型</h1><p>生成对抗网络模型（Generative Adversarial Nets，GAN）同时训练生成器 G 和判别器 D，判别器用来预测给定样本是来自于真实数据还是来自于生成模型。</p><p>利用对抗训练方法训练语义分割模型，将传统的多类交叉熵损失与对抗网络相结合，首先对对抗网络进行预训练，然后使用对抗性损失来微调分割网络，如下图所示。左边的分割网络将 RGB 图像作为输入，并产生每个像素的类别预测。右边的对抗网络将标签图作为输入并生成类标签（1 代表真实标注，0 代表合成标签）。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/u6DhPVCH3jqMIip.png alt=quicker_e26e71f8-565c-4c58-9a87-bc6a2e90f368.png></p><h1 id=基于全卷积语义分割模型对比>基于全卷积语义分割模型对比</h1><table><thead><tr><th style=text-align:left>名称</th><th style=text-align:left>优点</th><th style=text-align:left>缺点</th></tr></thead><tbody><tr><td style=text-align:left>FCN</td><td style=text-align:left>可以接受任意大小的图像输入；避免了采用像素块带来的重复存储和计算的问题</td><td style=text-align:left>得到的结果不太精确，对图像的细节不敏感，没有考虑像素与像素之间的关系，缺乏空间一致性</td></tr><tr><td style=text-align:left>SegNet</td><td style=text-align:left>使用去池化对特征图进行上采样，在分割中保持细节的完整性；去掉全连接层，拥有较少的参数</td><td style=text-align:left>当对低分辨率的特征图进行去池化时，会忽略邻近像素的信息</td></tr><tr><td style=text-align:left>Deconvnet</td><td style=text-align:left>对分割的细节处理要强于 FCN，位于低层的filter 能捕获目标的形状信息，位于高层的 filter能够捕获特定类别的细节信息，分割效果更好</td><td style=text-align:left>对细节的处理难度较大</td></tr><tr><td style=text-align:left>U-net</td><td style=text-align:left>简单地将编码器的特征图拼接至每个阶段解码器的上采样特征图，形成了一个梯形结构；采用跳跃连接架构，允许解码器学习在编码器池化中丢失的相关性</td><td style=text-align:left>在卷积过程中没有加pad，导致在每一次卷积后，特征长度就会减少两个像素，导致网络最后的输出与输入大小不一样</td></tr><tr><td style=text-align:left>DeepLab</td><td style=text-align:left>使用了空洞卷积；全连接条件随机场</td><td style=text-align:left>得到的预测结果只有原始输入的 1/8 大小</td></tr><tr><td style=text-align:left>RefineNet</td><td style=text-align:left>带有解码器模块的编码器-解码器结构；所有组件遵循残差连接的设计方式</td><td style=text-align:left>带有解码器模块的编码器-解码器结构；所有组件遵循残差连接的设计方式</td></tr><tr><td style=text-align:left>PSPNet</td><td style=text-align:left>提出金字塔模块来聚合背景信息；使用了附加损失</td><td style=text-align:left>采用四种不同的金字塔池化模块，对细节的处理要求较高</td></tr><tr><td style=text-align:left>GCN</td><td style=text-align:left>提出了带有大维度卷积核的编码器-解码器结构</td><td style=text-align:left>计算复杂，具有较多的结构参数</td></tr><tr><td style=text-align:left>DeepLabV3 ASPP</td><td style=text-align:left>采用了Multigrid；在原有的网络基础上增加了几个 block；提出了ASPP，加入了 BN</td><td style=text-align:left>不能捕捉图像大范围信息，图像层的特征整合只存在于 ASPP中</td></tr><tr><td style=text-align:left>GAN</td><td style=text-align:left>提出将分割网络作为判别器，GAN 扩展训练数据，提升训练效果；将判别器改造为 FCN，从将判别每一个样本的真假变为每一个像素的真假</td><td style=text-align:left>没有比较与全监督+半监督精调模型的实验结果，只体现了在本文中所提创新点起到了一定的作用，但并没有体现有效的程度</td></tr></tbody></table><h1 id=图像增强>图像增强</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#                                      OpenCV</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 水平翻转</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(cv2<span style=color:#f92672>.</span>flip(img, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 水平翻转</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(cv2<span style=color:#f92672>.</span>flip(img, <span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 随机裁剪</span>
</span></span><span style=display:flex><span>x, y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>256</span>), np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>256</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>imshow(img[x:x<span style=color:#f92672>+</span><span style=color:#ae81ff>256</span>, y:y<span style=color:#f92672>+</span><span style=color:#ae81ff>256</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#                                    albumentations           </span>
</span></span><span style=display:flex><span><span style=color:#75715e># 水平翻转</span>
</span></span><span style=display:flex><span>augments <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>HorizontalFlip(p<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)(image<span style=color:#f92672>=</span>img, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>img_aug, mask_aug <span style=color:#f92672>=</span> augments[<span style=color:#e6db74>&#39;image&#39;</span>], augments[<span style=color:#e6db74>&#39;mask&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#75715e># 随机裁剪</span>
</span></span><span style=display:flex><span>augments <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>RandomCrop(p<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, height<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>, width<span style=color:#f92672>=</span><span style=color:#ae81ff>256</span>)(image<span style=color:#f92672>=</span>img, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>img_aug, mask_aug <span style=color:#f92672>=</span> augments[<span style=color:#e6db74>&#39;image&#39;</span>], augments[<span style=color:#e6db74>&#39;mask&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#75715e># 旋转</span>
</span></span><span style=display:flex><span>augments <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>ShiftScaleRotate(p<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)(image<span style=color:#f92672>=</span>img, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>img_aug, mask_aug <span style=color:#f92672>=</span> augments[<span style=color:#e6db74>&#39;image&#39;</span>], augments[<span style=color:#e6db74>&#39;mask&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Compose            
</span></span><span style=display:flex><span>transforms<span style=color:#960050;background-color:#1e0010>：</span>转换类的数组<span style=color:#960050;background-color:#1e0010>，</span>list类型
</span></span><span style=display:flex><span>bbox_params<span style=color:#960050;background-color:#1e0010>：</span>用于 bounding boxes 转换的参数<span style=color:#960050;background-color:#1e0010>，</span>BboxPoarams 类型
</span></span><span style=display:flex><span>keypoint_params<span style=color:#960050;background-color:#1e0010>：</span>用于 keypoints 转换的参数<span style=color:#960050;background-color:#1e0010>，</span> KeypointParams 类型
</span></span><span style=display:flex><span>additional_targets<span style=color:#960050;background-color:#1e0010>：</span>key新target 名字<span style=color:#960050;background-color:#1e0010>，</span>value 为旧 target 名字的 dict<span style=color:#960050;background-color:#1e0010>，</span>如 {<span style=color:#e6db74>&#39;image2&#39;</span>: <span style=color:#e6db74>&#39;image&#39;</span>}<span style=color:#960050;background-color:#1e0010>，</span>dict 类型
</span></span><span style=display:flex><span>p<span style=color:#960050;background-color:#1e0010>：</span>使用这些变换的概率<span style=color:#960050;background-color:#1e0010>，</span>默认值为 <span style=color:#ae81ff>1.0</span>           
</span></span><span style=display:flex><span>组合与随机选择<span style=color:#960050;background-color:#1e0010>（</span>Compose <span style=color:#f92672>&amp;</span> OneOf<span style=color:#960050;background-color:#1e0010>）</span>           
</span></span><span style=display:flex><span>image4 <span style=color:#f92672>=</span> Compose([
</span></span><span style=display:flex><span>        RandomRotate90(),
</span></span><span style=display:flex><span>        <span style=color:#75715e># 翻转</span>
</span></span><span style=display:flex><span>        Flip(),
</span></span><span style=display:flex><span>        Transpose(),
</span></span><span style=display:flex><span>        OneOf([
</span></span><span style=display:flex><span>            <span style=color:#75715e># 高斯噪点</span>
</span></span><span style=display:flex><span>            IAAAdditiveGaussianNoise(),
</span></span><span style=display:flex><span>            GaussNoise(),
</span></span><span style=display:flex><span>        ], p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>),
</span></span><span style=display:flex><span>        OneOf([
</span></span><span style=display:flex><span>            <span style=color:#75715e># 模糊相关操作</span>
</span></span><span style=display:flex><span>            MotionBlur(p<span style=color:#f92672>=</span><span style=color:#ae81ff>.2</span>),
</span></span><span style=display:flex><span>            MedianBlur(blur_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>),
</span></span><span style=display:flex><span>            Blur(blur_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>),
</span></span><span style=display:flex><span>        ], p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>),
</span></span><span style=display:flex><span>        ShiftScaleRotate(shift_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0625</span>, scale_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, rotate_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>45</span>, p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>),
</span></span><span style=display:flex><span>        OneOf([
</span></span><span style=display:flex><span>            <span style=color:#75715e># 畸变相关操作</span>
</span></span><span style=display:flex><span>            OpticalDistortion(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>            GridDistortion(p<span style=color:#f92672>=</span><span style=color:#ae81ff>.1</span>),
</span></span><span style=display:flex><span>            IAAPiecewiseAffine(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>        ], p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>),
</span></span><span style=display:flex><span>        OneOf([
</span></span><span style=display:flex><span>            <span style=color:#75715e># 锐化、浮雕等操作</span>
</span></span><span style=display:flex><span>            CLAHE(clip_limit<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>),
</span></span><span style=display:flex><span>            IAASharpen(),
</span></span><span style=display:flex><span>            IAAEmboss(),
</span></span><span style=display:flex><span>            RandomBrightnessContrast(),            
</span></span><span style=display:flex><span>        ], p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>        HueSaturationValue(p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>),
</span></span><span style=display:flex><span>    ], p<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>augments <span style=color:#f92672>=</span> image4(image<span style=color:#f92672>=</span>img, mask<span style=color:#f92672>=</span>mask)
</span></span><span style=display:flex><span>img_aug, mask_aug <span style=color:#f92672>=</span> augments[<span style=color:#e6db74>&#39;image&#39;</span>], augments[<span style=color:#e6db74>&#39;mask&#39;</span>]
</span></span><span style=display:flex><span>           
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
cv-语义分割</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/ title=cv-语义分割>/post/cv-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/cv-%E5%8D%A1%E5%8F%B7%E8%AF%86%E5%88%AB/ rel=next title=cv-卡号识别><i class="fa fa-chevron-left"></i> cv-卡号识别</a></div><div class="post-nav-prev post-nav-item"><a href=/post/linux-hadoop%E7%8E%AF%E5%A2%83/ rel=prev title=linux-Hadoop环境>linux-Hadoop环境
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>