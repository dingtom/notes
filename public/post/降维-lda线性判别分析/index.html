<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="降维-LDA线性判别分析"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="降维-LDA线性判别分析"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2023-01-01 14:28:53 +0800 CST"><meta property="article:modified_time" content="2023-01-01 14:28:53 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90","permalink":"/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/","title":"降维-LDA线性判别分析","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>降维-LDA线性判别分析 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>125</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#lda模型是什么>LDA模型是什么</a><ul><li><a href=#5个分布的理解>5个分布的理解</a></li><li><a href=#3个基础模型的理解>3个基础模型的理解</a></li><li><a href=#lda模型>LDA模型</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>125</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>10</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=443429></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=952></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-01-01T14:28:53+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="降维-LDA线性判别分析"><meta itemprop=description content="LDA模型是什么 LDA可以分为以下5个步骤： 一个函数：gamma函数。 四个分布：二项分布、多项分布、beta分布、Dirichlet分布。 一"></span><header class=post-header><h1 class=post-title itemprop="name headline">降维-LDA线性判别分析
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e9%99%8d%e7%bb%b4-LDA%e7%ba%bf%e6%80%a7%e5%88%a4%e5%88%ab%e5%88%86%e6%9e%90.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-01-01 14:28:53 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-01-01 14:28:53 +0800 CST">2023-01-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E9%99%8D%E7%BB%B4 itemprop=url rel=index><span itemprop=name>降维</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>5859</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>12分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h2 id=lda模型是什么>LDA模型是什么</h2><p>LDA可以分为以下5个步骤：</p><ul><li>一个函数：gamma函数。</li><li>四个分布：二项分布、多项分布、beta分布、Dirichlet分布。</li><li>一个概念和一个理念：共轭先验和贝叶斯框架。</li><li>两个模型：pLSA、LDA。</li><li>一个采样：Gibbs采样</li></ul><p>关于LDA有两种含义，一种是线性判别分析（Linear Discriminant Analysis），一种是概率主题模型：<strong>隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）</strong>，本文讲后者。</p><p>按照wiki上的介绍，LDA由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出，是一种主题模型，它可以将文档集 中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。</p><p>人类是怎么生成文档的呢？首先先列出几个主题，然后以一定的概率选择主题，以一定的概率选择这个主题包含的词汇，最终组合成一篇文章。如下图所示(其中不同颜色的词语分别对应上图中不同主题下的词)。</p><p><img src=/imgs/img-lazy-loading.gif data-src=http://wx4.sinaimg.cn/mw690/00630Defgy1g5f9yubudij30gh05rq4o.jpg alt></p><p>那么LDA就是跟这个反过来：<strong>根据给定的一篇文档，反推其主题分布。</strong></p><p>在LDA模型中，一篇文档生成的方式如下：</p><ul><li>从狄利克雷分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha alt>中取样生成文档 i 的主题分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5ctheta_i alt>。</li><li>从主题的多项式分布<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5ctheta_i alt> 中取样生成文档i第 j 个词的主题<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z_%7bi,j%7d alt>。</li><li>从狄利克雷分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta alt>中取样生成主题 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z_%7bi,j%7d alt> 对应的词语分布<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cphi_%7bz_%7bi,j%7d%7d alt>。</li><li>从词语的多项式分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cphi_%7bz_%7bi,j%7d%7d alt>中采样最终生成词语 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?w_%7bi,j%7d alt>。</li></ul><p>其中，类似Beta分布是二项式分布的共轭先验概率分布，而狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布。此外，LDA的图模型结构如下图所示（类似贝叶斯网络结构）：</p><p><img src=/imgs/img-lazy-loading.gif data-src=http://wx2.sinaimg.cn/mw690/00630Defgy1g5facicxbej30bi0d30t5.jpg alt></p><h3 id=5个分布的理解>5个分布的理解</h3><p>先解释一下以上出现的概念。</p><ol><li><p><strong>二项分布（Binomial distribution）</strong></p><p>二项分布是从伯努利分布推进的。伯努利分布，又称两点分布或0-1分布，是一个离散型的随机分布，其中的随机变量只有两类取值，非正即负{+，-}。而二项分布即重复n次的伯努利试验，记为 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?X%5csim_%7b%7db%28n,p%29 alt>。简言之，<strong>只做一次实验，是伯努利分布，重复做了n次，是二项分布。</strong></p></li><li><p><strong>多项分布</strong></p><p>是二项分布扩展到多维的情况。多项分布是指单次试验中的随机变量的取值不再是0-1的，而是有多种离散值可能（1,2,3&mldr;,k）。比如投掷6个面的骰子实验，N次实验结果服从K=6的多项分布。其中：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?%5csum_%7bi=1%7d%5e%7bk%7dp_i=1,p_i%3e0" alt></p></li><li><p><strong>共轭先验分布</strong></p><p>在
<a href=https://baike.baidu.com/item/%e8%b4%9d%e5%8f%b6%e6%96%af%e7%bb%9f%e8%ae%a1/3431194 title=贝叶斯统计 rel="noopener external nofollow noreferrer" target=_blank class=exturl>贝叶斯统计
<i class="fa fa-external-link-alt"></i>
</a>中，如果
<a href=https://baike.baidu.com/item/%e5%90%8e%e9%aa%8c%e5%88%86%e5%b8%83/2022914 title=后验分布 rel="noopener external nofollow noreferrer" target=_blank class=exturl>后验分布
<i class="fa fa-external-link-alt"></i>
</a>与
<a href=https://baike.baidu.com/item/%e5%85%88%e9%aa%8c%e5%88%86%e5%b8%83/7513047 title=先验分布 rel="noopener external nofollow noreferrer" target=_blank class=exturl>先验分布
<i class="fa fa-external-link-alt"></i>
</a>属于同类，则先验分布与后验分布被称为<strong>共轭分布</strong>，而先验分布被称为似然函数的<strong>共轭先验</strong>。</p></li><li><p><strong>Beta分布</strong></p><p>二项分布的共轭先验分布。给定参数 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha%3e0 alt> 和 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta%3e0 alt>，取值范围为[0,1]的随机变量 x 的概率密度函数：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?f%28x;%5calpha,%5cbeta%29=%5cfrac%7b1%7d%7bB%28%5calpha,%5cbeta%29%7dx%5e%7b%5calpha-1%7d%281-x%29%5e%7b%5cbeta-1%7d" alt></p><p>其中：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?%5cfrac%7b1%7d%7bB%28%5calpha,%5cbeta%29%7d=%5cfrac%7b%5cGamma%28%5calpha+%5cbeta%29%7d%7b%5cGamma%28%5calpha%29%5cGamma%28%5cbeta%29%7d" alt></p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?%5cGamma%28z%29=%5cint_%7b0%7d%5e%7b%5cinfty%7dt%5e%7bz-1%7de%5e%7b-t%7ddt" alt></p><p><strong>注：这便是所谓的gamma函数，下文会具体阐述。</strong></p></li><li><p><strong>狄利克雷分布</strong></p><p>是beta分布在高维度上的推广。Dirichlet分布的的密度函数形式跟beta分布的密度函数如出一辙：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?f%28x_1,x_2,...,x_k;%5calpha_1,%5calpha_2,...,%5calpha_k%29=%5cfrac%7b1%7d%7bB%28%5calpha%29%7d%5cprod_%7bi=1%7d%5e%7bk%7dx_i%5e%7b%5calpha%5ei-1%7d" alt></p><p>其中</p><p><img src=/imgs/img-lazy-loading.gif data-src=http://wx4.sinaimg.cn/mw690/00630Defgy1g5fiqijle0j30bj02h0so.jpg alt></p></li></ol><p>至此，我们可以看到二项分布和多项分布很相似，Beta分布和Dirichlet 分布很相似。</p><p>如果想要深究其原理可以参考：
<a href=https://blog.csdn.net/v_july_v/article/details/41209515 title=通俗理解LDA主题模型 rel="noopener external nofollow noreferrer" target=_blank class=exturl>通俗理解LDA主题模型
<i class="fa fa-external-link-alt"></i>
</a>，也可以先往下走，最后在回过头来看详细的公式，就更能明白了。</p><p>总之，<strong>可以得到以下几点信息。</strong></p><ul><li><p>beta分布是二项式分布的共轭先验概率分布：对于非负实数 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha alt>和 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta alt>，我们有如下关系：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?Beta%28p%7c%5calpha,%5cbeta%29+Count%28m_1,m_2%29=Beta%28p%7c%5calpha+m_1,%5cbeta+m_2%29" alt></p><p>其中 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%28m_1,m_2%29 alt>对应的是二项分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?B%28m_1+m_2,p%29 alt>的记数。针对于这种观测到的数据符合二项分布，参数的先验分布和后验分布都是Beta分布的情况，就是Beta-Binomial 共轭。”</p></li><li><p>狄利克雷分布（Dirichlet分布）是多项式分布的共轭先验概率分布，一般表达式如下：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?Dir%28%5cvec%7bp%7d%7c%5cvec%5calpha%29+MultCount%28%5cvec%7bm%7d%29=Dir%28p%7c%5cvec%7b%5calpha%7d+%5cvec%7bm%7d%29" alt></p><p>针对于这种观测到的数据符合多项分布，参数的先验分布和后验分布都是Dirichlet 分布的情况，就是 Dirichlet-Multinomial 共轭。 ”</p></li><li><p>贝叶斯派思考问题的固定模式：</p><p>先验分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cpi%28%5ctheta%29 alt>+ 样本信息<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?X alt> = 后验分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cpi%28%5ctheta%7cx%29 alt>。</p></li></ul><h3 id=3个基础模型的理解>3个基础模型的理解</h3><p>在讲LDA模型之前，再循序渐进理解基础模型：Unigram model、mixture of unigrams model，以及跟LDA最为接近的pLSA模型。为了方便描述，首先定义一些变量：</p><ul><li><img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?w alt>表示词，<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?V alt>示所有单词的个数（固定值）。</li><li><img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z alt> 表示主题，<img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?k alt>主题的个数（预先给定，固定值）。</li><li><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?D=%28W_1,...,W_M%29" alt> 表示语料库，其中的M是语料库中的文档数（固定值）。</li><li><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?W=%28w_1,w_2,...,w_N%29" alt>表示文档，其中的N表示一个文档中的词数（随机变量）。</li></ul><ol><li><p><strong>Unigram model</strong></p><p>对于文档 <img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?W=%28w_1,w_2,...,w_N%29" alt>，用 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?p%28w_n%29 alt>表示词 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?w_n alt>的先验概率，生成文档w的概率为：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?p%28W%29=%5cprod_%7bn=1%7d%5e%7bN%7dp%28w_n%29" alt></p></li><li><p><strong>Mixture of unigrams model</strong></p><p>该模型的生成过程是：给某个文档先选择一个主题z,再根据该主题生成文档，该文档中的所有词都来自一个主题。假设主题有 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z_1,...,z_n alt>，生成文档w的概率为：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?p%28W%29=p%28z_1%29%5cprod_%7bn=1%7d%5e%7bN%7dp%28w_n%7cz_1%29+...+p%28z_k%29%5cprod_%7bn=1%7d%5e%7bN%7dp%28w_n%7cz_k%29=%5csum_%7bz%7dp%28z%29%5cprod_%7bn=1%7d%5e%7bN%7dp%28w_n%7cz%29" alt></p></li><li><p><strong>PLSA模型</strong></p><p>理解了pLSA模型后，到LDA模型也就一步之遥——给pLSA加上贝叶斯框架，便是LDA。</p><p>在上面的Mixture of unigrams model中，我们假定一篇文档只有一个主题生成，可实际中，一篇文章往往有多个主题，只是这多个主题各自在文档中出现的概率大小不一样。比如介绍一个国家的文档中，往往会分别从教育、经济、交通等多个主题进行介绍。那么在pLSA中，文档是怎样被生成的呢？</p><p>假定你一共有K个可选的主题，有V个可选的词，<strong>咱们来玩一个扔骰子的游戏。</strong></p><p>**一、**假设你每写一篇文档会制作一颗K面的“文档-主题”骰子（扔此骰子能得到K个主题中的任意一个），和K个V面的“主题-词项” 骰子（每个骰子对应一个主题，K个骰子对应之前的K个主题，且骰子的每一面对应要选择的词项，V个面对应着V个可选的词）。</p><p>比如可令K=3，即制作1个含有3个主题的“文档-主题”骰子，这3个主题可以是：教育、经济、交通。然后令V = 3，制作3个有着3面的“主题-词项”骰子，其中，教育主题骰子的3个面上的词可以是：大学、老师、课程，经济主题骰子的3个面上的词可以是：市场、企业、金融，交通主题骰子的3个面上的词可以是：高铁、汽车、飞机。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155351982937840324.png alt></p><p>**二、**每写一个词，先扔该“文档-主题”骰子选择主题，得到主题的结果后，使用和主题结果对应的那颗“主题-词项”骰子，扔该骰子选择要写的词。</p><p>先扔“文档-主题”的骰子，假设（以一定的概率）得到的主题是教育，所以下一步便是扔教育主题筛子，（以一定的概率）得到教育主题筛子对应的某个词：大学。</p><p>上面这个投骰子产生词的过程简化下便是：<strong>“先以一定的概率选取主题，再以一定的概率选取词”。</strong></p><p>**三、**最后，你不停的重复扔“文档-主题”骰子和”主题-词项“骰子，重复N次（产生N个词），完成一篇文档，重复这产生一篇文档的方法M次，则完成M篇文档。</p><p><strong>上述过程抽象出来即是PLSA的文档生成模型。在这个过程中，我们并未关注词和词之间的出现顺序，所以pLSA是一种词袋方法。生成文档的整个过程便是选定文档生成主题，确定主题生成词。</strong></p><p>反过来，既然文档已经产生，那么如何根据已经产生好的文档反推其主题呢？这个利用看到的文档推断其隐藏的主题（分布）的过程（其实也就是产生文档的逆过程），便是<strong>主题建模的目的：自动地发现文档集中的主题（分布）。</strong></p><p>文档d和词w是我们得到的样本，可观测得到，所以对于任意一篇文档，其 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28w_j%7cd_i%29 alt>是已知的。从而可以根据大量已知的文档-词项信息 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28w_j%7cd_i%29 alt>，训练出文档-主题 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28z_k%7cd_i%29 alt>和主题-词项 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28w_j%7cz_k%29 alt>，如下公式所示：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?P%28w_j%7cd_i%29=%5csum_%7bk=1%7d%5e%7bK%7dP%28w_j%7cz_k%29P%28z_k%7cd_i%29" alt></p><p>故得到文档中每个词的生成概率为：</p><p><img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?P%28d_i,w_j%29=P%28d_i%29P%28w_j%7cd_i%29=P%28d_i%29%5csum_%7bk=1%7d%5e%7bK%7dP%28w_j%7cz_k%29P%28z_k%7cd_i%29" alt></p><p>由于 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28d_i%29 alt>可事先计算求出，而 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28w_j%7cz_k%29%5e%7b%7d alt>和 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28z_k%7cd_i%29 alt>未知，所以 <img src=/imgs/img-lazy-loading.gif data-src="https://latex.codecogs.com/gif.latex?%5ctheta=%28P%28w_j%7cz_k%29,P%28z_k%7cd_i%29%29" alt>就是我们要估计的参数（值），通俗点说，就是要最大化这个θ。</p><p>用什么方法进行估计呢，常用的参数估计方法有极大似然估计MLE、最大后验证估计MAP、贝叶斯估计等等。因为该待估计的参数中含有隐变量z，所以我们可以考虑EM算法。详细的EM算法可以参考之前写过的
<a href=https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM title=EM算法 rel="noopener external nofollow noreferrer" target=_blank class=exturl>EM算法
<i class="fa fa-external-link-alt"></i></a> 章节。</p></li></ol><h3 id=lda模型>LDA模型</h3><p>事实上，理解了pLSA模型，也就差不多快理解了LDA模型，因为LDA就是在pLSA的基础上加层贝叶斯框架，即LDA就是pLSA的贝叶斯版本（正因为LDA被贝叶斯化了，所以才需要考虑历史先验知识，才加的两个先验参数）。</p><p>下面，咱们对比下本文开头所述的LDA模型中一篇文档生成的方式是怎样的：</p><ul><li>按照先验概率 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?P%28d_i%29 alt>选择一篇文档 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?d_i alt>。</li><li>从狄利克雷分布（即Dirichlet分布） <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha alt>中取样生成文档 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?d_i alt>的主题分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5ctheta_i alt>，换言之，主题分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5ctheta_i alt>由超参数为 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha alt>的Dirichlet分布生成。</li><li>从主题的多项式分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5ctheta_i alt>中取样生成文档 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?d_i alt>第 j 个词的主题 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z_%7bi,j%7d alt>。</li><li>从狄利克雷分布（即Dirichlet分布） <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta alt>中取样生成主题 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?z_%7bi,j%7d alt>对应的词语分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cphi_%7bz_%7bi,j%7d%7d alt>，换言之，词语分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cphi_%7bz_%7bi,j%7d%7d alt> 由参数为 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta alt>的Dirichlet分布生成。</li><li>从词语的多项式分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cphi_%7bz_%7bi,j%7d%7d alt>中采样最终生成词语 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?w_%7bi,j%7d alt>。</li></ul><p>LDA中，选主题和选词依然都是两个随机的过程，依然可能是先从主题分布{教育：0.5，经济：0.3，交通：0.2}中抽取出主题：教育，然后再从该主题对应的词分布{大学：0.5，老师：0.3，课程：0.2}中抽取出词：大学。</p><p>那PLSA跟LDA的区别在于什么地方呢？区别就在于：</p><p>PLSA中，主题分布和词分布是唯一确定的，能明确的指出主题分布可能就是{教育：0.5，经济：0.3，交通：0.2}，词分布可能就是{大学：0.5，老师：0.3，课程：0.2}。
但在LDA中，主题分布和词分布不再唯一确定不变，即无法确切给出。例如主题分布可能是{教育：0.5，经济：0.3，交通：0.2}，也可能是{教育：0.6，经济：0.2，交通：0.2}，到底是哪个我们不再确定（即不知道），因为它是随机的可变化的。但再怎么变化，也依然服从一定的分布，<strong>即主题分布跟词分布由Dirichlet先验随机确定。正因为LDA是PLSA的贝叶斯版本，所以主题分布跟词分布本身由先验知识随机给定。</strong></p><p>换言之，LDA在pLSA的基础上给这两参数 $(P(z_k|d_i)、P(w_j|z_k)))$加了两个先验分布的参数（贝叶斯化）：一个主题分布的先验分布Dirichlet分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5calpha alt>，和一个词语分布的先验分布Dirichlet分布 <img src=/imgs/img-lazy-loading.gif data-src=https://latex.codecogs.com/gif.latex?%5cbeta alt>。</p><p>综上，LDA真的只是pLSA的贝叶斯版本，文档生成后，两者都要根据文档去推断其主题分布和词语分布（即两者本质都是为了估计给定文档生成主题，给定主题生成词语的概率），只是用的参数推断方法不同，在pLSA中用极大似然估计的思想去推断两未知的固定参数，而LDA则把这两参数弄成随机变量，且加入dirichlet先验。</p><p>所以，pLSA跟LDA的本质区别就在于它们去估计未知参数所采用的思想不同，前者用的是频率派思想，后者用的是贝叶斯派思想。</p><p>LDA参数估计：<strong>Gibbs采样</strong>，详见文末的参考文献。</p><h1 id=怎么确定lda的topic个数>怎么确定LDA的topic个数？</h1><ol><li>基于经验 主观判断、不断调试、操作性强、最为常用。</li><li>基于困惑度（主要是比较两个模型之间的好坏）。</li><li>使用Log-边际似然函数的方法，这种方法也挺常用的。</li><li>非参数方法：Teh提出的基于狄利克雷过程的HDP法。</li><li>基于主题之间的相似度：计算主题向量之间的余弦距离，KL距离等。</li></ol><h1 id=如何用主题模型解决推荐系统中的冷启动问题>如何用主题模型解决推荐系统中的冷启动问题？</h1><p>推荐系统中的冷启动问题是指在没有大量用户数据的情况下如何给用户进行个性化推荐，目的是最优化点击率、转化率或用户 体验（用户停留时间、留存率等）。冷启动问题一般分为用户冷启动、物品冷启动和系统冷启动三大类。</p><ul><li>用户冷启动是指对一个之前没有行为或行为极少的新用户进行推荐；</li><li>物品冷启动是指为一个新上市的商品或电影（这时没有与之相关的 评分或用户行为数据）寻找到具有潜在兴趣的用户；</li><li>系统冷启动是指如何为一个 新开发的网站设计个性化推荐系统。</li></ul><p>解决冷启动问题的方法一般是基于内容的推荐。以Hulu的场景为例，对于用 户冷启动来说，我们希望根据用户的注册信息（如：年龄、性别、爱好等）、搜 索关键词或者合法站外得到的其他信息（例如用户使用Facebook账号登录，并得 到授权，可以得到Facebook中的朋友关系和评论内容）来推测用户的兴趣主题。 得到用户的兴趣主题之后，我们就可以找到与该用户兴趣主题相同的其他用户， 通过他们的历史行为来预测用户感兴趣的电影是什么。</p><p>同样地，对于物品冷启动问题，我们也可以根据电影的导演、演员、类别、关键词等信息推测该电影所属于的主题，然后基于主题向量找到相似的电影，并将新电影推荐给以往喜欢看这 些相似电影的用户。<strong>可以使用主题模型（pLSA、LDA等）得到用户和电影的主题。</strong></p><p>以用户为例，我们将每个用户看作主题模型中的一篇文档，用户对应的特征 作为文档中的单词，这样每个用户可以表示成一袋子特征的形式。通过主题模型 学习之后，经常共同出现的特征将会对应同一个主题，同时每个用户也会相应地 得到一个主题分布。每个电影的主题分布也可以用类似的方法得到。</p><p>**那么如何解决系统冷启动问题呢？**首先可以得到每个用户和电影对应的主题向量，除此之外，还需要知道用户主题和电影主题之间的偏好程度，也就是哪些主题的用户可能喜欢哪些主题的电影。当系统中没有任何数据时，我们需要一些先验知识来指定，并且由于主题的数目通常比较小，随着系统的上线，收集到少量的数据之后我们就可以对主题之间的偏好程度得到一个比较准确的估计。</p><h1 id=代码>代码</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import pandas as pd
</span></span><span style=display:flex><span>from gensim import corpora, models, similarities
</span></span><span style=display:flex><span>import gensim
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df = pd.read_csv(&#34;HillaryEmails.csv&#34;)
</span></span><span style=display:flex><span># 原邮件数据中有很多Nan的值，直接扔了。
</span></span><span style=display:flex><span>df = df[[&#39;Id&#39;,&#39;ExtractedBodyText&#39;]].dropna()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df.head().append(df.tail())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def clean_email_text(text):
</span></span><span style=display:flex><span>    text = text.replace(&#39;\n&#39;,&#34; &#34;) #新行，我们是不需要的
</span></span><span style=display:flex><span>    text = re.sub(r&#34;-&#34;, &#34; &#34;, text) #把 &#34;-&#34; 的两个单词，分开。（比如：pre-processing ==&gt; pre processing）
</span></span><span style=display:flex><span>    text = re.sub(r&#34;\d+/\d+/\d+&#34;, &#34;&#34;, text) #日期，对主体模型没什么意义
</span></span><span style=display:flex><span>    text = re.sub(r&#34;[0-2]?[0-9]:[0-6][0-9]&#34;, &#34;&#34;, text) #时间，没意义
</span></span><span style=display:flex><span>    text = re.sub(r&#34;[\w]+@[\.\w]+&#34;, &#34;&#34;, text) #邮件地址，没意义
</span></span><span style=display:flex><span>    text = re.sub(r&#34;/[a-zA-Z]*[:\//\]*[A-Za-z0-9\-_]+\.+[A-Za-z0-9\.\/%&amp;=\?\-_]+/i&#34;, &#34;&#34;, text) #网址，没意义
</span></span><span style=display:flex><span>    pure_text = &#39;&#39;
</span></span><span style=display:flex><span>    # 以防还有其他特殊字符（数字）等等，我们直接把他们loop一遍，过滤掉
</span></span><span style=display:flex><span>    for letter in text:
</span></span><span style=display:flex><span>        # 只留下字母和空格
</span></span><span style=display:flex><span>        if letter.isalpha() or letter==&#39; &#39;:
</span></span><span style=display:flex><span>            pure_text += letter
</span></span><span style=display:flex><span>    # 再把那些去除特殊字符后落单的单词，直接排除。
</span></span><span style=display:flex><span>    # 我们就只剩下有意义的单词了。
</span></span><span style=display:flex><span>    text = &#39; &#39;.join(word for word in pure_text.split() if len(word)&gt;1)
</span></span><span style=display:flex><span>    return text
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>docs = df[&#39;ExtractedBodyText&#39;]
</span></span><span style=display:flex><span>docs = docs.apply(lambda s: clean_email_text(s))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>doclist = docs.values
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>stoplist = [&#39;very&#39;, &#39;ourselves&#39;, &#39;am&#39;, &#39;doesn&#39;, &#39;through&#39;, &#39;me&#39;, &#39;against&#39;, &#39;up&#39;, &#39;just&#39;, &#39;her&#39;, &#39;ours&#39;, 
</span></span><span style=display:flex><span>            &#39;couldn&#39;, &#39;because&#39;, &#39;is&#39;, &#39;isn&#39;, &#39;it&#39;, &#39;only&#39;, &#39;in&#39;, &#39;such&#39;, &#39;too&#39;, &#39;mustn&#39;, &#39;under&#39;, &#39;their&#39;, 
</span></span><span style=display:flex><span>            &#39;if&#39;, &#39;to&#39;, &#39;my&#39;, &#39;himself&#39;, &#39;after&#39;, &#39;why&#39;, &#39;while&#39;, &#39;can&#39;, &#39;each&#39;, &#39;itself&#39;, &#39;his&#39;, &#39;all&#39;, &#39;once&#39;, 
</span></span><span style=display:flex><span>            &#39;herself&#39;, &#39;more&#39;, &#39;our&#39;, &#39;they&#39;, &#39;hasn&#39;, &#39;on&#39;, &#39;ma&#39;, &#39;them&#39;, &#39;its&#39;, &#39;where&#39;, &#39;did&#39;, &#39;ll&#39;, &#39;you&#39;, 
</span></span><span style=display:flex><span>            &#39;didn&#39;, &#39;nor&#39;, &#39;as&#39;, &#39;now&#39;, &#39;before&#39;, &#39;those&#39;, &#39;yours&#39;, &#39;from&#39;, &#39;who&#39;, &#39;was&#39;, &#39;m&#39;, &#39;been&#39;, &#39;will&#39;, 
</span></span><span style=display:flex><span>            &#39;into&#39;, &#39;same&#39;, &#39;how&#39;, &#39;some&#39;, &#39;of&#39;, &#39;out&#39;, &#39;with&#39;, &#39;s&#39;, &#39;being&#39;, &#39;t&#39;, &#39;mightn&#39;, &#39;she&#39;, &#39;again&#39;, &#39;be&#39;, 
</span></span><span style=display:flex><span>            &#39;by&#39;, &#39;shan&#39;, &#39;have&#39;, &#39;yourselves&#39;, &#39;needn&#39;, &#39;and&#39;, &#39;are&#39;, &#39;o&#39;, &#39;these&#39;, &#39;further&#39;, &#39;most&#39;, &#39;yourself&#39;, 
</span></span><span style=display:flex><span>            &#39;having&#39;, &#39;aren&#39;, &#39;here&#39;, &#39;he&#39;, &#39;were&#39;, &#39;but&#39;, &#39;this&#39;, &#39;myself&#39;, &#39;own&#39;, &#39;we&#39;, &#39;so&#39;, &#39;i&#39;, &#39;does&#39;, &#39;both&#39;, 
</span></span><span style=display:flex><span>            &#39;when&#39;, &#39;between&#39;, &#39;d&#39;, &#39;had&#39;, &#39;the&#39;, &#39;y&#39;, &#39;has&#39;, &#39;down&#39;, &#39;off&#39;, &#39;than&#39;, &#39;haven&#39;, &#39;whom&#39;, &#39;wouldn&#39;, 
</span></span><span style=display:flex><span>            &#39;should&#39;, &#39;ve&#39;, &#39;over&#39;, &#39;themselves&#39;, &#39;few&#39;, &#39;then&#39;, &#39;hadn&#39;, &#39;what&#39;, &#39;until&#39;, &#39;won&#39;, &#39;no&#39;, &#39;about&#39;, 
</span></span><span style=display:flex><span>            &#39;any&#39;, &#39;that&#39;, &#39;for&#39;, &#39;shouldn&#39;, &#39;don&#39;, &#39;do&#39;, &#39;there&#39;, &#39;doing&#39;, &#39;an&#39;, &#39;or&#39;, &#39;ain&#39;, &#39;hers&#39;, &#39;wasn&#39;, 
</span></span><span style=display:flex><span>            &#39;weren&#39;, &#39;above&#39;, &#39;a&#39;, &#39;at&#39;, &#39;your&#39;, &#39;theirs&#39;, &#39;below&#39;, &#39;other&#39;, &#39;not&#39;, &#39;re&#39;, &#39;him&#39;, &#39;during&#39;, &#39;which&#39;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>texts = [[word for word in doc.lower().split() if word not in stoplist] for doc in doclist]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>texts[0]
</span></span><span style=display:flex><span># [&#39;thursday&#39;,  &#39;march&#39;,  &#39;pm&#39;, &#39;latest&#39;, &#39;syria&#39;,  &#39;aiding&#39;,  &#39;qaddafi&#39;, &#39;march&#39;, &#39;hillary&#39;]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 用词袋的方法，把每个单词用一个数字index指代，并把我们的原文本变成一条长长的数组：
</span></span><span style=display:flex><span>dictionary = corpora.Dictionary(texts)
</span></span><span style=display:flex><span>corpus = [dictionary.doc2bow(text) for text in texts]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>corpus[13]
</span></span><span style=display:flex><span># [(51, 1), (505, 1), (506, 1), (507, 1), (508, 1)]
</span></span><span style=display:flex><span># 这个列表告诉我们，第14（从0开始是第一）个邮件中，一共6个有意义的单词（经过我们的文本预处理，并去除了停止词后）
</span></span><span style=display:flex><span># 其中，36号单词出现1次，505号单词出现1次，以此类推。。。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=20)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lda.print_topic(10, topn=5)
</span></span><span style=display:flex><span># 我们可以看到，第10号分类，其中最常出现的单词是：
</span></span><span style=display:flex><span># &#39;0.011*&#34;sent&#34; + 0.010*&#34;see&#34; + 0.010*&#34;good&#34; + 0.009*&#34;think&#34; + 0.008*&#34;blackberry&#34;&#39;
</span></span><span style=display:flex><span>lda.print_topics(num_topics=20, num_words=5)
</span></span><span style=display:flex><span># [(0,
</span></span><span style=display:flex><span>#   &#39;0.009*&#34;said&#34; + 0.009*&#34;would&#34; + 0.006*&#34;democrats&#34; + 0.006*&#34;party&#34; + 0.006*&#34;republicans&#34;&#39;),
</span></span><span style=display:flex><span>#  (1,
</span></span><span style=display:flex><span>#   &#39;0.009*&#34;percent&#34; + 0.008*&#34;youll&#34; + 0.008*&#34;calling&#34; + 0.006*&#34;know&#34; + 0.005*&#34;ri&#34;&#39;),
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
降维-LDA线性判别分析</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/ title=降维-LDA线性判别分析>/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/ rel=next title=非线性模型-朴素贝叶斯><i class="fa fa-chevron-left"></i> 非线性模型-朴素贝叶斯</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/ rel=prev title=降维-PCA主成分分析>降维-PCA主成分分析
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>