<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="code-Spark"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="code-Spark"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/code-spark/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2023-01-01 14:28:53 +0800 CST"><meta property="article:modified_time" content="2023-01-01 14:28:53 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"code-spark","permalink":"/post/code-spark/","title":"code-Spark","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>code-Spark - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>109</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><ul><li><a href=#parallelize>parallelize</a></li><li><a href=#textfile>textFile</a></li><li><a href=#getnumpartitions>getNumPartitions</a></li></ul></li><li><a href=#transformation算子>Transformation算子</a><ul><li><a href=#flatmap>flatMap</a></li><li><a href=#reducebykey>reduceByKey</a></li><li><a href=#groupby>groupBy</a></li><li><a href=#groupbykey>groupByKey</a></li><li><a href=#filter>filter</a></li><li><a href=#distinct>distinct</a></li><li><a href=#union>union</a></li><li><a href=#join>join</a></li><li><a href=#glom>glom</a></li><li><a href=#sortby>sortBy</a></li><li><a href=#sortbykey>sortByKey</a></li></ul></li><li><a href=#分区操作算子>分区操作算子</a><ul><li><a href=#mappartitions>mapPartitions</a></li><li><a href=#foreachpartition>foreachPartition</a></li><li><a href=#partitionby>partitionBy</a></li><li><a href=#partition>partition</a></li><li><a href=#coalesce>coalesce</a></li><li><a href=#mapvalues>mapValues</a></li><li><a href=#join-1>join</a></li></ul></li><li><a href=#action算子>Action算子</a><ul><li><a href=#countbykey>countByKey</a></li><li><a href=#collect>collect</a></li><li><a href=#reduce>reduce</a></li><li><a href=#fold>fold</a></li><li><a href=#first>first</a></li><li><a href=#take>take</a></li><li><a href=#count>count</a></li><li><a href=#takesample>takeSample</a></li><li><a href=#top>top</a></li><li><a href=#takeordered>takeOrdered</a></li><li><a href=#foreach>foreach</a></li><li><a href=#saveastextfile>saveAsTextFile</a></li></ul></li></ul><ul><li><a href=#缓存>缓存</a></li><li><a href=#checkpoint>CheckPoint</a></li></ul><ul><li><a href=#广播变量>广播变量</a></li><li><a href=#累加器>累加器</a></li></ul><ul><li><a href=#dag>DAG</a><ul><li><a href=#spark是怎么做内存计算的dag的作用stage阶段划分的作用>Spark是怎么做内存计算的？DAG的作用？Stage阶段划分的作用？</a></li><li><a href=#spark为什么比mapreduce快>Spark为什么比MapReduce快</a></li></ul></li><li><a href=#spark的并行>Spark的并行</a></li><li><a href=#spark程序的调度>Spark程序的调度</a></li></ul><ul><li><a href=#sparksql和hive的异同>SparkSQL和Hive的异同</a></li><li><a href=#sparksql的数据抽象>SparkSQL的数据抽象</a></li><li><a href=#读取文件>读取文件</a></li><li><a href=#方法>方法</a></li><li><a href=#保存>保存</a></li><li><a href=#jdbc读写数据库>JDBC读写数据库</a></li><li><a href=#udf函数>UDF函数</a></li><li><a href=#sparksql的运行流程>SparkSQL的运行流程</a></li><li><a href=#spark-on-hive>Spark On Hive</a></li><li><a href=#分布式sql执行引擎>分布式SQL执行引擎</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>109</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>7</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=413242></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=884></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-01-01T14:28:53+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/code-spark/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="code-Spark"><meta itemprop=description content="Spark **Spark是一款分布式内存计算的统一分析引擎。其特点就是对任意类型的数据进行自定义计算。**Spark的适用面非常广泛，所以，被称之为"></span><header class=post-header><h1 class=post-title itemprop="name headline">code-Spark
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/code-Spark.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-01-01 14:28:53 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-01-01 14:28:53 +0800 CST">2023-01-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/code itemprop=url rel=index><span itemprop=name>code</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>15851</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>32分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/code-spark/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h1 id=spark>Spark</h1><p>**Spark是一款分布式内存计算的统一分析引擎。其特点就是对任意类型的数据进行自定义计算。**Spark的适用面非常广泛，所以，被称之为 <strong>统一的（适用面广）的分析引擎（数据处理）</strong></p><p>Spark可以计算：结构化、半结构化、非结构化等各种类型的数据结构，同时也支持使用Python、Java、Scala、R以及SQL语言去开发应用程序计算数据。</p><p>RDD 是一种分布式内存抽象，其使得程序员能够在大规模集群中做内存运算，并且有一定的容错方式。而这也
是整个 Spark 的核心数据结构，Spark 整个平台都围绕着RDD进行。</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Hadoop</th><th style=text-align:center>Spark</th></tr></thead><tbody><tr><td style=text-align:center>类型</td><td style=text-align:center>基础平台, 包含计算, 存储, 调度</td><td style=text-align:center>纯计算工具（分布式）</td></tr><tr><td style=text-align:center>场景</td><td style=text-align:center>海量数据批处理（磁盘迭代计算）</td><td style=text-align:center>海量数据的批处理（内存迭代计算、交互式计算）、海量数据流计算</td></tr><tr><td style=text-align:center>价格</td><td style=text-align:center>对机器要求低, 便宜</td><td style=text-align:center>对内存有要求, 相对较贵</td></tr><tr><td style=text-align:center>编程范式</td><td style=text-align:center>Map+Reduce, API 较为底层, 算法适应性差</td><td style=text-align:center>RDD组成DAG有向无环图, API 较为顶层, 方便使用</td></tr><tr><td style=text-align:center>数据存储结构</td><td style=text-align:center>MapReduce中间计算结果在HDFS磁盘上, 延迟大</td><td style=text-align:center>RDD中间运算结果在内存中 , 延迟小</td></tr><tr><td style=text-align:center>运行方式</td><td style=text-align:center>Task以进程方式维护, 任务启动慢</td><td style=text-align:center>Task以线程方式维护, 任务启动快，可批量创建提高并行能力</td></tr></tbody></table><p>尽管Spark相对于Hadoop而言具有较大优势，但Spark并不能完全替代Hadoop</p><ul><li><p>在计算层面，Spark相比较MR（MapReduce）有巨大的性能优势，但至今仍有许多计算工具基于MR构架，比如非常成熟的Hive</p></li><li><p>Spark仅做计算，而Hadoop生态圈不仅有计算（MR）也有存储（HDFS）和资源管理调度（YARN），HDFS和YARN仍是许多大数据体系的核心架构。</p></li></ul><h1 id=spark四大特点>Spark四大特点</h1><ul><li><p>速度快</p><p>由于Apache Spark支持内存计算，并且通过DAG（有向无环图）执行引擎支持无环数据流，所以官方宣称其在内存中的运算速度要比Hadoop的MapReduce快100倍，在硬盘中要快10倍。</p><p>Spark 借鉴了 MapReduce 思想发展而来，保留了其分布式并行计算的优点并改进了其明显的缺陷。让中间数据存储在内存中提高了运行速度、并提供丰富的操作数据的API提高了开发速度。</p></li><li><p>易于使用</p><p>Spark 的版本已经更新到 Spark 3.2.0（截止日期2021.10.13），支持了包括 Java、Scala、Python 、R和SQL语言在内的多种语言。为了兼容Spark2.x企业级应用场景，Spark仍然持续更新Spark2版本。</p></li><li><p>通用性强</p><p>在 Spark 的基础上，Spark 还提供了包括Spark SQL、Spark Streaming、MLib 及GraphX在内的多个工具库，我们可以在一个应用中无缝
地使用这些工具库。</p></li><li><p>运行方式</p></li></ul><p>Spark 支持多种运行方式，包括在 Hadoop 和 Mesos 上，也支持 Standalone的独立运行模式，同时也可以运行在云Kubernetes（Spark 2.3开始支持）上。对于数据源而言，Spark 支持从HDFS、HBase、Cassandra 及 Kafka 等多种途径获取数据。</p><p>Hadoop的基于进程的计算和Spark基于线程方式优缺点？</p><blockquote><p>答案：Hadoop中的MR中每个map/reduce task都是一个java进程方式运行，好处在于进程之间是互相独立的，每个task独享进程资源，没有互相干扰，监控方便，但是问题在于task之间不方便共享数据，执行效率比较低。比如多个map task读取不同数据源文件需要将数据源加载到每个map task中，造成重复加载和浪费内存。而基于线程的方式计算是为了数据共享和提高执行效率，Spark采用了线程的最小的执行单位，但缺点是线程之间会有资源竞争。</p><ul><li>线程是CPU的基本调度单位</li><li>一个进程一般包含多个线程, 一个进程下的多个线程共享进程的资源</li><li>不同进程之间的线程相互不可见</li><li>线程不能独立执行</li><li>一个线程可以创建和撤销另外一个线程</li></ul></blockquote><p>Spark解决什么问题？</p><blockquote><p>海量数据的计算，可以进行离线批处理以及实时流计算</p></blockquote><p>Spark有哪些模块？</p><blockquote><p>核心SparkCore、SQL计算（SparkSQL）、流计算（SparkStreaming）、图计算（GraphX）、机器学习（MLlib）</p><ul><li>Spark Core：Spark的核心，Spark核心功能均由Spark Core模块提供，是Spark运行的基础。Spark Core以RDD为数据抽象，提供Python、Java、Scala、R语言的API，可以编程进行海量离线数据批处理计算。</li><li>SparkSQL：基于SparkCore之上，提供结构化数据的处理模块。SparkSQL支持以SQL语言对数据进行处理，SparkSQL本身针对离线计算场景。同时基于SparkSQL，Spark提供了Structured Streaming模块，可以以SparkSQL为基础，进行数据的流式计算。</li><li>SparkStreaming：以SparkCore为基础，提供数据的流式计算功能。</li><li>MLlib：以SparkCore为基础，进行机器学习计算，内置了大量的机器学习库和API算法等。方便用户以分布式计算的模式进行机器学习计算。</li><li>GraphX：以SparkCore为基础，进行图计算，提供了大量的图计算API，方便用于以分布式计算模式进行图计算。</li></ul></blockquote><p>Spark特点有哪些？</p><blockquote><p>速度快、使用简单、通用性强、多种模式运行</p></blockquote><p>Spark的运行模式？</p><blockquote><p>• 本地模式
• 集群模式（StandAlone、YARN、K8S）
• 云模式</p><ul><li><p>本地模式（单机）
本地模式就是以一个独立的进程，通过其内部的多个线程来模拟整个Spark运行时环境</p></li><li><p>Standalone模式（集群）
Spark中的各个角色以独立进程的形式存在，并组成Spark集群环境</p></li><li><p>Hadoop YARN模式（集群）
Spark中的各个角色运行在YARN的容器内部，并组成Spark集群环境</p></li><li><p>Kubernetes模式（容器集群）
Spark中的各个角色运行在Kubernetes的容器内部，并组成Spark集群环境</p></li><li><p>云服务模式（运行在云平台上）</p></li></ul></blockquote><p>Spark的运行角色（对比YARN）？</p><blockquote><blockquote><p>从2个层面划分：
资源管理层面：</p><ul><li>管理者： Spark是 Master角色，YARN是 Resourcemanager</li><li>工作中： Spark是 Workers角色，YARN是 Nodemanager</li></ul><p>从任务执行层面：</p><ul><li>某任务管理者： Spark是 Driver角色，YARN是 Applicationmaster</li><li>某任务执行者： Spark是 Executor角色，YARN是容器中运行的具体工作进程</li></ul><p>注：正常情况下Executor是干活的角色，不过在特殊场景下（Local模式）Driver可以即管理又干活</p></blockquote><blockquote><p>YARN主要有4类角色，从2个层面去看：</p><p>资源管理层面</p><ul><li><p>集群资源管理者（Master）：ResourceManager</p></li><li><p>单机资源管理者（Worker）：NodeManager</p></li></ul><p>任务计算层面</p><ul><li>单任务管理者（Master）：ApplicationMaster</li><li>单任务执行者（Worker）：Task（容器内计算框架的工作角色）</li></ul></blockquote></blockquote><h1 id=rdd>RDD</h1><p>RDD（Resilienlt Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，代表一个<strong>不可变、可分区、里面的元素可并行计算</strong>的集合。</p><ul><li>Dataset：一个数据集合，用于存放数据的</li><li>Distributed：RDD中的数据是分布式存储的，可用于分布式计算。</li><li>Resilient：RDD中的数据可以存储在内存中或者磁盘中。</li><li>所有的运算以及操作都建立在 RDD 数据结构的基础之上。</li></ul><p>RDD的五大特性</p><ul><li><p>RDD是有分区的</p><p>RDD的分区是RDD数据存储的最小单位，l一份RDD的数据，本质上是分隔成了多个分区</p></li><li><p>RDD的方法会作用在其所有的分区上</p></li><li><p>RDD之间是有依赖关系(RDD有血缘关系)</p></li><li><p>Key-Value型的RDD可以有分区器</p><p>默认分区器：Hash分区规则，可以手动设置一个分区器(rdd.partitionBy的方法来设置)</p></li><li><p>DD的分区规划，会尽量靠近数据所在的服务器</p><p>在初始RDD（读取数据的时候）规划的时候分区会尽量规划到存储数据所在的服务器上因为这样可以走本地读取，避免网铬读取</p></li></ul><h1 id=算子>算子</h1><p>分布式集合对象上的API称之为算子</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 这里可以选择本地PySpark环境执行Spark代码，也可以使用虚拟机中PySpark环境，通过os可以配置</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;SPARK_HOME&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;/export/servers/spark&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># PYSPARK_PYTHON = &#34;/root/anaconda3/envs/pyspark_env/bin/python&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 当存在多个版本时，不指定很可能会导致出错</span>
</span></span><span style=display:flex><span><span style=color:#75715e># os.environ[&#34;PYSPARK_PYTHON&#34;] = PYSPARK_PYTHON</span>
</span></span><span style=display:flex><span><span style=color:#75715e># os.environ[&#34;PYSPARK_DRIVER_PYTHON&#34;] = PYSPARK_PYTHON</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;PySpark First Program&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># TODO: 当应用运行在集群上的时候，MAIN函数就是Driver Program，必须创建SparkContext对象</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建SparkConf对象，设置应用的配置信息，比如应用名称和应用运行模式</span>
</span></span><span style=display:flex><span>conf <span style=color:#f92672>=</span> SparkConf()<span style=color:#f92672>.</span>setAppName(<span style=color:#e6db74>&#34;miniProject&#34;</span>)<span style=color:#f92672>.</span>setMaster(<span style=color:#e6db74>&#34;local[*]&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># TODO: 构建SparkContext上下文实例对象，读取数据和调度Job执行</span>
</span></span><span style=display:flex><span>sc <span style=color:#f92672>=</span> SparkContext(conf<span style=color:#f92672>=</span>conf)
</span></span></code></pre></div><h3 id=parallelize>parallelize</h3><p>演示通过并行化集合的方式去创建RDD, 本地集合 -> 分布式对象(RDD)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>])
</span></span></code></pre></div><h3 id=textfile>textFile</h3><p>读取数据</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 读取本地文件数据</span>
</span></span><span style=display:flex><span>file_rdd1 <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;../data/input/words.txt&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 加最小分区数参数的测试</span>
</span></span><span style=display:flex><span>file_rdd2 <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;../data/input/words.txt&#34;</span>, <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 最小分区数是参考值, Spark有自己的判断, 你给的太大Spark不会理会</span>
</span></span><span style=display:flex><span>file_rdd3 <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;../data/input/words.txt&#34;</span>, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 读取HDFS文件数据测试</span>
</span></span><span style=display:flex><span>hdfs_rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;hdfs://n1:8020/pydata/input/words.txt&#34;</span>)
</span></span></code></pre></div><h3 id=getnumpartitions>getNumPartitions</h3><p>获取分区数</p><h2 id=transformation算子>Transformation算子</h2><p>定义：RDD的算子，<strong>返回值仍旧是一个RDD的</strong>，称之为转换算子</p><p>特性：这类算子是Lazy懒加载的。如果没action.算子，Transformation算子是不工作的。</p><h3 id=flatmap>flatMap</h3><p>对rdd执行map操作，然后进行解除嵌套操作</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/26/Xsl7TpUPnqAudxa.png alt=quicker_43d4dca9-e229-44d1-b559-f2b6600ceeae.png></p><h3 id=reducebykey>reduceByKey</h3><p>针对KV型RDD,自动按照key分组，然后根据你提供的聚合逻辑，完成<strong>组内数据</strong>(valve)的聚合操作</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/23/WuPSmhzI9igHZb8.png alt=quicker_bf607a63-1c54-43cb-99a6-27b1a0b0cdcd.png></p><h3 id=groupby>groupBy</h3><p>将rdd的数据进行分组</p><p>拿到你的返回值后，将所有相同返回值的放入一个组中。每一个组是一个二元元组，<strong>key就是返回值，所有同组的数据放入一个迭代器对象中作为value</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>result <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>groupBy(<span style=color:#66d9ef>lambda</span> t: t[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>print(result<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> t:(t[<span style=color:#ae81ff>0</span>], list(t[<span style=color:#ae81ff>1</span>])))<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[(<span style=color:#e6db74>&#39;b&#39;</span>, [(<span style=color:#e6db74>&#39;b&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;b&#39;</span>, <span style=color:#ae81ff>2</span>), (<span style=color:#e6db74>&#39;b&#39;</span>, <span style=color:#ae81ff>3</span>)]), (<span style=color:#e6db74>&#39;a&#39;</span>, [(<span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;a&#39;</span>, <span style=color:#ae81ff>1</span>)])]
</span></span></code></pre></div><h3 id=groupbykey>groupByKey</h3><p>针对KV型RDD,自动按照key分组</p><h3 id=filter>filter</h3><p>过滤想要的数据进行保留</p><h3 id=distinct>distinct</h3><p>对RDD数据进行去重，返回新RDD</p><h3 id=union>union</h3><p>2个rdd合并成1个rdd返回，<strong>只合并，不会去重</strong></p><h3 id=join>join</h3><p>对两个RDD执行JOIN操作(可实现SQL的内外连接)<strong>注意：join算子只能用于二元元组</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>    rdd1 <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1001</span>, <span style=color:#e6db74>&#34;zhangsan&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1002</span>, <span style=color:#e6db74>&#34;lisi&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1003</span>, <span style=color:#e6db74>&#34;wangwu&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1004</span>, <span style=color:#e6db74>&#34;zhaoliu&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    rdd2 <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1001</span>, <span style=color:#e6db74>&#34;销售部&#34;</span>),
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>1002</span>, <span style=color:#e6db74>&#34;科技部&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    print(rdd1<span style=color:#f92672>.</span>join(rdd2)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>    print(rdd1<span style=color:#f92672>.</span>leftOuterJoin(rdd2)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>    print(rdd1<span style=color:#f92672>.</span>rightOuterJoin(rdd2)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    [(<span style=color:#ae81ff>1001</span>, (<span style=color:#e6db74>&#39;zhangsan&#39;</span>, <span style=color:#e6db74>&#39;销售部&#39;</span>)), (<span style=color:#ae81ff>1002</span>, (<span style=color:#e6db74>&#39;lisi&#39;</span>, <span style=color:#e6db74>&#39;科技部&#39;</span>))]
</span></span><span style=display:flex><span>[(<span style=color:#ae81ff>1004</span>, (<span style=color:#e6db74>&#39;zhaoliu&#39;</span>, <span style=color:#66d9ef>None</span>)), (<span style=color:#ae81ff>1001</span>, (<span style=color:#e6db74>&#39;zhangsan&#39;</span>, <span style=color:#e6db74>&#39;销售部&#39;</span>)), (<span style=color:#ae81ff>1002</span>, (<span style=color:#e6db74>&#39;lisi&#39;</span>, <span style=color:#e6db74>&#39;科技部&#39;</span>)), (<span style=color:#ae81ff>1003</span>, (<span style=color:#e6db74>&#39;wangwu&#39;</span>, <span style=color:#66d9ef>None</span>))]
</span></span><span style=display:flex><span>[(<span style=color:#ae81ff>1001</span>, (<span style=color:#e6db74>&#39;zhangsan&#39;</span>, <span style=color:#e6db74>&#39;销售部&#39;</span>)), (<span style=color:#ae81ff>1002</span>, (<span style=color:#e6db74>&#39;lisi&#39;</span>, <span style=color:#e6db74>&#39;科技部&#39;</span>))]
</span></span></code></pre></div><p>intersection</p><p>求RDD之间的交集</p><h3 id=glom>glom</h3><p>将RDD的数据，加上嵌套</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>], <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>  print(rdd<span style=color:#f92672>.</span>glom()<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: x)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span> [[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>], [<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>]]   
</span></span></code></pre></div><h3 id=sortby>sortBy</h3><p>对RDD数据进行排序，基于你指定的排序依据。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 按照value 数字进行排序</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数1函数, 表示的是 ,  告知Spark 按照数据的哪个列进行排序</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2: True表示升序 False表示降序</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数3: 排序的分区数</span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#34;&#34;&#34;注意: 如果要全局有序, 排序分区数请设置为1&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>sortBy(<span style=color:#66d9ef>lambda</span> x: x[<span style=color:#ae81ff>1</span>], ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, numPartitions<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>collect())
</span></span></code></pre></div><h3 id=sortbykey>sortByKey</h3><p>针对KV型RDD,按照key进行排序</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># ascending:升序or降序，True升序，Falsel降序，默认是升序</span>
</span></span><span style=display:flex><span><span style=color:#75715e># numPartitions:按照几个分区进行排序，如果全局有序，设置1</span>
</span></span><span style=display:flex><span><span style=color:#75715e># keyfunc:排序前处理一下key，让Key以你处理的样子进行排序（不影响数据本身）</span>
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>sortByKey(ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, numPartitions<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keyfunc<span style=color:#f92672>=</span><span style=color:#66d9ef>lambda</span> key: str(key)<span style=color:#f92672>.</span>lower())<span style=color:#f92672>.</span>collect())
</span></span></code></pre></div><h2 id=分区操作算子>分区操作算子</h2><h3 id=mappartitions>mapPartitions</h3><p>一次被传递的是一整个分区的数据作为一个迭代器(一次性list)对象传入过来</p><p>map一次处理一条，六次，mapPartitions三次</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/jFSWn9bfDaEcmBX.png alt=quicker_3aff15d9-b9da-4cde-94bf-40473841c484.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/26/7payLHS1lV54EXZ.png alt=quicker_6e6cf5b0-0d4c-45d6-b22d-0e320bafe6f5.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>6</span>], <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(iter):
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> list()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> it <span style=color:#f92672>in</span> iter:
</span></span><span style=display:flex><span>        result<span style=color:#f92672>.</span>append(it <span style=color:#f92672>*</span> <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>        print(result)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> result
</span></span><span style=display:flex><span>    print(rdd<span style=color:#f92672>.</span>mapPartitions(process)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>40</span>]
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>70</span>, <span style=color:#ae81ff>90</span>, <span style=color:#ae81ff>60</span>]
</span></span><span style=display:flex><span>    [<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>30</span>, <span style=color:#ae81ff>20</span>, <span style=color:#ae81ff>40</span>, <span style=color:#ae81ff>70</span>, <span style=color:#ae81ff>90</span>, <span style=color:#ae81ff>60</span>]
</span></span></code></pre></div><h3 id=foreachpartition>foreachPartition</h3><p>和普通foreach一致，一次处理的是一整个分区数据(就是一个没有返回值的mapPartitions)</p><h3 id=partitionby>partitionBy</h3><p>对RDD进行自定义分区操作</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 参数1重新分区后有几个分区</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2自定义分区规则，函数传入</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 返回值是int分区编号从0开始</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(k):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;hadoop&#39;</span> <span style=color:#f92672>==</span> k <span style=color:#f92672>or</span> <span style=color:#e6db74>&#39;hello&#39;</span> <span style=color:#f92672>==</span> k: <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#39;spark&#39;</span> <span style=color:#f92672>==</span> k: <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>partitionBy(<span style=color:#ae81ff>3</span>, process)<span style=color:#f92672>.</span>glom()<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>[[(<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;hello&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>1</span>)], [(<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>1</span>)], [(<span style=color:#e6db74>&#39;flink&#39;</span>, <span style=color:#ae81ff>1</span>)]]
</span></span></code></pre></div><h3 id=partition>partition</h3><p>对RDD的分区执行重新分区（仅数量)</p><h3 id=coalesce>coalesce</h3><p>对分区进行数量增减</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 参数1，分区数</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2,True or False。True表示允许shuffle,也就是可以加分区。False表示不允许shuffle,也就是不劭加分区，False是默认</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>], <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># repartition 修改分区</span>
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>repartition(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>getNumPartitions())
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>repartition(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>getNumPartitions())
</span></span><span style=display:flex><span><span style=color:#75715e># coalesce 修改分区</span>
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>coalesce(<span style=color:#ae81ff>5</span>)<span style=color:#f92672>.</span>getNumPartitions())
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>coalesce(<span style=color:#ae81ff>5</span>, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>getNumPartitions())
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>5</span>
</span></span></code></pre></div><p>如果你改分区了会影响并行计算（内存迭代的并行管道数量）后面学。分区如果增加，极大可能导致shuffle。对比repartition,一般使用coalesce较多，因为加分区要写参数2这样避免写repartition的时候手抖了加分区了。</p><h3 id=mapvalues>mapValues</h3><p>针对二元元组RDD,对其内部的二元元组的Valve执行map操作</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([(<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;hello&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;flink&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>1</span>), (<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>1</span>)])
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>mapValues(<span style=color:#66d9ef>lambda</span> x: x <span style=color:#f92672>*</span> <span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>collect())
</span></span><span style=display:flex><span>[(<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#39;hello&#39;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#39;flink&#39;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#39;hadoop&#39;</span>, <span style=color:#ae81ff>10</span>), (<span style=color:#e6db74>&#39;spark&#39;</span>, <span style=color:#ae81ff>10</span>)]
</span></span></code></pre></div><h3 id=join-1>join</h3><h2 id=action算子>Action算子</h2><p>定义：<strong>返回值不是rdd的就是action.算子</strong></p><h3 id=countbykey>countByKey</h3><p>统计key出现的次数(一般适用于KV型RDD)，返回字典</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>result <span style=color:#f92672>=</span> rdd2<span style=color:#f92672>.</span>countByKey()
</span></span><span style=display:flex><span>print(result)
</span></span><span style=display:flex><span>print(type(result))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>defaultdict(<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>class</span> <span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>int</span><span style=color:#e6db74>&#39;&gt;, {&#39;</span>hello<span style=color:#e6db74>&#39;: 3, &#39;</span>spark<span style=color:#e6db74>&#39;: 1, &#39;</span>hadoop<span style=color:#e6db74>&#39;: 1, &#39;</span>flink<span style=color:#e6db74>&#39;: 1})</span>
</span></span><span style=display:flex><span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>class</span> <span style=color:#960050;background-color:#1e0010>&#39;</span><span style=color:#a6e22e>collections</span><span style=color:#f92672>.</span>defaultdict<span style=color:#e6db74>&#39;&gt;</span>
</span></span></code></pre></div><h3 id=collect>collect</h3><p>将RDD各个分区内的数据，统一收集到Driver中，形成一个List对象</p><h3 id=reduce>reduce</h3><p>对RDD数据集按照你传入的逻辑进行聚合</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>])
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>reduce(<span style=color:#66d9ef>lambda</span> a, b: a <span style=color:#f92672>+</span> b))
</span></span><span style=display:flex><span><span style=color:#ae81ff>15</span>
</span></span></code></pre></div><h3 id=fold>fold</h3><p>和reduce一样，接受传入逻辑进行聚合，聚合是带有初始值的</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>这个初始值聚合<span style=color:#960050;background-color:#1e0010>，</span>会作用在<span style=color:#960050;background-color:#1e0010>：</span>分区内聚合<span style=color:#960050;background-color:#1e0010>、</span>分区间聚合
</span></span><span style=display:flex><span>比如<span style=color:#960050;background-color:#1e0010>：</span>[[<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>3</span>]<span style=color:#960050;background-color:#1e0010>，</span>[<span style=color:#ae81ff>4</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>6</span>]<span style=color:#960050;background-color:#1e0010>，</span>[<span style=color:#ae81ff>7</span>,<span style=color:#ae81ff>8</span>,<span style=color:#ae81ff>9</span>]]
</span></span><span style=display:flex><span>数据分布在3个分区
</span></span><span style=display:flex><span>分区1 <span style=color:#ae81ff>123</span>聚合的时候带上10作为初始值得到16
</span></span><span style=display:flex><span>分区2 <span style=color:#ae81ff>456</span>聚合的时候带上10作为初始值得到25
</span></span><span style=display:flex><span>分区3 <span style=color:#ae81ff>789</span>聚合的时候带上10作为初始值得到34
</span></span><span style=display:flex><span><span style=color:#ae81ff>3</span>个分区的结果做聚合也带上初始值10<span style=color:#960050;background-color:#1e0010>，</span>所以结果是<span style=color:#960050;background-color:#1e0010>：</span><span style=color:#ae81ff>10</span><span style=color:#f92672>+</span><span style=color:#ae81ff>16</span><span style=color:#f92672>+</span><span style=color:#ae81ff>25</span><span style=color:#f92672>+</span><span style=color:#ae81ff>34</span><span style=color:#f92672>=</span><span style=color:#ae81ff>85</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>9</span>], <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>fold(<span style=color:#ae81ff>10</span>, <span style=color:#66d9ef>lambda</span> a, b: a <span style=color:#f92672>+</span> b))
</span></span><span style=display:flex><span><span style=color:#ae81ff>85</span>
</span></span></code></pre></div><h3 id=first>first</h3><p>取出RDD的第一个元素</p><h3 id=take>take</h3><p>取RDD的前N个元素，组合成ist返回给你</p><h3 id=count>count</h3><p>计算RDD有多少条数据，返回值是一个数字</p><h3 id=takesample>takeSample</h3><p>随机抽样RDD的数据</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 参数1:TrUe表示运行取同一个数据，False表示不允许取同一个数据。</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2：抽样要几个</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数3：随机数种子，这个参数传入一个数字即可，随意给</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>6</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>], <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>takeSample(<span style=color:#66d9ef>False</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1</span>))
</span></span></code></pre></div><h3 id=top>top</h3><p>对RDD数据集进行降序排序，取前N个</p><h3 id=takeordered>takeOrdered</h3><p>对RDD进行排序取前N个</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 参数1要几个数据</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2对排序的数据进行更改(不会更改数据本身，只是在排序的时候换个样子)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>6</span>], <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>3</span>))
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>takeOrdered(<span style=color:#ae81ff>3</span>, <span style=color:#66d9ef>lambda</span> x: <span style=color:#f92672>-</span>x))
</span></span><span style=display:flex><span>print(rdd<span style=color:#f92672>.</span>top(<span style=color:#ae81ff>3</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>6</span>]
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>9</span>, <span style=color:#ae81ff>7</span>, <span style=color:#ae81ff>6</span>]
</span></span></code></pre></div><h3 id=foreach>foreach</h3><p>对RDD的每一个元素，执行你提供的逻辑的操作(和mp一个意思)，<strong>但是这个方法没有返回值</strong></p><h3 id=saveastextfile>saveAsTextFile</h3><p>将RDD的数据写入文本文件中。支持本地写出，hdfs等文件系统。</p><p><strong>写出数据是跳过Driver的每个分区直接写出</strong>，每个分区所在的Executor]直接控制数据写出到目标文件系统中所以才会<strong>一个分区产生1个结果文件</strong></p><p><strong>foreach、saveAsTextFile这两个算子是分区(Executor)直接执行的跳过Driver</strong>,.由分区所在的Executorj直接执行。其余的Action.算子都会将结果发送至Driver</p><ul><li><p>reduceByKey 和 groupByKey的区别?</p><p>reduceByKey自带聚合逻辑, groupByKey不带。如果做数据聚合reduceByKey的效率更好, 因为可以<strong>先聚合后shuffle再最终聚合, 传输的IO小</strong></p></li></ul><h1 id=提交到yarn集群中运行>提交到YARN集群中运行</h1><p>在PyCharml中直接执行</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 加入环境变量，让pycharmi直接提交yarn的时候，知道nadoop的配置在哪，可以去读取yarn的信息</span>
</span></span><span style=display:flex><span>os<span style=color:#f92672>.</span>environ[<span style=color:#e6db74>&#39;HADOOP_CONF_DIR&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;/export/server/hadoop/etc/hadoop&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>conf <span style=color:#f92672>=</span> SparkConf()<span style=color:#f92672>.</span>setAppName(<span style=color:#e6db74>&#34;test-yarn-1&#34;</span>)<span style=color:#f92672>.</span>setMaster(<span style=color:#e6db74>&#34;yarn&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># 如果提交到集群运行, 除了主代码以外, 还依赖了其它的代码文件.需要设置一个参数, 来告知spark ,还有依赖文件要同步上传到集群中</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数1做: spark.submit.pyFiles</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2的值可以是 单个.py文件,   也可以是.zip压缩包(有多个依赖文件的时候可以用zip压缩后上传)</span>
</span></span><span style=display:flex><span>conf<span style=color:#f92672>.</span>set(<span style=color:#e6db74>&#34;spark.submit.pyFiles&#34;</span>, <span style=color:#e6db74>&#34;defs_19.py&#34;</span>)
</span></span></code></pre></div><p>在服务器上通过spark-submit提交到集群运行</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#--py-f1Les可以帮你指定你依赖的其它python代码，支持。z1p（一堆），也可以单个。py文件都行。</span>
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>export<span style=color:#f92672>/</span>server<span style=color:#f92672>/</span>spark<span style=color:#f92672>/</span>bin<span style=color:#f92672>/</span>spark<span style=color:#f92672>-</span>submit <span style=color:#f92672>--</span>master yarn <span style=color:#f92672>--</span>py<span style=color:#f92672>-</span>files <span style=color:#f92672>./</span>defs<span style=color:#f92672>.</span>zip <span style=color:#f92672>./</span>main<span style=color:#f92672>.</span>py
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 要注意代码中：</span>
</span></span><span style=display:flex><span><span style=color:#75715e># master部分删除</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 读取的文件路径改为hdfs才可以</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;榨干集群性能提交,先查看集群的资源有多少：&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看CPU有几核：</span>
</span></span><span style=display:flex><span>cat <span style=color:#f92672>/</span>proc<span style=color:#f92672>/</span>cpuinfo <span style=color:#f92672>|</span> grep processor <span style=color:#f92672>|</span> wc <span style=color:#f92672>-</span>l
</span></span><span style=display:flex><span><span style=color:#75715e># 查看内存有多大：</span>
</span></span><span style=display:flex><span>free <span style=color:#f92672>-</span>g
</span></span><span style=display:flex><span><span style=color:#75715e># 通过命令，计算得知，当前我集群3台服务器总共提供：16G物理内存+6核心CPU的计算资源简单规划：这个Spark任务需要：</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 吃掉6核CPU,吃掉12个GB内存,规划后，希望使用6个executor来干活，每个executorl吃掉1核CPU2G内存</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bin<span style=color:#f92672>/</span>spark<span style=color:#f92672>-</span>submit <span style=color:#f92672>--</span>master yarn <span style=color:#f92672>--</span>py<span style=color:#f92672>-</span>files <span style=color:#f92672>/</span>root<span style=color:#f92672>/</span>defs<span style=color:#f92672>.</span>py
</span></span><span style=display:flex><span><span style=color:#f92672>--</span>executor<span style=color:#f92672>-</span>memory <span style=color:#ae81ff>2</span>g
</span></span><span style=display:flex><span><span style=color:#f92672>--</span>executor<span style=color:#f92672>-</span>cores <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>--</span>num<span style=color:#f92672>-</span>executors <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>root<span style=color:#f92672>/</span>main<span style=color:#f92672>.</span>py
</span></span><span style=display:flex><span><span style=color:#75715e>#每个executorl吃2g内存，吃1个cpu核心，总共6个executor</span>
</span></span></code></pre></div><h1 id=rdd的持久化>RDD的持久化</h1><p>RDD的数据是<strong>过程数据</strong>，RDD之间进行相互迭代计算(Transformation的转换)，当执行开启后**，新RDD的生成，代表老RDD的消失，RDD的数据只在处理的过程中存在**，一旦处理完成，就不见了。</p><p>这个特性可以最大化的利用资源，老旧DD没用了就从内存中清理，给后续的计算腾出内存空间。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/26/q7c3OtKLJuUkT4d.png alt=quicker_961662a0-d755-40c4-b30d-eb7fcee9cc6f.png></p><h2 id=缓存>缓存</h2><p>对于上述的场景，肯定要执行优化，优化就是：RDD3如果不消失，那么RDD1→RDD2→RDD3这个链条就不会执行2次，或者更多次</p><p>缓存的API</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#RDD3被2次使用，可以加入缓存进行优化</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>cache<span style=color:#960050;background-color:#1e0010>（）</span><span style=color:#75715e>#缓存到内存中</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>MEMORY_ONLY)<span style=color:#75715e>#仅内存缓存</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>MEMORY_ONLY_2)<span style=color:#75715e>#仅内存缓存，2个副本</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>DISK_ONLY)<span style=color:#75715e>#仅缓存硬盘上</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>DISK_ONLY_2)<span style=color:#75715e>#仅缓存硬盘上，2个副本</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>DISK_ONLY_3)<span style=color:#75715e>#仅缓存硬盘上，3个副本</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>MEMORY_AND_DISK)<span style=color:#75715e>#先放内存，不够放硬盘</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>pers1st(StorageLevel<span style=color:#f92672>.</span>MEMORY_AND_DISK2)<span style=color:#75715e>#先放内存，不够放硬盘，2个副本</span>
</span></span><span style=display:flex><span>rdd3<span style=color:#f92672>.</span>persist(StorageLevel<span style=color:#f92672>.</span>OFF_HEAP)<span style=color:#75715e>#堆外内存（系统内存）</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#如上API,自行选择使用即可</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#股建议使用rdd3.persist(StorageLevel.MEMORY_AND_DISK)</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#如果内存比较小的集群，建议使用rdd3.persist(StorageLevel.DISK0NLY)或者就别用缓存了用CheckPoint</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#主动清理缓存的API</span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>unpersist<span style=color:#960050;background-color:#1e0010>（）</span>
</span></span></code></pre></div><p>缓存技术可以将过程RDD数据，<strong>分散存储</strong>持久化<strong>保存到内存或者硬盘上</strong>。但是，这个保存在设定上是认为<strong>不安全</strong>的
缓存的数据在设计上是认为有丢失风险的。所以，缓存有一个特点就是：其<strong>保留RDD之间的血缘（依赖）关系</strong>
一旦缓存丢失，可以基于血缘关系的记录，重新计算这个RDD的数据</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/26/lfdeBkY7WtR54Uz.png alt=quicker_2f5166c8-78f5-4282-84fb-72c53b7288d7.png></p><p>RDD是将自己分区的数据，每个<strong>分区自行将其数据保存在其所在的Executor内存和硬盘上，这是分散存储</strong></p><h2 id=checkpoint>CheckPoint</h2><p>也是将RDD的数据，保存起来。但是它<strong>仅支持硬盘存储</strong>、它被设计认为是<strong>安全的、不保留血缘关系，集中收集各个分区数据进行存储</strong>。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/25/hDcKR7LfmNwzIFl.png alt=quicker_a810d31c-37ce-41eb-a6c7-85fd86579471.png></p><ul><li>CheckPoint不管分区数量多少，风险是一样的，缓存分区越多，风险越高</li><li>CheckPoint支持写入HDFS,缓存不行，HDFS是高可靠存储，CheckPoint被认为是安全的。</li><li>CheckPointz不支持内存，缓存可以，缓存如果写内存性能比CheckPoint要好一些</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#设置CheckPoint第一件事情，选择CP的保存路径</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#如果是Local模式，可以支持本地文件系统，如果在集群运行，千万要用HDFS</span>
</span></span><span style=display:flex><span>sc<span style=color:#f92672>.</span>setCheckpointDir(<span style=color:#e6db74>&#34;hdfs://node1:8020/output/bj52ckp&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>#用的时候，直接调用checkpoint算子即可。</span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>checkpoint<span style=color:#960050;background-color:#1e0010>（）</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rdd<span style=color:#f92672>.</span>unpersist()
</span></span></code></pre></div><p>CheckPoint是一种重量级的使用，也就是RDD的重新计算成本很高的时候，我们采用CheckPointh比较合适。或者数据量很大，用CheckPointh比较合适.<strong>如果数据量小，或者RDD重新计算是非常快的，用CheckPointi没啥必要，直接缓存即可。</strong></p><p>Cachei和CheckPoint两个API都不是Action类型，想要它俩工作，必须在后面接上Action。</p><ul><li><strong>CheckPoint不管分区数量多少，风险是一样的，缓存分区越多，风险越高</strong></li><li><strong>CheckPoint支持写入HDFS,缓存不行，HDFS是高可靠存储，CheckPoint被认为是安全的。</strong></li><li><strong>CheckPointz不支持内存，缓存可以，缓存如果写内存性能比CheckPoint要好一些</strong></li><li><strong>CheckPoint因为设计认为是安全的，所以不保留血缘关系，而缓存因为设计上认为不安全，所以保留</strong></li></ul><h1 id=共享变量>共享变量</h1><h2 id=广播变量>广播变量</h2><p>分布式集合RDD和本地集合进行关联使用的时候降低内存占用以及减少网络IO传输，提高性能</p><p><strong>本地list对象，被发送到每个分区的处理线程上使用，也就是一个executor内，其实存放了2份一样的数据</strong>。<strong>executor是进程，进程内资源共享</strong>，这2份数据没有必要，造成了内存浪费。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/MnDZ4CAHF9tV8Id.png alt=quicker_2fe3f89c-1f12-42f7-a92a-fbd11c66c44e.png></p><p>如果将本地list对象标记为广播变量对象，那么当上述场景出现的时候，Spark只会：给每个Executor来一份数据，而不是像原本那样，每一个分区的处理线程都来一份。节省内存。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e>#1,将本地机list标记成广播变量即可</span>
</span></span><span style=display:flex><span>broadcast <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>broadcast(stu_info_list)
</span></span><span style=display:flex><span><span style=color:#75715e>#2.使用广播变量，从broadcast对象中取出本地凯ist对象即可</span>
</span></span><span style=display:flex><span>value <span style=color:#f92672>=</span>  broadcast<span style=color:#f92672>.</span>value
</span></span><span style=display:flex><span><span style=color:#75715e>#也就是先放进去broadcast内部，然后从proadcast内部在取出来用，中间传输的是broadcasti这个对象了.只要中间传输的是broadcast对象，spark就会留意，只会给每个Executor发一份了，而不是傻傻的哪个分区要都给，</span>
</span></span></code></pre></div><h2 id=累加器>累加器</h2><p>分布式代码执行中，进行全局累加</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/k9imS5du1rXKxGC.png alt=quicker_5ad8353a-49ac-41f3-9d6c-779fd836d5a1.png></p><p>count来自driver对象，<strong>当在分布式的map算子中需要count对象的时候，driver会将count对象发送给每一个executor一份（复制发送）</strong>，每个executor各自收到一个，在最后执行print(count)的时候，这个被打印的count依旧是driver的那个，所以，不管executor中累加到多少，都和driver这个count无关</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>sc<span style=color:#f92672>.</span>accumulator
</span></span><span style=display:flex><span><span style=color:#75715e># 这个对象可以从各个Executort中收集它们的执行结果，作用回自己身上。</span>
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/2Iv1yrHRaDeJsAK.png alt=quicker_fabaa4b2-8d28-4dcf-9e52-6df75596cb20.png></p><p>也就是，使用累加器的时候，要注意，因为rdd是过程数据，如果rdd被多次使用，可能会重新构建此rdd，如果累加器累加代码，存在重新构建的步骤中，累加器累加代码就可能被多次执行</p><p>如何解决：加缓存或者CheckPoint即可。</p><h1 id=spark-内核调度>Spark 内核调度</h1><h2 id=dag>DAG</h2><p>DAG（有向无环图）是Spark代码的逻辑执行图，这个DAG的最终作用是；为了构建物理上的Spark详细执行计划而生，所以，由于Spark是分布式（多分区）的，那么DAG和分区之间也是有关联的。</p><ul><li>一个Spark环境可以运行多个Application</li><li>一个代码运行起来，会成为一个Application</li><li>Application内部可以有多个Job</li><li>每个Job由一个Action产生，并且每个Job有自己的DAG执行图</li><li>一个Job的DAG图会基于宽窄依赖划分成不同的阶段</li><li>不同阶段内基于分区数量，形成多个并行的内存迭代管道</li><li>每一个内存迭代管道形成一个Task(DAG调度器划分将Job内划分出具体的task任务，一个Job被划分出来的task在逻辑上称之为这个job的taskset)</li></ul><p>在SparkRDD前后之间的关系，分为：</p><ul><li>窄依赖：父RDD的一个分区，全部将数据发给子RDD的一个分区</li><li>宽依赖（shuffle）：父RDD的一个分区，将数据发给子RDD的多个分区</li></ul><p><strong>从后向前，遇到宽依赖就划分出一个阶段。称之为stage，在stage的内部，一定都是：窄依赖</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/Qby3GMPrqSCuHxi.png alt=quicker_959a4989-ad2f-4f11-b402-0ee3d2696e61.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/02/27/txZNWu6Ozn32KqQ.png alt=quicker_7043d0d6-61e5-415e-918c-fddc644ddb02.png></p><p>Spark默认受到全局并行度的限制，除了个别算子有特殊分区情况，大部分的算子，都会遵循全局并行度的要求，来规划自己的分区数。如果全局并行度是3，其实大部分算子分区都是3(否则可能影响内存计算管道长度)</p><p>注意：S<strong>park我们一般推荐只设置全局并行度，不要再算子上设置并行度除了一些排序算子外，计算算子就让他默认开分区就可以了。</strong></p><h3 id=spark是怎么做内存计算的dag的作用stage阶段划分的作用>Spark是怎么做内存计算的？DAG的作用？Stage阶段划分的作用？</h3><ul><li>Spark会产生DAG图</li><li>DAG图会基于分区和宽窄依赖关系划分阶段</li><li>一个阶段的内部都是窄依赖，窄依赖内，如果形成前后1:1的分区对应关系，就可以产生许多内存迭代计算的管道</li><li>这些内存迭代计算的管道，就是一个个具体的执行Task</li><li>一个Task是一个具体的线程，任务跑在一个线程内，就是走内存计算了。</li></ul><h3 id=spark为什么比mapreduce快>Spark为什么比MapReduce快</h3><ul><li>编程模型上Spark占优（算子够多）</li></ul><p>Spark的算子丰富，MapReduce.算子匮乏(Map和Reduce),MapReduce这个编程模型，很难在一套MR中处理复杂的任务,很多的复杂任务，是需要写多个MapReduce进行串联。多个MR串联通过磁盘交互数据</p><ul><li>算子交互上，和计算上可以尽量多的内存计算而非磁盘迭代</li></ul><p>Spark可以执行内存迭代，算子之间形成DAG基于依赖划分阶段后，在阶段内形成内存迭代管道。但是MapReduce的Map和Reduce之间的交互依旧是通过硬盘来交互的</p><h2 id=spark的并行>Spark的并行</h2><p>在同一时间内，有多少个task在同时运行。在有了6个task并行的前提下，rdd的分区就被规划成6个分区了。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 优先级从高到低：代码中、客户端提交参数中、配置文件中、默认(1，但是不会全部以1来跑，多数时候基于读取文件的分片数量来作为默认并行度)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;全局并行度是推荐设置，不要针对RDD改分区，可能会影响内存迭代管道的构建，或者会产生额外的Shuffle&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 配置文件中：conf/spark-defaults.conf中设置</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>default<span style=color:#f92672>.</span>parallelism <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 在客户端提交参数中：</span>
</span></span><span style=display:flex><span>bin<span style=color:#f92672>/</span>spark<span style=color:#f92672>-</span>submit <span style=color:#f92672>--</span>conf <span style=color:#e6db74>&#34;spark.default.parallelism=100&#34;</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 在代码中设置：</span>
</span></span><span style=display:flex><span>conf <span style=color:#f92672>=</span> SaprkConf()
</span></span><span style=display:flex><span>conf<span style=color:#f92672>.</span>set(<span style=color:#e6db74>&#34;spark.default.parallelism&#34;</span>,<span style=color:#e6db74>&#34;100&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#e6db74>&#39;&#39;&#39;针对RDD的并行度设置-不推荐&#39;&#39;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 只能在代码中写，算子：</span>
</span></span><span style=display:flex><span>repartition
</span></span><span style=display:flex><span>coalesce
</span></span><span style=display:flex><span>partitionBy
</span></span></code></pre></div><p>集群中如何规划并行度</p><p>结论：设置为CPU总核心的2~10倍</p><p>比如集群可用CPU核心是100个，我们建议并行度是200~1000
确保是CPU核心的整数倍即可，最小是2倍，最大一般10倍或更高适量）均可</p><h2 id=spark程序的调度>Spark程序的调度</h2><p>流程如图：</p><ul><li>Driver被构建出来</li><li>构建Spark Context（执行环境入口对象）</li><li>基于DAG Scheduler(DAG调度器)构建逻辑Task分配</li><li>基于Task Scheduler(Task调度器)将逻辑Task分配到各个Executor.上干活，并监控它们</li><li>Worker(Executor),被Task Scheduler管理监控，听从它们的指令干活，并定期汇报进度</li></ul><p>DAG调度器工作内容：将逻辑的DAG图进行处理，最终得到<strong>逻辑上的Task划分</strong></p><p>Task调度器工作内容：基于DAG Scheduler的产出，来<strong>规划这些逻辑的task,应该在哪些物理的executor.上运行，以及监控管理它们的运行</strong></p><h1 id=spark-shuffle>Spark Shuffle</h1><p>Spark在DAG调度阶段会将一个Job划分为多个Stage，上游Stage做map工作，下游Stage做reduce工作，其本质上还是MapReduce计算框架。Shuffle是连接map和reduce之间的桥梁，它将map的输出对应到reduce输入中，涉及到序列化反序列化、跨节点网络IO以及磁盘读写IO等。</p><p>Spark的Shuffle分为Write和Read两个阶段，分属于两个不同的Stage，前者是Parent Stage的最后一步，后者是Child Stage的第一步。</p><p>执行Shuffle的主体是Stage中的并发任务，这些任务分ShuffleMapTask和ResultTask两种，ShuffleMapTask要进行
Shuffle，ResultTask负责返回计算结果，一个Job中只有最后的Stage采用ResultTask，其他的均为ShuffleMapTask
。如果要按照map端和reduce端来分析的话，ShuffleMapTask可以即是map端任务，又是reduce端任务，因为
Spark中的Shuffle是可以串行的；ResultTask则只能充当reduce端任务的角色。</p><h1 id=sparksql>SparkSQL</h1><p>SparkSQL 是Spark的一个模块, 用于处理海量结构化数据</p><ul><li>融合性
SQ可以无缝集成在代码中，随时用SQL处理数据</li><li>统一数据访问
一套标准API可读写不同数据源</li><li>Hive兼容
可以使用SparkSQL直接计算并生成Hive数据表</li><li>标准化连接
支持标准化DBC1ODBC连接，方便和各种数据库进行数据交互。</li></ul><h2 id=sparksql和hive的异同>SparkSQL和Hive的异同</h2><p>Hive和Spark均是：“分布式SQL计算引擎”均是构建大规模结构化数据计算的绝佳利器，同时SparkSQL拥有更好的性能。</p><p>都可以运行在YARN上</p><table><thead><tr><th>SparkSQL</th><th>Hive</th></tr></thead><tbody><tr><td>内存计算</td><td>磁盘迭代</td></tr><tr><td>无元数据管理</td><td>Metastore</td></tr><tr><td>SQL/代码混合执行</td><td>SQL</td></tr><tr><td>底层运行Spark RDD</td><td>底层运行MapReduce</td></tr></tbody></table><h2 id=sparksql的数据抽象>SparkSQL的数据抽象</h2><p>SparkSQL 其实有3类数据抽象对象</p><ul><li><p>SchemaRDD对象（已废弃）</p></li><li><p>DataSet对象：可用于Java、Scala语言</p></li><li><p>DataFrame对象：可用于Java、Scala、Python、R</p></li><li><p>Pandas-DataFrame</p><p>二维表数据结构</p><p>单机（本地）集合</p></li><li><p>SparkCore-RDD</p><p>无标准数据结构，存储什么数据均可</p><p>分布式集合（分区）</p></li><li><p>SparkSQL-DataFrame</p><p>二维表数据结构</p><p>分布式集合（分区）</p></li></ul><h2 id=读取文件>读取文件</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 构建SparkSession执行环境入口对象</span>
</span></span><span style=display:flex><span>spark <span style=color:#f92672>=</span> SparkSession<span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>appName(<span style=color:#e6db74>&#34;test&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	master(<span style=color:#e6db74>&#34;local[*]&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>    config(<span style=color:#e6db74>&#34;spark.sql.shuffle.partitions&#34;</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	getOrCreate()
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    spark.sql.shuffle.partitions 在sql计算中, shuffle算子阶段默认的分区数是200个.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    对于集群模式来说, 200个默认也算比较合适
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    如果在local下运行, 200个很多, 在调度上会带来额外的损耗
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    所以在local下建议修改比较低 比如2</span><span style=color:#ae81ff>\4\10</span><span style=color:#e6db74>均可
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    这个参数和Spark RDD中设置并行度的参数 是相互独立的.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#75715e># 通过SparkSession对象 获取 SparkContext对象</span>
</span></span><span style=display:flex><span>sc <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>sparkContext
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 基于RDD转换成DataFrame</span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>textFile(<span style=color:#e6db74>&#34;../data/input/sql/people.txt&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	map(<span style=color:#66d9ef>lambda</span> x: x<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;,&#34;</span>))<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	map(<span style=color:#66d9ef>lambda</span> x: (x[<span style=color:#ae81ff>0</span>], int(x[<span style=color:#ae81ff>1</span>])))
</span></span><span style=display:flex><span><span style=color:#75715e># 构建DataFrame对象</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数1 被转换的RDD</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2 指定列名, 通过list的形式指定, 按照顺序依次提供字符串名称即可</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(rdd, schema<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;name&#39;</span>, <span style=color:#e6db74>&#39;age&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># toDF的方式构建DataFrame</span>
</span></span><span style=display:flex><span>df1 <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>toDF([<span style=color:#e6db74>&#34;name&#34;</span>, <span style=color:#e6db74>&#34;age&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 基于Pandas的DataFrame构建SparkSQL的DataFrame对象</span>
</span></span><span style=display:flex><span>pdf <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>DataFrame(
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;id&#34;</span>: [<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;name&#34;</span>: [<span style=color:#e6db74>&#34;张大仙&#34;</span>, <span style=color:#e6db74>&#34;王晓晓&#34;</span>, <span style=color:#e6db74>&#34;吕不为&#34;</span>],
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;age&#34;</span>: [<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>21</span>, <span style=color:#ae81ff>11</span>]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(pdf)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># 构建表结构的描述对象: StructType对象</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType()<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;name&#34;</span>, StringType(), nullable<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	add(<span style=color:#e6db74>&#34;age&#34;</span>, IntegerType(), nullable<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 基于StructType对象去构建RDD到DF的转换</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>createDataFrame(rdd, schema<span style=color:#f92672>=</span>schema)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 构建StructType, text数据源, 读取数据的特点是, 将一整行只作为`一个列`读取, 默认列名是value 类型是String</span>
</span></span><span style=display:flex><span>schema <span style=color:#f92672>=</span> StructType()<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;data&#34;</span>, StringType(), nullable<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;text&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	schema(schema<span style=color:#f92672>=</span>schema)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>	load(<span style=color:#e6db74>&#34;../data/input/sql/people.txt&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># JSON类型自带有Schema信息</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;json&#34;</span>)<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;../data/input/sql/people.json&#34;</span>)        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 读取CSV文件   </span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;csv&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;sep&#34;</span>, <span style=color:#e6db74>&#34;;&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;header&#34;</span>, <span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>\  
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;encoding&#34;</span>, <span style=color:#e6db74>&#34;utf-8&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        schema(StructType([
</span></span><span style=display:flex><span>        StructField(<span style=color:#e6db74>&#34;name&#34;</span>, StringType()),
</span></span><span style=display:flex><span>        StructField(<span style=color:#e6db74>&#34;age&#34;</span>, IntegerType()),
</span></span><span style=display:flex><span>        StructField(<span style=color:#e6db74>&#34;job&#34;</span>, StringType())]))<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        load(<span style=color:#e6db74>&#34;../data/input/sql/people.csv&#34;</span>)    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 读取parquet类型的文件</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;parquet&#34;</span>)<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;../data/input/sql/users.parquet&#34;</span>) 
</span></span></code></pre></div><h2 id=方法>方法</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># 打印DataFrame的表结构</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>printSchema()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 打印df中的数据</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数1 表示 展示出多少条数据, 默认不传的话是20</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 参数2 表示是否对列进行截断, 如果列的数据长度超过20个字符串长度, 后续的内容不显示以...代替</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 如果给False 表示不阶段全部显示, 默认是True</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>show(<span style=color:#ae81ff>20</span>, <span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># SQL 风格</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        SELECT * FROM score WHERE name=&#39;语文&#39; LIMIT 5
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># DSL 风格</span>
</span></span><span style=display:flex><span>    df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#34;movie_id&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        agg(
</span></span><span style=display:flex><span>            F<span style=color:#f92672>.</span>count(<span style=color:#e6db74>&#34;movie_id&#34;</span>)<span style=color:#f92672>.</span>alias(<span style=color:#e6db74>&#34;cnt&#34;</span>),
</span></span><span style=display:flex><span>            F<span style=color:#f92672>.</span>round(F<span style=color:#f92672>.</span>avg(<span style=color:#e6db74>&#34;rank&#34;</span>), <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>alias(<span style=color:#e6db74>&#34;avg_rank&#34;</span>)
</span></span><span style=display:flex><span>        )<span style=color:#f92672>.</span>where(<span style=color:#e6db74>&#34;cnt &gt; 100&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        orderBy(<span style=color:#e6db74>&#34;avg_rank&#34;</span>, ascending<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        limit(<span style=color:#ae81ff>10</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        show()
</span></span><span style=display:flex><span>        <span style=color:#75715e># agg: 它是GroupedData对象的API, 作用是 在里面可以写多个聚合</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># alias: 它是Column对象的API, 可以针对一个列 进行改名</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># first: DataFrame的API, 取出DF的第一行数据,</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> <span style=color:#75715e># Column对象的获取</span>
</span></span><span style=display:flex><span>id_column <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;id&#39;</span>]
</span></span><span style=display:flex><span>subject_column <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;subject&#39;</span>]
</span></span><span style=display:flex><span><span style=color:#75715e># DLS风格演示</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>select([<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;subject&#34;</span>])<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>select(<span style=color:#e6db74>&#34;id&#34;</span>, <span style=color:#e6db74>&#34;subject&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>select(id_column, subject_column)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># filter API</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>filter(<span style=color:#e6db74>&#34;score &lt; 99&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>filter(df[<span style=color:#e6db74>&#39;score&#39;</span>] <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>99</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># where API</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>where(<span style=color:#e6db74>&#34;score &lt; 99&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>where(df[<span style=color:#e6db74>&#39;score&#39;</span>] <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>99</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># group By API</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(<span style=color:#e6db74>&#34;subject&#34;</span>)<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>groupBy(df[<span style=color:#e6db74>&#39;subject&#39;</span>])<span style=color:#f92672>.</span>count()<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># df.groupBy API的返回值 GroupedData</span>
</span></span><span style=display:flex><span><span style=color:#75715e># GroupedData对象 不是DataFrame</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 它是一个 有分组关系的数据结构, 有一些API供我们对分组做聚合</span>
</span></span><span style=display:flex><span><span style=color:#75715e># SQL: group by 后接上聚合: sum avg count min man</span>
</span></span><span style=display:flex><span><span style=color:#75715e># GroupedData 类似于SQL分组后的数据结构, 同样有上述5种聚合方法</span>
</span></span><span style=display:flex><span><span style=color:#75715e># GroupedData 调用聚合方法后, 返回值依旧是DataFrame</span>
</span></span><span style=display:flex><span><span style=color:#75715e># GroupedData 只是一个中转的对象, 最终还是要获得DataFrame的结果</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 注册成临时表</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>createTempView(<span style=color:#e6db74>&#34;score&#34;</span>) <span style=color:#75715e># 注册临时视图(表)</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>createOrReplaceTempView(<span style=color:#e6db74>&#34;score_2&#34;</span>) <span style=color:#75715e># 注册 或者 替换  临时视图</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>createGlobalTempView(<span style=color:#e6db74>&#34;score_3&#34;</span>) <span style=color:#75715e># 注册全局临时视图 全局临时视图在使用的时候 需要在前面带上global_temp. 前缀</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 可以通过SparkSession对象的sql api来完成sql语句的执行</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT subject, COUNT(*) AS cnt FROM score GROUP BY subject&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT subject, COUNT(*) AS cnt FROM score_2 GROUP BY subject&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT subject, COUNT(*) AS cnt FROM global_temp.score_3 GROUP BY subject&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># withColumn方法</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 方法功能: 对已存在的列进行操作, 返回一个新的列, 如果名字和老列相同, 那么替换, 否则作为新列存在</span>
</span></span><span style=display:flex><span>df2 <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>withColumn(<span style=color:#e6db74>&#34;value&#34;</span>, F<span style=color:#f92672>.</span>explode(F<span style=color:#f92672>.</span>split(df[<span style=color:#e6db74>&#39;value&#39;</span>], <span style=color:#e6db74>&#34; &#34;</span>)))
</span></span><span style=display:flex><span>withColumnRenamed(<span style=color:#e6db74>&#34;value&#34;</span>, <span style=color:#e6db74>&#34;word&#34;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 数据清洗: 数据去重</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dropDuplicates 是DataFrame的API, 可以完成数据去重</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 无参数使用, 对全部的列 联合起来进行比较, 去除重复值, 只保留一条</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>dropDuplicates()<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>dropDuplicates([<span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#e6db74>&#39;job&#39;</span>])<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 数据清洗: 数据去重</span>
</span></span><span style=display:flex><span><span style=color:#75715e># dropDuplicates 是DataFrame的API, 可以完成数据去重</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 无参数使用, 对全部的列 联合起来进行比较, 去除重复值, 只保留一条</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>dropDuplicates()<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>dropDuplicates([<span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#e6db74>&#39;job&#39;</span>])<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 缺失值处理也可以完成对缺失值进行填充</span>
</span></span><span style=display:flex><span><span style=color:#75715e># DataFrame的 fillna 对缺失的列进行填充</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>fillna(<span style=color:#e6db74>&#34;loss&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># 指定列进行填充</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>fillna(<span style=color:#e6db74>&#34;N/A&#34;</span>, subset<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;job&#39;</span>])<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span><span style=color:#75715e># 设定一个字典, 对所有的列 提供填充规则</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>fillna({<span style=color:#e6db74>&#34;name&#34;</span>: <span style=color:#e6db74>&#34;未知姓名&#34;</span>, <span style=color:#e6db74>&#34;age&#34;</span>: <span style=color:#ae81ff>1</span>, <span style=color:#e6db74>&#34;job&#34;</span>: <span style=color:#e6db74>&#34;worker&#34;</span>})<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>spliti<span style=color:#960050;background-color:#1e0010>：</span>切分字符串
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>split(被切分的列<span style=color:#960050;background-color:#1e0010>，</span>切分字符串)
</span></span><span style=display:flex><span>explode<span style=color:#960050;background-color:#1e0010>：</span>数组转列
</span></span><span style=display:flex><span>F<span style=color:#f92672>.</span>explode<span style=color:#960050;background-color:#1e0010>（</span>被转换的列<span style=color:#960050;background-color:#1e0010>）</span>
</span></span></code></pre></div><h2 id=保存>保存</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Write text 写出, 只能写出一个列的数据, 需要将df转换为单列df</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>select(F<span style=color:#f92672>.</span>concat_ws(<span style=color:#e6db74>&#34;---&#34;</span>, <span style=color:#e6db74>&#34;user_id&#34;</span>, <span style=color:#e6db74>&#34;movie_id&#34;</span>, <span style=color:#e6db74>&#34;rank&#34;</span>, <span style=color:#e6db74>&#34;ts&#34;</span>))<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        write<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        format(<span style=color:#e6db74>&#34;text&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        save(<span style=color:#e6db74>&#34;../data/output/sql/text&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write csv</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        format(<span style=color:#e6db74>&#34;csv&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;sep&#34;</span>, <span style=color:#e6db74>&#34;;&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;header&#34;</span>, <span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        save(<span style=color:#e6db74>&#34;../data/output/sql/csv&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write json</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        format(<span style=color:#e6db74>&#34;json&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        save(<span style=color:#e6db74>&#34;../data/output/sql/json&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Write parquet</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        format(<span style=color:#e6db74>&#34;parquet&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        save(<span style=color:#e6db74>&#34;../data/output/sql/parquet&#34;</span>)
</span></span></code></pre></div><h2 id=jdbc读写数据库>JDBC读写数据库</h2><p>对于windows:系统（使用本地解释器）(以Anaconda环境演示)
将jar包放在：Anaconda3的安装路径下\envs\虚拟环境\Lib\site-packages\pyspark\jars
对于Linux系统（使用远程解释器执行）(以Anaconda环境演示)
将jar包放在：Anaconda:3的安装路径下/envs/虚拟环境/Lib/python3.8/site-packages/pyspark/jars</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span> <span style=color:#75715e># 1. 写出df到mysql数据库中</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>write<span style=color:#f92672>.</span>mode(<span style=color:#e6db74>&#34;overwrite&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        format(<span style=color:#e6db74>&#34;jdbc&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;url&#34;</span>, <span style=color:#e6db74>&#34;jdbc:mysql://n1:3306/bigdata?useSSL=false&amp;useUnicode=true&#34;</span>)<span style=color:#f92672>.</span>\  
</span></span><span style=display:flex><span>        <span style=color:#75715e># 使用useSSL=false确保连接可以正常连接（不使用SSL安全协议进行连接</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 使用useUnicode=true来确保传输中不出现乱码</span>
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;dbtable&#34;</span>, <span style=color:#e6db74>&#34;movie_data&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        <span style=color:#75715e># dbtable属性：指定写出的表名</span>
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;root&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;password&#34;</span>, <span style=color:#e6db74>&#34;123456&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        save()
</span></span><span style=display:flex><span>        <span style=color:#75715e># save()不要填参数，没有路径，是写出数据库</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df2 <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>read<span style=color:#f92672>.</span>format(<span style=color:#e6db74>&#34;jdbc&#34;</span>)<span style=color:#f92672>.</span> \
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;url&#34;</span>, <span style=color:#e6db74>&#34;jdbc:mysql://n1:3306/bigdata?useSSL=false&amp;useUnicode=true&#34;</span>)<span style=color:#f92672>.</span> \
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;dbtable&#34;</span>, <span style=color:#e6db74>&#34;movie_data&#34;</span>)<span style=color:#f92672>.</span> \
</span></span><span style=display:flex><span>        <span style=color:#75715e># 读出来是自带schema,不需要设置schema,因为数据库就有schema</span>
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;user&#34;</span>, <span style=color:#e6db74>&#34;root&#34;</span>)<span style=color:#f92672>.</span> \
</span></span><span style=display:flex><span>        option(<span style=color:#e6db74>&#34;password&#34;</span>, <span style=color:#e6db74>&#34;123456&#34;</span>)<span style=color:#f92672>.</span> \
</span></span><span style=display:flex><span>        load()
</span></span><span style=display:flex><span>        <span style=color:#75715e># load（）不需要加参数，没有路径，从数据库中读取的啊</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df2<span style=color:#f92672>.</span>printSchema()
</span></span><span style=display:flex><span>df2<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h2 id=udf函数>UDF函数</h2><p>回顾Hive中自定义函数有三种类型：</p><ul><li><p>UDF(User-Defined-Function)函数</p><p>一对一的关系，输入一个值经过函数以后输出一个值；</p><p>在Hive中继承UDF类，方法名称为evaluate,返回值不能为void,其实就是实现一个方法；</p></li><li><p>UDAF(User-Defined Aggregation Function)聚合函数</p><p>多对一的关系，输入多个值输出一个值，通常与groupBy联合使用；</p></li><li><p>UDTF(User-Defined Table-Generating Functions)函数</p><p>一对多的关系，输入一个值输出多个值（一行变为多行）；</p><p>用户自定义生成函数，有点像flatMap;</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span>sparksession<span style=color:#f92672>.</span>udf<span style=color:#f92672>.</span>register(参数1<span style=color:#960050;background-color:#1e0010>，</span>参数2<span style=color:#960050;background-color:#1e0010>，</span>参数3<span style=color:#960050;background-color:#1e0010>）</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  参数1：UDF名称，可用于SQL风格</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  参数2：被注册成UDF的方法名</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  参数3：声明UDF的返回值类型</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>udf对象 <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>udf(参数1<span style=color:#960050;background-color:#1e0010>，</span> 参数2)
</span></span><span style=display:flex><span><span style=color:#75715e>#  参数1：被注册成UDF的方法名</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  参数2：声明UDF的返回值类型</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  udf对象： 返回值对象，是一个UDF对象，可用于DSL风格</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#  其中F是：from pyspark.sql import functions as F</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 构建一个RDD</span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([[<span style=color:#e6db74>&#34;hadoop spark flink&#34;</span>], [<span style=color:#e6db74>&#34;hadoop flink java&#34;</span>]])
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>toDF([<span style=color:#e6db74>&#34;line&#34;</span>])
</span></span><span style=display:flex><span><span style=color:#75715e># 注册UDF, UDF的执行函数定义</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>split_line</span>(data):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34; &#34;</span>)  <span style=color:#75715e># 返回值是一个Array对象</span>
</span></span><span style=display:flex><span><span style=color:#75715e># TODO1 方式1 构建UDF</span>
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>udf<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;udf1&#34;</span>, split_line, ArrayType(StringType()))
</span></span><span style=display:flex><span><span style=color:#75715e># SQL风格</span>
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>createTempView(<span style=color:#e6db74>&#34;lines&#34;</span>)
</span></span><span style=display:flex><span>spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT udf1(line) FROM lines&#34;</span>)<span style=color:#f92672>.</span>show(truncate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># TODO 2 方式2的形式构建UDF</span>
</span></span><span style=display:flex><span>udf3 <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>udf(split_line, ArrayType(StringType()))
</span></span><span style=display:flex><span>df<span style=color:#f92672>.</span>select(udf3(df[<span style=color:#e6db74>&#39;line&#39;</span>]))<span style=color:#f92672>.</span>show(truncate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>                          
</span></span><span style=display:flex><span>                          
</span></span><span style=display:flex><span>                          
</span></span><span style=display:flex><span>                          
</span></span><span style=display:flex><span>    rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([[<span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>2</span>], [<span style=color:#ae81ff>3</span>]])
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>toDF([<span style=color:#e6db74>&#34;num&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#75715e># 注册UDF</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(data):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#34;num&#34;</span>: data, <span style=color:#e6db74>&#34;letters&#34;</span>: string<span style=color:#f92672>.</span>ascii_letters[data]}
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    UDF的返回值是字典的话, 需要用StructType来接收
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    udf1 <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>udf<span style=color:#f92672>.</span>register(<span style=color:#e6db74>&#34;udf1&#34;</span>, process,
</span></span><span style=display:flex><span>                              StructType()<span style=color:#f92672>.</span>add(<span style=color:#e6db74>&#34;num&#34;</span>, IntegerType(), nullable<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>                              add(<span style=color:#e6db74>&#34;letters&#34;</span>, StringType(), nullable<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>))
</span></span><span style=display:flex><span>    df<span style=color:#f92672>.</span>selectExpr(<span style=color:#e6db74>&#34;udf1(num)&#34;</span>)<span style=color:#f92672>.</span>show(truncate<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)     
</span></span><span style=display:flex><span><span style=color:#f92672>+---------+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>udf1(num)<span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---------+</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>[<span style=color:#ae81ff>1</span>,b]    <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>[<span style=color:#ae81ff>2</span>,c]    <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>|</span>[<span style=color:#ae81ff>3</span>,d]    <span style=color:#f92672>|</span>
</span></span><span style=display:flex><span><span style=color:#f92672>+---------+</span>                          
</span></span><span style=display:flex><span>                          
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 模拟UDAF 实现聚合</span>
</span></span><span style=display:flex><span>rdd <span style=color:#f92672>=</span> sc<span style=color:#f92672>.</span>parallelize([<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>4</span>, <span style=color:#ae81ff>5</span>], <span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> rdd<span style=color:#f92672>.</span>map(<span style=color:#66d9ef>lambda</span> x: [x])<span style=color:#f92672>.</span>toDF([<span style=color:#e6db74>&#39;num&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 折中的方式 就是使用RDD的mapPartitions 算子来完成聚合操作</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 如果用mapPartitions API 完成UDAF聚合, 一定要单分区</span>
</span></span><span style=display:flex><span>single_partition_rdd <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>rdd<span style=color:#f92672>.</span>repartition(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>process</span>(iter):
</span></span><span style=display:flex><span>    sum <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> iter:
</span></span><span style=display:flex><span>        sum <span style=color:#f92672>+=</span> row[<span style=color:#e6db74>&#39;num&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [sum]    <span style=color:#75715e># 一定要嵌套list, 因为mapPartitions方法要求的返回值是list对象</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(single_partition_rdd<span style=color:#f92672>.</span>mapPartitions(process)<span style=color:#f92672>.</span>collect())                          
</span></span></code></pre></div><h2 id=sparksql的运行流程>SparkSQL的运行流程</h2><ul><li><p>RDD的运行会完全按照开发者的代码执行， 如果开发者水平有限，RDD的执行效率也会受到影响。</p></li><li><p>而SparkSQL会对写完的代码，执行“自动优化”， 以提升代码运行效率，避免开发者水平影响到代码执行效率。</p></li><li><p>为什么SparkSQL可以自动优化而RDD不可以？</p></li></ul><p>​ RDD：内含数据类型不限格式和结构</p><p>​ DataFrame：100% 是二维表结构，可以被针对</p><ul><li>SparkSQL的自动优化，依赖于：Catalyst优化器</li></ul><p>为了解决过多依赖Hive的问题，SparkSQL使用了一个新的SQL优化器替代Hive中的优化器，这个优化器就是Catalyst,.整个
SparkSQL的架构大致如下：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/6uJdIHO8CqjZR4S.png alt=quicker_b07a1325-9e59-4a0c-afba-140fc97781c6.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/THNICV93ZSPMwae.png alt=quicker_f1977f58-1c16-47d0-aacc-118b086edf58.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/rGavj4XQfqEiY3P.png alt=quicker_c4104bcb-d353-463e-9cb1-28973c05df9e.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/oE1fRmpl5aP6HuA.png alt=quicker_53076345-f6b1-408b-833e-0d71ff1b3491.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/LQsdebPEinUhxMH.png alt=quicker_2ac0ae6b-4bf7-456c-bc6b-d400721650c3.png></p><p>catalyst的各种优化细节非常多，大方面的优化点有2个：</p><ul><li>谓词下推(Predicate Pushdown)\断言下推：将逻辑判断提前到前面，以减少shuffle阶段的数据量(行过滤，提前执行where)</li><li>列值裁剪(Column Pruning):将加载的列进行裁剪，尽量减少被处理数据的宽度(列过滤，提前规划select的字段数量，非常合适的存储系统：parquet)</li></ul><h2 id=spark-on-hive>Spark On Hive</h2><p>对于Spark来说，自身是一个执行引擎，但是Spark自己没有元数据管理功能，Spark完全有能力将SQL变成RDD提交</p><p>但是问题是，Person的数据在哪？Person有哪些字段？字段啥类型？Spark完全不知道了。不知道这些东西，如何翻译RDD运行。</p><p>在SparkSQL代码中可以写SQL那是因为，表是来自DataFrame注册的。DataFrame中有数据，有字段，有类型，足够Spark用来翻译RDD用。如果以不写代码的角度来看，SELECT*FROM person WHERE age>10。 spark无法翻译，因为没有元数据</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/02/AaXZPsI1mk6Bf9i.png alt=quicker_d86db96d-7abe-43ae-932f-97d6eacb6f23.png></p><p>Spark On Hive就是把Hive的MetaStore服务拿过来给Spark做元数据管理用而已。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span> spark <span style=color:#f92672>=</span> SparkSession<span style=color:#f92672>.</span>builder<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        appName(<span style=color:#e6db74>&#34;test&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        master(<span style=color:#e6db74>&#34;local[*]&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        config(<span style=color:#e6db74>&#34;spark.sql.shuffle.partitions&#34;</span>, <span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        config(<span style=color:#e6db74>&#34;spark.sql.warehouse.dir&#34;</span>, <span style=color:#e6db74>&#34;hdfs://n1:8020/user/hive/warehouse&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        config(<span style=color:#e6db74>&#34;hive.metastore.uris&#34;</span>, <span style=color:#e6db74>&#34;thrift://n1:9083&#34;</span>)<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        enableHiveSupport()<span style=color:#f92672>.</span>\
</span></span><span style=display:flex><span>        getOrCreate()
</span></span><span style=display:flex><span>    sc <span style=color:#f92672>=</span> spark<span style=color:#f92672>.</span>sparkContext
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    spark<span style=color:#f92672>.</span>sql(<span style=color:#e6db74>&#34;SELECT * FROM student&#34;</span>)<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><h2 id=分布式sql执行引擎>分布式SQL执行引擎</h2><p>Spark中有一个服务叫做：ThriftServer服务，可以启动并监听在10000端口，这个服务对外提供功能，我们可以用数据库工具或者代码连接上来直接写SQL即可操作spark</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/03/Cg7LVHutzTpmYS3.png alt=quicker_68317947-afa8-4b40-abe3-2d0bef4e0b18.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#75715e>#如果是你们自己的虚拟机</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#直接在r00t账户下启动即可</span>
</span></span><span style=display:flex><span>$SPARK_HOME/sbin/start-thriftserver.sh <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--hiveconf hive.server2.thrift.port<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--hiveconf hive.server2.thrift.bind.host<span style=color:#f92672>=</span>n1 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--master local<span style=color:#f92672>[</span>2<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#masteri选择local,每一条sql都是local进程执行</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#master选择yarn,每一条sql都是在YARN集群中执行</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 为了安装pyhive包需要安装一堆inux软件，执行如下命令进行linux:软件安装：</span>
</span></span><span style=display:flex><span>yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel libffi-devel gcc make gcc-c++ python-devel cyrus-sasl-devel cyrus-sasl-plain cyrus-sasl-gssapi -y
</span></span><span style=display:flex><span><span style=color:#75715e># 安装好前置依赖软件后，安装pyhive包</span>
</span></span><span style=display:flex><span>/export/server/anaconda3/envs/pyspark/bin/python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyhive pymysql sasl thrift thrift_sasl
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyhive <span style=color:#f92672>import</span> hive
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    <span style=color:#75715e># 获取到Hive(Spark ThriftServer的链接)</span>
</span></span><span style=display:flex><span>    conn <span style=color:#f92672>=</span> hive<span style=color:#f92672>.</span>Connection(host<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;n1&#34;</span>, port<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>, username<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;hadoop&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 获取一个游标对象</span>
</span></span><span style=display:flex><span>    cursor <span style=color:#f92672>=</span> conn<span style=color:#f92672>.</span>cursor()
</span></span><span style=display:flex><span>    <span style=color:#75715e># 执行SQL</span>
</span></span><span style=display:flex><span>    cursor<span style=color:#f92672>.</span>execute(<span style=color:#e6db74>&#34;SELECT * FROM student&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 通过fetchall API 获得返回值</span>
</span></span><span style=display:flex><span>    result <span style=color:#f92672>=</span> cursor<span style=color:#f92672>.</span>fetchall()
</span></span><span style=display:flex><span>    print(result)
</span></span><span style=display:flex><span>j
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
code-Spark</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/code-spark/ title=code-Spark>/post/code-spark/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/code-selenium/ rel=next title=code-selenium><i class="fa fa-chevron-left"></i> code-selenium</a></div><div class="post-nav-prev post-nav-item"><a href=/post/code-vue/ rel=prev title=code-vue>code-vue
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>