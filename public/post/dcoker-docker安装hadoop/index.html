<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="dcoker-Docker"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="dcoker-Docker"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-02 02:11:14 +0800 CST"><meta property="article:modified_time" content="2022-12-02 02:11:14 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"dcoker-docker%E5%AE%89%E8%A3%85hadoop","permalink":"/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/","title":"dcoker-Docker","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>dcoker-Docker - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>72</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#进入-ubuntu-容器>进入 Ubuntu 容器</a></li><li><a href=#修改-apt-源>修改 apt 源</a></li><li><a href=#安装-jdk-18>安装 jdk 1.8</a></li><li><a href=#安装-scala>安装 Scala</a></li><li><a href=#安装-vim-与-网络工具包>安装 Vim 与 网络工具包</a></li><li><a href=#安装-ssh>安装 SSH</a></li></ul><ul><li><a href=#下载-hadoop>下载 Hadoop</a></li><li><a href=#解压>解压</a></li><li><a href=#添加环境变量>添加环境变量</a></li><li><a href=#修改配置文件>修改配置文件</a></li><li><a href=#在-docker-中启动集群>在 Docker 中启动集群</a><ul><li><a href=#将当前容器导出为镜像>将当前容器导出为镜像</a></li><li><a href=#启动-5-个终端>启动 5 个终端</a></li><li><a href=#在-h01-主机中启动-haddop-集群>在 h01 主机中，启动 Haddop 集群</a></li></ul></li><li><a href=#运行内置wordcount例子>运行内置WordCount例子</a></li></ul><ul><li><a href=#下载-hbase-213>下载 Hbase 2.1.3</a></li><li><a href=#解压-1>解压</a></li><li><a href=#修改环境变量文件>修改环境变量文件</a></li><li><a href=#修改配置文件-1>修改配置文件</a></li><li><a href=#启动-hbase>启动 Hbase</a></li></ul><ul><li><a href=#下载-spark-240>下载 Spark 2.4.0</a></li><li><a href=#解压-2>解压</a></li><li><a href=#修改-环境变量>修改 环境变量</a></li><li><a href=#启动-spark>启动 Spark</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>72</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=276830></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=590></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-02T02:11:14+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="dcoker-Docker"><meta itemprop=description content="Docker、Java、Scala、Hadoop、 Hbase、Spark。 集群共有5台机器，主机名分别为 h01、h02、h03、h04、h0"></span><header class=post-header><h1 class=post-title itemprop="name headline">dcoker-Docker
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/dcoker-Docker%e5%ae%89%e8%a3%85Hadoop.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-02 02:11:14 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-02 02:11:14 +0800 CST">2022-12-02</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/dcoker itemprop=url rel=index><span itemprop=name>dcoker</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>3051</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>7分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p>Docker、Java、Scala、Hadoop、 Hbase、Spark。
集群共有5台机器，主机名分别为 h01、h02、h03、h04、h05。其中 h01 为 master，其他的为 slave。
JDK 1.8、Scala 2.11.6、Hadoop 3.2.1、Hbase 2.1.3、Spark 2.4.0</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># Ubuntu 安装 Docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 在 Ubuntu 下安装 Docker 的时候需在管理员的账号下操作。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```wget -qO- https://get.docker.com/ | sh```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 以 sudo 启动 Docker 服务。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo service docker start```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 显示 Docker 中所有正在运行的容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker ps``` 
</span></span><span style=display:flex><span>现在的 Docker 网络能够提供 DNS 解析功能，使用如下命令为接下来的 Hadoop 集群单独构建一个虚拟的网络。
</span></span><span style=display:flex><span>​```sudo docker network create --driver=bridge hadoop```
</span></span><span style=display:flex><span>以上命令创建了一个名为 Hadoop 的虚拟桥接网络，该虚拟网络内部提供了自动的DNS解析服务。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 查看 Docker 中的网络
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker network ls```
</span></span><span style=display:flex><span>可以看到刚刚创建的名为 hadoop 的虚拟桥接网络。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 查找 ubuntu 容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker search ubuntu```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 下载 ubuntu 16.04 版本的镜像文件
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker pull ubuntu:16.04```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 根据镜像启动一个容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker run -it ubuntu:16.04 /bin/bash```
</span></span><span style=display:flex><span>可以看出 shell 已经是容器的 shell 了
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 退出容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​``` exit```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 查看本机上所有的容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker ps -a```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 启动容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker start fab4da838c2f```
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>## 关闭容器
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>​```sudo docker stop  fab4da838c2f
</span></span></code></pre></div><h1 id=安装-java-与-scala>安装 Java 与 Scala</h1><p>在当前容器中将配置配好，导入出为镜像。以此镜像为基础创建五个容器，并赋予 hostname
进入 h01 容器，启动 Hadoop。</p><h2 id=进入-ubuntu-容器>进入 Ubuntu 容器</h2><p><code>sudo docker exec -it fab4da838c2f /bin/bash</code></p><h2 id=修改-apt-源>修改 apt 源</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>备份源
</span></span><span style=display:flex><span>cp /etc/apt/sources.list /etc/apt/sources_init.list
</span></span><span style=display:flex><span>先删除旧源文件
</span></span><span style=display:flex><span>rm /etc/apt/sources.list
</span></span><span style=display:flex><span>这个时候没有 vim 工具，使用 echo 命令将源写入新文件
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo &#34;deb http://mirrors.aliyun.com/ubuntu/ xenial main
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial main
</span></span><span style=display:flex><span>deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main
</span></span><span style=display:flex><span>deb http://mirrors.aliyun.com/ubuntu/ xenial universe
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial universe
</span></span><span style=display:flex><span>deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
</span></span><span style=display:flex><span>deb http://mirrors.aliyun.com/ubuntu/ xenial-security main
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main
</span></span><span style=display:flex><span>deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe
</span></span><span style=display:flex><span>deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security universe&#34;
</span></span><span style=display:flex><span>&gt; /etc/apt/sources.list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>更新源
</span></span><span style=display:flex><span>apt update
</span></span></code></pre></div><h2 id=安装-jdk-18>安装 jdk 1.8</h2><p><code>apt install openjdk-8-jdk</code>
测试
<code>java -version</code></p><h2 id=安装-scala>安装 Scala</h2><p><code>apt install scala</code>
测试
<code>scala</code></p><h2 id=安装-vim-与-网络工具包>安装 Vim 与 网络工具包</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>安装 vim，用来编辑文件
</span></span><span style=display:flex><span>apt install vim
</span></span><span style=display:flex><span>安装 net-tools
</span></span><span style=display:flex><span>apt install net-tools
</span></span></code></pre></div><h2 id=安装-ssh>安装 SSH</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>安装 SSH，并配置免密登录，由于后面的容器之间是由一个镜像启动的，所以在当前容器里配置 SSH 自身免密登录就 OK 了。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>安装 SSH
</span></span><span style=display:flex><span>apt-get install openssh-server
</span></span><span style=display:flex><span>安装 SSH 的客户端
</span></span><span style=display:flex><span>apt-get install openssh-client
</span></span><span style=display:flex><span>进入当前用户的用户根目录
</span></span><span style=display:flex><span>cd ~
</span></span><span style=display:flex><span>生成密钥，一直回车就行
</span></span><span style=display:flex><span>ssh-keygen -t rsa -P &#34;&#34;
</span></span><span style=display:flex><span>生成的密钥在当前用户根目录下的 .ssh 文件夹中以 . 开头的文件与文件夹 ls 是看不懂的，需要ls -al才能查看。
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>将公钥追加到 authorized_keys 文件中
</span></span><span style=display:flex><span>cat .ssh/id_rsa.pub &gt;&gt; .ssh/authorized_keys
</span></span><span style=display:flex><span>启动 SSH 服务
</span></span><span style=display:flex><span>service ssh start
</span></span><span style=display:flex><span>免密登录自己
</span></span><span style=display:flex><span>ssh 127.0.0.1
</span></span><span style=display:flex><span>修改 .bashrc 文件，启动 shell 的时候，自动启动 SSH 服务
</span></span><span style=display:flex><span>vim ~/.bashrc
</span></span><span style=display:flex><span>添加一行
</span></span><span style=display:flex><span>service ssh start
</span></span></code></pre></div><h1 id=安装-hadoop>安装 Hadoop</h1><h2 id=下载-hadoop>下载 Hadoop</h2><p><code>wget http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz</code></p><h2 id=解压>解压</h2><p>到 /usr/local 目录下面并重命名文件夹
<code>tar -zxvf hadoop-3.2.1.tar.gz -C /usr/local/</code>
<code>cd /usr/local/</code>
<code>mv hadoop-3.2.1 hadoop</code></p><h2 id=添加环境变量>添加环境变量</h2><p><code>vim /etc/profile</code>
追加以下内容，JAVA_HOME 为 JDK 安装路径，使用 apt 安装就是这个，用 <code>update-alternatives --config java </code>可查看</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>#java
</span></span><span style=display:flex><span>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
</span></span><span style=display:flex><span>export JRE_HOME=${JAVA_HOME}/jre    
</span></span><span style=display:flex><span>export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    
</span></span><span style=display:flex><span>export PATH=${JAVA_HOME}/bin:$PATH
</span></span><span style=display:flex><span>#hadoop
</span></span><span style=display:flex><span>export HADOOP_HOME=/usr/local/hadoop
</span></span><span style=display:flex><span>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</span></span><span style=display:flex><span>export HADOOP_COMMON_HOME=$HADOOP_HOME 
</span></span><span style=display:flex><span>export HADOOP_HDFS_HOME=$HADOOP_HOME 
</span></span><span style=display:flex><span>export HADOOP_MAPRED_HOME=$HADOOP_HOME
</span></span><span style=display:flex><span>export HADOOP_YARN_HOME=$HADOOP_HOME 
</span></span><span style=display:flex><span>export HADOOP_INSTALL=$HADOOP_HOME 
</span></span><span style=display:flex><span>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
</span></span><span style=display:flex><span>export HADOOP_CONF_DIR=$HADOOP_HOME 
</span></span><span style=display:flex><span>export HADOOP_LIBEXEC_DIR=$HADOOP_HOME/libexec 
</span></span><span style=display:flex><span>export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
</span></span><span style=display:flex><span>export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
</span></span><span style=display:flex><span>export HDFS_DATANODE_USER=root
</span></span><span style=display:flex><span>export HDFS_DATANODE_SECURE_USER=root
</span></span><span style=display:flex><span>export HDFS_SECONDARYNAMENODE_USER=root
</span></span><span style=display:flex><span>export HDFS_NAMENODE_USER=root
</span></span><span style=display:flex><span>export YARN_RESOURCEMANAGER_USER=root
</span></span><span style=display:flex><span>export YARN_NODEMANAGER_USER=root
</span></span></code></pre></div><p>使环境变量生效
<code>source /etc/profile</code></p><h2 id=修改配置文件>修改配置文件</h2><p>在目录 /usr/local/hadoop/etc/hadoop 下，修改 hadoop-env.sh 文件，在文件末尾添加以下信息
<code>cd /usr/local/hadoop/etc/hadoop</code>
<code>vim hadoop-env.sh</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
</span></span><span style=display:flex><span>export HDFS_NAMENODE_USER=root
</span></span><span style=display:flex><span>export HDFS_DATANODE_USER=root
</span></span><span style=display:flex><span>export HDFS_SECONDARYNAMENODE_USER=root
</span></span><span style=display:flex><span>export YARN_RESOURCEMANAGER_USER=root
</span></span><span style=display:flex><span>export YARN_NODEMANAGER_USER=root
</span></span></code></pre></div><p>修改 core-site.xml，修改为
<code>vim core-site.xml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;configuration&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;fs.default.name&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;hdfs://h01:9000&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;/home/hadoop3/hadoop/tmp&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>&lt;/configuration&gt;
</span></span></code></pre></div><p><code>chmod 777 /home/hadoop3/hadoop/tmp</code>
修改 hdfs-site.xml，修改为
<code>vim hdfs-site.xml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;configuration&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;dfs.replication&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;2&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;/home/hadoop3/hadoop/hdfs/name&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;/home/hadoop3/hadoop/hdfs/data&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>&lt;/configuration&gt;
</span></span></code></pre></div><p>修改 mapred-site.xml，修改为
<code>vim mapred-site.xml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;configuration&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;yarn&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;
</span></span><span style=display:flex><span>            /usr/local/hadoop/etc/hadoop,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/common/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/common/lib/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/hdfs/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/hdfs/lib/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/mapreduce/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/mapreduce/lib/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/yarn/*,
</span></span><span style=display:flex><span>            /usr/local/hadoop/share/hadoop/yarn/lib/*
</span></span><span style=display:flex><span>        &lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>&lt;/configuration&gt;
</span></span></code></pre></div><p>修改 yarn-site.xml，修改为
<code>vim yarn-site.xml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;configuration&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;h01&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>    &lt;property&gt;
</span></span><span style=display:flex><span>        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
</span></span><span style=display:flex><span>        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
</span></span><span style=display:flex><span>    &lt;/property&gt;
</span></span><span style=display:flex><span>&lt;/configuration&gt;
</span></span></code></pre></div><p>修改 worker 为
<code>vim workers</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>h01
</span></span><span style=display:flex><span>h02
</span></span><span style=display:flex><span>h03
</span></span><span style=display:flex><span>h04
</span></span><span style=display:flex><span>h05
</span></span></code></pre></div><p>此时，hadoop已经配置好了</p><h2 id=在-docker-中启动集群>在 Docker 中启动集群</h2><h3 id=将当前容器导出为镜像>将当前容器导出为镜像</h3><p><code>exit</code>
<code>sudo docker commit -m "hadoop" -a "tomding" fab4da838c2f newuhadoop</code>
查看镜像
<code>sudo docker images</code></p><h3 id=启动-5-个终端>启动 5 个终端</h3><p>启动 h01 做 master 节点，所以暴露了端口，以供访问 web 页面，&ndash;network hadoop 参数是将当前容器加入到名为 hadoop 的虚拟桥接网络中，此网站提供自动的 DNS 解析功能
<code>sudo docker run -it --network hadoop -h "h01" --name "h01" -p 9870:9870 -p 8088:8088 newuhadoop /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h02" --name "h02" newuhadoop /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h03" --name "h03" newuhadoop /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h04" --name "h04" newuhadoop /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h05" --name "h05" newuhadoop /bin/bash</code></p><h3 id=在-h01-主机中启动-haddop-集群>在 h01 主机中，启动 Haddop 集群</h3><p>先进行格式化操作，不格式化操作，hdfs会起不来
<code>cd /usr/local/hadoop/bin</code>
<code>./hadoop namenode -format</code>
进入 hadoop 的 sbin 目录
<code>cd /usr/local/hadoop/sbin/</code>
启动
<code>./start-all.sh</code>
查看分布式文件系统的状态
<code>cd /usr/local/hadoop/bin</code>
<code>./hadoop dfsadmin -report</code></p><h2 id=运行内置wordcount例子>运行内置WordCount例子</h2><p>把license作为需要统计的文件
<code>cd /usr/local/hadoop</code>
<code>cat LICENSE.txt > file1.txt</code>
在 HDFS 中创建 input 文件夹
<code>cd /usr/local/hadoop/bin</code>
<code>./hadoop fs -mkdir /input</code>
上传 file1.txt 文件到 HDFS 中
<code>./hadoop fs -put ../file1.txt /input</code>
查看 HDFS 中 input 文件夹里的内容
<code>./hadoop fs -ls /input</code>
运作 wordcount 例子程序
<code>./hadoop jar ../share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /input /output</code>
查看 HDFS 中的 /output 文件夹的内容
<code>./hadoop fs -ls /output</code>
查看 part-r-00000 文件的内容
<code>./hadoop fs -cat /output/part-r-00000</code>
Hadoop 部分结束了</p><h1 id=安装-hbase>安装 Hbase</h1><p>在 Hadoop 集群的基础上安装 Hbase</p><h2 id=下载-hbase-213>下载 Hbase 2.1.3</h2><p>root@h01:~# <code>wget http://archive.apache.org/dist/hbase/2.1.3/hbase-2.1.3-bin.tar.gz</code></p><h2 id=解压-1>解压</h2><p>到 /usr/local 目录下面
<code>tar -zxvf hbase-2.1.3-bin.tar.gz -C /usr/local/</code></p><h2 id=修改环境变量文件>修改环境变量文件</h2><p><code>vim /etc/profile</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>export HBASE_HOME=/usr/local/hbase-2.1.3
</span></span><span style=display:flex><span>export PATH=$PATH:$HBASE_HOME/bin
</span></span></code></pre></div><p><code>source /etc/profile</code>
使用 ssh h02/3/4/5 进入其他四个容器，依次在 /etc/profile 文件后追加那两行环境变量</p><p>在目录 /usr/local/hbase-2.1.3/conf 修改配置
<code>cd /usr/local/hbase-2.1.3/conf</code>
修改 hbase-env.sh，追加
<code>vim hbase-env.sh</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
</span></span><span style=display:flex><span>export HBASE_MANAGES_ZK=true
</span></span></code></pre></div><h2 id=修改配置文件-1>修改配置文件</h2><p>修改 hbase-site.xml 为
<code>vim hbase-site.xml</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>&lt;configuration&gt;
</span></span><span style=display:flex><span>        &lt;property&gt;
</span></span><span style=display:flex><span>                &lt;name&gt;hbase.rootdir&lt;/name&gt;
</span></span><span style=display:flex><span>                &lt;value&gt;hdfs://h01:9000/hbase&lt;/value&gt;
</span></span><span style=display:flex><span>        &lt;/property&gt;
</span></span><span style=display:flex><span>        &lt;property&gt;
</span></span><span style=display:flex><span>                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
</span></span><span style=display:flex><span>                &lt;value&gt;true&lt;/value&gt;
</span></span><span style=display:flex><span>        &lt;/property&gt;
</span></span><span style=display:flex><span>        &lt;property&gt;
</span></span><span style=display:flex><span>                &lt;name&gt;hbase.master&lt;/name&gt;
</span></span><span style=display:flex><span>                &lt;value&gt;h01:60000&lt;/value&gt;
</span></span><span style=display:flex><span>        &lt;/property&gt;
</span></span><span style=display:flex><span>        &lt;property&gt;
</span></span><span style=display:flex><span>                &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
</span></span><span style=display:flex><span>                &lt;value&gt;h01,h02,h03,h04,h05&lt;/value&gt;
</span></span><span style=display:flex><span>        &lt;/property&gt;
</span></span><span style=display:flex><span>        &lt;property&gt;
</span></span><span style=display:flex><span>                &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
</span></span><span style=display:flex><span>                &lt;value&gt;/home/hadoop/zoodata&lt;/value&gt;
</span></span><span style=display:flex><span>        &lt;/property&gt;
</span></span><span style=display:flex><span>&lt;/configuration&gt;
</span></span></code></pre></div><p>修改 regionservers 文件为
<code>vim regionservers</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>h01
</span></span><span style=display:flex><span>h02
</span></span><span style=display:flex><span>h03
</span></span><span style=display:flex><span>h04
</span></span><span style=display:flex><span>h05
</span></span></code></pre></div><p>使用 scp 命令将配置好的 Hbase 复制到其他 4 个容器中
<code>scp -r /usr/local/hbase-2.1.3 root@h02:/usr/local/</code>
<code>scp -r /usr/local/hbase-2.1.3 root@h03:/usr/local/</code>
<code>scp -r /usr/local/hbase-2.1.3 root@h04:/usr/local/</code>
<code>scp -r /usr/local/hbase-2.1.3 root@h05:/usr/local/</code></p><h2 id=启动-hbase>启动 Hbase</h2><p><code>cd /usr/local/hbase-2.1.3/bin</code>
<code>./start-hbase.sh</code>
打开 Hbase 的 shell
<code>hbase shell</code></p><h1 id=安装-spark>安装 Spark</h1><p>在 Hadoop 的基础上安装 Spark</p><h2 id=下载-spark-240>下载 Spark 2.4.0</h2><p><code>wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</code></p><h2 id=解压-2>解压</h2><p>到 /usr/local 目录下面
<code>tar -zxvf spark-2.4.0-bin-hadoop2.7.tgz -C /usr/local/</code>
修改文件夹的名字
<code>cd /usr/local/</code>
<code>mv spark-2.4.0-bin-hadoop2.7 spark-2.4.0</code></p><h2 id=修改-环境变量>修改 环境变量</h2><p><code>vim /etc/profile</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>export SPARK_HOME=/usr/local/spark-2.4.0
</span></span><span style=display:flex><span>export PATH=$PATH:$SPARK_HOME/bin
</span></span></code></pre></div><p><code>source /etc/profile</code></p><p>使用 ssh h02/3/4/5 可进入其他四个容器，依次在 /etc/profile 文件后追加那两行环境变量</p><p>在目录 /usr/local/spark-2.4.0/conf 修改配置
<code>cd /usr/local/spark-2.4.0/conf</code>
修改文件名
<code>mv spark-env.sh.template spark-env.sh</code>
修改 spark-env.sh，追加
<code>vim spark-env.sh</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
</span></span><span style=display:flex><span>export HADOOP_HOME=/usr/local/hadoop
</span></span><span style=display:flex><span>export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
</span></span><span style=display:flex><span>export SCALA_HOME=/usr/share/scala
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>export SPARK_MASTER_HOST=h01
</span></span><span style=display:flex><span>export SPARK_MASTER_IP=h01
</span></span><span style=display:flex><span>export SPARK_WORKER_MEMORY=4g
</span></span></code></pre></div><p>修改文件名
<code>mv slaves.template slaves</code>
修改 slaves 如下
<code>vim slaves</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>h01
</span></span><span style=display:flex><span>h02
</span></span><span style=display:flex><span>h03
</span></span><span style=display:flex><span>h04
</span></span><span style=display:flex><span>h05
</span></span></code></pre></div><p>使用 scp 命令将配置好的 Hbase 复制到其他 4 个容器中
<code>scp -r /usr/local/spark-2.4.0 root@h02:/usr/local/</code>
<code>scp -r /usr/local/spark-2.4.0 root@h03:/usr/local/</code>
<code>scp -r /usr/local/spark-2.4.0 root@h04:/usr/local/</code>
<code>scp -r /usr/local/spark-2.4.0 root@h05:/usr/local/</code></p><h2 id=启动-spark>启动 Spark</h2><p><code>cd /usr/local/spark-2.4.0/sbin</code>
<code>./start-all.sh</code></p><p>其他
3.1 HDFS 重格式化问题
参考
<a href=https://blog.csdn.net/gis_101/article/details/52821946 title=https://blog.csdn.net/gis_101/article/details/52821946 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://blog.csdn.net/gis_101/article/details/52821946
<i class="fa fa-external-link-alt"></i></a>
<del>重新格式化意味着集群的数据会被全部删除，格式化前需考虑数据备份或转移问题</del>；
先删除主节点（即namenode节点），Hadoop的临时存储目录tmp、namenode存储永久性元数据目录dfs/name、Hadoop系统日志文件目录log 中的内容 （注意是删除目录下的内容不是目录）；
删除所有数据节点(即datanode节点) ，Hadoop的临时存储目录tmp、namenode存储永久性元数据目录dfs/name、Hadoop系统日志文件目录log 中的内容；</p><p>格式化一个新的分布式文件系统：
<code>cd /usr/local/hadoop/bin</code>
<code>./hadoop namenode -format</code></p><p>注意事项</p><blockquote><p>Hadoop的临时存储目录tmp（即core-site.xml配置文件中的hadoop.tmp.dir属性，默认值是/tmp/hadoop-{user.name}），
如果没有配置hadoop.tmp.dir属性，那么hadoop格式化时将会在/tmp目录下创建一个目录，例如在cloud用户下安装配置hadoop，那么Hadoop的临时存储目录就位于/tmp/hadoop-cloud目录下Hadoop的namenode元数据目录（即hdfs-site.xml配置文件中的dfs.namenode.name.dir属性，默认{hadoop.tmp.dir}/dfs/name），
同样如果没有配置该属性，那么hadoop在格式化时将自行创建。必须注意的是在格式化前必须清楚所有子节点（即DataNode节点）dfs/name下的内容，否则在启动hadoop时子节点的守护进程会启动失败。这是由于，每一次format主节点namenode，dfs/name/current目录下的VERSION文件会产生新的clusterID、namespaceID。
但是如果子节点的dfs/name/current仍存在，hadoop格式化时就不会重建该目录，因此形成子节点的clusterID、namespaceID与主节点（即namenode节点）的clusterID、namespaceID不一致。最终导致hadoop启动失败。</p></blockquote><p>进入：
<code>sudo docker run -it --network hadoop -h "h01" --name "h01" -p 9870:9870 -p 8088:8088 master /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h02" --name "h02" slave1 /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h03" --name "h03" slave2 /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h04" --name "h04" slave3 /bin/bash</code>
<code>sudo docker run -it --network hadoop -h "h05" --name "h05" slave14 /bin/bash</code>
退出：
<code>sudo docker commit -m "master" -a "tomding" h01 master</code>
<code>sudo docker commit -m "slave1" -a "tomding" h02 slave1</code>
<code>sudo docker commit -m "slave2" -a "tomding" h03 slave2</code>
<code>sudo docker commit -m "slave3" -a "tomding" h04 slave3</code>
<code>sudo docker commit -m "slave4" -a "tomding" h05 slave4</code></p></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
dcoker-Docker</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/ title=dcoker-Docker>/post/dcoker-docker%E5%AE%89%E8%A3%85hadoop/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/dcoker-docker%E5%91%BD%E4%BB%A4/ rel=next title=dcoker-Docker><i class="fa fa-chevron-left"></i> dcoker-Docker</a></div><div class="post-nav-prev post-nav-item"><a href=/post/linux-linux%E5%91%BD%E4%BB%A4/ rel=prev title=dcoker-Docker>dcoker-Docker
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>