<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="聚类-SOM神经网络算法"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="聚类-SOM神经网络算法"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2023-01-01 14:28:53 +0800 CST"><meta property="article:modified_time" content="2023-01-01 14:28:53 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95","permalink":"/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/","title":"聚类-SOM神经网络算法","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>聚类-SOM神经网络算法 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>126</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><ul><li><a href=#neighborhood-function>neighborhood function</a></li><li><a href=#学习率α邻域范围σ随时间衰减>学习率α、邻域范围σ随时间衰减</a></li></ul></li></ul><ul><li><ul><li><ul><li></li></ul></li><li><a href=#2iris-数据集实验>2.iris 数据集实验</a><ul><li></li></ul></li><li><a href=#3异常值>3.异常值</a></li><li><a href=#4numpy>4.numpy</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>126</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>10</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=446002></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=957></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-01-01T14:28:53+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="聚类-SOM神经网络算法"><meta itemprop=description content="自组织映射（Self Organizing Map，SOM）。蓝色斑点是训练数据的分布，而小白色斑点是从该分布中抽取得到的当前训练数据。首先（左图）SOM节点被任"></span><header class=post-header><h1 class=post-title itemprop="name headline">聚类-SOM神经网络算法
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e8%81%9a%e7%b1%bb-SOM%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-01-01 14:28:53 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-01-01 14:28:53 +0800 CST">2023-01-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E8%81%9A%E7%B1%BB itemprop=url rel=index><span itemprop=name>聚类</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>4682</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>10分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-135ea1f9251fdb66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>自组织映射（Self Organizing Map，SOM）。<strong>蓝色斑点是训练数据的分布</strong>，而<strong>小白色斑点是从该分布中抽取得到的当前训练数据</strong>。首先（左图）<strong>SOM节点被任意地定位在数据空间中</strong>。我们选择<strong>最接近训练数据的节点作为获胜节点</strong>（用黄色突出显示）。<strong>它被移向训练数据，包括（在较小的范围内）其网格上的相邻节点</strong>。经过多次迭代后，网格趋于接近数据分布（右图）。</p><h1 id=算法流程>算法流程</h1><p>我们有一个空间连续的输入空间，其中包含我们的输入向量。我们的目的是将其映射到低维的离散输出空间，其拓扑结构是通过在网格中布置一系列神经元形成的。我们的SOM算法提供了称为特征映射的非线性变换。</p><ul><li><p>随机化映射(map)中的节点权重向量</p></li><li><p>随机选取输入向量${{D}(t)}$</p></li><li><p>遍历映射中每一个节点
计算${\displaystyle{D}(t)}$与映射节点之间的相似度(通常使用欧式距离);选取距离最小的节点作为优胜节点(winner node)$，有的时也叫BMU(best matching unit)</p></li><li><p>更新BMU（包括BMU本身）附近节点的权重向量，方法是将它们拉近输入向量,当前权重向量为$\mathbf{W}{v}$的输出节点$v$其权值更新公式为
$${ W_{v}(s+1)=W_{v}(s)+\theta(u, v, ，，s)\cdot\alpha(s)\cdot (D(t)-W_{v}(s))}$$
其中$s$为迭代次数，$D(t)$是当前输入向量$u$为获胜节点
$\theta(u，v，s)$是 $s$ 下给出 $u $和$ v $之间距离的邻近函数，用来确定获胜节点对其近邻节点的影响强弱，$\alpha(s)$是一个单调递减的学习率</p></li><li><p>增加${\displaystyle s}$并在${\displaystyle s&lt;\lambda}$时从步骤2开始重复</p></li></ul><p>###　评估
一种衡量SOM优劣的指标是量化误差（quantization error），输入样本 与 对应的winner神经元的weight 之间的 平方根.可以用下式表示：
$E_{q}=\sum_{i=1}^{n}\left|x_{i}-w_{j}\right|^{2}$</p><h3 id=neighborhood-function>neighborhood function</h3><p>neighborhood函数用来确定优胜节点对其近邻节点的影响强弱，即优胜邻域中每个节点的更新幅度。最常见的选择是高斯函数，它可以表征优胜邻域内，影响强弱与距离的关系。</p><p>假设中心节点是优胜节点</p><ul><li>高斯近邻函数：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-c0eb8c58c684fbf1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></li></ul><p>是连续的，因此sigma的有效取值范围也是连续的
当选sigma设为1时，所有的节点都有一定的更新幅度，中心优胜节点是1，越远离优胜节点，更新幅度越低，
当sigma取值很小时，只有优胜节点更新幅度是1，其余几乎都接近0
当sigma取值较大时，即使是边缘的节点，也有较大的更新幅度</p><ul><li>Bubble近邻函数：只要是在优胜邻域内的神经元，更新系数都是相同的
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-1dedc77c3ebea64e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
因此, sigma的有效取值是离散的：
0.5：仅优胜节点
1.5：周围一圈
2.5：周围2圈</li></ul><h3 id=学习率α邻域范围σ随时间衰减>学习率α、邻域范围σ随时间衰减</h3><p>SOM网络的另一个特点是，学习率和邻域范围随着迭代次数会逐渐衰减</p><h1 id=优缺点>优缺点</h1><p>能够保持拓扑结构不变，泛化能力好、抗噪音能力强</p><h1 id=minisomhttpsgithubcomjustglowingminisom><a href=https://github.com/JustGlowing/minisom title=MiniSom rel="noopener external nofollow noreferrer" target=_blank class=exturl>MiniSom
<i class="fa fa-external-link-alt"></i></a></h1><ul><li>som.get_weights()： Returns the weights of the neural network</li><li>som.distance_map()：Returns the distance map of the weights</li><li>som.activate(X)： Returns the activation map to x 值越小的神经元，表示与输入样本 越匹配</li><li>som.quantization(X)：Assigns a code book 给定一个 输入样本，找出该样本的优胜节点，然后返回该神经元的权值向量(每个元素对应一个输入单元)</li><li>som.winner(X)： 给定一个 输入样本，找出该样本的优胜节点 格式：输出平面中的位置</li><li>som.win_map(X)：将各个样本，映射到平面中对应的位置 返回一个dict { position: samples_list }</li><li>som.activation_response(X)： 返回输出平面中，各个神经元成为 winner的次数 格式为 1个二维矩阵</li><li>quantization_error(量化误差)： 输入样本 与 对应的winner神经元的weight 之间的 平方根</li></ul><h6 id=1创建一个简单的颜色量化模型>1.创建一个简单的颜色量化模型</h6><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6c5698a178fd111c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>from minisom import MiniSom
</span></span><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import matplotlib.pyplot as plt
</span></span><span style=display:flex><span>%matplotlib inline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>img = plt.imread(r&#39;C:\Users\tomding\Pictures\640.jpg&#39;)
</span></span><span style=display:flex><span>pixels = np.reshape(img, (img.shape[0]*img.shape[1], 3))/255
</span></span><span style=display:flex><span>print(img.shape)
</span></span><span style=display:flex><span># (170, 296, 3)
</span></span></code></pre></div><p><code>som = MiniSom(x=3, y=3, input_len=3, sigma=0.1, learning_rate=0.2)</code></p><blockquote><ul><li>第一个参数是 SOM 的维度。在我们的例子中，我们将构建一个 3<em>3 的 SOM。这意味着我们得到的最终颜色将是 3 * 3，即 9。这里的数字可以随意尝试，看看你会得到哪些不同的结果。为降维任务设置网格大小的经验法则是，它应该包含**5</em>sqrt（N）**个神经元，其中N是要分析的数据集中的样本数。</li><li>第二个参数是 input_len，它是我们数据集中的特征数量。在我们的例子中，我们使用 3，对应了像素数组的形状。</li><li>下一个参数是 sigma，它是 SOM 中不同相邻节点的半径，默认值为 1.0。</li><li>最后一个参数是 learning_rate，它确定每次迭代期间权重的调整幅度。</li><li>Neighborhood_function:&lsquo;gaussian&rsquo;、&lsquo;mexican_hat&rsquo;、&lsquo;bubble&rsquo;</li></ul></blockquote><p><code>som.random_weights_init(pixels)</code></p><blockquote><p>将SOM的权重初始化为小的标准化随机值。我们使用random_weights_init函数并传入我们的数据（像素）来实现这一点。</p></blockquote><p><code>starting_weights = som.get_weights().copy()</code></p><blockquote><p>我们现在需要保存起始权重，它们代表图像的初始颜色。稍后会将它们可视化。我们保存这些权重的方法是使用 get_weights 函数和 Python 的副本来确保在权重更新之前得到它们。</p></blockquote><p><code>som.train_random(pixels, 100)</code></p><blockquote><p>然后我们通过运行 train_random 函数来训练像素。它需要两个参数，第一个参数是需要训练的数据集，第二个参数是迭代次数。</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>qnt = som.quantization(pixels)
</span></span><span style=display:flex><span>print(pixels.shape, starting_weights.shape, qnt.shape)
</span></span><span style=display:flex><span># (165000, 3) (3, 3, 3) (165000, 3)
</span></span><span style=display:flex><span># 计算分配给地图上的观察 x的坐标，方法为 winner(x)。
</span></span><span style=display:flex><span># 使用 distance_map() 方法计算地图上权重的平均距离图。
</span></span><span style=display:flex><span># 计算每个神经元被认为是新数据集观察的赢家的次数，方法是 activation_response(data)。
</span></span><span style=display:flex><span># 用 quantization_error(data) 方法计算量化误差。
</span></span><span style=display:flex><span># 矢量量化是由 quantization 方法实现的,颜色量化就是减少图像中所用的特殊颜色,所谓矢量量化算法就是找到一个原型矢量数据集w_i，i=1…，m。用它最大程度的近似表示原始的数据集。最著名的算法k-均值（k-means）算法，它可以很方便的找到原型矢量数据集，并使其量化误差最小。原型矢量的数据密度决定于待训练样本数据密度，总的来说，它满足下式：
</span></span></code></pre></div><blockquote><p>量化图像的每个像素。在此过程中，我们将减少图像中的颜色数量。我们使用 MiniSom 中的 quantization 实用程序来完成这一步。</p></blockquote><p><code>clustered = np.zeros(img.shape)</code></p><blockquote><p>将图像构建为 3D 图像。</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>for i, q in enumerate(qnt):
</span></span><span style=display:flex><span>  clustered[np.unravel_index(i, dims=(img.shape[0], img.shape[1]))] = q
</span></span></code></pre></div><blockquote><p>接下来我们需要做的是将量化后的值放入新图像中。
numpy.unravel_index()函数的作用是获取一个/组int类型的索引值在一个多维数组中的位置。这个函数有三个参数：
indices：一个整数数组，其元素是维度变暗数组的平铺版本的索引。
dims：整数元组用于解开 indices.dex 值的数组的形状
Order：{&lsquo;C&rsquo;，&lsquo;F&rsquo;}，可选确定是否应将索引视为以行主（C样式）或列主（Fortran样式）顺序编制索引。</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>result = np.zeros(img.shape)
</span></span><span style=display:flex><span>for i, q in enumerate(qnt):
</span></span><span style=display:flex><span>     result[np.unravel_index(i, dims=(img.shape[0], img.shape[1]))] = q
</span></span><span style=display:flex><span>plt.figure(figsize=(12, 6))
</span></span><span style=display:flex><span>plt.subplot(221)
</span></span><span style=display:flex><span>plt.title(&#39;Original&#39;)
</span></span><span style=display:flex><span>plt.imshow(img)
</span></span><span style=display:flex><span>plt.subplot(222)
</span></span><span style=display:flex><span>plt.title(&#39;Result&#39;)
</span></span><span style=display:flex><span>plt.imshow(result)
</span></span><span style=display:flex><span>plt.subplot(223)
</span></span><span style=display:flex><span>plt.title(&#39;Initial Colors&#39;)
</span></span><span style=display:flex><span>plt.imshow(starting_weights)
</span></span><span style=display:flex><span>plt.subplot(224)
</span></span><span style=display:flex><span>plt.title(&#39;Learnt Colors&#39;)
</span></span><span style=display:flex><span>plt.imshow(som.get_weights())
</span></span><span style=display:flex><span>plt.tight_layout()
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-d3005164327dee54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h3 id=2iris-数据集实验>2.iris 数据集实验</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import sys
</span></span><span style=display:flex><span>import math
</span></span><span style=display:flex><span>from minisom import MiniSom
</span></span><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import matplotlib.pyplot as plt
</span></span><span style=display:flex><span>from matplotlib.gridspec import GridSpec
</span></span><span style=display:flex><span>import pandas as pd
</span></span><span style=display:flex><span>from matplotlib.patches import Patch
</span></span><span style=display:flex><span>from sklearn import datasets
</span></span><span style=display:flex><span>from sklearn.model_selection import train_test_split
</span></span><span style=display:flex><span>from sklearn.metrics import classification_report
</span></span><span style=display:flex><span>iris = datasets.load_iris()
</span></span><span style=display:flex><span>feature_names = iris.feature_names
</span></span><span style=display:flex><span>class_names = iris.target_names
</span></span><span style=display:flex><span>X = iris.data
</span></span><span style=display:flex><span>y = iris.target
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)
</span></span><span style=display:flex><span>N = X_train.shape[0]  # 样本数量
</span></span><span style=display:flex><span>M = X_train.shape[1]  # 维度/特征数量
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>size = math.ceil(np.sqrt(5 * np.sqrt(N)))  # 经验公式：决定输出层尺寸
</span></span><span style=display:flex><span>print(&#34;训练样本个数:{}  测试样本个数:{}&#34;.format(N,X_test.shape[0]))
</span></span><span style=display:flex><span>print(&#34;输出网格最佳边长为:&#34;,size)
</span></span><span style=display:flex><span>max_iter = 200
</span></span><span style=display:flex><span>som = MiniSom(size, size, M, sigma=3, learning_rate=0.5, 
</span></span><span style=display:flex><span>              neighborhood_function=&#39;bubble&#39;)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 初始化权重以跨越前两个主分量。这种初始化不依赖于随机过程，使训练过程收敛得更快。
</span></span><span style=display:flex><span>#　强烈建议在初始化权重之前对数据进行规范化，并对训练数据使用相同的规范化。
</span></span><span style=display:flex><span>som.pca_weights_init(X_train)
</span></span><span style=display:flex><span>som.train_batch(X_train, 50000)  
</span></span><span style=display:flex><span>#  som.win_map(X_train)  返回字典wm，其中wm[（i，j）]是一个列表，其中包含在位置i，j中映射的所有模式。
</span></span><span style=display:flex><span># (7, 3): [array([5. , 2. , 3.5, 1. ]), array([4.9, 2.4, 3.3, 1. ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># som.labels_map返回字典wm，其中wm[（i，j）]是一个字典，它包含在位置i，j中映射的给定标签的样本数。{
</span></span><span style=display:flex><span># {(7, 3): Counter({1: 3}), (2, 2): Counter({2: 1}), (0, 3): Counter({2: 3})
</span></span><span style=display:flex><span>win_map = som.labels_map(X_train,y_train)# 
</span></span></code></pre></div><h6 id=根据权重矩阵w我们可以计算每个神经元-距离它的邻近神经元们的距离计算好的矩阵就是u-matrix>根据权重矩阵W,我们可以计算每个神经元 距离它的邻近神经元们的距离，计算好的矩阵就是U-Matrix</h6><p>在矩阵中较小的值表示该节点与其邻近节点在输入空间靠得近因此，U-matrix可以看作输入空间中数据点概率密度在二维平面上的映射</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>heatmap = som.distance_map()  #生成U-Matrix
</span></span><span style=display:flex><span>print(heatmap.shape)
</span></span><span style=display:flex><span>plt.imshow(heatmap, cmap=&#39;bone_r&#39;)      #miniSom案例中用的pcolor函数,需要调整坐标
</span></span><span style=display:flex><span>plt.colorbar()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6bf0577ce7cedb54.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>plt.figure(figsize=(9, 9))
</span></span><span style=display:flex><span>heatmap = som.distance_map()
</span></span><span style=display:flex><span>plt.pcolor(heatmap, cmap=&#39;bone_r&#39;)  # plotting the distance map as background
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 定义不同标签的图案标记
</span></span><span style=display:flex><span>markers = [&#39;o&#39;, &#39;s&#39;, &#39;D&#39;]
</span></span><span style=display:flex><span>colors = [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;]
</span></span><span style=display:flex><span>category_color = {&#39;setosa&#39;: &#39;C0&#39;,
</span></span><span style=display:flex><span>                  &#39;versicolor&#39;: &#39;C1&#39;,
</span></span><span style=display:flex><span>                  &#39;virginica&#39;: &#39;C2&#39;}
</span></span><span style=display:flex><span>for cnt, xx in enumerate(X_train):
</span></span><span style=display:flex><span>    position = som.winner(xx)  # getting the winner
</span></span><span style=display:flex><span>    # 在样本Heat的地方画上标记
</span></span><span style=display:flex><span>    plt.plot(position[0]+.5,  position[1]+.5, markers[y_train[cnt]], markerfacecolor=&#39;None&#39;,
</span></span><span style=display:flex><span>             markeredgecolor=colors[y_train[cnt]], markersize=12, markeredgewidth=2)
</span></span><span style=display:flex><span>plt.axis([0, size, 0, size])
</span></span><span style=display:flex><span>ax = plt.gca()
</span></span><span style=display:flex><span>ax.invert_yaxis() #颠倒y轴方向
</span></span><span style=display:flex><span>legend_elements = [Patch(facecolor=clr, edgecolor=&#39;w&#39;, label=l) 
</span></span><span style=display:flex><span>                   for l, clr in category_color.items()]
</span></span><span style=display:flex><span>plt.legend(handles=legend_elements, loc=&#39;center left&#39;, bbox_to_anchor=(1, .95))
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4cec2a60540aaf9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h6 id=不清楚每个格子的样本数量-如果有个类别落到同一个格子看不出比例>不清楚每个格子的样本数量 ,如果有个类别落到同一个格子，看不出比例</h6><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>label_name_map_number = {&#34;setosa&#34;:0,&#34;versicolor&#34;:1,&#34;virginica&#34;:2}
</span></span><span style=display:flex><span>from matplotlib.gridspec import GridSpec
</span></span><span style=display:flex><span>plt.figure(figsize=(9, 9))
</span></span><span style=display:flex><span># 画非对称的子图
</span></span><span style=display:flex><span>the_grid = GridSpec(size, size)
</span></span><span style=display:flex><span>for position in win_map.keys():
</span></span><span style=display:flex><span>    label_num = [win_map[position][label] 
</span></span><span style=display:flex><span>                   for label in [0, 1, 2]] # 该位置各个类别的数量
</span></span><span style=display:flex><span>    plt.subplot(the_grid[position[1], position[0]], aspect=1)
</span></span><span style=display:flex><span>    patches, _ = plt.pie(label_num) 
</span></span><span style=display:flex><span>    # 如果没有设置autopct,返回(patches, texts)如果设置autopct,返回(patches, texts, autotexts)
</span></span><span style=display:flex><span>    # patches -- list --matplotlib.patches.Wedge对象 texts autotexts -- matplotlib.text.Text对象autotexts：列表。A是数字标签的Text实例列表。
</span></span><span style=display:flex><span>    plt.text(position[0]/100, position[1]/100,  str(len(list(win_map[position].elements()))),
</span></span><span style=display:flex><span>              color=&#39;black&#39;, fontdict={&#39;weight&#39;: &#39;bold&#39;,  &#39;size&#39;: 15}, va=&#39;center&#39;,ha=&#39;center&#39;)
</span></span><span style=display:flex><span>plt.legend(patches, class_names, loc=&#39;center right&#39;, bbox_to_anchor=(-1,9), ncol=3)
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-b29b0e07468fc2be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h6 id=component-plane每个神经元对单个特征什么取值最敏感wi则取出特征i对应的权值矩阵>Component Plane每个神经元对单个特征什么取值最敏感。W[:,:,i]则取出特征i对应的权值矩阵</h6><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>W = som.get_weights()
</span></span><span style=display:flex><span>plt.figure(figsize=(10, 10))
</span></span><span style=display:flex><span>for i, f in enumerate(feature_names):
</span></span><span style=display:flex><span>    plt.subplot(3, 3, i+1)
</span></span><span style=display:flex><span>    plt.title(f)
</span></span><span style=display:flex><span>    plt.imshow(W[:,:,i], cmap=&#39;coolwarm&#39;)
</span></span><span style=display:flex><span>    plt.colorbar()
</span></span><span style=display:flex><span>    plt.xticks(np.arange(size))
</span></span><span style=display:flex><span>    plt.yticks(np.arange(size))
</span></span><span style=display:flex><span>#plt.tight_layout()
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-861ad9af2e54ff41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h3 id=3异常值>3.异常值</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>from minisom import MiniSom
</span></span><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import matplotlib.pyplot as plt
</span></span><span style=display:flex><span>from matplotlib.gridspec import GridSpec
</span></span><span style=display:flex><span>from sklearn.datasets import make_blobs
</span></span><span style=display:flex><span>from sklearn.preprocessing import scale
</span></span><span style=display:flex><span>outliers_percentage = 0.35
</span></span><span style=display:flex><span>inliers = 300
</span></span><span style=display:flex><span>outliers = int(inliers * outliers_percentage)
</span></span><span style=display:flex><span>data = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[.3, .3], n_samples=inliers, random_state=0)[0]
</span></span><span style=display:flex><span>#print(data.shape)  # (300, 2)
</span></span><span style=display:flex><span>data = scale(data)
</span></span><span style=display:flex><span>data = np.concatenate([data, (np.random.rand(outliers, 2)-.5)*4.])
</span></span><span style=display:flex><span>#print(data.shape)  # (405, 2)
</span></span><span style=display:flex><span>som = MiniSom(2, 1, data.shape[1], sigma=1, learning_rate=0.5, neighborhood_function=&#39;triangle&#39;, random_seed=10)
</span></span><span style=display:flex><span>som.train_batch(data, 100, verbose=True)  # random training
</span></span></code></pre></div><p>设置阈值超过该值的即为异常</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span># 计算二范数
</span></span><span style=display:flex><span>quantization_errors = np.linalg.norm(som.quantization(data) - data, axis=1)
</span></span><span style=display:flex><span># print(quantization_errors.shape)  (405,)
</span></span><span style=display:flex><span># 计算一个多维数组的任意百分比分位数
</span></span><span style=display:flex><span>error_treshold = np.percentile(quantization_errors,  100*(1-outliers_percentage)+5)
</span></span><span style=display:flex><span># print(error_treshold) 0.3417397144318405  超过该值的即为异常
</span></span><span style=display:flex><span>is_outlier = quantization_errors &gt; error_treshold
</span></span><span style=display:flex><span>#　print(is_outlier)  [False False False 
</span></span><span style=display:flex><span>plt.hist(quantization_errors)
</span></span><span style=display:flex><span>plt.axvline(error_treshold, color=&#39;k&#39;, linestyle=&#39;--&#39;)
</span></span><span style=display:flex><span>plt.xlabel(&#39;error&#39;)
</span></span><span style=display:flex><span>plt.ylabel(&#39;frequency&#39;)
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-179f311d1efc78cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>plt.figure(figsize=(8, 8))
</span></span><span style=display:flex><span>plt.scatter(data[~is_outlier, 0], data[~is_outlier, 1],
</span></span><span style=display:flex><span>            label=&#39;inlier&#39;)
</span></span><span style=display:flex><span>plt.scatter(data[is_outlier, 0], data[is_outlier, 1],
</span></span><span style=display:flex><span>            label=&#39;outlier&#39;)
</span></span><span style=display:flex><span>plt.legend()
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6b5505f56e816c41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h3 id=4numpy>4.numpy</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import pylab as pl
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>class SOM(object):
</span></span><span style=display:flex><span>    def __init__(self, X, output, iteration, batch_size):
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        :param X: 形状是N*D， 输入样本有N个,每个D维
</span></span><span style=display:flex><span>        :param output: (n,m)一个元组，为输出层的形状是一个n*m的二维矩阵
</span></span><span style=display:flex><span>        :param iteration:迭代次数
</span></span><span style=display:flex><span>        :param batch_size:每次迭代时的样本数量
</span></span><span style=display:flex><span>        初始化一个权值矩阵，形状为D*(n*m)，即有n*m权值向量，每个D维
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        self.X = X  
</span></span><span style=display:flex><span>        self.output = output  
</span></span><span style=display:flex><span>        self.iteration = iteration  
</span></span><span style=display:flex><span>        self.batch_size = batch_size  
</span></span><span style=display:flex><span>        self.W = np.random.rand(
</span></span><span style=display:flex><span>            X.shape[1], output[0] * output[1])  
</span></span><span style=display:flex><span>        print(&#34;W mat shape is&#34;, self.W.shape)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def GetN(self, t):
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        :param t:时间t, 这里用迭代次数来表示时间
</span></span><span style=display:flex><span>        :return: 返回一个整数，表示拓扑距离，时间越大，拓扑邻域越小
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        a = min(self.output)  
</span></span><span style=display:flex><span>        return int(a-float(a)*t/self.iteration)  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def Geteta(self, t, n):
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        :param t: 时间t, 这里用迭代次数来表示时间
</span></span><span style=display:flex><span>        :param n: 拓扑距离
</span></span><span style=display:flex><span>        :return: 返回学习率，
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        return np.power(np.e, -n)/(t+2)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def updata_W(self, X, t, winner):
</span></span><span style=display:flex><span>        &#34;&#34;&#34; 
</span></span><span style=display:flex><span>        用于更新权值矩阵
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        N = self.GetN(t)  
</span></span><span style=display:flex><span>        for x, i in enumerate(winner):  
</span></span><span style=display:flex><span>            to_update = self.getneighbor(i[0], N)  
</span></span><span style=display:flex><span>            for j in range(N+1):
</span></span><span style=display:flex><span>                e = self.Geteta(t, j)
</span></span><span style=display:flex><span>                for w in to_update[j]:
</span></span><span style=display:flex><span>                    self.W[:, w] = np.add(
</span></span><span style=display:flex><span>                        self.W[:, w], e*(X[x, :] - self.W[:, w]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def getneighbor(self, index, N):
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        :param index:获胜神经元的下标
</span></span><span style=display:flex><span>        :param N: 邻域半径
</span></span><span style=display:flex><span>        :return ans: 返回一个集合列表，分别是不同邻域半径内需要更新的神经元坐标
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        a, b = self.output
</span></span><span style=display:flex><span>        length = a*b  
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        def distence(index1, index2):
</span></span><span style=display:flex><span>            i1_a, i1_b = index1 // a, index1 % b
</span></span><span style=display:flex><span>            i2_a, i2_b = index2 // a, index2 % b
</span></span><span style=display:flex><span>            return np.abs(i1_a - i2_a), np.abs(i1_b - i2_b)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        ans = [set() for i in range(N+1)]
</span></span><span style=display:flex><span>        for i in range(length):
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            dist_a, dist_b = distence(i, index)
</span></span><span style=display:flex><span>            if dist_a &lt;= N and dist_b &lt;= N:  
</span></span><span style=display:flex><span>                ans[max(dist_a, dist_b)].add(i)  
</span></span><span style=display:flex><span>        return ans
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def train(self):
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        train_Y:训练样本与形状为batch_size*(n*m)
</span></span><span style=display:flex><span>        winner:一个一维向量，batch_size个获胜神经元的下标
</span></span><span style=display:flex><span>        :return:返回值是调整后的W
</span></span><span style=display:flex><span>        &#34;&#34;&#34;
</span></span><span style=display:flex><span>        count = 0  
</span></span><span style=display:flex><span>        while self.iteration &gt; count:
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            train_X = self.X[np.random.choice(
</span></span><span style=display:flex><span>                self.X.shape[0], self.batch_size)]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            normal_W(self.W)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            normal_X(train_X)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            train_Y = train_X.dot(self.W)  
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            winner = np.argmax(train_Y, axis=1).tolist()  
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            self.updata_W(train_X, count, winner)
</span></span><span style=display:flex><span>            count += 1
</span></span><span style=display:flex><span>        return self.W
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    def train_result(self):
</span></span><span style=display:flex><span>        normal_X(self.X)  
</span></span><span style=display:flex><span>        train_Y = self.X.dot(self.W)  
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        winner = np.argmax(train_Y, axis=1).tolist()  
</span></span><span style=display:flex><span>        print(winner)
</span></span><span style=display:flex><span>        return winner
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def normal_X(X):
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    :param X:二维矩阵，N*D，N个D维的数据
</span></span><span style=display:flex><span>    :return: 将X归一化的结果
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    N, D = X.shape
</span></span><span style=display:flex><span>    for i in range(N):
</span></span><span style=display:flex><span>        temp = np.sum(np.multiply(X[i], X[i]))
</span></span><span style=display:flex><span>        X[i] /= np.sqrt(temp)
</span></span><span style=display:flex><span>    return X
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def normal_W(W):
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    :param W:二维矩阵，D*(n*m)，D个n*m维的数据
</span></span><span style=display:flex><span>    :return: 将W归一化的结果
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    for i in range(W.shape[1]):
</span></span><span style=display:flex><span>        temp = np.sum(np.multiply(W[:, i], W[:, i]))
</span></span><span style=display:flex><span>        W[:, i] /= np.sqrt(temp)
</span></span><span style=display:flex><span>    return W
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>def draw(C):
</span></span><span style=display:flex><span>    colValue = [&#39;r&#39;, &#39;y&#39;, &#39;g&#39;, &#39;b&#39;, &#39;c&#39;, &#39;k&#39;, &#39;m&#39;]
</span></span><span style=display:flex><span>    for i in range(len(C)):
</span></span><span style=display:flex><span>        coo_X = []  
</span></span><span style=display:flex><span>        coo_Y = []  
</span></span><span style=display:flex><span>        for j in range(len(C[i])):
</span></span><span style=display:flex><span>            coo_X.append(C[i][j][0])
</span></span><span style=display:flex><span>            coo_Y.append(C[i][j][1])
</span></span><span style=display:flex><span>        pl.scatter(coo_X, coo_Y, marker=&#39;x&#39;,
</span></span><span style=display:flex><span>                   color=colValue[i % len(colValue)], label=i)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pl.legend(loc=&#39;upper right&#39;)
</span></span><span style=display:flex><span>    pl.show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data = &#34;&#34;&#34;
</span></span><span style=display:flex><span>1,0.697,0.46,2,0.774,0.376,3,0.634,0.264,4,0.608,0.318,5,0.556,0.215,
</span></span><span style=display:flex><span>6,0.403,0.237,7,0.481,0.149,8,0.437,0.211,9,0.666,0.091,10,0.243,0.267,
</span></span><span style=display:flex><span>11,0.245,0.057,12,0.343,0.099,13,0.639,0.161,14,0.657,0.198,15,0.36,0.37,
</span></span><span style=display:flex><span>16,0.593,0.042,17,0.719,0.103,18,0.359,0.188,19,0.339,0.241,20,0.282,0.257,
</span></span><span style=display:flex><span>21,0.748,0.232,22,0.714,0.346,23,0.483,0.312,24,0.478,0.437,25,0.525,0.369,
</span></span><span style=display:flex><span>26,0.751,0.489,27,0.532,0.472,28,0.473,0.376,29,0.725,0.445,30,0.446,0.459&#34;&#34;&#34;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>a = data.split(&#39;,&#39;)
</span></span><span style=display:flex><span>dataset = np.mat([[float(a[i]), float(a[i+1])] for i in range(1, len(a)-1, 3)])
</span></span><span style=display:flex><span>dataset_old = dataset.copy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>som = SOM(dataset, (5, 5), 1, 30)
</span></span><span style=display:flex><span>som.train()
</span></span><span style=display:flex><span>res = som.train_result()  
</span></span><span style=display:flex><span>classify = {}  
</span></span><span style=display:flex><span>for i, win in enumerate(res):
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    if not classify.get(win[0]):
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        classify.setdefault(win[0], [i])
</span></span><span style=display:flex><span>    else:
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        classify[win[0]].append(i)
</span></span><span style=display:flex><span>C = []  
</span></span><span style=display:flex><span>D = []  
</span></span><span style=display:flex><span>for i in classify.values():
</span></span><span style=display:flex><span>    C.append(dataset_old[i].tolist())
</span></span><span style=display:flex><span>    D.append(dataset[i].tolist())
</span></span><span style=display:flex><span>draw(C) 
</span></span><span style=display:flex><span>draw(D) 
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
聚类-SOM神经网络算法</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/ title=聚类-SOM神经网络算法>/post/%E8%81%9A%E7%B1%BB-som%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E8%81%9A%E7%B1%BB-kmeans/ rel=next title=聚类-kmeans><i class="fa fa-chevron-left"></i> 聚类-kmeans</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E8%81%9A%E7%B1%BB-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB/ rel=prev title=聚类-层次聚类>聚类-层次聚类
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>