<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="非线性模型-朴素贝叶斯"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="非线性模型-朴素贝叶斯"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2023-01-01 14:28:53 +0800 CST"><meta property="article:modified_time" content="2023-01-01 14:28:53 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF","permalink":"/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/","title":"非线性模型-朴素贝叶斯","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>非线性模型-朴素贝叶斯 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>126</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#条件概率>条件概率：</a></li><li><a href=#先验概率prior-probability>先验概率（Prior probability）</a></li><li><a href=#可能性函数likelyhood>可能性函数（Likelyhood）</a></li><li><a href=#后验概率posterior-probability>后验概率（Posterior probability）</a></li><li><a href=#全概率公式>全概率公式：</a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>126</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>10</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=446699></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=958></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-01-01T14:28:53+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="非线性模型-朴素贝叶斯"><meta itemprop=description content="贝叶斯定理 条件概率： A 在另外一个事件 B 已经发生条件下的发生概率 $P(A|B) = \frac{P(A B)}{P(B)} $ 贝叶斯公式： #####后验概率（新信息出现后的A概率） ＝ 先验概率（A"></span><header class=post-header><h1 class=post-title itemprop="name headline">非线性模型-朴素贝叶斯
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e9%9d%9e%e7%ba%bf%e6%80%a7%e6%a8%a1%e5%9e%8b-%e6%9c%b4%e7%b4%a0%e8%b4%9d%e5%8f%b6%e6%96%af.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-01-01 14:28:53 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-01-01 14:28:53 +0800 CST">2023-01-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B itemprop=url rel=index><span itemprop=name>非线性模型</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>2921</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>6分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h1 id=贝叶斯定理>贝叶斯定理</h1><h2 id=条件概率>条件概率：</h2><p>A 在另外一个事件 B 已经发生条件下的发生概率
$P(A|B) = \frac{P(A
B)}{P(B)} $
贝叶斯公式：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e6984cf1ec908314.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-aec51b9403a8e8ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-429436970ab7e2d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>#####后验概率（新信息出现后的A概率）　＝　先验概率（A概率） ｘ 可能性函数（新信息带来的调整）</p><h2 id=先验概率prior-probability>先验概率（Prior probability）</h2><p><strong>不知道B事件的前提下，我们对A事件概率的一个主观判断。</strong>
对应这个例子里就是在不知道女神经常对你笑的前提下，来主观判断出女神喜欢一个人的概率。这里我们假设是50%，也就是不喜欢你，可能不喜欢你的概率都是一半。</p><h2 id=可能性函数likelyhood>可能性函数（Likelyhood）</h2><p><strong>这是一个调整因子，也就是将先验概率（之前的主观判断）调整到更接近真实概率。</strong>
可能性函数你可以理解为新信息过来后，对先验概率的一个调整。比如我们刚开始看到“人工智能”这个信息，你有自己的理解（先验概率-主观判断），但是当你学习了一些数据分析，或者看了些这方面的书后（新的信息），然后你根据掌握的最新信息优化了自己之前的理解（可能性函数-调整因子），最后重新理解了“人工智能”这个信息（后验概率）如果"可能性函数"P(B|A)/P(B)>1，意味着"先验概率"被增强，事件A的发生的可能性变大；如果"可能性函数"=1，意味着B事件无助于判断事件A的可能性；如果"可能性函数"&lt;1，意味着"先验概率"被削弱，事件A的可能性变小。</p><h2 id=后验概率posterior-probability>后验概率（Posterior probability）</h2><p><strong>B事件发生之后，我们对A事件概率的重新评估。</strong>
这个例子里就是在女神冲你笑后，对女神喜欢你的概率重新预测。带入贝叶斯公式计算出P(A|B)=P(A)* P(B|A)/P(B)=50% *1.5=75%因此，女神经常冲你笑，喜欢上你的概率是75%。这说明，女神经常冲你笑这个新信息的推断能力很强，将50%的"先验概率"一下子提高到了75%的"后验概率"。</p><h2 id=全概率公式>全概率公式：</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-7b17d669d7c66286.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><blockquote><p>有两个一模一样的碗，1号碗里有30个巧克力和10个水果糖，2号碗里有20个巧克力和20个水果糖。
然后把碗盖住。随机选择一个碗，从里面摸出一个巧克力。
问题：这颗巧克力来自1号碗的概率是多少？</p></blockquote><p>$P(A_1|B) = \frac{P(A_1) }{P(B)} P(B|A_1) =0.6$</p><p>$P(A_1)=0.5$
$P(B)=P(B|A_1)P(A_1)+ P(B |A_2)P(A_2)=3/4<em>0.5+1/2</em>0.5=0.625$
$P(B|A_1) =0.75 $</p><h1 id=朴素贝叶斯>朴素贝叶斯</h1><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-629ef335a268f85b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>它是一种简单但极为强大的预测建模算法。之所以称为朴素贝叶斯，是因为它<strong>假设每个输入变量是独立的</strong>。这是一个强硬的假设，实际情况并不一定</p><p>$P(A_{i} \mid B)=\frac{P(A_{i}) P(B \mid A_{i})}{\sum_{i=1}^{n} P(A_{i}) P(B \mid A_{i})}$</p><blockquote><p>朴素贝叶斯做的就是，假设这些身高，体重，性别这些特征之间是没有关系的，互相不影响。那么我们算同时符合这三个特征概率的时候，就可以分开算了$P(ABC) = P(A)* P(B)* P(C）$就是这个道理了。</p></blockquote><p>1）计算$P(A_i|B_1B_2B_3)$最大的
即最大的$\frac{P(A_i)*P(B_1B_2B_3|A_i)}{P(B_1B_2B_3)}$
即最大的$P(A_i)*P(B_1B_2B_3|A_i)$
2）$P(B_1B_2B_3|A_i)=P(B_1|A_i)*P(B_2|A_i)*P(B_3|A_i)$</p><blockquote><p><strong>离散数据案例</strong>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-1fe07ed7d3209c24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
给你一个新的数据：身高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e1838acfcf9caec6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p></blockquote><blockquote><p><strong>连续数据案例</strong>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-c5fd9b64c168837d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？
这里的困难在于，由于身高、体重、鞋码都是连续变量，不能采用离散变量的方法计算概率。而且由于样本太少，所以也无法分成区间计算。怎么办呢？
这时，可以假设男性和女性的身高、体重、鞋码都是正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。<strong>有了密度函数，就可以把值代入，算出某一点的密度函数的值。</strong>（求连续型随机变量在某一个取值点的概率的时候，可以看当前概率密度函数在该点的函数值，值越大，概率越大。但<strong>当前概率密度函数的值不和概率相等，只可以比大小用</strong>）
比如，男性的身高是均值 179.5、标准差为 3.697 的正态分布。所以男性的身高为 180 的概率为 0.1069，男性体重为 120 的概率为 0.000382324，男性鞋码为 41 号的概率为 0.120304111。
所以我们可以计算得出：$P(B_1B_2B_3|A_1)=P(B_1|C1)P(B_2|C1)P(B_3|A_1)$=0.1069 * 0.000382324 * 0.120304111=4.9169e-6
同理我们也可以计算出来该人为女的可能性：$P(B_1B_2B_3|A_2)=P(A1|A_2)P(B_2|A_2)P(B_3|A_2)$=0.00000147489 * 0.015354144 * 0.120306074=2.7244e-9
很明显这组数据分类为男的概率大于分类为女的概率。</p></blockquote><h1 id=sklearn>sklearn</h1><p>朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。</p><blockquote><ul><li>高斯朴素贝叶斯：<strong>特征变量是连续变量，符合高斯分布</strong>，比如说<strong>人的身高，物体的长度</strong>。</li><li>多项式朴素贝叶斯：<strong>特征变量是离散变量，符合多项分布</strong>，在文档分类中特征变量体现在<strong>一个单词出现的次数</strong>，或者是单词的 TF-IDF 值等。注意， 多项式朴素贝叶斯实际上符合多项式分布，不会存在负数，所以传入输入的时候，别用StandardScaler进行归一化数据，可以使用MinMaxScaler进行归一化</li><li>伯努利朴素贝叶斯：特征变量是<strong>布尔变量，符合 0/1 分布</strong>，在文档分类中特征是<strong>单词是否出现</strong>。</li></ul></blockquote><p><code>MultinomialNB(*, alpha=1.0, fit_prior=True, class_prior=None)</code></p><ul><li>alpha :为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。
当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。
当 0&lt;alpha&lt;1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import pandas as pd
</span></span><span style=display:flex><span>import os
</span></span><span style=display:flex><span>import jieba
</span></span><span style=display:flex><span>from sklearn.model_selection import train_test_split
</span></span><span style=display:flex><span>from sklearn.feature_extraction.text import TfidfVectorizer
</span></span><span style=display:flex><span>from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB
</span></span><span style=display:flex><span>from sklearn.metrics import accuracy_score
</span></span><span style=display:flex><span>data = pd.read_csv(&#39;D:\gitcode\python\data\online_shopping_10_cats.csv&#39;)
</span></span><span style=display:flex><span>print(data.head(10))
</span></span><span style=display:flex><span>print(data.isnull().sum())
</span></span><span style=display:flex><span>data.dropna(inplace=True)
</span></span><span style=display:flex><span>print(data[&#39;cat&#39;].value_counts())
</span></span><span style=display:flex><span>data = data.sample(frac=0.1, random_state=1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 加载停用词
</span></span><span style=display:flex><span>with open(&#39;D:\gitcode\python\data\chineseStopWords.txt&#39;, &#39;rb&#39;) as f:
</span></span><span style=display:flex><span>    STOP_WORDS = [line.strip() for line in f.readlines()]
</span></span><span style=display:flex><span>LABEL_MAP = {&#39;洗发水&#39; : 0, &#39;水果&#39; : 1, &#39;酒店&#39; : 2, &#39;衣服&#39; : 3, &#39;平板&#39; : 4, &#39;计算机&#39; : 5, &#39;书籍&#39; : 6, &#39;手机&#39; : 7, &#39;蒙牛&#39; : 8, &#39;热水器&#39; : 9}
</span></span><span style=display:flex><span>documents, labels = [], []
</span></span><span style=display:flex><span>for i in range(len(data)):
</span></span><span style=display:flex><span>    d = data.iloc[i]
</span></span><span style=display:flex><span>    try:
</span></span><span style=display:flex><span>        word_list = list(jieba.cut(d[&#39;review&#39;]))
</span></span><span style=display:flex><span>        words = [w for w in word_list if w not in STOP_WORDS]
</span></span><span style=display:flex><span>        documents.append(&#39; &#39;.join(words))
</span></span><span style=display:flex><span>        labels.append(LABEL_MAP[d[&#39;cat&#39;]])
</span></span><span style=display:flex><span>    except:
</span></span><span style=display:flex><span>        print(i)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_x, test_x, train_y, text_y = train_test_split(documents, labels)
</span></span><span style=display:flex><span># 计算TF-IDF矩阵
</span></span><span style=display:flex><span>tfidf_vec = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5)
</span></span><span style=display:flex><span>new_train_x = tfidf_vec.fit_transform(train_x)   #  (1570, 15065)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 测试集用训练集的词典
</span></span><span style=display:flex><span>test_tfidf_vec = TfidfVectorizer(stop_words=STOP_WORDS, max_df=0.5, vocabulary=tfidf_vec.vocabulary_)
</span></span><span style=display:flex><span>new_test_x = test_tfidf_vec.fit_transform(test_x)
</span></span><span style=display:flex><span># 建立模型
</span></span><span style=display:flex><span>bayes_model = {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bayes_model[&#39;MultinomialNB&#39;] = MultinomialNB(alpha=0.001)
</span></span><span style=display:flex><span>bayes_model[&#39;BernoulliNB&#39;] = BernoulliNB(alpha=0.001)
</span></span><span style=display:flex><span>bayes_model[&#39;GaussianNB&#39;] = GaussianNB()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>for item in bayes_model.keys():
</span></span><span style=display:flex><span>    clf = bayes_model[item]
</span></span><span style=display:flex><span>    clf.fit(new_train_x.toarray(), train_y)
</span></span><span style=display:flex><span>    pred = clf.predict(new_test_x.toarray())
</span></span><span style=display:flex><span>    print(item, &#34;accuracy_score: &#34;, accuracy_score(text_y, pred))
</span></span></code></pre></div><p>numpy</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>
</span></span><span style=display:flex><span>import numpy as  np
</span></span><span style=display:flex><span>import pandas as pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>class Naive_Bayes:
</span></span><span style=display:flex><span>    def __init__(self):
</span></span><span style=display:flex><span>        pass
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    # 朴素贝叶斯训练过程
</span></span><span style=display:flex><span>    def nb_fit(self, X, y):
</span></span><span style=display:flex><span>        classes = y[y.columns[0]].unique()
</span></span><span style=display:flex><span>        class_count = y[y.columns[0]].value_counts()
</span></span><span style=display:flex><span>        # 类先验概率
</span></span><span style=display:flex><span>        class_prior = class_count / len(y)
</span></span><span style=display:flex><span>        # 计算类条件概率
</span></span><span style=display:flex><span>        prior = dict()
</span></span><span style=display:flex><span>        for col in X.columns:
</span></span><span style=display:flex><span>            for j in classes:
</span></span><span style=display:flex><span>                p_x_y = X[(y == j).values][col].value_counts()
</span></span><span style=display:flex><span>                for i in p_x_y.index:
</span></span><span style=display:flex><span>                    prior[(col, i, j)] = p_x_y[i] / class_count[j]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        return classes, class_prior, prior
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    # 预测新的实例
</span></span><span style=display:flex><span>    def predict(self, X_test):
</span></span><span style=display:flex><span>        res = []
</span></span><span style=display:flex><span>        for c in classes:
</span></span><span style=display:flex><span>            p_y = class_prior[c]
</span></span><span style=display:flex><span>            p_x_y = 1
</span></span><span style=display:flex><span>            for i in X_test.items():
</span></span><span style=display:flex><span>                p_x_y *= prior[tuple(list(i) + [c])]
</span></span><span style=display:flex><span>            res.append(p_y * p_x_y)
</span></span><span style=display:flex><span>        return classes[np.argmax(res)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>if __name__ == &#34;__main__&#34;:
</span></span><span style=display:flex><span>    x1 = [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3]
</span></span><span style=display:flex><span>    x2 = [&#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;S&#39;, &#39;S&#39;, &#39;S&#39;, &#39;M&#39;, &#39;M&#39;, &#39;L&#39;, &#39;L&#39;, &#39;L&#39;, &#39;M&#39;, &#39;M&#39;, &#39;L&#39;, &#39;L&#39;]
</span></span><span style=display:flex><span>    y = [-1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1]
</span></span><span style=display:flex><span>    df = pd.DataFrame({&#39;x1&#39;: x1, &#39;x2&#39;: x2, &#39;y&#39;: y})
</span></span><span style=display:flex><span>    X = df[[&#39;x1&#39;, &#39;x2&#39;]]
</span></span><span style=display:flex><span>    y = df[[&#39;y&#39;]]
</span></span><span style=display:flex><span>    X_test = {&#39;x1&#39;: 2, &#39;x2&#39;: &#39;S&#39;}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    nb = Naive_Bayes()
</span></span><span style=display:flex><span>    classes, class_prior, prior = nb.nb_fit(X, y)
</span></span><span style=display:flex><span>    print(&#39;测试数据预测类别为：&#39;, nb.predict(X_test))
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
非线性模型-朴素贝叶斯</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/ title=非线性模型-朴素贝叶斯>/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B-k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/ rel=next title=非线性模型-K近邻算法><i class="fa fa-chevron-left"></i> 非线性模型-K近邻算法</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/ rel=prev title=降维-LDA线性判别分析>降维-LDA线性判别分析
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>