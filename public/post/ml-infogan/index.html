<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="ml-infoGAN"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="ml-infoGAN"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/ml-infogan/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"ml-infogan","permalink":"/post/ml-infogan/","title":"ml-infoGAN","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>ml-infoGAN - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>61</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><ul><li><ul><li></li></ul></li></ul></li><li><a href=#即让d_loss减小注意在训练判别器的时候生成器中的所有参数要固定住即不参加训练><strong>先训练判别器k（比如3）次：</strong>
1. 从噪声分布（比如高斯分布）中随机采样出m个噪声向量：
<img src=https://upload-images.jianshu.io/upload_images/18339009-c5d01fc306415848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
2.从真实样本x中随机采样出m个样本：
<img src=https://upload-images.jianshu.io/upload_images/18339009-b349e54ff0e191ce.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
3. 用梯度下降法使损失函数real_loss：$logD(x^{(i)})$与1之间的二分类交叉熵减小（因为最后判别器最后一层的激活函数为sigmoid，所以要与0或者1做二分类交叉熵，这也是为什么损失函数要取log的原因）。
4.用梯度下降法使损失函数fake_loss： $logD(z^{(i)})$与0之间的二分类交叉熵减小。
5. 所以判别器的总损失函数d_loss:
<img src=https://upload-images.jianshu.io/upload_images/18339009-42dd10e627a85885.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
即让d_loss减小。注意<strong>在训练判别器的时候生成器中的所有参数要固定住，即不参加训练。</strong></a></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>61</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>4</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=250188></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=530></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-01T19:59:47+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/ml-infogan/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="ml-infoGAN"><meta itemprop=description content="InfoGAN的发布时间应该在是DCGAN之后没多久，可以算是在大部分的GAN模型的前面的。时间上看，InfoGAN应该是在Semi-GAN"></span><header class=post-header><h1 class=post-title itemprop="name headline">ml-infoGAN
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/ml-infoGAN.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ml itemprop=url rel=index><span itemprop=name>ml</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>3867</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>8分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/ml-infogan/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-c52910e7276a8366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><blockquote><p>InfoGAN的发布时间应该在是DCGAN之后没多久，可以算是在大部分的GAN模型的前面的。时间上看，InfoGAN应该是在Semi-GAN和Cat-GAN之后提出，在ImprovedGAN和ACGAN之前提出。从算法分类上看，InfoGAN属于半监督模型，但是不同于一般的半监督模型，比如，SemiGAN，CatGAN, ImprovedGAN，ACGAN等。后面的这些模型添加半监督的思路，主要是想要将GAN中D扩张为一个可以分类图像label，而不是单纯的分是否是bogus data（即，是否来自于G）。</p><ul><li>Semi-GAN：D输出变成K+1。（1为原来的fake or not的判断， K为分类器的目标分类类数）</li><li>CatGAN：D的输出变成K。结合信息熵，认为概率在每个类上越接近等概率，表示data来自于G。当然，越是集中在某个类别上，这样就可以描述具体的类别了。</li><li>ImprovedGAN：D的输出变成K。做两层的softmax。一层是在D上做，这里只是将D当做一个分类器来看待。之后，再假设有还有一个类别，即fake，K+1。由于其他K个类别数字都在变，因此假设最后一个类别数值固定也可以再加一层softmax完成。</li><li>ACGAN：将D分解，D的卷积层作为特征挖掘的层（一般也是这么认为的）。之后，对于这样的特征再做不同的映射。一个将特征映射到K上（分类器）， 一个是将特征映射到0/1上（判别器）。（判别器本质上也是分类器，这里主要是为了区别说明）</li></ul></blockquote><p>但是，会注意到，其实，无论怎么改，大家在半监督的GAN上的挖掘都是停留在D上。而忽视了G（当然也是G上不太好做文章的原因）。
一般来说，G的输入只有z 。GAN的训练方式，是将一个随机变量，通过博弈的方式，让z在G上具有意义。也就是使得没有特定信息的变量z，在通过G的映射之后，变得具有某种含义。这种含义使得z的变化会影响到G的生成效果。<strong>InfoGAN的操作，就是尝试添加其他的输入，使得这参数也具有意义。</strong></p><p>既然要让输入的噪声向量z带有一定的语义信息，那就人为的为它添加上一些限制，于是作者把G的输入看成两部分：</p><blockquote><ul><li>一部分就是噪声z，可以将它看成是不可压缩的噪声向量。</li><li>另一部分是若干个离散的和连续的latent variables（潜变量）所拼接而成的向量c，用于代表生成数据的不同语意信息。</li></ul></blockquote><p>以MNIST数据集为例，可以用一个离散的随机变量（0-9，用于表示生成数字的具体数值）和两个连续的随机变量（假设用于表示笔划的粗细与倾斜程度）。所以此时的c由一个离散的向量（长度为10）、两个连续的向量（长度为1）拼接而成，即c长度为12。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-affbd37ec32c7d81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h6 id=机器学习中的各种熵>机器学习中的各种熵</h6><blockquote><p>a) 自信息：事件提供的信息量，与概率成反比
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6ebfba4060211a31?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
b) 信息熵，自信息关于概率的期望，反映不确定度
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a5384a0c529b6eec?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
c) 联合熵，两个事件间的不确定度
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fee61ac4c22785d5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
d) 条件熵，已知X下，Y的不确定度
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-29fc307a34eeee15?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
同时条件熵和联合熵，信息熵的关系如下：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fe115bd12a450bba?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
e) 交叉熵，衡量两个分布的差异程度
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a6aacccf52b9dd32?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
f) 相对熵，KL散度，<strong>交叉熵和KL散度成正相关</strong>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a17da3d3cfadf61c?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
g) 互信息，<strong>已知一个变量后，另一个变量减小了多少不确定度</strong>，本文重点
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6c0343ee44831390?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p></blockquote><p>互信息式中，$H$表示计算熵值，所以$I(X;Y)$是两个熵值的差。$H(X|Y)$衡量的是“给定随机变量的情况下，随机变量$X$的不确定性”。从公式中可以看出，若$X$和$Y$是独立的，此时$H(X)=H(X|Y)$，得到$I(X;Y)=0$，为最小值。若$X$和$Y$有非常强的关联时，即已知$Y$时，$X$没有不确定性，则$H(X|Y)=0$, $I(X;Y)$达到最大值。所以为了让$G(z,c)$和$c$之间产生尽量明确的语义信息，必须要让它们二者的互信息足够的大，所以我们对GAN的损失函数添加一个正则项，就可以改写为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-beeea3a7453c419b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>**注意$I(c;G(z,c))$属于G的损失函数的一部分，所以这里为负号，即让该项越大越好，使得G的损失函数变小。**其中 $\lambda$ 为平衡两个损失函数的权重。但是，在计算$I(c;G(z,c))$的过程中，需要知道后验概率分布$P(c|x)$ ，而这个分布在实际中是很难获取的，因此作者在解决这个问题时采用了变分推理的思想，引入变分分布 $Q(c|x)$来逼近$P(c|x)$
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-2d9b41b807e6289d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-baedccfee0e1666b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a45877a383af6f8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>故$ L_I (G, Q) $是互信息的一个下界。作者指出，用蒙特卡罗模拟（Monte Carlo simulation）去逼近$ L_I (G, Q)$ 是较为方便的，这样我们的优化问题就可以表示为：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-0af2cf52cd0609ac.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>从上图可以清晰的看出，虽然在设计InfoGAN时的数学推导比较复杂，但是网络架构还是非常简单明了的。G和D的网络结构和DCGAN保持一致，均由CNN构成。在此基础上，改动的地方主要有：</p><blockquote><p>1.G的输入不仅仅是噪声向量z了，而是z和具有语意信息的浅变量c进行拼接后的向量输入给G。
2.D的输出在原先的基础上添加了一个新的输出分支Q，Q和D共享全部分卷积层，然后各自通过不同的全连接层输出不同的内容：Q的输出对应于$X_{fake}$的c的概率分布，D则仍然判别真伪。</p></blockquote><p><strong>InfoGAN的训练流程</strong></p><p>假设batch_size=m，数据集为MNIST，则根据作者的方法，不可压缩噪声向量的长度为62，离散潜变量的个数为1，取值范围为[0, 9]，代表0-9共10个数字，连续浅变量的个数为2，代表了生成数字的倾斜程度和笔划粗细，最好服从[-2, 2]上的均匀分布，因为这样能够显式的通过改变其在[-2,2]上的数值观察到生成数据相应的变化，便于实验，所以此时输入变量的长度为62+10+2=74。</p><p>则在每一个epoch中：</p><blockquote><h2 id=即让d_loss减小注意在训练判别器的时候生成器中的所有参数要固定住即不参加训练><strong>先训练判别器k（比如3）次：</strong>
1. 从噪声分布（比如高斯分布）中随机采样出m个噪声向量：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-c5d01fc306415848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
2.从真实样本x中随机采样出m个样本：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-b349e54ff0e191ce.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
3. 用梯度下降法使损失函数real_loss：$logD(x^{(i)})$与1之间的二分类交叉熵减小（因为最后判别器最后一层的激活函数为sigmoid，所以要与0或者1做二分类交叉熵，这也是为什么损失函数要取log的原因）。
4.用梯度下降法使损失函数fake_loss： $logD(z^{(i)})$与0之间的二分类交叉熵减小。
5. 所以判别器的总损失函数d_loss:
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-42dd10e627a85885.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
即让d_loss减小。注意<strong>在训练判别器的时候生成器中的所有参数要固定住，即不参加训练。</strong></h2><p><strong>再训练生成器1次：</strong>
1. 从噪声分布中随机采样出m个噪声向量：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-cb79ec1d72bccb5c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
2. 从离散随机分布中随机采样m个长度为10、one-hot编码格式的向量：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-f452ad27eb0d4b03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
3. 从两个连续随机分布中各随机采样m个长度为1的向量：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ae5f105206acbc6a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4faccda3bb883a53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
4. 将上面的所有向量进行concat操作，得到长度为74的向量，共m个，并记录每个向量所在的位置，便于计算损失函数。
5. 此时g_loss由三部分组成：一个是 $logD(z^{(i)})$与1之间的二分类交叉熵、一个是Q分支输出的离散浅变量的预测值和相应的输入部分的交叉熵以及Q分支输出的连续浅变量的预测值和输入部分的互信息，并为这三部分乘上适当的平衡因子，其中互信息项的系数是负的。
6. 用梯度下降法使越小越好。注意在训练生成器的时候判别器中的所有参数要固定住，即不参加训练。</p></blockquote><p>直到所有epoch执行完毕，训练结束。</p><p><strong>（四）总结</strong></p><p>1.G的输入不再是一个单一的噪声向量，而是噪声向量与潜变量的拼接。
2.对于潜变量来说，G和D组成的大网络就好比是一个AutoEncoder，不同之处只是将信息编码在了图像中，而非向量，最后通过D解码还原回。
3.D的输出由原先的单一分支变为两个不同的分支。
4.从信息熵的角度对噪声向量和潜变量的关系完成建模，并通过数学推导以及实验的方式证明了该方法确实有效。
5.通过潜变量，使得G生成的数据具有一定的可解释性。</p><p>在实现中，$D(x)、G(z, c) $和 $Q(x)$ 分别用一个 CNN (Convolutional Neural Networks)、DCNN (DeConv Neural Networks) 、CNN来实现。</p><p>同时，潜码 c 也包含两部分：一部分是类别，服从$ Cat(K = N,p = 1/N)$，其中 N 为类别数量；另一部分是连续的与生成数据有关的参数，服从$ Unif(−1,1)$ 的分布。</p><p>在此应指出，$Q(c|x) $可以表示为一个神经网络 Q(x) 的输出。对于输入随机变量 z 和类别潜码 c，实际的$ L_I(G, Q) $可以表示为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fce94af7b9cbaa22.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
其中$ · $表示内积（inner product），$c $是一个选择计算哪个$ log $的参数，例如 $c_i = 1$ 而 $c_j = 0(∀j = 1,2,···,i − 1,i + 1,···,n)$，那么 z 这时候计算出的$ L_I(G,Q)$ 就等于$ log(Q(z,c)i)$。这里我们可以消去 $H(c)$，因为$ c $的分布是固定的，即优化目标与 $H(c) $无关：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-85f12073e5d23b30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>而对于参数潜码，我们假设它符合正态分布，神经网络$ Q(x)$ 则输出其预测出的该潜码的均值和标准差， 我们知道，对于均值$ μ$，标准差$ σ $的随机变量，其概率密度函数为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fe235563b7fa0d8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>要计算参数潜码 c 的<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6917fdf8a0f55091?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=img> ，就是要计算 $log p(c)$，即：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-98f8751d19aba6f2?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>设$ Q(x) $输出的参数潜码$ c$ 的均值$ μ$，标准差 $σ$ 分别为$ Q(x)μ$ 和 $Q(x)σ$，那么对于参数潜码$ c$：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e62cda793a5275e3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
同样的，我们可以消去 $H(c)$，因为 $c $的分布是固定的，那么：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-73a42b5c861bbfc6?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>读完论文，我们发现，对于类别潜码，这个$ L_I $本质上是$ x$ 与$ G(z, c) $之间的 KL 散度：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-687934ec7ceb6fc5?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>也就是说：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-c61296d5b3d4e2dc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=img></p><p>而$ min DKL(c||Q(G(z, c)))$ 意味着减小$ c$ 与$ Q(G(z, c)) $的差别。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-6236068b4c7e8ac0?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>▲ 图7. 普通GAN和InfoGAN的LI在训练过程中的比较</p><p>如果我们不考虑 $Q(x)σ $的影响，$L_I$ 的优化过程：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-7a9eab94cd5cdd22?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>而<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-8d9b5fbeb6c73798?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=img> 也意味着减小$ c $与 $Q(G(z, c))μ$ 的差。</p><p>再纵观整个模型，我们会发现这一对$ L_I $优化的过程，实质上是以 G 为编码器（Encoder）， Q 为解码器（Decoder），生成的图像作为我们要编码的码（code），训练一个自编码器（Autoencoder），也就是说，作者口中的信息论优化问题，本质上是无监督训练问题。</p></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
ml-infoGAN</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/ml-infogan/ title=ml-infoGAN>/post/ml-infogan/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/ml-gpu%E7%89%88pytorch%E5%AE%89%E8%A3%85/ rel=next title=ml-gpu版pytorch安装><i class="fa fa-chevron-left"></i> ml-gpu版pytorch安装</a></div><div class="post-nav-prev post-nav-item"><a href=/post/ml-jupyter-notebook/ rel=prev title=ml-Jupyter-Notebook>ml-Jupyter-Notebook
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>