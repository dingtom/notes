<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="降维-PCA主成分分析"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="降维-PCA主成分分析"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2023-01-01 14:28:53 +0800 CST"><meta property="article:modified_time" content="2023-01-01 14:28:53 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90","permalink":"/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/","title":"降维-PCA主成分分析","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>降维-PCA主成分分析 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>126</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>126</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>10</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=446699></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=958></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2023-01-01T14:28:53+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="降维-PCA主成分分析"><meta itemprop=description content="一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。 两个维数相同的向量的内积被定义为： 常见形式：$A·B=|A||B| cos(\alpha)$ A与"></span><header class=post-header><h1 class=post-title itemprop="name headline">降维-PCA主成分分析
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e9%99%8d%e7%bb%b4-PCA%e4%b8%bb%e6%88%90%e5%88%86%e5%88%86%e6%9e%90.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2023-01-01 14:28:53 +0800 CST" itemprop="dateCreated datePublished" datetime="2023-01-01 14:28:53 +0800 CST">2023-01-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E9%99%8D%E7%BB%B4 itemprop=url rel=index><span itemprop=name>降维</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>4288</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>9分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p>一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-3c2f6f6dcd84d308.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
两个维数相同的向量的内积被定义为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-59d8fafc342b9832.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
常见形式：$A·B=|A||B| cos(\alpha)$
A与B的内积等于A到B的投影长度乘以B的模。
如果我们假设B的模为1，即让$|B|=1$，那么就变成了：
$A·B=|A|cos(\alpha)$
其中$|A|=\sqrt{x_1^2+y_1^2}$是向量A的模，也就是A线段的标量长度。
也就是说，设向量B的模为1，<strong>则A与B的内积值等于A向B所在直线投影的矢量长度</strong></p><p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的B向量可以表示为（3，2），这是我们再熟悉不过的向量表示。
不过我们常常忽略，只有一个（3，2）本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量（3，2）实际是说在x轴投影为3而y轴的投影为2。<strong>注意投影是一个矢量，所以可以为负。</strong>
更正式的说，向量（x，y）实际上表示线性组合：
$x(1，0)^T+y(0，1)^T$
不难证明所有二维向量都可以表示为这样的线性组合。此处（1，0）和（0，1）叫做二维空间中的一组基。</p><p><strong>所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。</strong></p><p>**实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。**如，上面的基可以变为。$(-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}})$,$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$现在，我们想获得（3，2）在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算（3，2）和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}})$。</p><p>我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p><p>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-14a086910ee97633.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-b65727ceaee09800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p><p>如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？
假设我们的数据由五条记录组成，将它们表示成矩阵形式：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-82f95beae4f89f40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-24025a212bdac384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4d759713bed34c0e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p><p>我们希望投影后投影值尽可能分散，而这种<strong>分散程度，可以用数学上的方差来表述</strong>。一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-35bf140e26bc888e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-640d4bfaf54eca69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<strong>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</strong></p><p>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。<strong>数学上可以用两个字段的协方差表示其相关性</strong>，由于已经让每个字段均值为0，则：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-d81a6ce3f597d093.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。至此，我们得到了降维问题的优化目标：<strong>将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</strong></p><p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p><p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-7f469b002a6a0769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
然后我们用X乘以X的转置，并乘上系数1/m：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a56b6d29bf0dbcb2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p><p>根据上述推导，我们发现要达到优化目地，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-0e588186fbfb5ea1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^T$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p><p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：
1）实对称矩阵不同特征值对应的特征向量必然正交。
2）设特征向量$\lambda$重数为r，则必然存在r个线性无关的特征向量对应于$\lambda$，因此可以将这r个特征向量单位正交化。</p><p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,&mldr;e_n$，我们将其按列组成矩阵：
则对协方差矩阵C有如下结论：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ab74b18111f33d8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
其中$\Lambda$为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。
到这里，我们发现我们已经找到了需要的矩阵P：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-fd0b3cc0d176c275.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照$\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p><h1 id=pca算法><strong>PCA算法</strong></h1><p>设有m条n维数据。
1）将原始数据按列组成n行m列矩阵X
2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值
3）求出协方差矩阵$C= \frac{1}{m} X X^{\top}$
4）求出协方差矩阵的特征值及对应的特征向量
5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
6）Y=PX即为降维到k维后的数据</p><p>以上文提到的
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a2f3ecae948aecdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
为例，我们用PCA方法将这组二维数据其降到一维。
因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-9fc5461577c9b811.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-809b694cff2295cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
其对应的特征向量分别是：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ab8175034693607a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数。那么标准化后的特征向量为：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-b294ffa7737e12c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
因此我们的矩阵P是：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-d95dba6feb19bbd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
可以验证协方差矩阵C的对角化：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-92ded9ecec0fb09a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ed9af8999cf9fefc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
降维投影结果如下图：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-9869ed5f3b3a5de0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import numpy as np
</span></span><span style=display:flex><span>import matplotlib.pyplot as plt
</span></span><span style=display:flex><span>x = np.array([1.5, 1.5, 2.4, 2,3.3, 2.3, 2, 1, 1.5, 1.5])
</span></span><span style=display:flex><span>y = np.array([2.1, 1.7, 2.9, 2.2, 3, 2.7, 1.6, 1.1, 1.6, 0.9])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值
</span></span><span style=display:flex><span>mean_x = np.mean(x)
</span></span><span style=display:flex><span>mean_y = np.mean(y)
</span></span><span style=display:flex><span>scaled_x = x-mean_x
</span></span><span style=display:flex><span>scaled_y = y-mean_y
</span></span><span style=display:flex><span>data = np.matrix([[scaled_x[i], scaled_y[i]] for i in range(len(scaled_x))])
</span></span><span style=display:flex><span># print(data.shape)  (10, 2)
</span></span><span style=display:flex><span># 规范化后的数据分布情况
</span></span><span style=display:flex><span>plt.plot(scaled_x, scaled_y, &#39;o&#39;, color=&#39;red&#39;)
</span></span><span style=display:flex><span>plt.show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 求协方差
</span></span><span style=display:flex><span>cov = np.cov(scaled_x, scaled_y)
</span></span><span style=display:flex><span># print(cov)   (2,2)
</span></span><span style=display:flex><span># [[0.42666667 0.4       ]
</span></span><span style=display:flex><span># [0.4        0.53066667]]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 求出协方差矩阵的特征值及对应的特征向量 eig()和eigh()。一个计算的特征值为复数形式，一个为实数形式。
</span></span><span style=display:flex><span>eig_val, eig_vec = np.linalg.eig(cov)
</span></span><span style=display:flex><span># print(eig_val, eig_vec.shape) [0.07530083 0.88203251]
</span></span><span style=display:flex><span># [[-0.75130394 -0.65995635]
</span></span><span style=display:flex><span>#  [ 0.65995635 -0.75130394]]
</span></span><span style=display:flex><span>eig_pairs = [(np.abs(eig_val[i]), eig_vec[:, i]) for i in range(len(eig_val))]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
</span></span><span style=display:flex><span>eig_pairs.sort(reverse=True)
</span></span><span style=display:flex><span># print(eig_pairs)
</span></span><span style=display:flex><span># [(0.882032505577202, array([-0.65995635, -0.75130394])), (0.07530082775613123, array([-0.75130394,  0.65995635]))]
</span></span><span style=display:flex><span>feature = eig_pairs[0][1]
</span></span><span style=display:flex><span># print(feature)  （1,2）
</span></span><span style=display:flex><span># [-0.65995635 -0.75130394]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span># Y=PX即为降维到k维后的数据
</span></span><span style=display:flex><span>new_data_reduced = np.transpose(np.dot(feature, np.transpose(data)))
</span></span><span style=display:flex><span># print(new_data_reduced.shape) (10, 1)
</span></span><span style=display:flex><span># [[ 0.17382607]
</span></span><span style=display:flex><span># [ 0.47434764]
</span></span><span style=display:flex><span># [-1.0211778 ]
</span></span><span style=display:flex><span># [-0.2312825 ]
</span></span><span style=display:flex><span># [-1.69026891]
</span></span><span style=display:flex><span># [-0.80492138]
</span></span><span style=display:flex><span># [ 0.21949986]
</span></span><span style=display:flex><span># [ 1.25510819]
</span></span><span style=display:flex><span># [ 0.54947804]
</span></span><span style=display:flex><span># [ 1.0753908 ]]
</span></span><span style=display:flex><span>new_data = np.transpose(np.dot(eig_vec, np.transpose(data)))
</span></span><span style=display:flex><span>plt.plot(scaled_x, scaled_y, &#39;o&#39;, color=&#39;red&#39;)
</span></span><span style=display:flex><span>plt.plot([eig_vec[:, 0][0], 0], [eig_vec[:, 0][1], 0], color=&#39;red&#39;)
</span></span><span style=display:flex><span># print([eig_vec[:, 0][0], 0], [eig_vec[:, 0][1], 0])
</span></span><span style=display:flex><span># [-0.7513039432395591, 0] [0.6599563507329023, 0]
</span></span><span style=display:flex><span>plt.plot([eig_vec[:, 1][0], 0], [eig_vec[:, 1][1], 0], color=&#39;blue&#39;)
</span></span><span style=display:flex><span>plt.plot(new_data[:, 0], new_data[:, 1], &#39;^&#39;, color=&#39;blue&#39;)
</span></span><span style=display:flex><span>plt.plot(new_data_reduced[:, 0], [1.5]*10, &#39;*&#39;, color=&#39;green&#39;)
</span></span><span style=display:flex><span>plt.show()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>
</span></span><span style=display:flex><span>from sklearn.datasets import make_blobs
</span></span><span style=display:flex><span>from sklearn.decomposition import PCA
</span></span><span style=display:flex><span># 生成随机数据，样本量为10000，维度为10
</span></span><span style=display:flex><span>X, y = make_blobs(n_samples=10000, n_features=10)
</span></span><span style=display:flex><span># print(X.shape, y.shape)
</span></span><span style=display:flex><span># (10000, 10) (10000,)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>pca = PCA(n_components=&#39;mle&#39;)
</span></span><span style=display:flex><span># PCA(copy=True, n_components=2, whiten=F
</span></span><span style=display:flex><span># copy：bool类型，是否将原始数据复制一份，默认为TRUE。
</span></span><span style=display:flex><span># n_components =k（可以是int型数字或者阈值，这里的‘mle’表示自动选择降维的维数）
</span></span><span style=display:flex><span># whiten：bool类型，是否进行白化，默认为FALSE。
</span></span><span style=display:flex><span>new_data = pca.fit_transform(X)
</span></span><span style=display:flex><span># 训练
</span></span><span style=display:flex><span>print(new_data.shape, pca.components_.shape)  # 返回具有最大方差的成分
</span></span><span style=display:flex><span># (10000, 2) (2, 10)
</span></span><span style=display:flex><span>print(pca.explained_variance_ratio_)   # 返回 所保留的n个成分各自的方差百分比。
</span></span><span style=display:flex><span>print(pca.explained_variance_)  # 返回 所保留的n个成分各自的方差
</span></span><span style=display:flex><span>print(pca.n_components_)    # 查看自动选择降到的维数
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
降维-PCA主成分分析</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/ title=降维-PCA主成分分析>/post/%E9%99%8D%E7%BB%B4-pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E9%99%8D%E7%BB%B4-lda%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/ rel=next title=降维-LDA线性判别分析><i class="fa fa-chevron-left"></i> 降维-LDA线性判别分析</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E9%99%8D%E7%BB%B4-svd%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/ rel=prev title=降维-SVD奇异值分解>降维-SVD奇异值分解
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>