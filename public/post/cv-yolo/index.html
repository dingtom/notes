<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="cv-yolo"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="cv-yolo"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/cv-yolo/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"cv-yolo","permalink":"/post/cv-yolo/","title":"cv-yolo","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>cv-yolo - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>74</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#yolo-vs-faster-r-cnn>YOLO vs Faster R-CNN</a></li></ul><ul><li><a href=#思想>思想</a></li><li><a href=#网络结构>网络结构</a></li><li><a href=#后处理>后处理</a></li><li><a href=#损失函数>损失函数</a></li><li><a href=#缺陷>缺陷</a></li></ul><ul><li><a href=#思想-1>思想</a></li><li><a href=#网络结构-1>网络结构</a></li><li><a href=#改进>改进</a><ul><li><a href=#batch-normalization>Batch Normalization</a></li><li><a href=#高分辨率分类器>高分辨率分类器</a></li><li><a href=#anchor>Anchor</a></li><li><a href=#聚类anchor>聚类anchor</a></li><li><a href=#direct-location-prediction>Direct location prediction</a></li><li><a href=#fine-grained-features>Fine-Grained Features</a></li><li><a href=#多尺度训练>多尺度训练</a></li><li><a href=#heading></a></li></ul></li><li><a href=#损失函数-1>损失函数</a></li></ul><ul><li><a href=#网络结构-2>网络结构</a></li><li><a href=#目标边界框的预测>目标边界框的预测</a></li><li><a href=#损失函数-2>损失函数</a></li></ul><ul><li><a href=#网络结构-3>网络结构</a></li><li><a href=#cspdarknet53网络结构>CSPDarknet53网络结构</a></li><li><a href=#网络详细结构>网络详细结构</a></li><li><a href=#改进-1>改进</a><ul><li><a href=#输入端>输入端</a><ul><li><a href=#mosaic数据增强>Mosaic数据增强</a></li></ul></li><li><a href=#backbone>Backbone</a><ul><li><a href=#cspdarknet53>CSPDarknet53</a></li><li><a href=#mish激活函数>Mish激活函数</a></li><li><a href=#dropblock>Dropblock</a></li></ul></li><li><a href=#neck>Neck</a><ul><li><a href=#spp模块>SPP模块</a></li><li><a href=#fpnpan>FPN+PAN</a></li></ul></li><li><a href=#prediction>Prediction</a><ul><li><a href=#ciou_loss>CIOU_Loss</a></li><li><a href=#diou_nms>DIOU_nms</a></li></ul></li></ul></li><li><a href=#优化策略>优化策略</a></li></ul><ul><li><a href=#网络结构-4>网络结构</a></li><li><a href=#改进-2>改进</a><ul><li><a href=#输入端-1>输入端</a><ul><li><a href=#mosaic数据增强-1>Mosaic数据增强</a></li><li><a href=#自适应锚框计算>自适应锚框计算</a></li><li><a href=#自适应图片缩放>自适应图片缩放</a></li></ul></li><li><a href=#backbone-1>Backbone</a><ul><li><a href=#focus结构>Focus结构</a></li><li><a href=#csp结构>CSP结构</a></li></ul></li><li><a href=#neck-1>Neck</a><ul><li><a href=#fpnpan-1>FPN+PAN</a></li></ul></li><li><a href=#prediction-1>Prediction</a><ul><li></li></ul></li></ul></li><li><a href=#yolov5四种网络结构的不同点>Yolov5四种网络结构的不同点</a><ul><li><a href=#yolov5四种网络的深度>Yolov5四种网络的深度</a></li><li><a href=#yolov5四种网络的宽度>Yolov5四种网络的宽度</a></li></ul></li><li><a href=#损失函数-3>损失函数</a></li></ul><ul><li><a href=#数据增强>数据增强</a></li></ul><ul><li><a href=#网络结构-5>网络结构</a></li><li><a href=#改进-3>改进</a><ul><li><a href=#输入端-2>输入端</a><ul><li><a href=#mosaic数据增强-2>Mosaic数据增强</a></li><li><a href=#mixup数据增强>MixUp数据增强</a></li></ul></li><li><a href=#backbone-2>Backbone</a></li><li><a href=#neck-2>Neck</a></li><li><a href=#prediction-2>Prediction</a><ul><li><a href=#decoupled-head>Decoupled Head</a></li><li><a href=#anchor-free>Anchor-free</a></li><li><a href=#标签分配>标签分配</a></li></ul></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>74</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>23</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-06-01T15:59:41+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=269220></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=576></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-01T20:06:32+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/cv-yolo/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="cv-yolo"><meta itemprop=description content="YOLO vs Faster R-CNN Faster R-CNN将检测结果分为两部分求解：物体类别（分类问题）、物体位置即bounding box（回归问题），YOLO统一为一个回归问题"></span><header class=post-header><h1 class=post-title itemprop="name headline">cv-yolo
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/cv-yolo.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/cv itemprop=url rel=index><span itemprop=name>cv</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>19697</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>40分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/cv-yolo/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h2 id=yolo-vs-faster-r-cnn>YOLO vs Faster R-CNN</h2><p>Faster R-CNN<strong>将检测结果分为两部分求解</strong>：物体类别（分类问题）、物体位置即bounding box（回归问题），YOLO<strong>统一为一个回归问题</strong>。</p><p>统一网络：YOLO没有显示求取region proposal的过程。Faster R-CNN中尽管RPN与fast rcnn共享卷积层，但是在模型训练过程中，需要反复训练RPN网络和fast rcnn网络。相对于R-CNN系列的"看两眼"(候选框提取与分类)，YOLO只需要Look Once.</p><blockquote><ul><li>YOLOv1论文名以及论文地址：<strong>You Only Look Once:Unified, Real-Time Object Detection</strong>、
You Only Look Once:Unified, Real-Time Object Detection: <em><a href=https://arxiv.org/pdf/1506.02640.pdf title=https://arxiv.org/pdf/1506.02640.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1506.02640.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv1开源代码：<strong>YOLOv1-Darkent</strong>
YOLOv1-Darkent: <em><a href=https://github.com/pjreddie/darknet title=https://github.com/pjreddie/darknet rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pjreddie/darknet
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv2论文名以及论文地址：<strong>YOLO9000:Better, Faster, Stronger</strong>
YOLO9000:Better, Faster, Stronger: <em><a href=https://arxiv.org/pdf/1612.08242v1.pdf title=https://arxiv.org/pdf/1612.08242v1.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1612.08242v1.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv2开源代码：<strong>YOLOv2-Darkent</strong>
YOLOv2-Darkent: <em><a href=https://github.com/pjreddie/darknet title=https://github.com/pjreddie/darknet rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/pjreddie/darknet
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv3论文名以及论文地址：<strong>YOLOv3: An Incremental Improvement</strong>
YOLOv3: An Incremental Improvement: <em><a href=https://arxiv.org/pdf/1804.02767.pdf title=https://arxiv.org/pdf/1804.02767.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1804.02767.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv3开源代码：<strong>YOLOv3-PyTorch</strong>
YOLOv3-PyTorch: <em><a href=https://github.com/ultralytics/yolov3 title=https://github.com/ultralytics/yolov3 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/ultralytics/yolov3
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv4论文名以及论文地址：<strong>YOLOv4: Optimal Speed and Accuracy of Object Detection</strong>
YOLOv4: Optimal Speed and Accuracy of Object Detection: <em><a href=https://arxiv.org/pdf/2004.10934.pdf title=https://arxiv.org/pdf/2004.10934.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2004.10934.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv4开源代码：<strong>YOLOv4-Darkent</strong>
YOLOv4-Darkent: <em><a href=https://github.com/AlexeyAB/darknet title=https://github.com/AlexeyAB/darknet rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/AlexeyAB/darknet
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv5论文名以及论文地址：无
YOLOv5开源代码：<strong>YOLOv5-PyTorch</strong>
YOLOv5-PyTorch: <em><a href=https://github.com/ultralytics/yolov5 title=https://github.com/ultralytics/yolov5 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/ultralytics/yolov5
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOx论文名以及论文地址：<strong>YOLOX: Exceeding YOLO Series in 2021</strong>
YOLOX: Exceeding YOLO Series in 2021: <em><a href=https://arxiv.org/pdf/2107.08430.pdf title=https://arxiv.org/pdf/2107.08430.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2107.08430.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOx开源代码：<strong>YOLOx-PyTorch</strong>
YOLOx-PyTorch: <em><a href=https://github.com/Megvii-BaseDetection/YOLOX title=https://github.com/Megvii-BaseDetection/YOLOX rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/Megvii-BaseDetection/YOLOX
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv6论文名以及论文地址：<strong>YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications</strong>
YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications: <em><a href=https://arxiv.org/pdf/2209.02976.pdf title=https://arxiv.org/pdf/2209.02976.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2209.02976.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv6开源代码：<strong>YOLOv6-PyTorch</strong>
YOLOv6-PyTorch: <em><a href=https://github.com/meituan/YOLOv6 title=https://github.com/meituan/YOLOv6 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/meituan/YOLOv6
<i class="fa fa-external-link-alt"></i></a></em></li><li>YOLOv7论文名以及论文地址：<strong>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</strong>
YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors: <em><a href=https://arxiv.org/pdf/2207.02696.pdf title=https://arxiv.org/pdf/2207.02696.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/2207.02696.pdf
<i class="fa fa-external-link-alt"></i></a></em>
YOLOv7开源代码：<strong>Official YOLOv7-PyTorch</strong>
Official YOLOv7-PyTorch: <em><a href=https://github.com/WongKinYiu/yolov7 title=https://github.com/WongKinYiu/yolov7 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/WongKinYiu/yolov7
<i class="fa fa-external-link-alt"></i></a></em></li></ul></blockquote><h1 id=v1>V1</h1><p><strong>you only look once:unified, real-time object detection</strong></p><h2 id=思想>思想</h2><p><strong>yolo的核心思想是将输入的图像经过backbone特征提取后，将的到的特征图划分为S x S的网格，物体的中心落在哪一个网格内，这个网格就负责预测该物体的置信度、类别以及坐标位置。</strong></p><ul><li><p>将一幅图像分成SxS个网格(grid cell),如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。</p></li><li><p>每个网格要预测B个bounding box,每个bounding box除了要预测位置之外，还要附带预测一个confidence值。每个网格还要预测C个类别的分数。</p></li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/30/fmPys3jO1xXeaS4.png alt=quicker_009f6375-cf72-45a5-8c9a-71d462a9fca2.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/07/Q28uDgZbSz59Kf3.png alt=quicker_e565eedf-5844-4a6d-9f72-9f5d292438b9.png></p><p>​ 该表达式含义：如果有object落在一个grid cell里，则第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/30/vt8dCxbKTMXD3jo.png alt=quicker_8554cab6-43ed-43ba-b576-e57ea2ae429a.png></p><p>B=2， 7*7*30包含了坐标、置信度、类别结果</p><p>x.y是相对每一个gird cell左上角点的坐标，w,h是相对整幅图像的宽高，都是0-1的值。</p><h2 id=网络结构>网络结构</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/07/wGxvH52dr4uq3jQ.png alt=quicker_bbfc17a0-fd55-46e7-a924-608320874e3c.png></p><p>最后一层全连接层用线性激活函数，其余层采用 Leaky ReLU。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/sl2un1bEIVZgktB.png alt=quicker_ddc8f262-fece-4ee4-aeb9-276fb623a62d.png></p><h2 id=后处理>后处理</h2><p>2个框重合度很高，大概率是一个目标，那就只取一个框。</p><p>首先从所有的检测框中找到置信度最大的那个，然后遍历剩余的框，计算其与最大框之间的IOU。如果其值大于一定阈值，则表示重合度过高，那么就将该框就会被剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/03/30/Q8WsNpUHTr7S6xC.png alt=quicker_d32d2664-49e5-4c50-9f54-c8b28e313f25.png></p><h2 id=损失函数>损失函数</h2><p>损失函数有三个部分组成，分别是边框损失，置信度损失，以及类别损失。并且三个损失函数都使用均方差损失函数(MSE)。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-21/2022-10-21_16-43-29-953.png alt></p><p>只有当某个网格中有object的时候才对classification error进行惩罚。</p><p>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大</p><p>紫色的框是当gird cell有真实框的中心点的时候取1，否则取0</p><p>红色的框为第i个gird cell第j个bounding box是负责预测物体，是为1否则为0</p><p>绿色的框呢是第i个gird cell第j个bounding box不负责预测物体为1否则为0</p><p>再说两个λ，对于负责预测物体的框呢要严重惩罚所以赋值为５，不负责的呢意思意思为０.５再说每一项，</p><p>第一项是中心点定位误差，</p><p>第二项是宽高误差，开根号主要是为了惩罚小框，对大框公平一点</p><p>第三项是置信度误差，标签是通过计算这个框与ground truth的iou</p><p>最后一项是类别预测误差，这个概率就是条件概率乘以置信度，所得到的20维度的类别概率</p><blockquote><p>宽和高开根号，是因为偏移相同的距离，对小目标影响更大<img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/MdCFshx5IwDUYty.png alt=quicker_fe20384e-9a44-4c4b-b1d2-8ac93ab34f7b.png></p></blockquote><h2 id=缺陷>缺陷</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/uASTXq365lkvEjF.png alt=quicker_ea58d9a6-762d-431b-84da-54c0e2cabe7a.png></p><ul><li>YOLO对<strong>相互靠的很近的物体和很小的群体检测效果不好</strong>，这是因为一个网格中只预测了两个框，并且只属于一类；</li><li>准确度低，recall低</li><li>同一类物体出现的新的<strong>不常见的长宽比和其他情况时，泛化能力偏弱</strong>；</li><li><strong>由于损失函数的问题，定位误差是影响检测效果的主要原因</strong>。尤其是大小物体的处理上，还有待加强。</li></ul><h1 id=v2>v2</h1><p>YOLOv2 也叫 YOLO9000，因为使用了 COCO 数据集以及 Imagenet 数据集来联合训练，最终可以检测9000个类别。</p><h2 id=思想-1>思想</h2><p>使用 Darknet19 作为网络的主干网络。Darknet19 有点类似 VGG，在 Darknet19 中，使用的是 3 x 3 大小的卷积核，并且在每次Pooling 之后都增加一倍通道数，以及将特征图的宽高缩减为原来的一半。网络中有19个卷积层，所以叫 Darknet19，以及有5个 Max Pooling 层，所以这里进行了32倍的下采样。</p><h2 id=网络结构-1>网络结构</h2><p>使用 Darknet19 作为网络的主干网络。Darknet19 有点类似 VGG，在 Darknet19 中，使用的是 3 x 3 大小的卷积核，并0且在每次Pooling 之后都增加一倍通道数，以及将特征图的宽高缩减为原来的一半。网络中有19个卷积层，所以叫 Darknet19，以及有5个 Max Pooling 层，所以这里进行了32倍的下采样。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/cENjmszArXtovaY.png alt=quicker_e071e5e8-99ff-40ab-9c03-b511a0572d92.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/HnX14fM7NuaCwmV.png alt=quicker_4ff8f074-ade7-42a8-b122-cb0c80d438d9.png></p><h2 id=改进>改进</h2><h3 id=batch-normalization>Batch Normalization</h3><p>在卷积层的后面、激活函数的前面加上了BN层，</p><p>使得网络可以加速收敛，</p><p>解决了梯度弥散的问题。</p><p>不太受初始化的影响，</p><p>还可以起到正则化的作用。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/lV83pBA7FsOHE6G.png alt=quicker_a21a2608-2876-4a16-80b6-8d6a59448be0.png></p><p>零均值标准差为1，sigmod、双曲正切函数(tanh)在0附近有比较大的梯度</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-32-57-469.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-36-22-728.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-41-01-767.png alt></p><h3 id=高分辨率分类器>高分辨率分类器</h3><p>High Resolution Classifier</p><p>先来看看yolov1是怎么做的，首先让ImageNet的图像resize到224*224训练特征提取层，使用更高的448*448的分辨率训练检测头</p><p>yolov2避免了这个突变，首先对224*224的预训练之后，然后在448*448的分辨率上微调10个epoch，这样的过度使得mAP又增加了3.7个百分点</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/7xGFfyglPTtpber.png alt=quicker_cc2c9f30-c06a-4066-8cad-1c85007659c8.png></p><h3 id=anchor>Anchor</h3><p>Convolutional With Anchor Boxes</p><p>先受Faster-rcnn的启发，使用<strong>锚框然后预测它的偏移量，而不是直接预测坐标位置</strong>，这样网络学习起来会更容易。所谓锚框，就是之前设定好不同宽高比的先验框</p><p>为了更好的理解anchor，我们先来回忆一下yolov1没有anchor的时候是怎么回事？四个坐标呢完全就是由网络计算出来的，(x, y)坐标表示相对于网格单元格边界的方框中心，宽高是相对于整个图像的宽度和高度，这个值生成的宽高比、中心点的位置幔帐图像乱跑的，明显更难预测。</p><p>作者输入的是416*416，而不是448*448，因为下采样32倍之后416*416的图像是13*13，它是一个奇数，为什么我们想要奇数呢？因为图片的中心往往存在大物体，作者希望有一个单独的grid cell来负责预测这个物体，而不是周围的4个</p><p>作者将原图划分为13*13个gird cell，每个grid cell会有5个预测框，13*13*5这个数远远大于v1中的7*7*2，所以作者去掉了全连接层，参数太多，计算太慢</p><p>但是添加了锚框之后mAP下降了0.2%，这依然是一个很好的改进，因为他解决了V1的召回率低的问题，使recall增加到了88%</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/OQAohyxdTgPZ1Ir.png alt=quicker_ebf88288-5a3f-4e1d-8ac0-1acd1551802a.png></p><h3 id=聚类anchor>聚类anchor</h3><p>Dimension Clusters</p><p>先验框先验框，这个宽高比该怎么定呢，该选择几个锚框呢，3个、5个？手动选择吗？显然不是那么合理，所以作何采用了对训练集的bounding_box进行<strong>k_means聚类</strong>的方法。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/jSmZx9cCOErkywX.png alt=quicker_2940e48e-7b55-4a37-89cd-55562f7f9846.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-48-31-857.png alt></p><h3 id=direct-location-prediction>Direct location prediction</h3><p>虽然说现在使用了锚框，但是它的中心点还是可以乱跑的。作者对边框进行了限制，</p><p>保证锚框的中心点只能在负责预测的这个grid cell里面（通过sigmoid函数映射到0-1），宽高不设限制。</p><p>这里的cx,cy是经过归一化了的，一个cx,cy是1，也就是如果要可视化在原图上还要乘以13</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_16-57-58-123.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_16-57-04-305.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/642gsR7lVd9nf1x.png alt=quicker_eddfe836-9897-4e0a-ae21-04ae470c5e8c.png></p><h3 id=fine-grained-features>Fine-Grained Features</h3><p><img src=/imgs/img-lazy-loading.gif data-src=C:%5cUsers%5cWENCHAO%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5c1652019960723.png alt=1652019960723></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_17-20-10-865.png alt></p><p>类似 Pixel-shuffle，**融合高层和低层的信息，这样可以保留一些细节信息，这样可以更好检测小物体。**具体来说，就是进行一拆四的操作，直接传递到池化后的特征图中，进行卷积后再叠加两者，最后一起作为输出特征图进行输出。通过使用 Pass through 层来检测细粒度特征使 mAP 提升了1个点。</p><img src=https://s2.loli.net/2022/05/08/e3TdWYOHxlI2QDA.png title=quicker_e58852f3-aa65-4471-af7e-881bbeae12b1.png><h3 id=多尺度训练>多尺度训练</h3><p>Multi-Scale Training</p><p>由于这个模型只有卷积层、池化层，所以可以动态的调整输入图像的大小，为了让网络适应不同的尺度的图像，<strong>每10个batch，网络随机选择一个新的图像维度大小</strong>，他们都是32的倍数,这一步是在检测数据集上fine tune时候采用的</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/dPnTxls9EhVo4FM.png alt=quicker_ee5ea3b8-9757-4c4e-ac61-4d1be128c63e.png></p><h3 id=heading></h3><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_17-25-21-331.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/zmoA9DNHcVFRkeg.png alt=quicker_be167748-486f-4193-a962-45a55c4fa985.png></p><h2 id=损失函数-1>损失函数</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-22-37-301.png alt></p><h1 id=v3>V3</h1><h2 id=网络结构-2>网络结构</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_11-16-28-274.png alt></p><p>上图三个蓝色方框内表示Yolov3的三个基本组件：</p><ol><li><strong>CBL：<strong>Yolov3网络结构中的最小组件，由</strong>Conv+Bn+Leaky_relu</strong>激活函数三者组成。</li><li><strong>Res unit：<strong>借鉴</strong>Resnet</strong>网络中的残差结构，让网络可以构建的更深。</li><li><strong>ResX：<strong>由一个</strong>CBL</strong>和<strong>X</strong>个残差组件构成，是Yolov3中的大组件。每个Res模块前面的CBL都起到下采样的作用，因此经过5次Res模块后，得到的特征图是<strong>608->304->152->76->38->19大小</strong>。</li></ol><p>其他基础操作：</p><ol><li>**Concat：**张量拼接，会扩充两个张量的维度，例如26*26*256和26*26*512两个张量拼接，结果是26*26*768。Concat和cfg文件中的route功能一样。</li><li>**add：**张量相加，张量直接相加，不会扩充维度，例如104*104*128和104*104*128相加，结果还是104*104*128。add和cfg文件中的shortcut功能一样。</li></ol><p>每个ResX中包含1+2*X个卷积层，因此整个主干网络Backbone中一共包含<strong>1+（1+2*1）+（1+2*2）+（1+2*8）+（1+2*8）+（1+2*4）=52</strong>，再加上一个FC全连接层，即可以组成一个<strong>Darknet53分类网络</strong>。不过在目标检测Yolov3中，去掉FC层，不过为了方便称呼，仍然把<strong>Yolov3</strong>的主干网络叫做<strong>Darknet53结构</strong>。</p><p>卷积的strides默认为（1，1），padding默认为same，当strides为（2，2）时padding为valid。</p><p>上图是以输入图像256 x 256进行预训练来进行介绍的，常用的尺寸是416 x 416，都是32的倍数。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-51-36-317.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/Q1AWzDPpg4sr2jC.png alt=quicker_63fb0c5f-b01b-48ea-8421-3db82da2b5a3.png></p><p><strong>原Darknet53中的尺寸是在图片分类训练集上训练的，所以输入的图像尺寸是256x256，下图是以YOLO v3 416模型进行绘制的，所以输入的尺寸是416x416，预测的三个特征层大小分别是52，26，13。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/6vFiPaYE5kXMQDZ.png alt=quicker_1e5320d6-33b2-4bdf-8484-555e43228621.png></p><p>在上图中我们能够很清晰的看到三个预测层分别来自的什么地方，以及Concatenate层与哪个层进行深度方向拼接（FPN对应维度上相加）。<strong>注意Convolutional是指Conv2d+BN+LeakyReLU，和Darknet53图中的一样，而生成预测结果的最后三层都只是Conv2d。</strong></p><h2 id=目标边界框的预测>目标边界框的预测</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/08/rPwdmlf4nOvXDjU.png alt=quicker_07c7df41-0dfc-4598-ac25-dfff4392091f.png></p><h2 id=损失函数-2>损失函数</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-22/2022-10-22_15-08-59-922.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=C:%5cUsers%5cWENCHAO%5cAppData%5cRoaming%5cTypora%5ctypora-user-images%5c1652025879465.png alt=1652025879465></p><h1 id=v3-spp>V3 SPP</h1><p>而Yolov3和Yolov3_spp的不同点在于，Yolov3的主干网络后面，<strong>添加了spp组件</strong>，这里需要注意。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_16-51-57-117.png alt></p><h1 id=v4>V4</h1><p><a href="https://so.csdn.net/so/search?q=YOLOv4&spm=1001.2101.3001.7020" title=YOLOv4 rel="noopener external nofollow noreferrer" target=_blank class=exturl>YOLOv4
<i class="fa fa-external-link-alt"></i>
</a>: Optimal Speed and Accuracy of Object Detection</p><h2 id=网络结构-3>网络结构</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-23/2022-10-23_16-51-42-777.png alt></p><p><strong>先整理下Yolov4的五个基本组件</strong>：</p><ol><li>**CBM：**Yolov4网络结构中的最小组件，由Conv+Bn+Mish激活函数三者组成。</li><li>**CBL：**由Conv+Bn+Leaky_relu激活函数三者组成。</li><li>**Res unit：**借鉴Resnet网络中的残差结构，让网络可以构建的更深。</li><li>**CSPX：**借鉴CSPNet网络结构，由卷积层和X个Res unint模块Concate组成。</li><li>**SPP：**采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。</li></ol><p><strong>其他基础操作：</strong></p><ol><li>**Concat：**张量拼接，维度会扩充，和Yolov3中的解释一样，对应于cfg文件中的route操作。</li><li>**add：**张量相加，不会扩充维度，对应于cfg文件中的shortcut操作。</li></ol><p><strong>Backbone中卷积层的数量：</strong></p><p>和Yolov3一样，再来数一下Backbone里面的卷积层数量。</p><p>每个CSPX中包含5+2*X个卷积层，因此整个主干网络Backbone中一共包含1+（5+2*1）+（5+2*2）+（5+2*8）+（5+2*8）+（5+2*4）=72。</p><p>Backbone: CSPDarknet53
Neck: SPP，PAN
Head: YOLOv3</p><p>相比之前的<code>YOLOv3</code>，改进了下Backbone，在<code>Darknet53</code>中引入了<code>CSP</code>模块（来自<code>CSPNet</code>）。在Neck部分，采用了<code>SPP</code>模块（<code>Ultralytics</code>版的<code>YOLOv3 SPP</code>就使用到了）以及<code>PAN</code>模块（来自<code>PANet</code>）。Head部分没变还是原来的检测头。</p><h2 id=cspdarknet53网络结构>CSPDarknet53网络结构</h2><p>CSPDarknet53就是将CSP结构融入了Darknet53中。CSP结构是在CSPNet（Cross Stage Partial Network）论文中提出的，CSPNet作者说在目标检测任务中使用CSP结构有如下好处：</p><p>Strengthening learning ability of a CNN
Removing computational bottlenecks
Reducing memory costs
<strong>即减少网络的计算量以及对显存的占用，同时保证网络的能力不变或者略微提升。<strong>CSP结构的思想参考原论文中绘制的CSPDenseNet，<strong>进入每个stage（一般在下采样后）先将数据划分成俩部分</strong>，如下图所示的Part1和Part2。但具体怎么划分呢，<strong>在CSPNet中是直接按照通道均分</strong>，但在</strong>YOLOv4网络中是通过两个1x1的卷积层来实现的</strong>。<strong>在Part2后跟一堆Blocks然后在通过1x1的卷积层（图中的Transition），接着将两个分支的信息在通道方向进行Concat拼接，最后再通过1x1的卷积层进一步融合</strong>（图中的Transition）。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/drBFhsqePQcnuYi.png alt=quicker_9c3da51b-5d06-4c84-a90b-c08a371a0edf.png></p><p>CSPDarknet53详细结构（以输入图片大小为416 × 416 × 3 为例）</p><ul><li><p>注意，<code>CSPDarknet53</code> Backbone中所有的激活函数都是<code>Mish</code>激活函数</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/mlPByM4hdgoIS71.png alt=quicker_d078c76a-ae00-4272-b93a-f4a5a4944b02.png></p></li></ul><h2 id=网络详细结构>网络详细结构</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/S7vgi5XTmFNOM21.png alt=quicker_e0ec51d7-d578-4fb5-8a36-e590863d705f.png></p><h2 id=改进-1>改进</h2><p>YoloV4的创新之处进行讲解，让大家一目了然。</p><ol><li><strong>输入端：<strong>这里指的创新主要是训练时对输入端的改进，主要包括</strong>Mosaic数据增强、cmBN、SAT自对抗训练</strong></li><li>**BackBone主干网络：**将各种新的方式结合起来，包括：<strong>CSPDarknet53、Mish激活函数、Dropblock</strong></li><li><strong>Neck：<strong>目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的</strong>SPP模块</strong>、<strong>FPN+PAN结构</strong></li><li><strong>Prediction：<strong>输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数</strong>CIOU_Loss</strong>，以及预测框筛选的nms变为<strong>DIOU_nms</strong></li></ol><h3 id=输入端>输入端</h3><p>这里指的创新主要是训练时对输入端的改进，主要包括<strong>Mosaic数据增强、cmBN、SAT自对抗训练</strong></p><h4 id=mosaic数据增强>Mosaic数据增强</h4><p><strong>Yolov4</strong>中使用的<strong>Mosaic</strong>是参考2019年底提出的<strong>CutMix数据增强</strong>的方式，但<strong>CutMix</strong>只使用了两张图片进行拼接，而<strong>Mosaic数据增强</strong>则采用了4张图片，<strong>随机缩放、随机裁剪、随机排布</strong>的方式进行拼接。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-23/2022-10-23_16-54-55-361.png alt></p><p>主要有几个优点：</p><ol><li><strong>丰富数据集：<strong>随机使用</strong>4张图片</strong>，随机缩放，再随机分布进行拼接，大大丰富了检测数据集，特别是随机缩放增加了很多小目标，让网络的鲁棒性更好。</li><li>**减少GPU：**可能会有人说，随机缩放，普通的数据增强也可以做，但作者考虑到很多人可能只有一个GPU，因此Mosaic增强训练时，可以直接计算4张图片的数据，使得Mini-batch大小并不需要很大，一个GPU就可以达到比较好的效果。</li></ol><h3 id=backbone>Backbone</h3><h4 id=cspdarknet53>CSPDarknet53</h4><p><strong>CSPDarknet53</strong>是在Yolov3主干网络<strong>Darknet53</strong>的基础上，借鉴<strong>2019年CSPNet</strong>的经验，产生的<strong>Backbone</strong>结构，其中包含了<strong>5个CSP</strong>模块。</p><p>每个CSP模块前面的卷积核的大小都是3*3，stride=2，因此可以起到下采样的作用。因为Backbone有5个<strong>CSP模块</strong>，输入图像是<strong>608*608</strong>，所以特征图变化的规律是：<strong>608->304->152->76->38->19</strong>.经过5次CSP模块后得到19*19大小的特征图。</p><p>CSPNet论文地址：
<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.11929.pdf" title=https://arxiv.org/pdf/1911.11929.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1911.11929.pdf
<i class="fa fa-external-link-alt"></i></a></p><p>CSPNet全称是Cross Stage Paritial Network，CSPNet的作者认为推理计算过高的问题是由于网络优化中的<strong>梯度信息重复</strong>导致的。因此采用CSP模块先将基础层的特征映射划分为两部分，然后通过<strong>跨阶段层次结构将它们合并</strong>，在<strong>减少了计算量的同时可以保证准确率</strong>。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-24/2022-11-24_19-39-16-343.png alt></p><h4 id=mish激活函数>Mish激活函数</h4><p>而且作者只在Backbone中采用了<strong>Mish激活函数</strong>，网络后面仍然采用<strong>Leaky_relu激活函数。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-23/2022-10-23_19-23-58-105.png alt></p><h4 id=dropblock>Dropblock</h4><p>Yolov4中使用的<strong>Dropblock</strong>，其实和常见网络中的Dropout功能类似，也是缓解过拟合的一种正则化方式。</p><p>Dropblock在2018年提出，论文地址：
<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1810.12890.pdf" title=https://arxiv.org/pdf/1810.12890.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1810.12890.pdf
<i class="fa fa-external-link-alt"></i></a></p><p>传统的Dropout很简单，一句话就可以说的清：<strong>随机删除减少神经元的数量，使网络变得更简单。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-32-03-855.png alt></p><p>中间Dropout的方式会随机的删减丢弃一些信息，但<strong>Dropblock的研究者</strong>认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：<strong>卷积+激活+池化层</strong>，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到<strong>相同的信息</strong>。</p><p>因此，在全连接层上效果很好的Dropout在卷积层上<strong>效果并不好</strong>。</p><p>所以<strong>右图Dropblock的研究者</strong>则干脆整个局部区域进行删减丢弃。</p><p>这种方式其实是借鉴<strong>2017年的cutout数据增强</strong>的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程<strong>线性的增加这个比率</strong>。</p><p><strong>Dropblock</strong>的研究者与<strong>Cutout</strong>进行对比验证时，发现有几个特点：</p><p>**优点一：**Dropblock的效果优于Cutout</p><p>**优点二：**Cutout只能作用于输入层，而Dropblock则是将Cutout应用到网络中的每一个特征图上</p><p>**优点三：**Dropblock可以定制各种组合，在训练的不同阶段可以修改删减的概率，从空间层面和时间层面，和Cutout相比都有更精细的改进。</p><p><strong>Yolov4</strong>中直接采用了更优的<strong>Dropblock</strong>，对网络的正则化过程进行了全面的升级改进。</p><h3 id=neck>Neck</h3><p>目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的<strong>SPP模块</strong>、<strong>FPN+PAN结构</strong></p><h4 id=spp模块>SPP模块</h4><p>第一个预测特征层添加了SPP（Spatial Pyramid Pooling）模块，实现了不同尺度的特征融合
注意：这里的SPP和SPPnet中的SPP结构不一样</p><p>SPP模块，其实在Yolov3中已经存在了，在<strong>Yolov4</strong>的C++代码文件夹中有一个<strong>Yolov3_spp版本</strong>，但有的同学估计从来没有使用过，在Yolov4中，SPP模块仍然是在Backbone主干网络之后：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-36-24-126.png alt></p><p>作者在SPP模块中，使用**k={1*1,5*5,9*9,13*13}**的最大池化的方式，再将不同尺度的特征图进行Concat操作。</p><p><strong>注意：<strong>这里最大池化采用</strong>padding操作</strong>，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，<strong>padding=2</strong>，因此池化后的特征图仍然是13×13大小。</p><h4 id=fpnpan>FPN+PAN</h4><p><strong>PAN结构</strong>比较有意思，看了网上Yolov4关于这个部分的讲解，大多都是讲的比较笼统的，而PAN是借鉴
<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.01534" title=图像分割领域PANet rel="noopener external nofollow noreferrer" target=_blank class=exturl>图像分割领域PANet
<i class="fa fa-external-link-alt"></i>
</a>的创新点，有些同学可能不是很清楚。</p><p>下面大白将这个部分拆解开来，看下Yolov4中是如何设计的。</p><p><strong>Yolov3结构：</strong></p><p>我们先来看下Yolov3中Neck的FPN结构</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-51-36-317.png alt></p><p>可以看到经过几次下采样，三个紫色箭头指向的地方，输出分别是<strong>76*76、38*38、19*19。</strong></p><p>以及最后的<strong>Prediction</strong>中用于预测的三个特征图<strong>①19*19*255、②38*38*255、③76*76*255。[注：255表示80类别(1+4+80)×3=255]</strong></p><p>我们将Neck部分用立体图画出来，更直观的看下两部分之间是如何通过<strong>FPN结构</strong>融合的。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-53-31-895.png alt></p><p>如图所示，FPN是自顶向下的，将高层的特征信息通过<strong>上采样</strong>的方式进行传递融合，得到进行预测的特征图。</p><p><strong>Yolov4结构：</strong></p><p>而Yolov4中Neck这部分除了使用FPN外，还在此基础上使用了PAN结构：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-56-16-588.png alt></p><p>前面CSPDarknet53中讲到，每个CSP模块前面的卷积核都是<strong>3*3大小</strong>，<strong>步长为2</strong>，相当于下采样操作。</p><p>因此可以看到三个紫色箭头处的特征图是<strong>76*76、38*38、19*19。</strong></p><p>以及最后Prediction中用于预测的三个特征图：<strong>①76*76*255，②38*38*255，③19*19*255。</strong></p><p>我们也看下<strong>Neck</strong>部分的立体图像，看下两部分是如何通过<strong>FPN+PAN结构</strong>进行融合的。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_09-55-30-821.png alt></p><p>和Yolov3的FPN层不同，Yolov4在FPN层的后面还添加了一个<strong>自底向上的特征金字塔。</strong></p><p>其中包含两个<strong>PAN结构。</strong></p><p>这样结合操作，FPN层自顶向下传达<strong>强语义特征</strong>，而特征金字塔则自底向上传达<strong>强定位特征</strong>，两两联手，从不同的主干层对不同的检测层进行参数聚合,这样的操作确实很皮。</p><p><strong>FPN+PAN</strong>借鉴的是18年CVPR的<strong>PANet</strong>，当时主要应用于<strong>图像分割领域</strong>，但Alexey将其拆分应用到Yolov4中，进一步提高特征提取的能力。</p><p>不过这里需要注意几点：</p><p><strong>注意一：</strong></p><p>Yolov3的FPN层输出的三个大小不一的特征图①②③直接进行预测</p><p>但Yolov4的FPN层，只使用最后的一个76*76特征图①，而经过两次PAN结构，输出预测的特征图②和③。</p><p>这里的不同也体现在cfg文件中，这一点有很多同学之前不太明白，</p><p>比如Yolov3.cfg最后的三个Yolo层，</p><p>第一个Yolo层是最小的特征图<strong>19*19</strong>，mask=<strong>6,7,8</strong>，对应<strong>最大的anchor box。</strong></p><p>第二个Yolo层是中等的特征图<strong>38*38</strong>，mask=<strong>3,4,5</strong>，对应<strong>中等的anchor box。</strong></p><p>第三个Yolo层是最大的特征图<strong>76*76</strong>，mask=<strong>0,1,2</strong>，对应<strong>最小的anchor box。</strong></p><p>而Yolov4.cfg则<strong>恰恰相反</strong></p><p>第一个Yolo层是最大的特征图<strong>76*76</strong>，mask=<strong>0,1,2</strong>，对应<strong>最小的anchor box。</strong></p><p>第二个Yolo层是中等的特征图<strong>38*38</strong>，mask=<strong>3,4,5</strong>，对应<strong>中等的anchor box。</strong></p><p>第三个Yolo层是最小的特征图<strong>19*19</strong>，mask=<strong>6,7,8</strong>，对应<strong>最大的anchor box。</strong></p><p><strong>注意点二：</strong></p><p>原本的PANet网络的<strong>PAN结构</strong>中，两个特征图结合是采用<strong>shortcut</strong>操作，而Yolov4中则采用**concat（route）**操作，特征图融合后的尺寸发生了变化。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-13-42-477.png alt></p><h3 id=prediction>Prediction</h3><p>输出层的锚框机制和Yolov3相同，主要改进的是训练时的损失函数<strong>CIOU_Loss</strong>，以及预测框筛选的nms变为<strong>DIOU_nms</strong></p><p>目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。</p><p>Bounding Box Regeression的Loss近些年的发展过程是：</p><p>Smooth L1 Loss</p><p>IoU Loss（2016）</p><p>GIoU Loss（2019）</p><p>DIoU Loss（2020）</p><p>CIoU Loss（2020）</p><h4 id=ciou_loss>CIOU_Loss</h4><h5 id=iou_loss>IOU_Loss</h5><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-14-34-147.png alt></p><p>可以看到IOU的loss其实很简单，主要是<strong>交集/并集</strong>，但其实也存在两个问题。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-16-46-272.png alt></p><p>问题1：即状态1的情况，当<strong>预测框和目标框不相交时，IOU=0，无法反应两个框距离的远近</strong>，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。</p><p>问题2：即状态2和状态3的情况，当两个预测框大小相同，两个<strong>IOU也相同，IOU_Loss无法区分两者相交情况的不同。</strong></p><p>GIOU_Loss</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-17-36-178.png alt></p><p>可以看到右图GIOU_Loss中，增加了<strong>相交尺度的衡量方式</strong>，缓解了单纯IOU_Loss时的尴尬。</p><p>但为什么仅仅说缓解呢？因为还存在一种不足：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-18-23-212.png alt></p><p>问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。</p><h5 id=diou_loss>DIOU_Loss</h5><p>好的目标框回归函数应该考虑三个重要几何因素：<strong>重叠面积、中心点距离，长宽比。</strong></p><p>针对IOU和GIOU存在的问题，作者从两个方面进行考虑</p><p><strong>一：如何最小化预测框和目标框之间的归一化距离？</strong></p><p><strong>二：如何在预测框和目标框重叠时，回归的更准确？</strong></p><p>针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-19-59-320.png alt></p><p>DIOU_Loss考虑了<strong>重叠面积</strong>和<strong>中心点距离</strong>，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。</p><p>但就像前面好的目标框回归函数所说的，没有考虑到长宽比。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-20-43-483.png alt></p><p>比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。</p><p>但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。</p><h5 id=ciou_loss-1>CIOU_Loss</h5><p>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-52-58-544.png alt></p><p>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。</p><p>再来综合的看下各个Loss函数的不同点：</p><p>**IOU_Loss：**主要考虑检测框和目标框重叠面积。</p><p>**GIOU_Loss：**在IOU的基础上，解决边界框不重合时的问题。</p><p>**DIOU_Loss：**在IOU和GIOU的基础上，考虑边界框中心点距离的信息。</p><p>**CIOU_Loss：**在DIOU的基础上，考虑边界框宽高比的尺度信息。</p><p>Yolov4中采用了<strong>CIOU_Loss</strong>的回归方式，使得预测框回归的<strong>速度和精度</strong>更高一些。</p><h4 id=diou_nms>DIOU_nms</h4><p>Nms主要用于预测框的筛选，常用的目标检测算法中，一般采用普通的nms的方式，Yolov4则借鉴上面D/CIOU loss的论文：
<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.08287.pdf" title=https://arxiv.org/pdf/1911.08287.pdf rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://arxiv.org/pdf/1911.08287.pdf
<i class="fa fa-external-link-alt"></i>
</a>将其中计算IOU的部分替换成DIOU的方式：</p><p>再来看下实际的案例</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-53-59-691.png alt></p><p>在上图重叠的摩托车检测中，中间的摩托车因为考虑边界框中心点的位置信息，也可以回归出来。因此在重叠目标的检测中，<strong>DIOU_nms</strong>的效果优于<strong>传统的nms</strong>。</p><p><strong>注意：有读者会有疑问，这里为什么不用CIOU_nms，而用DIOU_nms?</strong></p><p>**答：**因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。</p><p>**总体来说，**YOLOv4的论文称的上良心之作，将近几年关于深度学习领域最新研究的改进s移植到Yolov4中做验证测试，将Yolov3的精度提高了不少。虽然没有全新的创新，但很多改进之处都值得借鉴，借用Yolov4作者的总结。</p><p>Yolov4 主要带来了 3 点新贡献：</p><p>（1）提出了一种高效而强大的目标检测模型，使用 1080Ti 或 2080Ti 就能训练出超快、准确的目标检测器。</p><p>（2）在检测器训练过程中，验证了最先进的一些研究成果对目标检测器的影响。</p><p>（3）改进了 SOTA 方法，使其更有效、更适合单 GPU 训练。</p><h2 id=优化策略>优化策略</h2><p>有关训练Backbone时采用的优化策略就不讲了有兴趣自己看下论文的<code>4.2</code>章节，这里直接讲下训练检测器时作者采用的一些方法。在论文<code>4.3</code>章节，作者也罗列了一堆方法，并做了部分消融实验。这里我只介绍确实在代码中有使用到的一些方法。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/NxXor2OA1mbRiWT.png alt=quicker_ec7b98a6-e801-41ca-abaa-593108f91593.png></p><h1 id=v5>V5</h1><h2 id=网络结构-4>网络结构</h2><p>Yolov5s网络最小，速度最快，AP精度也最低。但如果检测的以大目标为主，追求速度，倒也是个不错的选择。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_10-59-23-477.png alt></p><p>大家可能对<strong>Yolov3</strong>比较熟悉，因此大白列举它和Yolov3的一些主要的不同点，并和Yolov4进行比较。</p><p>**（1）输入端：**Mosaic数据增强、自适应锚框计算、自适应图片缩放
**（2）Backbone：**Focus结构，CSP结构
**（3）Neck：**FPN+PAN结构
**（4）Prediction：**GIOU_Loss</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/kCZgA5jshbKy7lv.png alt=quicker_d52e935b-94ae-4c77-b7b2-d2bd508eb31f.png></p><h2 id=改进-2>改进</h2><h3 id=输入端-1>输入端</h3><h4 id=mosaic数据增强-1>Mosaic数据增强</h4><p><strong>Mosaic数据增强</strong>则采用了4张图片，<strong>随机缩放、随机裁剪、随机排布</strong>的方式进行拼接。</p><h4 id=自适应锚框计算>自适应锚框计算</h4><p>在Yolo算法中，针对不同的数据集，都<strong>会有初始设定长宽的锚框。在网络训练中，网络在初始锚框的基础上输出预测框，进而和真实框groundtruth进行比对，计算两者差距，再反向更新</strong>，迭代网络参数。</p><p>因此初始锚框也是比较重要的一部分，比如Yolov5在Coco数据集上初始设定的锚框：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-03-19-552.png alt></p><p>在Yolov3、Yolov4中，训练不同的数据集时，计算初始锚框的值是通过单独的程序运行的。</p><p>但Yolov5中将此功能嵌入到代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。</p><p>当然，如果觉得计算的锚框效果不是很好，也可以在代码中将自动计算锚框功能关闭。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-05-20-513.png alt></p><h4 id=自适应图片缩放>自适应图片缩放</h4><p>在常用的目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。</p><p>比如Yolo算法中常用416*416，608*608等尺寸，比如对下面800*600的图像进行缩放。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-06-35-430.png alt></p><p>作者认为，在项目实际使用时，很多图片的长宽比不同，因此缩放填充后，两端的黑边大小都不同，而如果填充的比较多，则存在信息冗余，影响推理速度。因此在Yolov5的代码中datasets.py的letterbox函数中进行了修改，对原始图像<strong>自适应的添加最少的黑边</strong>。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-06-44-568.png alt></p><p><strong>图像高度上两端的黑边变少了，在推理时，计算量也会减少，即目标检测速度会得到提升。</strong></p><p>这种方式在之前github上Yolov3中也进行了讨论：</p><p><a href="https://link.zhihu.com/?target=https%3A//wx.qq.com/cgi-bin/mmwebwx-bin/webwxcheckurl%3Frequrl%3Dhttps%3A%2F%2Fgithub.com%2Fultralytics%2Fyolov3%2Fissues%2F232%26skey%3D%40crypt_96d23a7c_7a713cdc64109256773c39e67ce4a665%26deviceid%3De850832231813449%26pass_ticket%3DTgSQoHNgevOIg9%252B8R3aPNK%252F5sw6ZIUuR2A96p1sbiAGBktXTseCh8r9U9jZAQojj%26opcode%3D2%26scene%3D1%26username%3D%408bbd87b4deb686cd79c1471b85752510" title=https://github.com/ultralytics/yolov3/issues/232 rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/ultralytics/yolov3/issues/232
<i class="fa fa-external-link-alt"></i>
</a>在讨论中，通过这种简单的改进，推理速度得到了37%的提升，可以说效果很明显。</p><p>但是有的同学可能会有<strong>大大的问号？？<strong>如何进行计算的呢？大白按照Yolov5中的思路详细的讲解一下，在</strong>datasets.py的letterbox函数中</strong>也有详细的代码。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-13-39-077.png alt></p><p>此外，需要注意的是：</p><ul><li><p>这里大白填充的是黑色，即**（0，0，0）<strong>，而Yolov5中填充的是灰色，即</strong>（114,114,114）**，都是一样的效果。</p></li><li><p><strong>训练时没有采用缩减黑边的方式，还是采用传统填充的方式</strong>，即缩放到416*416大小。只是在测试，使用<strong>模型推理时，才采用缩减黑边的方式，提高目标检测，推理的速度</strong>。</p></li><li><p>为什么np.mod函数的后面用32？因为Yolov5的网络经过5次下采样，而2的5次方，等于32。所以至少要去掉32的倍数，再进行取余。</p></li></ul><h3 id=backbone-1>Backbone</h3><h4 id=focus结构>Focus结构</h4><p>具体操作为把一张feature map每隔一个像素拿到一个值，类似于邻近下采样，这样我们就拿到了4张feature map</p><p>在减少特征信息损失的情况下，<strong>减少特征图尺寸的大小提高计算力</strong>。通道多少对计算量影响不大</p><p>切片顺序不同 focus列优先 passthrough行优先</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-15-15-904.png alt></p><p>Focus结构，在Yolov3&Yolov4中并没有这个结构，其中比较关键是切片操作。比如右图的切片示意图，4*4*3的图像切片后变成2*2*12的特征图。以Yolov5s的结构为例，原始608*608*3的图像输入Focus结构，采用切片操作，先变成304*304*12的特征图，再经过一次32个卷积核的卷积操作，最终变成304*304*32的特征图。</p><p><strong>需要注意的是</strong>：Yolov5s的Focus结构最后使用了32个卷积核，而其他三种结构，使用的数量有所增加，先注意下，后面会讲解到四种结构的不同点。</p><h4 id=csp结构>CSP结构</h4><p>Yolov4网络结构中，借鉴了CSPNet的设计思路，在主干网络中设计了CSP结构。Yolov5与Yolov4不同点在于，Yolov4中只有主干网络使用了CSP结构。</p><p>而Yolov5中设计了两种CSP结构，以<strong>Yolov5s网络</strong>为例，<strong>CSP1_X结构</strong>应用于<strong>Backbone主干网络</strong>，另一种<strong>CSP2_X</strong>结构则应用于<strong>Neck</strong>中。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-11-24/2022-11-24_19-38-14-562.png alt></p><h3 id=neck-1>Neck</h3><h4 id=fpnpan-1>FPN+PAN</h4><p>Yolov5现在的Neck和Yolov4中一样，<strong>都采用FPN+PAN的结构</strong>，但在Yolov5刚出来时，只使用了FPN结构，后面才增加了PAN结构，此外网络中其他部分也进行了调整。</p><p>但如上面CSPNet结构中讲到，Yolov5和Yolov4的不同点在于，Yolov4的Neck结构中，采用的都是普通的卷积操作。而Yolov5的Neck结构中，采用借鉴CSPnet设计的CSP2结构，加强网络特征融合的能力。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-19-28-972.png alt></p><h3 id=prediction-1>Prediction</h3><h5 id=ciou_lossbounding-box损失函数>CIOU_Loss—Bounding box损失函数</h5><p>Yolov5中采用其中的CIOU_Loss做Bounding box的损失函数。</p><h5 id=diou_nms-1>DIOU_nms</h5><p>在目标检测的后处理过程中，针对很多目标框的筛选，通常需要nms操作。</p><p>因为CIOU_Loss中包含影响因子v，涉及groudtruth的信息，而测试推理时，是没有groundtruth的。所以Yolov4在DIOU_Loss的基础上采用DIOU_nms的方式，而Yolov5中采用加权nms的方式。</p><p>可以看出，采用DIOU_nms，下方中间箭头的黄色部分，原本被遮挡的摩托车也可以检出。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-31-35-919.png alt></p><h2 id=yolov5四种网络结构的不同点>Yolov5四种网络结构的不同点</h2><p>Yolov5代码中的四种网络，和之前的Yolov3，Yolov4中的<strong>cfg文件</strong>不同，都是以<strong>yaml</strong>的形式来呈现。</p><p>而且四个文件的内容基本上都是一样的，只有最上方的<strong>depth_multiple</strong>和<strong>width_multiple</strong>两个参数不同，很多同学看的<strong>一脸懵逼</strong>，不知道只通过两个参数是如何控制四种结构的？</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-33-11-309.png alt></p><h3 id=yolov5四种网络的深度>Yolov5四种网络的深度</h3><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_11-36-19-982.png alt></p><p>在上图中，大白画了两种CSP结构，CSP1和CSP2，其中CSP1结构主要应用于Backbone中，CSP2结构主要应用于Neck中。</p><p><strong>需要注意的是，四种网络结构中每个CSP结构的深度都是不同的。</strong></p><ul><li><p>以yolov5s为例，第一个CSP1中，使用了1个残差组件，因此是<strong>CSP1_1</strong>。而在Yolov5m中，则增加了网络的深度，在第一个CSP1中，使用了2个残差组件，因此是<strong>CSP1_2</strong>。而Yolov5l中，同样的位置，则使用了<strong>3个残差组件</strong>，Yolov5x中，使用了<strong>4个残差组件</strong>。其余的第二个CSP1和第三个CSP1也是同样的原理。</p></li><li><p>在第二种CSP2结构中也是同样的方式，以第一个CSP2结构为例，Yolov5s组件中使用了2×X=2×1=2个卷积，因为Ｘ=1，所以使用了1组卷积，因此是<strong>CSP2_1</strong>。而Yolov5m中使用了2<strong>组</strong>，Yolov5l中使用了3<strong>组</strong>，Yolov5x中使用了4**组。**其他的四个CSP2结构，也是同理。</p></li></ul><p>Yolov5中，网络的不断加深，也在不断<strong>增加网络特征提取</strong>和<strong>特征融合</strong>的能力。</p><p>控制四种网络结构的核心代码是<strong>yolo.py</strong>中下面的代码，存在两个变量，<strong>n和gd</strong>。</p><p>我们再将<strong>n和gd</strong>带入计算，看每种网络的变化结果。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_14-41-09-595.png alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_15-24-36-818.png alt></p><h3 id=yolov5四种网络的宽度>Yolov5四种网络的宽度</h3><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_14-41-42-893.png alt>如上图表格中所示，四种yolov5结构在不同阶段的卷积核的数量都是不一样的，因此也直接影响卷积后特征图的第三维度，即<strong>厚度</strong>，大白这里表示为网络的<strong>宽度</strong>。</p><ul><li><p>以Yolov5s结构为例，第一个Focus结构中，最后卷积操作时，卷积核的数量是32个，因此经过<strong>Focus结构</strong>，特征图的大小变成<strong>304*304*32</strong>。而yolov5m的<strong>Focus结构</strong>中的卷积操作使用了48个卷积核，因此<strong>Focus结构</strong>后的特征图变成<strong>304*304*48</strong>。yolov5l，yolov5x也是同样的原理。</p></li><li><p>第二个卷积操作时，yolov5s使用了64个卷积核，因此得到的特征图是<strong>152*152*64</strong>。而yolov5m使用96个特征图，因此得到的特征图是<strong>152*152*96</strong>。yolov5l，yolov5x也是同理。</p></li><li><p>后面三个卷积下采样操作也是同样的原理，这样大白不过多讲解。</p></li></ul><p>四种不同结构的卷积核的数量不同，这也直接影响网络中，比如<strong>CSP1，CSP2等结构</strong>，以及各个普通卷积，卷积操作时的卷积核数量也同步在调整，影响整体网络的计算量。当然卷积核的数量越多，特征图的厚度，即<strong>宽度越宽</strong>，网络提取特征的<strong>学习能力也越强</strong>。</p><p>在yolov5的代码中，控制宽度的核心代码是<strong>yolo.py</strong>文件里面的这一行：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_14-44-36-512.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-24/2022-10-24_15-25-18-348.png alt></p><h2 id=损失函数-3>损失函数</h2><h1 id=code>code</h1><p>和YOLOv4对比，其实YOLOv5在Backbone部分没太大变化。但是YOLOv5在v6.0版本后相比之前版本有一个很小的改动，把网络的第一层（原来是Focus模块）换成了一个6x6大小的卷积层。两者在理论上其实等价的，但是对于现有的一些GPU设备（以及相应的优化算法）使用6x6大小的卷积层比使用Focus模块更加高效。详情可以参考这个issue #4825。下图是原来的Focus模块(和之前Swin Transformer中的Patch Merging类似)，<strong>将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map，然后在接上一个3x3大小的卷积层。这和直接使用一个6x6大小的卷积层等效。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/8I3kFMYjO2RhL6e.png alt=quicker_1d6266e5-514d-4ce5-b18b-609bcb1bda92.png></p><p>在Neck部分的变化还是相对较大的，首先是将SPP换成成了SPPF（Glenn Jocher自己设计的），这个改动我个人觉得还是很有意思的，<strong>两者的作用是一样的，但后者效率更高</strong>。<code>SPPF</code>比<code>SPP</code>计算速度快了不止两倍。SPP结构如下图所示，是将输入并行通过多个不同大小的MaxPool，然后做进一步融合，能在一定程度上解决目标多尺度问题。</p><p><code>SPPF</code>结构是将输入串行通过多个<code>5x5</code>大小的<code>MaxPool</code>层，这里需要注意的是<strong>串行两个<code>5x5</code>大小的<code>MaxPool</code>层是和一个<code>9x9</code>大小的<code>MaxPool</code>层计算结果是一样的，串行三个<code>5x5</code>大小的<code>MaxPool</code>层是和一个<code>13x13</code>大小的<code>MaxPool</code>层计算结果是一样的。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/ugM3fX7L6myd9ZN.png alt=quicker_20886403-6055-4761-a782-afdf76d1476b.png></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/SiqLO4nXUDtrak9.png alt=quicker_99ea837a-ba94-4c1c-86e1-39582eab5c6b.png></p><p>在<strong>Neck</strong>部分另外一个不同点就是<code>New CSP-PAN</code>了，在YOLOv4中，<strong>Neck</strong>的<code>PAN</code>结构是没有引入<code>CSP</code>结构的，但在YOLOv5中作者在<code>PAN</code>结构中加入了<code>CSP</code>。每个<code>C3</code>模块里都含有<code>CSP</code>结构。在<strong>Head</strong>部分，YOLOv3, v4, v5都是一样的</p><h2 id=数据增强>数据增强</h2><p>这里简单罗列部分方法：</p><ul><li><strong>Mosaic</strong>，将四张图片拼成一张图片，讲过很多次了</li><li><strong>Random affine(Rotation, Scale, Translation and Shear)</strong>，随机进行仿射变换，但根据配置文件里的超参数发现只使用了<code>Scale</code>和<code>Translation</code>即缩放和平移。</li><li><strong>MixUp</strong>，就是将两张图片按照一定的透明度融合在一起，具体有没有用不太清楚，毕竟没有论文，也没有消融实验。代码中只有较大的模型才使用到了<code>MixUp</code>，而且每次只有10%的概率会使用到。</li><li><strong>Albumentations</strong>，主要是做些滤波、直方图均衡化以及改变图片质量等等，我看代码里写的只有安装了<code>albumentations</code>包才会启用，但在项目的<code>requirements.txt</code>文件中<code>albumentations</code>包是被注释掉了的，所以默认不启用。</li><li><strong>Augment HSV(Hue, Saturation, Value)</strong>，随机调整色度，饱和度以及明度。</li><li><strong>Random horizontal flip</strong>，随机水平翻转</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/05/09/UhNwK1bSXdei8RA.png alt=quicker_696f7eaa-9fc9-4263-ae6a-59a9b40f18c9.png></p><h1 id=x>X</h1><h2 id=网络结构-5>网络结构</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_16-57-39-950.png alt></p><p>**输入端：**Strong augmentation数据增强</p><p>**BackBone主干网络：**主干网络没有什么变化，还是Darknet53。</p><p>**Neck：**没有什么变化，Yolov3 baseline的Neck层还是FPN结构。</p><p>**Prediction：**Decoupled Head、End-to-End YOLO、Anchor-free、Multi positives。</p><h2 id=改进-3>改进</h2><h3 id=输入端-2>输入端</h3><p>在网络的输入端，Yolox主要采用了<strong>Mosaic、Mixup两种数据增强方法。</strong></p><p>而采用了这两种数据增强，直接将Yolov3 baseline，提升了2.4个百分点。</p><h4 id=mosaic数据增强-2>Mosaic数据增强</h4><p>Mosaic增强的方式，是U版YOLOv3引入的一种非常有效的增强策略。而且在Yolov4、Yolov5算法中，也得到了广泛的应用。通过<strong>随机缩放</strong>、<strong>随机裁剪</strong>、<strong>随机排布</strong>的方式进行<strong>拼接</strong>，对于<strong>小目标</strong>的检测效果提升，还是很不错的。</p><h4 id=mixup数据增强>MixUp数据增强</h4><p><strong>调整透明度两张图像叠加在一起。</strong></p><p>主要来源于2017年，顶会ICLR的一篇论文《mixup: Beyond Empirical Risk Minimization》。当时主要应用在图像分类任务中，可以在几乎无额外计算开销的情况下，稳定提升1个百分点的分类精度。而在Yolox中，则也应用到目标检测中，代码在yolox/datasets/mosaicdetection.py这个文件中。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-01-42-515.png alt></p><p>其实方式很简单，比如我们在做人脸检测的任务。先读取一张图片，图像两侧填充，缩放到640*640大小，即Image_1，人脸检测框为红色框。再随机选取一张图片，图像上下填充，也缩放到640*640大小，即Image_2，人脸检测框为蓝色框。然后设置一个融合系数，比如上图中，设置为0.5，将Image_1和Image_2，加权融合，最终得到右面的Image。从右图可以看出，人脸的红色框和蓝色框是叠加存在的。</p><p>我们知道，在Mosaic和Mixup的基础上，Yolov3 baseline增加了2.4个百分点。不过有两点需要注意：</p><p>（1）在训练的<strong>最后15个epoch，这两个数据增强会被关闭掉</strong>。而在此之前，Mosaic和Mixup数据增强，都是打开的，这个细节需要注意。</p><p>（2）由于采取了更强的数据增强方式，作者在研究中发现，ImageNet预训练将毫无意义，因此，<strong>所有的模型，均是从头开始训练的</strong>。</p><h3 id=backbone-2>Backbone</h3><p>Yolox-Darknet53的Backbone主干网络，和原本的Yolov3 baseline的主干网络都是一样的。</p><h3 id=neck-2>Neck</h3><p>在Neck结构中，Yolox-Darknet53和Yolov3 baseline的Neck结构，也是一样的，都是采用<strong>FPN的结构</strong>进行融合。</p><p>而在Yolov4、Yolov5、甚至后面讲到的Yolox-s、l等版本中，都是采用<strong>FPN+PAN的形式</strong>，这里需要注意。</p><h3 id=prediction-2>Prediction</h3><p>在输出层中，主要从四个方面进行讲解：<strong>Decoupled Head</strong>、<strong>Anchor Free</strong>、<strong>标签分配、Loss计算。</strong></p><h4 id=decoupled-head>Decoupled Head</h4><p>在很多一阶段网络中都有类似应用，比如<strong>RetinaNet、FCOS等</strong>。</p><p>而在Yolox中，作者增加了三个Decoupled Head，俗称“解耦头”。大白这里从两个方面对Decoupled Head进行讲解：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-06-20-655.png alt></p><p>从上图右面的Prediction中，我们可以看到，有三个Decoupled Head分支。</p><p><strong>① 为什么使用Decoupled Head？</strong></p><p>在了解原理前，我们先了解下改进的原因。为什么将原本的<strong>Yolo head</strong>，修改为<strong>Decoupled Head</strong>呢？</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-08-07-827.png alt></p><p>作者想继续改进，比如输出端改进为End-to-end的方式（即无NMS的形式）。在实验中还发现，不单单是精度上的提高。替换为Decoupled Head后，网络的收敛速度也加快了。</p><p>**但是需要注意的是：将检测头解耦，会增加运算的复杂度。**因此作者经过速度和性能上的权衡，最终使用 1个1x1 的卷积先进行降维，并在后面两个分支里，各使用了 2个3x3 卷积，最终调整到仅仅增加一点点的网络参数。而且这里解耦后，还有一个更深层次的重要性：<strong>Yolox的网络架构，可以和很多算法任务，进行一体化结合。</strong></p><p>比如：</p><p>（1）YOLOX + Yolact/CondInst/SOLO ，<strong>实现端侧的实例分割。</strong></p><p>（2）YOLOX + 34 层输出，实现端侧人体的 <strong>17 个关键点检测。</strong></p><p><strong>Decoupled Head的细节？</strong></p><p>我们将Yolox-Darknet53中，Decoupled Head①提取出来，经过前面的Neck层，这里Decouple Head①输入的长宽为20*20。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-14-04-120.png alt></p><p>从图上可以看出，Concat前总共有三个分支：</p><p>（1）cls_output：主要对目标框的类别，预测分数。因为COCO数据集总共有80个类别，且主要是N个二分类判断，因此经过Sigmoid激活函数处理后，变为20<em>20</em>80大小。</p><p>（2）obj_output：主要判断目标框是前景还是背景，因此经过Sigmoid处理好，变为20<em>20</em>1大小。</p><p>（3）reg_output：主要对目标框的坐标信息（x，y，w，h）进行预测，因此大小为20<em>20</em>4。</p><p>最后三个output，经过Concat融合到一起，得到20*20*85的特征信息。</p><p>当然，这只是Decoupled Head①的信息，再对Decoupled Head②和③进行处理。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-16-07-450.png alt></p><p>Decoupled Head②输出特征信息，并进行Concate，得到40*40*85特征信息。</p><p>Decoupled Head③输出特征信息，并进行Concate，得到80*80*85特征信息。</p><p>再对①②③三个信息，进行Reshape操作，并进行总体的Concat，得到8400*85的预测信息。</p><p>并经过一次Transpose，变为85*8400大小的二维向量信息。</p><p>这里的8400，指的是预测框的数量，而85是每个预测框的信息（reg，obj，cls）。</p><p>有了预测框的信息，下面我们再了解，如何将这些预测框和标注的框，即groundtruth进行关联，从而计算Loss函数，更新网络参数呢？</p><h4 id=anchor-free>Anchor-free</h4><p>在Yolov3、Yolov4、Yolov5中，通常都是采用Anchor Based的方式，来提取目标框，进而和标注的groundtruth进行比对，判断两者的差距。</p><p><strong>① Anchor Based方式</strong></p><p>比如输入图像，经过Backbone、Neck层，最终将特征信息，传送到输出的Feature Map中。</p><p>这时，就要设置一些Anchor规则，将预测框和标注框进行关联。从而在训练中，计算两者的差距，即损失函数，再更新网络参数。比如在下图的，最后的三个Feature Map上，基于每个单元格，都有三个不同尺寸大小的锚框。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-19-23-910.png alt></p><p>当输入为416*416时，网络最后的三个特征图大小为13*13，26*26，52*52。我们可以看到，黄色框为小狗的Groundtruth，即标注框。而蓝色的框，为小狗中心点所在的单元格，所对应的锚框，每个单元格都有3个蓝框。当采用COCO数据集，即有80个类别时。基于每个锚框，都有x、y、w、h、obj（前景背景）、class（80个类别），共85个参数。</p><p>因此会产生3*(13*13+26*26+52*52）*85=904995个预测结果。</p><p>如果将输入从416*416，变为640*640，最后的三个特征图大小为20*20,40*40,80*80。</p><p>则会产生3*（20*20+40*40+80*80）*85=2142000个预测结果。</p><p><strong>② Anchor Free方式</strong></p><p>而Yolox-Darknet53中，则采用Anchor Free的方式。我们从两个方面，来对Anchor Free进行了解。</p><p>a.输出的参数量</p><p>当输入为640*640时，最终输出得到的特征向量是85*8400。通过计算，8400*85=714000个预测结果，<strong>比基于Anchor Based的方式，少了2/3的参数量</strong>。</p><p>b.Anchor框信息</p><p>在前面Anchor Based中，我们知道，每个Feature map的单元格，都有3个大小不一的锚框。</p><p>那么Yolox-Darknet53就没有吗？其实并不然，这里只是巧<strong>妙的，将前面Backbone中，下采样的大小信息引入进来。</strong></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-41-54-908.png alt></p><p>比如上图中，最上面的分支，下采样了5次，2的5次方为32。并且Decoupled Head①的输出，为20*20*85大小。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-25/2022-10-25_17-46-38-244.png alt></p><p>最后8400个预测框中，其中有400个框，所对应锚框的大小，为32*32。同样的原理，中间的分支，最后有1600个预测框，所对应锚框的大小，为16*16。最下面的分支，最后有6400个预测框，所对应锚框的大小，为8*8。当有了8400个预测框的信息，每张图片也有标注的目标框的信息。</p><p>这时的锚框，就相当于桥梁。这时需要做的，就是将8400个锚框，和图片上所有的目标框进行关联，挑选出正样本锚框。而相应的，正样本锚框所对应的位置，就可以将正样本预测框，挑选出来。这里采用的关联方式，就是标签分配。</p><h4 id=标签分配>标签分配</h4><p>当有了8400个Anchor锚框后，这里的每一个锚框，都对应85*8400特征向量中的预测框信息。</p><p>这些预测框只有<strong>少部分是正样本，绝大多数是负样本。</strong></p><p><strong>那么到底哪些是正样本呢？</strong></p><p>这里需要利用锚框和实际目标框的关系，挑选出<strong>一部分适合的正样本锚框。</strong></p><p>比如第3、10、15个锚框是正样本锚框，则对应到网络输出的8400个预测框中，第3、10、15个预测框，就是相应的**正样本预测框。**训练过程中，在锚框的基础上，不断的预测，然后不断的迭代，从而更新网络参数，让网络预测的越来越准。</p><p>那么在Yolox中，是如何挑选正样本锚框的呢？</p><p><strong>① 初步筛选</strong></p><p>初步筛选的方式主要有两种：<strong>根据中心点来判断</strong>、<strong>根据目标框来判断</strong>；</p><p>这部分的代码，在models/yolo_head.py的get_in_boxes_info函数中。</p><p><strong>a. 根据中心点来判断：</strong></p><p><strong>规则：寻找anchor_box中心点，落在groundtruth_boxes矩形范围的所有anchors。</strong></p><p>比如在get_in_boxes_info的代码中，通过groundtruth的[x_center,y_center，w，h]，计算出每张图片的每个groundtruth的左上角、右下角坐标。</p><p><a href=https://zhuanlan.zhihu.com/p/397993315 title="深入浅出Yolo系列之Yolox核心基础完整讲解 - 知乎 (zhihu.com)" rel="noopener external nofollow noreferrer" target=_blank class=exturl>深入浅出Yolo系列之Yolox核心基础完整讲解 - 知乎 (zhihu.com)
<i class="fa fa-external-link-alt"></i></a></p></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
cv-yolo</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/cv-yolo/ title=cv-yolo>/post/cv-yolo/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/cv-vision-transformervit/ rel=next title="cv-Vision Transformer(ViT)"><i class="fa fa-chevron-left"></i> cv-Vision Transformer(ViT)</a></div><div class="post-nav-prev post-nav-item"><a href=/post/cv-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/ rel=prev title=cv-残差网络(ResNet)>cv-残差网络(ResNet)
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>