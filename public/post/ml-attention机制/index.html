<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="ml-Attention机制"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="ml-Attention机制"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/ml-attention%E6%9C%BA%E5%88%B6/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"ml-attention%E6%9C%BA%E5%88%B6","permalink":"/post/ml-attention%E6%9C%BA%E5%88%B6/","title":"ml-Attention机制","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>ml-Attention机制 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>74</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#通用形式>通用形式</a></li><li><a href=#分类>分类</a><ul><li><a href=#关注范围>关注范围</a><ul><li><a href=#全局注意力>全局注意力</a></li><li><a href=#局部注意力>局部注意力</a></li><li><a href=#硬注意力>硬注意力</a></li><li><a href=#稀疏注意力>稀疏注意力</a></li><li><a href=#结构注意力>结构注意力</a></li></ul></li><li><a href=#组合方式>组合方式</a><ul><li><a href=#层级注意力>层级注意力</a></li><li><a href=#双向注意力>双向注意力</a></li><li><a href=#多头注意力>多头注意力</a></li></ul></li></ul></li><li><a href=#自注意力>自注意力</a><ul><li><a href=#自注意力机制>自注意力机制</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>74</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=273678></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=585></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-05T20:43:26+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/ml-attention%E6%9C%BA%E5%88%B6/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="ml-Attention机制"><meta itemprop=description content="https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py 用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或"></span><header class=post-header><h1 class=post-title itemprop="name headline">ml-Attention机制
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/ml-Attention%e6%9c%ba%e5%88%b6.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/ml itemprop=url rel=index><span itemprop=name>ml</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>2383</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>5分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/ml-attention%E6%9C%BA%E5%88%B6/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><p><a href=https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md title=https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md
<i class="fa fa-external-link-alt"></i></a>
<a href=https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py title=https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py rel="noopener external nofollow noreferrer" target=_blank class=exturl>https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py
<i class="fa fa-external-link-alt"></i></a></p><p>用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p><ol><li>时间片 <em>t</em> 的计算依赖 <em>t-1</em> 时刻的计算结果，这样<code>限制了模型的并行能力；</code></li><li>顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，<code>但是对于特别长期的依赖现象,LSTM依旧无能为力。</code></li></ol><h1 id=attention>Attention</h1><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-a4e0fedeb414aca9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=通用形式>通用形式</h2><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ede3ccdaa82f4810.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>如果将下游任务抽象成查询（query），就可以归纳出注意力机制的通用形式，即将源文本看成是键-值&lt;Key,Value>对序列。给定Target中的某个元素Query（解码器隐层向量 $s_{t-1}$ ）用$K=（k_1，…，k_N）$和$V=（v_1，…，v_N）$分别表示键序列和值序列，用 $Q=（q_1，…，q_M）$表示查询序列，那么针对查询$q_t$的注意力可以被描述为键-值对序列在该查询上的映射。如图2所示，计算过程可分为三步：</p><ul><li>计算查询 $q_t$和每个键 $k_i$的注意力得分 $e_{ti}$，常用的计算方法包括点积、缩放点积、拼接以及相加等，如公式（1）所示；</li><li>使用 Softmax 等函数对注意力得分做归一化处理，得到每个键的权重$ α_{ti}$，如公式（2）所示；</li><li>将权重$ α_{ti}$和其对应的值$ v_i$加权求和作为注意力输出，如公式（3）所示模型输出的注意力是源文本序列基于查询 $q_t$的表示，不同的查询会给源文本序列带来不同的权重分布。
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-abd39baf986400fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-d792b865a3618a94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=分类>分类</h2><h3 id=关注范围>关注范围</h3><table><thead><tr><th style=text-align:center>注意力</th><th style=text-align:center>关注范围</th></tr></thead><tbody><tr><td style=text-align:center>全局注意力</td><td style=text-align:center>全部元素</td></tr><tr><td style=text-align:center>局部注意力</td><td style=text-align:center>以对齐位置为中心的窗口</td></tr><tr><td style=text-align:center>硬注意力</td><td style=text-align:center>一个元素</td></tr><tr><td style=text-align:center>稀疏注意力</td><td style=text-align:center>稀疏分布的部分元素</td></tr><tr><td style=text-align:center>结构注意力</td><td style=text-align:center>结构上相关的一系列元素</td></tr></tbody></table><h4 id=全局注意力>全局注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-f793257ac4ad0311.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=局部注意力>局部注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-57ffee086fdafc02.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=硬注意力>硬注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e02f1c94e9657b3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=稀疏注意力>稀疏注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-b12de5f47f732559.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=结构注意力>结构注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-2bbd796c8f791d15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h3 id=组合方式>组合方式</h3><h4 id=层级注意力>层级注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e6dbd47038d548d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-e7e9381a65fcbd6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=双向注意力>双向注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-9251c37e5ec1c6e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-faa1bea4b8d72ff1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-79c720997d172703.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h4 id=多头注意力>多头注意力</h4><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-545a2735c5bd6fdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ea638c381cf29b8c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><h2 id=自注意力>自注意力</h2><h3 id=自注意力机制>自注意力机制</h3><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-978848a3ce30e4ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-0f88d6ccaecf7d1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-050f12f047a4174a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-cd967218e97f2fe4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-4a6bdd3849b846e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><p>在self-attention中，每个单词有3个不同的向量，它们分别是Query向量 $Q$，Key向量$K$和Value向量$V$。它们是通过3个不同的权值矩阵由嵌入向量$X$乘以三个不同的权值矩阵$W_Q,W_K,W_V$  得到，其中三个矩阵的尺寸也是相同的。</p><p><code>Attention的计算方法，整个过程可以分成7步：</code></p><ol><li>将输入单词转化成嵌入向量；</li><li>根据嵌入向量得到 三个向量$Q,K,V$ ；</li><li>为每个向量计算一个attention score：$Q*K$；</li><li>为了梯度的稳定，Transformer使用了score归一化，即除以 $\sqrt{d_k}$  ；</li><li>对score施以softmax激活函数；</li><li>softmax点乘Value值$V$ ，得到加权的每个输入向量的评分 weighted values $V$ ；</li><li>相加之后得到最终的输出结果$Z=\sum(v)$。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>class AttentionLayer(Layer):
</span></span><span style=display:flex><span> &#34;&#34;&#34;
</span></span><span style=display:flex><span>        # Input shape  3D tensor with shape: `(samples, steps, features)`.
</span></span><span style=display:flex><span>        # Output shape 3D tensor with shape: `(samples, steps, output_dim)`.
</span></span><span style=display:flex><span>&#34;&#34;&#34;
</span></span><span style=display:flex><span>    def __init__(self, output_dim, `kwargs):
</span></span><span style=display:flex><span>        self.output_dim = output_dim
</span></span><span style=display:flex><span>        super(AttentionLayer, self).__init__(`kwargs)
</span></span><span style=display:flex><span>    def build(self, input_shape):
</span></span><span style=display:flex><span>        # 为该层创建一个可训练的权重
</span></span><span style=display:flex><span>        #inputs.shape = (batch_size, time_steps, seq_len)
</span></span><span style=display:flex><span>        self.kernel = self.add_weight(name=&#39;kernel&#39;, shape=(3, input_shape[2], self.output_dim), initializer=&#39;uniform&#39;, trainable=True)
</span></span><span style=display:flex><span>        super(AttentionLayer, self).build(input_shape)  # 一定要在最后调用它
</span></span><span style=display:flex><span>    def call(self, x):
</span></span><span style=display:flex><span>        WQ = K.dot(x, self.kernel[0])  # (None, input_shape[1]，input_shape[2])   (input_shape[2], output_dim)  (None, input_shape[1]，output_dim)
</span></span><span style=display:flex><span>        WK = K.dot(x, self.kernel[1])  # (None, input_shape[1]，output_dim)
</span></span><span style=display:flex><span>        WV = K.dot(x, self.kernel[2])  # (None, input_shape[1]，output_dim)
</span></span><span style=display:flex><span>        score = K.batch_dot(WQ, K.permute_dimensions(WK, [0, 2, 1])) / (input_shape[0]`0.5)  #  (None, input_shape[1], input_shape[1])
</span></span><span style=display:flex><span>        alpha = K.softmax(score)  
</span></span><span style=display:flex><span>        V = K.batch_dot(alpha, WV)  # (None, input_shape[1], input_shape[1])  (None, input_shape[1]，output_dim) (None, input_shape[1]，output_dim)
</span></span><span style=display:flex><span>        return V
</span></span></code></pre></div><h1 id=总结>总结</h1><p>优点：（1）虽然Transformer最终也没有逃脱传统学习的套路，Transformer也只是一个全连接（或者是一维卷积）加Attention的结合体。但是其设计已经足够有创新，因为其抛弃了在NLP中最根本的RNN或者CNN并且取得了非常不错的效果，算法的设计非常精彩，值得每个深度学习的相关人员仔细研究和品位。（2）Transformer的设计最大的带来性能提升的<code>关键是将任意两个单词的距离是1，</code>这对解决NLP中棘手的长期依赖问题是非常有效的。（3）Transformer不仅仅可以应用在NLP的机器翻译领域，甚至可以不局限于NLP领域，是非常有科研潜力的一个方向。（4）算法的并行性非常好，符合目前的硬件（主要指GPU）环境。</p><p>缺点：（1）粗暴的抛弃RNN和CNN虽然非常炫技，但是它也使模型<code>丧失了捕捉局部特征的能力</code>，RNN + CNN + Transformer的结合可能会带来更好的效果。（2）<code>Transformer失去的位置信息其实在NLP中非常重要</code>，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。+</p><p>给定一个在每个时间步产生隐藏状态$h_t$的模型，基于注意的模型计算一个“上下文”向量$c_t$作为状态序列h的加权平均值</p><p>$c_{t}=\sum_{j=1}^{T} \alpha_{t j} h_{j}$</p><p>式中，$T$是输入序列中的时间步总数，$α_{tj}$是针对每个状态$h_j$在每个时间步$t$处计算的权重。然后使用这些上下文向量来计算新的状态序列$s$，其中$s_t$依赖于$s_{t−1}$、$c_t$和$t−1$处的模型输出。然后通过以下公式计算权重$α_{tj}$：
$e_{t j}=a\left(s_{t-1}, h_{j}\right), \alpha_{t j}=\frac{\exp \left(e_{t j}\right)}{\sum_{k=1}^{T} \exp \left(e_{t k}\right)}$</p><p>其中，$a$是一个学习函数，可以认为是给定$h_j$值和先前状态$s_{t−1}$计算$h_j$的标量重要性值。该公式允许新的状态序列$s$更直接地访问整个状态序列$h$。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-f1ef9cc0c7443b7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-fallback data-lang=fallback><span style=display:flex><span>import numpy
</span></span><span style=display:flex><span>import keras
</span></span><span style=display:flex><span>import tensorflow as tf
</span></span><span style=display:flex><span>from keras import backend as K
</span></span><span style=display:flex><span>from keras import activations
</span></span><span style=display:flex><span>from keras.engine.topology import Layer
</span></span><span style=display:flex><span>from keras.preprocessing import sequence
</span></span><span style=display:flex><span>from keras.models import Sequential
</span></span><span style=display:flex><span>from keras.models import Model
</span></span><span style=display:flex><span>from keras.layers import Input, Dense, Embedding, LSTM, Bidirectional
</span></span><span style=display:flex><span>K.clear_session()
</span></span><span style=display:flex><span>tf.compat.v1.disable_eager_execution()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>class AttentionLayer(Layer):
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    # Input shape  3D tensor with shape: `(samples, steps, hidden_size)`.
</span></span><span style=display:flex><span>    # Output shape 2D tensor with shape: `(samples, hidden_size)`.
</span></span><span style=display:flex><span>    &#34;&#34;&#34;
</span></span><span style=display:flex><span>    def __init__(self, attention_size=None, `kwargs):
</span></span><span style=display:flex><span>        self.attention_size = attention_size
</span></span><span style=display:flex><span>        super(AttentionLayer, self).__init__(`kwargs)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    def get_config(self):
</span></span><span style=display:flex><span>        config = super().get_config()
</span></span><span style=display:flex><span>        config[&#39;attention_size&#39;] = self.attention_size
</span></span><span style=display:flex><span>        return config
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    def build(self, input_shape):
</span></span><span style=display:flex><span>        assert len(input_shape) == 3
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        self.time_steps = input_shape[1]
</span></span><span style=display:flex><span>        hidden_size = input_shape[2]
</span></span><span style=display:flex><span>        if self.attention_size is None:
</span></span><span style=display:flex><span>            self.attention_size = hidden_size
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        self.W = self.add_weight(name=&#39;att_weight&#39;, shape=(hidden_size, self.attention_size), initializer=&#39;uniform&#39;, trainable=True)
</span></span><span style=display:flex><span>        self.b = self.add_weight(name=&#39;att_bias&#39;, shape=(self.attention_size,), initializer=&#39;uniform&#39;, trainable=True)
</span></span><span style=display:flex><span>        self.V = self.add_weight(name=&#39;att_var&#39;, shape=(self.attention_size,), initializer=&#39;uniform&#39;, trainable=True)
</span></span><span style=display:flex><span>        super(AttentionLayer, self).build(input_shape)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    def call(self, inputs):
</span></span><span style=display:flex><span>        self.V = K.reshape(self.V, (-1, 1))   # (attention_size，1)
</span></span><span style=display:flex><span>        score =   K.dot(K.tanh(K.dot(inputs, self.W) + self.b), self.V) #  (None, 30, hidden_size)   (hidden_size, attention_size)   (None, 30, attention_size)
</span></span><span style=display:flex><span>        alpha = K.softmax(score, axis=1)                                #   //       (None, 30, attention_size) (attention_size，1) (None, 30, 1)
</span></span><span style=display:flex><span>        outputs = K.sum(alpha * inputs, axis=1)   #   (None, 30, 1)   (None, 30, hidden_size)        (None, hidden_size)
</span></span><span style=display:flex><span>        return outputs
</span></span><span style=display:flex><span>    def compute_output_shape(self, input_shape):
</span></span><span style=display:flex><span>        return input_shape[0], input_shape[2]
</span></span></code></pre></div></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
ml-Attention机制</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/ml-attention%E6%9C%BA%E5%88%B6/ title=ml-Attention机制>/post/ml-attention%E6%9C%BA%E5%88%B6/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/ml-gpu%E7%89%88pytorch%E5%AE%89%E8%A3%85/ rel=next title=linux-显卡安装驱动><i class="fa fa-chevron-left"></i> linux-显卡安装驱动</a></div><div class="post-nav-prev post-nav-item"><a href=/post/linux-%E6%98%BE%E5%8D%A1%E5%AE%89%E8%A3%85%E9%A9%B1%E5%8A%A8/ rel=prev title=ml-gpu版pytorch安装>ml-gpu版pytorch安装
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>