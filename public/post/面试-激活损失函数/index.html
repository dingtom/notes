<!doctype html><html lang=zh-cn data-theme=dark><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=theme-color content="#222" media="(prefers-color-scheme: dark)"><meta name=generator content="Hugo 0.107.0"><link rel="shortcut icon" type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/x-icon href=/imgs/icons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=/imgs/icons/favicon_16x16_next.png><link rel=icon type=image/png sizes=32x32 href=/imgs/icons/favicon_32_32_next.png><link rel=apple-touch-icon sizes=180x180 href=/imgs/icons/apple_touch_icon_next.png><meta itemprop=name content="面试-激活、损失函数"><meta itemprop=description content="Love and Peace"><meta itemprop=datePublished zgotmplz><meta itemprop=dateModified zgotmplz><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=keywords content="Hugo,NexT,主题,简单,强大"><meta property="og:type" content="article"><meta property="og:title" content="面试-激活、损失函数"><meta property="og:description" content="Love and Peace"><meta property="og:image" content="/imgs/hugo_next_avatar.png"><meta property="og:image:width" content="312"><meta property="og:image:height" content="312"><meta property="og:image:type" content="image/jpeg/png/svg/jpg"><meta property="og:url" content="/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"><meta property="og:site_name" content="Tomding's Blog"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="Tomding"><meta property="article:published_time" content="2022-12-01 19:59:47 +0800 CST"><meta property="article:modified_time" content="2022-12-01 19:59:47 +0800 CST"><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/font-awesome/6.1.2/css/all.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/animate.css/3.1.1/animate.min.css><link type=text/css rel=stylesheet href=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.css><link rel=stylesheet href=/css/main.min.0ca97083b39f4eda7430d39b99194aa738d8f8db4090f00bc7e814013df699cb.css><style type=text/css>.post-footer,.flinks-list-footer hr:after{content:"~ 我可是有底线的哟 ~"}</style><script type=text/javascript>(function(){localDB={set:function(e,t,n){if(n===0)return;const s=new Date,o=n*864e5,i={value:t,expiry:s.getTime()+o};localStorage.setItem(e,JSON.stringify(i))},get:function(e){const t=localStorage.getItem(e);if(!t)return void 0;const n=JSON.parse(t),s=new Date;return s.getTime()>n.expiry?(localStorage.removeItem(e),void 0):n.value}},theme={active:function(){const e=localDB.get("theme");if(e==null)return;theme.toggle(e),window.matchMedia("(prefers-color-scheme: dark)").addListener(function(e){theme.toggle(e.matches?"dark":"light")})},toggle:function(e){document.documentElement.setAttribute("data-theme",e),localDB.set("theme",e,2);const t=document.querySelector("iframe.giscus-frame");if(t){const n={setConfig:{theme:e}};t.contentWindow.postMessage({giscus:n},"https://giscus.app")}}},theme.active()})(window)</script><script class=next-config data-name=page type=application/json>{"comments":false,"isHome":false,"isPage":true,"path":"%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0","permalink":"/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/","title":"面试-激活、损失函数","waline":{"js":[{"alias":"waline","alias_name":"@waline/client","file":"dist/pageview.js","name":"pageview","version":"2.13.0"},{"alias":"waline","alias_name":"@waline/client","file":"dist/comment.js","name":"comment","version":"2.13.0"}]}}</script><script type=text/javascript>document.addEventListener("DOMContentLoaded",()=>{var e=document.createElement("script");e.charset="UTF-8",e.src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js",e.async=!1,e.defer=!0,document.head.appendChild(e),e.onload=function(){NexT.utils.fmtBusuanzi()}})</script><title>面试-激活、损失函数 - Tomding's Blog</title><noscript><link rel=stylesheet href=/css/noscript.css></noscript></head><body itemscope itemtype=http://schema.org/WebPage class=use-motion><div class=headband></div><main class=main><header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle aria-label=切换导航栏 role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class=site-meta><a href=/ class=brand rel=start><i class=logo-line></i><h1 class=site-title>Tomding's Blog</h1><i class=logo-line></i></a><p class=site-subtitle itemprop=description>Just do it!</p></div><div class=site-nav-right><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul class="main-menu menu"><li class="menu-item menu-item-home"><a href=/ class=hvr-icon-pulse rel=section><i class="fa fa-home hvr-icon"></i>首页</a></li><li class="menu-item menu-item-about"><a href=/about.html class=hvr-icon-pulse rel=section><i class="fa fa-user hvr-icon"></i>关于</a></li><li class="menu-item menu-item-archives"><a href=/archives/ class=hvr-icon-pulse rel=section><i class="fa fa-archive hvr-icon"></i>归档
<span class=badge>73</span></a></li><li class="menu-item menu-item-search"><a role=button class="popup-trigger hvr-icon-pulse"><i class="fa fa-search fa-fw hvr-icon"></i>搜索</a></li></ul></nav><div class=search-pop-overlay><div class="popup search-popup"><div class=search-header><span class=search-icon><i class="fa fa-search"></i></span><div class=search-input-container><input autocomplete=off autocapitalize=off maxlength=80 placeholder=搜索... spellcheck=false type=search class=search-input></div><span class=popup-btn-close role=button><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class=search-result-icon><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle" role=button><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div><aside class=sidebar><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class=sidebar-nav><li class=sidebar-nav-toc>文章目录</li><li class=sidebar-nav-overview>站点概览</li></ul><div class=sidebar-panel-container><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><nav id=TableOfContents><ul><li><a href=#激活函数>激活函数</a><ul><li><a href=#sigmoid函数>sigmoid函数：</a></li><li><a href=#tanh函数>tanh函数</a></li><li><a href=#relu函数>ReLU函数</a></li></ul></li><li><a href=#损失函数>损失函数</a><ul><li><a href=#cross-entropy-loss>Cross Entropy Loss</a></li><li><a href=#label-smoothing>Label Smoothing</a></li><li><a href=#focal-loss>Focal Loss</a></li><li><a href=#lovász-softmax>Lovász-Softmax</a></li></ul></li></ul></nav></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author site-overview-item animated" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image alt=Tomding src=/imgs/img-lazy-loading.gif data-src=/imgs/hugo_next_avatar.png><p class=site-author-name itemprop=name>Tomding</p><div class=site-description itemprop=description>Love and Peace</div></div><div class="site-state-wrap site-overview-item animated"><nav class=site-state><div class="site-state-item site-state-posts"><a href=/archives/><span class=site-state-item-count>73</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>5</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>0</span>
<span class=site-state-item-name>标签</span></a></div></nav></div><div class="links-of-social site-overview-item animated"><span class=links-of-social-item><a href=https://github.com/ title="Github → https://github.com/" rel=noopener class=hvr-icon-pulse target=_blank><i class="fab fa-github fa-fw hvr-icon"></i>Github</a></span>
<span class=links-of-social-item><a href=https://www.zhihu.com/hot title="知乎 → https://www.zhihu.com/hot" rel=noopener class=hvr-icon-pulse target=_blank><i class="fa fa-book fa-fw hvr-icon"></i>知乎</a></span></div><div class="cc-license animated" itemprop=license><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh class=cc-opacity rel=noopener target=_blank title=共享知识><img src=/imgs/img-lazy-loading.gif data-src=/imgs/cc/big/by_nc_sa.svg alt=共享知识></a></div><div class="links-of-blogroll site-overview-item animated"><div class=links-of-blogroll-title><i class="fa fa-globe fa-fw"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://gitee.com/hugo-next/hugo-theme-next title=https://gitee.com/hugo-next/hugo-theme-next target=_blank>Hugo-NexT</a></li><li class=links-of-blogroll-item><a href=https://lisenhui.cn title=https://lisenhui.cn target=_blank>凡梦星尘空间站</a></li></ul></div></div></div></div><div id=siteinfo-card-widget class=sidebar-card-widget><div class=item-headline><i class="fas fa-chart-line"></i>
<span>网站资讯</span></div><div class=siteinfo><div class=siteinfo-item><div class=item-name><i class="fa-solid fa-calendar-check"></i>已运行：</div><div class=item-count id=runTimes data-publishdate=2022-12-01T19:59:47+08:00></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-user"></i>总访客数：</div><div class=item-count id=busuanzi_value_site_uv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fas fa fa-eye"></i>页面浏览：</div><div class=item-count id=busuanzi_value_site_pv><i class="fa fa-sync fa-spin"></i></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-font"></i>总字数：</div><div class=item-count id=wordsCount data-count=272457></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-mug-hot"></i>阅读约：</div><div class=item-count id=readTimes data-times=581></div></div><div class=siteinfo-item><div class=item-name><i class="fa fa-clock-rotate-left"></i>最后更新于：</div><div class=item-count id=last-push-date data-lastpushdate=2022-12-08T09:49:29+08:00></div></div></div></div></aside><div class=sidebar-dimmer></div></header><div class=tool-buttons><div id=toggle-theme class=button title=深浅模式切换><i class="fas fa-adjust"></i></div><div class=back-to-top role=button title=返回顶部><i class="fa fa-arrow-up"></i>
<span>0%</span></div></div><div class=reading-progress-bar></div><noscript><div class=noscript-warning>Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner post posts-expand"><div class=post-block><article itemscope itemtype=http://schema.org/Article class=post-content lang><link itemprop=mainEntityOfPage href=/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/><span hidden itemprop=author itemscope itemtype=http://schema.org/Person><meta itemprop=image content="/imgs/hugo_next_avatar.png"><meta itemprop=name content="Tomding"></span><span hidden itemprop=publisher itemscope itemtype=http://schema.org/Organization><meta itemprop=name content="Tomding"><meta itemprop=description content="Love and Peace"></span><span hidden itemprop=post itemscope itemtype=http://schema.org/CreativeWork><meta itemprop=name content="面试-激活、损失函数"><meta itemprop=description content="激活函数 如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网"></span><header class=post-header><h1 class=post-title itemprop="name headline">面试-激活、损失函数
<a href=https://github.com/user-name/repo-name/tree/branch-name/subdirectory-name/post/%e9%9d%a2%e8%af%95-%e6%bf%80%e6%b4%bb%e3%80%81%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0.md rel="noopener external nofollow noreferrer" target=_blank class="exturl post-edit-link" title=编辑><i class="fa fa-pen-nib"></i></a></h1><div class=post-meta-container><div class=post-meta-items><span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-calendar"></i></span>
<span class=post-meta-item-text>发表于：</span>
<time title="发表于：2022-12-01 19:59:47 +0800 CST" itemprop="dateCreated datePublished" datetime="2022-12-01 19:59:47 +0800 CST">2022-12-01</time></span>
<span class=post-meta-item><span class=post-meta-item-icon><i class="far fa-folder-open"></i></span>
<span class=post-meta-item-text>分类于：</span>
<span itemprop=about itemscope itemtype=http://schema.org/Thing><a href=/categories/%E9%9D%A2%E8%AF%95 itemprop=url rel=index><span itemprop=name>面试</span></a></span></span></div><div class=post-meta-items><span class=post-meta-item title=字数><span class=post-meta-item-icon><i class="far fa-file-word"></i></span>
<span class=post-meta-item-text>字数：</span><span>4032</span></span>
<span class=post-meta-item title=阅读><span class=post-meta-item-icon><i class="far fa-clock"></i></span>
<span class=post-meta-item-text>阅读：&ap;</span>
<span>9分钟</span></span>
<span class=post-meta-item title=浏览><span class=post-meta-item-icon><i class="far fa-eye"></i></span>
<span class=post-meta-item-text>浏览：</span>
<span id=busuanzi_value_page_pv class=waline-pageview-count data-path=/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/><i class="fa fa-sync fa-spin"></i></span></span></div></div></header><div class="post-body autonumber" itemprop=articleBody><h2 id=激活函数>激活函数</h2><blockquote><p>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）
激活函数是用来加入非线性因素的，解决线性模型所不能解决的问题；
在神经网络中，我们可以经常看到对于某一个隐藏层的节点，该节点的激活之计算一般分为两步：
（1） 输入该节点的值后先进行一个线性变换，计算出值
（2）再进行一个非线性变换，也就是经过一个非线性激活函数</p></blockquote><p>常用的激活函数包括：sigmoid函数、tanh函数、ReLU函数。</p><h3 id=sigmoid函数>sigmoid函数：</h3><p>当目标是解决一个二分类问题，可在输出层使用sigmoid函数进行二分类。</p><p>该函数数将取值为(−∞,+∞) 的数映射到(0,1)之间，其公式以及函数图如下所示：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-5bd89cfb15325e55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300 alt=image.png>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-ce4352e48c3d9899.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300 alt>
缺点</p><blockquote><p>1.<code>当值非常大或者非常小是sigmoid的导数会趋近为0</code>，则会导致梯度消失
2.<code>函数的输出不是以0位均值，不便于下一层的计算。</code>这是不可取的，因为这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：<code>???????????如x>0, f=wTx+b 那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果，使得收敛缓慢。</code> 当然了，如果按batch去训练，那么那个batch可能得到不同的信号，所以这个问题还是可以缓解一下的。因此，非0均值这个问题虽然会产生一些不好的影响，不过跟上面提到的梯度消失问题相比还是要好很多的。
3.<code>解析式中含有幂运算</code>，计算机求解时相对来讲比较耗时。对于规模比较大的深度网络，这会较大地增加训练时间</p></blockquote><h3 id=tanh函数>tanh函数</h3><p>tanh读作Hyperbolic Tangent，它解决了Sigmoid函数的不是zero-centered输出问题，然而，梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在。
该函数数将取值为(−∞,+∞) 的数映射到(-1,1)之间，其公式以及函数图如下所示：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-23dc6503cdc17b26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300 alt>
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-037a253b499d575d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/300 alt></p><h3 id=relu函数>ReLU函数</h3><p>ReLU函数又称为修正线性单元, 是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下：
<img src=/imgs/img-lazy-loading.gif data-src=https://upload-images.jianshu.io/upload_images/18339009-f27b342366a20935.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240 alt=image.png></p><blockquote><p>优点： 1. 当输入大于0时，不存在梯度消失的问题2. 由于ReLU函数只有线性关系，所以计算速度要快很多
缺点：1.当输入小于0时，梯度为0，会产生梯度消失问题。</p></blockquote><p>ReLU函数其实就是一个取最大值函数，注意这并不是全区间可导的，但是我们可以取sub-gradient，如上图所示。ReLU虽然简单，但却是近几年的重要成果，有以下几大优点：
1） 解决了gradient vanishing问题 (在正区间)
2）计算速度非常快，只需要判断输入是否大于0
3）收敛速度远快于sigmoid和tanh</p><p>ReLU也有几个需要特别注意的问题：
1）ReLU的输出不是zero-centered
2）Dead ReLU Problem，指的是某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。有两个主要原因可能导致这种情况产生: (1) 非常不幸的参数初始化，这种情况比较少见 (2) learning rate太高导致在训练过程中参数更新太大，不幸使网络进入这种状态。解决方法是可以采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。</p><p>尽管存在这两个问题，ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！</p><h2 id=损失函数>损失函数</h2><h3 id=cross-entropy-loss>Cross Entropy Loss</h3><p>交叉熵损失函数。马上就能说出它的二分类公式：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-20-49-802.png alt></p><p>在二分类问题模型，真实样本的标签为 [0，1]，分别表示负类和正类。模型的最后通常会经过一个 Sigmoid 函数，输出一个概率值，这个概率值反映了预测为正类的可能性：概率越大，可能性越大。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-24-09-587.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-07-355.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-30-51-982.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_11-32-54-001.png alt></p><p>横坐标是预测输出，纵坐标是交叉熵损失函数 L。显然，预测输出越接近真实样本标签 1，损失函数 L 越小；预测输出越接近 0，L 越大。因此，函数的变化趋势完全符合实际需要的情况。</p><p>无论真实样本标签 y 是 0 还是 1，L 都表征了预测输出与 y 的差距。</p><p>BCE</p><p>BCE损失函数（Binary Cross-Entropy Loss）是交叉熵损失函数（Cross-Entropy Loss）的一种特例，BCE Loss只应用在二分类任务中。针对分类问题，单样本的交叉熵损失为：</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/hbuasvVTqPFCcop.png alt=quicker_46e51217-3a10-40a0-a80b-a74c7eb0ee93.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>bce <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BCELoss()
</span></span><span style=display:flex><span>bce_sig <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>BCEWithLogitsLoss()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>target <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>random_(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>pre <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sigmoid()(input)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>loss_bce <span style=color:#f92672>=</span> bce(pre, target)
</span></span><span style=display:flex><span>loss_bce_sig <span style=color:#f92672>=</span> bce_sig(input, target)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 同时，pytorch还提供了已经结合了Sigmoid函数的BCE损失：torch.nn.BCEWithLogitsLoss()，相当于免去了实现进行Sigmoid激活的操作。</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>input <span style=color:#f92672>=</span> tensor([[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2296</span>],
</span></span><span style=display:flex><span>        		[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.6389</span>],
</span></span><span style=display:flex><span>        		[<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2405</span>],
</span></span><span style=display:flex><span>        		[ <span style=color:#ae81ff>1.3451</span>],
</span></span><span style=display:flex><span>        		[ <span style=color:#ae81ff>0.7580</span>]], requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> tensor([[<span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>        		 [<span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>        		 [<span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>        		 [<span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>        		 [<span style=color:#ae81ff>1.</span>]])
</span></span><span style=display:flex><span>pre <span style=color:#f92672>=</span> tensor([[<span style=color:#ae81ff>0.4428</span>],
</span></span><span style=display:flex><span>        	  [<span style=color:#ae81ff>0.3455</span>],
</span></span><span style=display:flex><span>        	  [<span style=color:#ae81ff>0.4402</span>],
</span></span><span style=display:flex><span>        	  [<span style=color:#ae81ff>0.7933</span>],
</span></span><span style=display:flex><span>        	  [<span style=color:#ae81ff>0.6809</span>]], grad_fn<span style=color:#f92672>=&lt;</span>SigmoidBackward<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(loss_bce)
</span></span><span style=display:flex><span>tensor(<span style=color:#ae81ff>0.4869</span>, grad_fn<span style=color:#f92672>=&lt;</span>BinaryCrossEntropyBackward<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(loss_bce_sig)
</span></span><span style=display:flex><span>tensor(<span style=color:#ae81ff>0.4869</span>, grad_fn<span style=color:#f92672>=&lt;</span>BinaryCrossEntropyWithLogitsBackward<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><h3 id=label-smoothing>Label Smoothing</h3><p>对于标注数据来说，这个时候我们认为其标注结果是准确的（不然这个结果就没意义了）。<code>但实际上，有一些标注数据并不一定是准确的</code>。那么这时候，使用交叉熵损失函数作为目标函数并不一定是最优的。这会导致模型对正确分类的情况奖励最大，错误分类惩罚最大。如果训练数据能覆盖所有情况，或者是完全正确，那么这种方式没有问题。但事实上，这不可能。<code>所以这种方式可能会带来泛化能力差的问题，即过拟合。</code></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_14-45-42-974.png alt></p><h3 id=focal-loss>Focal Loss</h3><p>在目标检测领域对于one-stage的检测器准确率不高的问题，论文作者给出了解释：由于正负样本不均衡的问题（感觉理解成简单-难分样本不均衡比较好）。 什么意思呢，就是说one-stage中能够匹配到目标的候选框（正样本）个数一般只用十几个或几十个，而没匹配到的候选框（负样本）大概有$10^4 - 10^5$个。而负样本大多数都是简单易分的，对训练起不到什么作用，但是数量太多会淹没掉少数但是对训练有帮助的困难样本。</p><p>那么正负样本不均衡，会带来什么问题呢？</p><ul><li>训练效率低下。 training is inefficient as most locations are easy negatives that contribute no useful learning signal;</li><li>模型精度变低。 过多的负样本会主导训练，使模型退化。en masse,the easy negatives can overwhelm training and lead to degenerate models.</li></ul><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-24-56-371.png alt></p><p><img src=/imgs/img-lazy-loading.gif data-src=https://gitee.com/dingtom1995/picture/raw/master/2022-10-26/2022-10-26_16-27-48-233.png alt=quicker_b30327b2-4641-40a9-b9d2-53e0ed7c1ad2.png></p><p>如上图，横坐标代表$p_t$，纵坐标表示各种样本所占的loss权重。对于正样本，我们希望p越接近1越好，也就是$p_t$越接近1越好；对于负样本，我们希望p越接近0越好，也就是$p_t$越接近1越好。所以不管是正样本还是负样本，我们总是希望他预测得到的$p_t$ 越大越好。如上图所示，$p_t\in[0.6, 1]$就是我们预测效果比较好的样本（也就是易分样本）了。显然可以想象这部分的样本数量很多，所以占比是比较高的（如图中蓝色线区域），我们用$(1-p_t)^\gamma$ 来降低易分样本的损失占比 / 损失贡献（如图其他颜色的曲线）。</p><p>优点：</p><ul><li><p>解决了one-stage object detection中图片中正负样本（前景和背景）不均衡的问题；</p></li><li><p>降低简单样本的权重，使损失函数更关注困难样本；</p></li></ul><p>缺点：</p><ul><li>模型很容易收到噪声干扰：会将噪声当成复杂样本，使模型过拟合退化；</li><li>模型的初期，数量多的一类可能主导整个loss，所以训练初期可能训练不稳定；</li><li>两个参数$\alpha_t$ 和$\gamma$具体的值很难定义，需要自己调参，调的不好可能效果会更差（论文中的 $\alpha_t$=0.25，$\gamma$=2最好）。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>FocalLoss</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, gamma<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, logits<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, reduce<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        super(FocalLoss, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>=</span> alpha
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>=</span> gamma
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>logits <span style=color:#f92672>=</span> logits    <span style=color:#75715e># 如果BEC带logits则损失函数在计算BECloss之前会自动计算softmax/sigmoid将其映射到[0,1]</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>reduce <span style=color:#f92672>=</span> reduce
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, inputs, targets):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>logits:
</span></span><span style=display:flex><span>            BCE_loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>binary_cross_entropy_with_logits(inputs, targets, reduce<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            BCE_loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>binary_cross_entropy(inputs, targets, reduce<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        pt <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>BCE_loss)
</span></span><span style=display:flex><span>        F_loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>alpha <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>pt)<span style=color:#960050;background-color:#1e0010>`</span>self<span style=color:#f92672>.</span>gamma <span style=color:#f92672>*</span> BCE_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>reduce:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>mean(F_loss)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> F_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>FL1 <span style=color:#f92672>=</span> FocalLoss(logits<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>FL2 <span style=color:#f92672>=</span> FocalLoss(logits<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>inputs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>randn(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>targets <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>empty(<span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>random_(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>pre <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sigmoid()(inputs)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>f_loss_1 <span style=color:#f92672>=</span> FL1(pre, targets)
</span></span><span style=display:flex><span>f_loss_2 <span style=color:#f92672>=</span> FL2(inputs, targets)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;inputs:&#39;</span>, inputs)
</span></span><span style=display:flex><span>inputs: tensor([[<span style=color:#f92672>-</span><span style=color:#ae81ff>1.3521</span>],
</span></span><span style=display:flex><span>        [ <span style=color:#ae81ff>0.4975</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>1.0178</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.3859</span>],
</span></span><span style=display:flex><span>        [<span style=color:#f92672>-</span><span style=color:#ae81ff>0.2923</span>]], requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;targets:&#39;</span>, targets)
</span></span><span style=display:flex><span>targets: tensor([[<span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1.</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>1.</span>]])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;pre:&#39;</span>, pre)
</span></span><span style=display:flex><span>pre: tensor([[<span style=color:#ae81ff>0.2055</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.6219</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.2655</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.4047</span>],
</span></span><span style=display:flex><span>        [<span style=color:#ae81ff>0.4274</span>]], grad_fn<span style=color:#f92672>=&lt;</span>SigmoidBackward<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;f_loss_1:&#39;</span>, f_loss_1)
</span></span><span style=display:flex><span>f_loss_1: tensor(<span style=color:#ae81ff>0.3375</span>, grad_fn<span style=color:#f92672>=&lt;</span>MeanBackward0<span style=color:#f92672>&gt;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;f_loss_2&#39;</span>, f_loss_2)
</span></span><span style=display:flex><span>f_loss_2 tensor(<span style=color:#ae81ff>0.3375</span>, grad_fn<span style=color:#f92672>=&lt;</span>MeanBackward0<span style=color:#f92672>&gt;</span>)
</span></span></code></pre></div><h3 id=lovász-softmax>Lovász-Softmax</h3><p>IoU是评价分割模型分割结果质量的重要指标，因此很自然想到能否用1−IoU（即Jaccard loss）来做损失函数，但是它是一个离散的loss，不能直接求导，所以无法直接用来作为损失函数。为了克服这个离散的问题，可以采用<code>Lovász extension将离散的Jaccard loss 变得连续，从而可以直接求导，使得其作为分割网络的loss function</code>。Lovász-Softmax相比于交叉熵函数具有更好的效果。</p><p><img src=/imgs/img-lazy-loading.gif data-src=https://s2.loli.net/2022/04/01/JwYkyMQp19mqhRa.png alt=quicker_b818041f-ffa0-4458-b1b3-988c1d4b3dd6.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.autograd <span style=color:#f92672>import</span> Variable
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn.functional <span style=color:#66d9ef>as</span> F
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> itertools <span style=color:#f92672>import</span>  ifilterfalse
</span></span><span style=display:flex><span><span style=color:#66d9ef>except</span> <span style=color:#a6e22e>ImportError</span>: <span style=color:#75715e># py3k</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>from</span> itertools <span style=color:#f92672>import</span>  filterfalse <span style=color:#66d9ef>as</span> ifilterfalse
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># --------------------------- MULTICLASS LOSSES ---------------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lovasz_softmax</span>(probas, labels, classes<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;present&#39;</span>, per_image<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, ignore<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Multi-class Lovasz-Softmax loss
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).
</span></span></span><span style=display:flex><span><span style=color:#e6db74>              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      classes: &#39;all&#39; for all, &#39;present&#39; for classes present in labels, or a list of classes to average.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      per_image: compute the loss per image instead of per batch
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      ignore: void class labels
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> per_image:
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> mean(lovasz_softmax_flat(<span style=color:#f92672>*</span>flatten_probas(prob<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>), lab<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>), ignore), classes<span style=color:#f92672>=</span>classes)
</span></span><span style=display:flex><span>                          <span style=color:#66d9ef>for</span> prob, lab <span style=color:#f92672>in</span> zip(probas, labels))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> lovasz_softmax_flat(<span style=color:#f92672>*</span>flatten_probas(probas, labels, ignore), classes<span style=color:#f92672>=</span>classes)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lovasz_softmax_flat</span>(probas, labels, classes<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;present&#39;</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Multi-class Lovasz-Softmax loss
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      labels: [P] Tensor, ground truth labels (between 0 and C - 1)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      classes: &#39;all&#39; for all, &#39;present&#39; for classes present in labels, or a list of classes to average.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> probas<span style=color:#f92672>.</span>numel() <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># only void pixels, the gradients should be 0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> probas <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>    C <span style=color:#f92672>=</span> probas<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    losses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    class_to_sum <span style=color:#f92672>=</span> list(range(C)) <span style=color:#66d9ef>if</span> classes <span style=color:#f92672>in</span> [<span style=color:#e6db74>&#39;all&#39;</span>, <span style=color:#e6db74>&#39;present&#39;</span>] <span style=color:#66d9ef>else</span> classes
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> class_to_sum:
</span></span><span style=display:flex><span>        fg <span style=color:#f92672>=</span> (labels <span style=color:#f92672>==</span> c)<span style=color:#f92672>.</span>float() <span style=color:#75715e># foreground for class c</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> (classes <span style=color:#f92672>is</span> <span style=color:#e6db74>&#39;present&#39;</span> <span style=color:#f92672>and</span> fg<span style=color:#f92672>.</span>sum() <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> C <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> len(classes) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#39;Sigmoid output possible only with 1 class&#39;</span>)
</span></span><span style=display:flex><span>            class_pred <span style=color:#f92672>=</span> probas[:, <span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            class_pred <span style=color:#f92672>=</span> probas[:, c]
</span></span><span style=display:flex><span>        errors <span style=color:#f92672>=</span> (Variable(fg) <span style=color:#f92672>-</span> class_pred)<span style=color:#f92672>.</span>abs()
</span></span><span style=display:flex><span>        errors_sorted, perm <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sort(errors, <span style=color:#ae81ff>0</span>, descending<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        perm <span style=color:#f92672>=</span> perm<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>        fg_sorted <span style=color:#f92672>=</span> fg[perm]
</span></span><span style=display:flex><span>        losses<span style=color:#f92672>.</span>append(torch<span style=color:#f92672>.</span>dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> mean(losses)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>flatten_probas</span>(probas, labels, ignore<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Flattens predictions in the batch
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> probas<span style=color:#f92672>.</span>dim() <span style=color:#f92672>==</span> <span style=color:#ae81ff>3</span>:
</span></span><span style=display:flex><span>        <span style=color:#75715e># assumes output of a sigmoid layer</span>
</span></span><span style=display:flex><span>        B, H, W <span style=color:#f92672>=</span> probas<span style=color:#f92672>.</span>size()
</span></span><span style=display:flex><span>        probas <span style=color:#f92672>=</span> probas<span style=color:#f92672>.</span>view(B, <span style=color:#ae81ff>1</span>, H, W)
</span></span><span style=display:flex><span>    B, C, H, W <span style=color:#f92672>=</span> probas<span style=color:#f92672>.</span>size()
</span></span><span style=display:flex><span>    probas <span style=color:#f92672>=</span> probas<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>contiguous()<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, C)  <span style=color:#75715e># B * H * W, C = P, C</span>
</span></span><span style=display:flex><span>    labels <span style=color:#f92672>=</span> labels<span style=color:#f92672>.</span>view(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> ignore <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> probas, labels
</span></span><span style=display:flex><span>    valid <span style=color:#f92672>=</span> (labels <span style=color:#f92672>!=</span> ignore)
</span></span><span style=display:flex><span>    vprobas <span style=color:#f92672>=</span> probas[valid<span style=color:#f92672>.</span>nonzero()<span style=color:#f92672>.</span>squeeze()]
</span></span><span style=display:flex><span>    vlabels <span style=color:#f92672>=</span> labels[valid]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> vprobas, vlabels
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>xloss</span>(logits, labels, ignore<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Cross entropy loss
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>cross_entropy(logits, Variable(labels), ignore_index<span style=color:#f92672>=</span><span style=color:#ae81ff>255</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --------------------------- HELPER FUNCTIONS ---------------------------</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>isnan</span>(x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>!=</span> x
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>mean</span>(l, ignore_nan<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, empty<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    nanmean compatible with generators.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    l <span style=color:#f92672>=</span> iter(l)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> ignore_nan:
</span></span><span style=display:flex><span>        l <span style=color:#f92672>=</span> ifilterfalse(isnan, l)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>        n <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        acc <span style=color:#f92672>=</span> next(l)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>StopIteration</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> empty <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;raise&#39;</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>raise</span> <span style=color:#a6e22e>ValueError</span>(<span style=color:#e6db74>&#39;Empty mean&#39;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> empty
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> n, v <span style=color:#f92672>in</span> enumerate(l, <span style=color:#ae81ff>2</span>):
</span></span><span style=display:flex><span>        acc <span style=color:#f92672>+=</span> v
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> n <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> acc
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> acc <span style=color:#f92672>/</span> n
</span></span></code></pre></div><h1 id=检测速度>检测速度</h1><p><code>前传耗时</code>：从输入一张图像到输出最终结果所消耗的时间，<code>包括前处理耗时（如图</code>
<code>像归一化）、网络前传耗时、后处理耗时（如非极大值抑制）</code>
<code>每秒帧数</code>FPS(Frames Per Second)：每秒钟能处理的图像数量
<code>浮点运算量</code>(FLOPS floating-point operations per second)：<code>处理一张图像所需要的浮点运算数量</code>，跟具体软硬件没有关系，可以公平地比较不同算法之间的检测速度。</p></div><footer class=post-footer><div class=addthis_inline_share_toolbox style=text-align:center></div><hr><div class=post-copyright><img src=/imgs/cc/cc.svg width=75 height=75 align=right><ul><li class=post-copyright-title><strong>文章标题：</strong>
面试-激活、损失函数</li><li class=post-copyright-author><strong>本文作者：</strong>
Tomding</li><li class=post-copyright-link><strong>本文链接：</strong>
<a id=post-cr-link href=/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/ title=面试-激活、损失函数>/post/%E9%9D%A2%E8%AF%95-%E6%BF%80%E6%B4%BB%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</a></li><li class=post-copyright-license><strong>版权声明：</strong>
本博客所有文章除特别声明外，均采用 <i class="fab fa-fw fa-creative-commons"></i><a target=_blank href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class=followme><span>欢迎关注我的其它发布渠道</span><div class=social-list><div class=social-item><a target=_blank class=social-link href=/images/wechat_channel.jpg><span class=icon><i class="fab fa-weixin"></i></span>
<span class=label>WeChat</span></a></div><div class=social-item><a target=_blank class=social-link href=/atom.xml><span class=icon><i class="fa fa-rss"></i></span>
<span class=label>RSS</span></a></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=/post/%E9%9D%A2%E8%AF%95-%E6%96%B9%E5%B7%AE%E5%81%8F%E5%B7%AE%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88/ rel=next title=面试-方差、偏差、过拟合与欠拟合><i class="fa fa-chevron-left"></i> 面试-方差、偏差、过拟合与欠拟合</a></div><div class="post-nav-prev post-nav-item"><a href=/post/%E9%9D%A2%E8%AF%95-%E8%AE%A1%E7%AE%97%E9%87%8F%E5%92%8C%E5%8F%82%E6%95%B0%E9%87%8F/ rel=prev title=面试-计算量和参数量>面试-计算量和参数量
<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div></main><footer class=footer><div class=footer-inner><div class=copyright>&copy;
<span itemprop=copyrightYear>2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=author itemprop=copyrightHolder>Tomding</span></div></div></footer><script type=text/javascript src=https://cdn.staticfile.org/animejs/3.2.1/anime.min.js defer></script>
<script type=text/javascript src=https://cdn.staticfile.org/viewerjs/1.11.0/viewer.min.js defer></script>
<script class=next-config data-name=main type=application/json>{"bookmark":{"color":"#222","enable":false,"save":"manual"},"busuanzi":{"pageview":true},"copybtn":true,"darkmode":true,"giscus":{"cfg":{"category":"Comments","categoryid":null,"emit":false,"inputposition":"top","mapping":"title","reactions":false,"repo":"username/repo-name","repoid":null,"theme":"preferred_color_scheme"},"js":"https://giscus.app/client.js"},"hostname":"/","i18n":{"ds_day":" 天前","ds_days":" 天 ","ds_hour":" 小时前","ds_hours":" 小时 ","ds_just":"刚刚","ds_min":" 分钟前","ds_mins":" 分钟","ds_month":" 个月前","ds_years":" 年 ","empty":"没有找到任何搜索结果：${query}","hits":"","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","placeholder":"搜索..."},"lang":"zh-CN","lazyload":false,"localSearch":{"enable":true,"path":"/searchindexes.xml","preload":false,"topnperarticle":-1,"trigger":"auto","unescape":false},"motion":{"async":true,"enable":true,"transition":{"collheader":"fadeInLeft","postblock":"fadeIn","postbody":"fadeInDown","postheader":"fadeInDown","sidebar":"fadeInUp"}},"root":"/","scheme":"Gemini","sidebar":{"display":"post","offset":12,"padding":18,"position":"left","width":256},"statis":{"enable":true,"plugin":"busuanzi"},"vendor":{"plugins":"qiniu","router":"https://cdn.staticfile.org"},"version":"4.4.1","waline":{"cfg":{"comment":true,"emoji":false,"imguploader":false,"pageview":true,"placeholder":"请文明发言哟 ヾ(≧▽≦*)o","reaction":true,"reactiontext":["点赞","踩一下","得意","不屑","尴尬","睡觉"],"reactiontitle":"你认为这篇文章怎么样？","requiredmeta":["nick","mail"],"serverurl":null,"sofa":"快来发表你的意见吧 (≧∀≦)ゞ","wordlimit":200},"css":{"alias":"waline","file":"dist/waline.css","name":"@waline/client","version":"2.13.0"},"js":{"alias":"waline","file":"dist/waline.js","name":"@waline/client","version":"2.13.0"}}}</script><script type=text/javascript src=/js/main.min.e34208af37d3db3a875e6fbad04c2f1c241185af925a9c6ebf79dd1ba2730cdd.js defer></script></body></html>