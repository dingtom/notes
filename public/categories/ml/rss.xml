<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ml on Tomding's Blog</title><link>/categories/ml/</link><description>Recent content in ml on Tomding's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Thu, 01 Dec 2022 19:59:47 +0800</lastBuildDate><atom:link href="/categories/ml/rss.xml" rel="self" type="application/rss+xml"/><item><title>ml-Attention机制</title><link>/post/ml-attention%E6%9C%BA%E5%88%B6/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-attention%E6%9C%BA%E5%88%B6/</guid><description>https://github.com/CyberZHG/keras-self-attention/blob/master/README.zh-CN.md https://github.com/CyberZHG/keras-self-attention/blob/master/keras_self_attention/seq_self_attention.py 用Attention机制的原因是考虑到RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从左向右依次计算或</description></item><item><title>ml-gpu版pytorch安装</title><link>/post/ml-gpu%E7%89%88pytorch%E5%AE%89%E8%A3%85/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-gpu%E7%89%88pytorch%E5%AE%89%E8%A3%85/</guid><description>驱动安装 [NVIDIA 驱动程序下载]( 官方 GeForce 驱动程序 | NVIDIA ) 查看显卡型号 lspci | grep -i nvidia 安装驱动 sudo bash NVIDIA-Linux-x86_64-455.23.04.run 查看显卡信息 nvidia-smi 卸载显卡驱动重新安装 命令行界面 Ctrl+Alt+F1 sudo apt-get --purge remove nvidia* sudo</description></item><item><title>ml-infoGAN</title><link>/post/ml-infogan/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-infogan/</guid><description>InfoGAN的发布时间应该在是DCGAN之后没多久，可以算是在大部分的GAN模型的前面的。时间上看，InfoGAN应该是在Semi-GAN</description></item><item><title>ml-Jupyter-Notebook</title><link>/post/ml-jupyter-notebook/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-jupyter-notebook/</guid><description>安装 jupyter conda install ipython conda install jupyter 换主题 pip install jupyterthemes 查看可用的 Jupyter 主题 jt -l 更换 Jupyter 主题 jt -t onedork -f fira -fs 13 -cellw 90% -ofs 11 -dfs 11 -T -T -m 10 -t 主题 -f(字体) -fs(字体大小) -cell</description></item><item><title>ml-keras</title><link>/post/ml-keras/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-keras/</guid><description>tensorboard writer=tf.summary.FileWriter(&amp;#39;/path/to/logs&amp;#39;, tf.get_default_graph()) writer.close() 在上面程序的8、9行中，创建一个writer，将tensorboard summary写入文件夹/path/to/logs， 然后运行上</description></item><item><title>ml-keras-utils-plot_model报错</title><link>/post/ml-keras-utils-plot_model%E6%8A%A5%E9%94%99/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-keras-utils-plot_model%E6%8A%A5%E9%94%99/</guid><description>OSError: pydot failed to call GraphViz.Please install GraphViz ( https://www.graphviz.org/ ) and ensure that its executables are in the $PATH. 或 Failed to import pydot. You must install pydot and graphviz win10 ####1.安装 GraphViz 下载msi文件安装 与python关联 pip install graphviz ####2.添加</description></item><item><title>ml-pytorch</title><link>/post/ml-pytorch/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-pytorch/</guid><description>1.安装 https://pytorch.org pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # ２.基础 创建一个 5x3 矩阵, 但是未初始化: torch.empty(5, 3) 创建一个随机初始化的矩阵: torch.rand(5, 3) 创建一个0填充的矩阵，数据类型为long torch.zeros(5, 3, dtype=torch.long)</description></item><item><title>ml-Seq2Seq-模型及-Attention-机制</title><link>/post/ml-seq2seq-%E6%A8%A1%E5%9E%8B%E5%8F%8A-attention-%E6%9C%BA%E5%88%B6/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-seq2seq-%E6%A8%A1%E5%9E%8B%E5%8F%8A-attention-%E6%9C%BA%E5%88%B6/</guid><description>什么是 Seq2Seq ？ Seq2Seq 任务指的是输入和输出都是序列的任务。例如说英语翻译成中文。 Seq2Seq任务最常见的是使用Encoder+Decoder的模式。</description></item><item><title>ml-sklearn</title><link>/post/ml-sklearn/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-sklearn/</guid><description>中文手册 英文手册 在Sklearn当中有三大模型：Transformer 转换器、Estimator 估计器、Pipeline 管道 估计器 (estimator) 可以基于</description></item><item><title>ml-Tensorflow官方debug--tfdbg</title><link>/post/ml-tensorflow%E5%AE%98%E6%96%B9debug--tfdbg/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-tensorflow%E5%AE%98%E6%96%B9debug--tfdbg/</guid><description>example code： import numpy as np import tensorflow as tf from tensorflow.python import debug as tf_debug xs = np.linspace(-0.5, 0.49, 100) x = tf.placeholder(tf.float32, shape=[None], name=&amp;#34;x&amp;#34;) y = tf.placeholder(tf.float32, shape=[None], name=&amp;#34;y&amp;#34;) k = tf.Variable([0.0], name=&amp;#34;k&amp;#34;) y_hat = tf.multiply(k, x, name=&amp;#34;y_hat&amp;#34;) sse = tf.reduce_sum((y - y_hat) * (y - y_hat), name=&amp;#34;sse&amp;#34;) train_op = tf.train.GradientDescentOptimizer(learning_rate=0.02).minimize(sse) sess = tf.Session() sess.run(tf.global_variables_initializer()) sess = tf_debug.LocalCLIDebugWrapperSession(sess,ui_type=&amp;#34;readline&amp;#34;) for _ in range(10): sess.run(y_hat,feed_dict={x:xs,y:10*xs}) sess.run(train_op,</description></item><item><title>ml-插值</title><link>/post/ml-%E6%8F%92%E5%80%BC/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E6%8F%92%E5%80%BC/</guid><description>一维插值 插值不同于拟合。插值函数经过样本点，拟合函数一般基于最小二乘法尽量靠近所有样本点穿过。常见插值方法有拉格朗日插值法、分段插值法、样条</description></item><item><title>ml-迁移学习(Transfer-Learning)</title><link>/post/ml-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0transfer-learning/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0transfer-learning/</guid><description>训练复杂的卷积神经网络需要非常多的标注数据。 所谓迁移学习，就是讲一个问题上训练好的模型通过简单的调整使其适用于一个新的问题。 根据论文DeCA</description></item><item><title>ml-神经网络</title><link>/post/ml-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>反向传播(Back Propagation) 用样本的特征$x$，计算出神经网络中每个隐藏层节点的输出$a_i$以及输出层每个节点的输出$y_i$然后，我们按照下面的</description></item><item><title>ml-生成对抗网络(GAN)</title><link>/post/ml-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9Cgan/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9Cgan/</guid><description>Goodfellow的Generative Adversarial Networks https://arxiv.org/abs/1406.2661 **本质上， GAN 目标是训练出一个好的生成模型，来模拟训练集中的数据。**不同的是， 一般的生成模</description></item><item><title>ml-循环神经网络(RNN)</title><link>/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/</guid><description>RNN 神经网络只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后</description></item><item><title>ml-自动机器学习</title><link>/post/ml-%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><pubDate>Thu, 01 Dec 2022 19:59:47 +0800</pubDate><guid>/post/ml-%E8%87%AA%E5%8A%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</guid><description>AutoKeras是一个开源的，基于 Keras 的新型 AutoML 库。AutoKeras 是一个用于自动化机器学习的开源软件库，提供自动搜索深度学习模型的架构和超</description></item></channel></rss>