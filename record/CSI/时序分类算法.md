# 一、传统方法（需要手工设计）

### 1、DTW（dynamic time warping）& KNN


欧式距离不能很好地针对时间序列的波动模式进行分类，研发更适合时间序列分类的距离度量就成为关键，这其中最经典的时间序列距离度量就是Dynamic Time Warping （DTW）。 DTW的原理如下：
![](https://upload-images.jianshu.io/upload_images/18339009-2985b445c86768a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
比如说，给定一个样本序列X和比对序列Y,Z：
X：3，5，6，7，7，1
Y：3，6，6，7，8，1，1
Z：2，5，7，7，7，7，2
请问是X和Y更相似还是X和Z更相似？
DTW首先会根据序列点之间的距离(欧氏距离)，获得一个序列距离矩阵 MM，其中行对应X序列，列对应Y序列，矩阵元素为对应行列中X序列和Y序列点到点的欧氏距离：
![](https://upload-images.jianshu.io/upload_images/18339009-71a13afdeb719bc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
![](https://upload-images.jianshu.io/upload_images/18339009-58684fed809dd5f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

DTW通过对时间序列波动模式的分析可得到更好的时间序列分类结果。研究表明，在时间序列分类问题上，DTW距离度量配合简单的最小距离分类法（nearest neighbor）就可以取得较传统欧式距离算法（如SVM、经典多层神经网络、决策树、Adaboost）压倒性的优势。
![](https://upload-images.jianshu.io/upload_images/18339009-112ac8e373f42f12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


  DTW更进一步衍生出多种不同的变种，例如由Keogh和 Pazzani 提出的基于序列一阶导数的改进便取得了良好的效果；其中一种简单的方法叫Complexity Invariant distance (CID)，其利用一阶导数信息对DTW距离做计算，在某些问题上具有突出效果。

  除了DTW，还有其他考量时间序列的波动模式算法。例如Ye 和Keogh提出的Shapelet方法：考察序列中具有代表意义的子序列来作为Shapelet特征而进行分类。Lin等人提出了基于字典的方法，将序列根据特定的字典转化为词序列，从而进行分类。Deng提出了基于区间的方法，从区间中提取波动的特征。

  除了上述方法外，聚合算法（将多种不同算法聚合在一起）的研究也有了长足的进步。最近提出的COTE算法几乎将上述所有不同分类算法聚合在一起，得到了优异的分类效果。

### 2、基于特征的方法

 这一类的方法都是一些通过某种度量关系来提取相关特征的方法，如词袋法，通过找到该时间序列中是否有符合已有词袋中的特征（序列的样子），将一个序列用词来表示，再对词进行分类。而其他的基于特征的方法都是利用了类似的方法，如提取统计量，基于规则等，再通过分类模型进行分类。

# 二、深度学习
1、MLP、FCN、ResNet
MLP的输入是一个向量（数组），通过全连接的形式对整体数组的每一个元素逐层赋予权重，并求得最后的分类，这种方法是一种比较粗暴的学习方法，直接学习所有元素直接的线性或非线性相关关系，但是并没有去深度挖掘数组中更好的表现特征，分类效果不佳。

FCN是将MLP中的全链接层用卷积层进行替代，Resnet也是，但是其中的卷积层都用一维卷积核进行了替代。
![](https://upload-images.jianshu.io/upload_images/18339009-f34fb12ff3ad98cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
来自于Time Series Classifification from Scratch with Deep Neural Networks: A Strong Baseline.可以看到深度学习的方法效果基本上与传统方法相接近，甚至有所超过，其中整体表现最好的是FCN。

### 2、LSTM_FCN、BiGRU-CNN
![](https://upload-images.jianshu.io/upload_images/18339009-a527e05a29e21b5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
 LSTM_FCN的方法比较简单，是将输入分别输入到两个分支中，LSTM和FCN，并在最后将两个输出分支进行concat进行softmax获得分类结果。在这篇论文中，作者说这种方法取得了比FCN更好的效果。
      在其他的一些比赛方案中，也有resnet+LSTM+FC的组合形式，通过Resnet的一维卷积先提取相关特征，然后通过LSTM学习一维特征向量的相关关系，再进行分类，可能针对于不同的问题还是要试试才知道哪个的效果更加好。
      BiGRU-CNN与以上方法相比实际上并没有做什么大的改进，就是将LSTM分支替换成双向的GRU分支。
### 3、MC-CNN（multi-channel CNN）、MCNN(multi-scale CNN)
![](https://upload-images.jianshu.io/upload_images/18339009-6b6b01e5e121820f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)









