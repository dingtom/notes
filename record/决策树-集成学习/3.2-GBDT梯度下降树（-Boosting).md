GBDT（Gradient Boosting Decision Tree）梯度提升决策树，是一种基于 Boosting 的算法，采用以决策树为基学习器的加法模型，通过**不断拟合上一个弱学习器的残差，最终实现分类或回归**的模型。关键在于**利用损失函数的负梯度在当前模型的值作为残差的近似值，从而拟合一个回归树**。

GBDT 为什么拟合负梯度而不是残差
函数中下降最快的方向是导数方向，同理：GBDT 中，损失函数减小最快的方向也是本身的导数方向。当损失函数为均方误差时，损失函数的负梯度和残差一样，但当损失函数不为均方误差时，本质上拟合的还是负梯度。

对于分类问题：常使用指数损失函数；对于回归问题：常使用平方误差损失函数（此时，其负梯度就
是通常意义的残差），对于一般损失函数来说就是残差的近似。

# 思想

训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵 CART 树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

GBDT 由三个概念组成：Regression Decision Tree（即 DT）、Gradient Boosting（即 GB），和 Shrinkage（一个重要演变）

## 回归树（Regression Decision Tree）

如果认为 GBDT 有很多分类树那就大错特错了（虽然调整后也可以分类）。对于**分类树而言，其值加减无意义**（如性别），而对于**回归树而言，其值加减才是有意义的**（如说年龄）。GBDT 的**核心在于累加所有树的结果作为最终结果，所以GBDT 中的树都是回归树，不是分类树**，这一点相当重要。

回归树在分枝时会穷举每一个特征的每个阈值以找到最好的分割点，衡量标准是最小化均方误差。

## 梯度迭代（Gradient Boosting）

上面说到 GBDT 的核心在于累加所有树的结果作为最终结果，**GBDT 的每一棵树都是以之前树得到的残差来更新目标值，这样每一棵树的值加起来即为 GBDT 的预测值**。
![](https://upload-images.jianshu.io/upload_images/18339009-b961b7d246e7ed7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![](https://upload-images.jianshu.io/upload_images/18339009-a101b0f0d5942da4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



GBDT 的 Boosting 不同于 Adaboost 的 Boosting，**GBDT 的每一步残差计算其实变相地增大了被分错样本的权重，而对于分对样本的权重趋于 0**，这样后面的树就能专注于那些被分错的样本。

## 缩减（Shrinkage）

Shrinkage 的思想认为，每走**一小步逐渐逼近结果的效果要比每次迈一大步很快逼近结果的方式更容易避免过拟合**。即它并不是完全信任每一棵残差树。

![](https://upload-images.jianshu.io/upload_images/18339009-a67276ac07fd47c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


Shrinkage 不直接用残差修复误差，而是只修复一点点，把大步切成小步。本质上 **Shrinkage 为每棵树设置了一个 weight，累加时要乘以这个 weight，当 weight 降低时，基模型数会配合增大**。

# 优缺点

## 优点
1. **预测阶段的计算速度快**，树与树之间可并行化计算。

2. 在**分布稠密的数据集上，泛化能力和表达能力都很好**，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。 

3. 采用**决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性**，能够自动**发现特征间的高阶关系**，并且也**不需要对数据进行特殊的预处理如归一化**等。

   > 为什么树模型不需要归一化
   >
   > 因为数值缩放不影响分裂点位置，对树模型的结构不造成影响，而且是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化

## 缺点
1. GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
2. GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。 
3. 训练过程需要串行训练。 **对异常点敏感**。



# 问题



## RF(随机森林)与GBDT之间的区别与联系

**相同点**：

都是由多棵树组成，最终的结果都是由多棵树一起决定。

都是 Bootsrap有放回的抽样(Baging 采用有放回的均匀取样，而 Bosting 根据错误率来取样)

**不同点**：

2. 组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成
3. 组成随机森林的树可以并行生成，而GBDT是串行生成
4. 对于最终的输出结果而言，随机森林采用多数投票或简单平均等；而GBDT则是将所有结果累加起来，或者加权累加起来
5. 随机森林对异常值不敏感，而GBDT对异常值比较敏感
6. 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
7. 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能，但是xgb引入了正则项和列采样等等正则化手段之后，可以在少量增加偏差的情况下大幅度缩减模型的方差。

## GBDT与 Adaboost 的对比


### 相同：

1.  都是**Boosting 家族成员，使用弱分类器**；
2.  都使用**前向分布算法**；

###  同：

1.  **迭代思路不同**：Adaboost 是通过**提升错分数据点的权重来弥补模型的不足（利用错分样本）**，而 GBDT 是通过**算梯度来弥补模型的不足（利用残差）**；
2.  **损失函数不同**：AdaBoost 采用的是**指数损失**，GBDT 使用的是**绝对损失或者 Huber 损失函数**；

## 逻辑回归(LR)和GBDT的区别

- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；
- 对于高维稀疏数据，GBDT效果不如LR。

